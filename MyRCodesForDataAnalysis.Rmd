--- 
title: "My R Codes for Data Analysis"
author: "Serdar Balcı"
date: "`{r #  Sys.Date()`"
site: bookdown::bookdown_site
documentclass: book
bibliography: ['bib/book.bib', 'bib/packages.bib']
biblio-style: apalike
link-citations: yes
description: "This is a collection of R codes I use for data analysis"
---

# Prerequisites

This is a _sample_ book written in **Markdown**. You can use anything that Pandoc's Markdown supports, e.g., a math equation $a^2 + b^2 = c^2$.

The **bookdown** package can be installed from CRAN or Github:

```{r eval=FALSE, include=FALSE, echo=TRUE}
install.packages("bookdown")
# or the development version
# devtools::install_github("rstudio/bookdown")
```

Remember each Rmd file contains one and only one chapter, and a chapter is defined by the first-level heading `#`.

To compile this example to PDF, you need XeLaTeX. You are recommended to install TinyTeX (which includes XeLaTeX): <https://yihui.name/tinytex/>.

```{r include=FALSE}
# automatically create a bib database for R packages
knitr::write_bib(c(
  .packages(), 'bookdown', 'knitr', 'rmarkdown'
), 'bib/packages.bib')
```

<!--chapter:end:index.Rmd-->

# Introduction {#intro}

You can label chapter and section titles using `{#label}` after them, e.g., we can reference Chapter \@ref(intro). If you do not manually label them, there will be automatic labels anyway, e.g., Chapter \@ref(methods).

Figures and tables with captions will be placed in `figure` and `table` environments, respectively.

```{r , fig.cap='Here is a nice figure!', out.width='80%', fig.asp=.75, fig.align='center'}
par(mar = c(4, 4, .1, .1))
plot(pressure, type = 'b', pch = 19)
```

Reference a figure by its code chunk label with the `fig:` prefix, e.g., see Figure \@ref(fig:). Similarly, you can reference tables generated from `knitr::kable()`, e.g., see Table \@ref(tab:).

```{r , tidy=FALSE}
knitr::kable(
  head(iris, 20), caption = 'Here is a nice table!',
  booktabs = TRUE
)
```

You can write citations, too. For example, we are using the **bookdown** package [@R-bookdown] in this sample book, which was built on top of R Markdown and **knitr** [@xie2015].

<!--chapter:end:01-intro.Rmd-->

# Literature

Here is a review of existing methods.

<!--chapter:end:02-literature-1.Rmd-->

# Literature

Here is a review of existing methods.

<!--chapter:end:02-literature.Rmd-->

# Methods

We describe our methods in this chapter.

<!--chapter:end:03-method.Rmd-->

# Applications

Some _significant_ applications are demonstrated in this chapter.

## Example one

## Example two

<!--chapter:end:04-application.Rmd-->

# Final Words

We have finished a nice book.

<!--chapter:end:05-summary.Rmd-->

`{r #  if (knitr::is_html_output()) '
# References {-}
'`

<!--chapter:end:06-references.Rmd-->

---
title: "Bibliographic Studies"
subtitle: "Articles per journals per country"
author: "Serdar Balcı, MD, Pathologist"
date: '`{r #  format(Sys.Date())`'
---

```
output: 
  html_notebook: 
    code_folding: hide
    fig_caption: yes
    highlight: kate
    number_sections: yes
    theme: cerulean
    toc: yes
    toc_float: yes
  html_document: 
    code_folding: hide
    df_print: kable
    keep_md: yes
    number_sections: yes
    theme: cerulean
    toc: yes
    toc_float: yes
    highlight: kate
```

If you want to see the code used in the analysis please click the code button on the right upper corner or throughout the page.

---

# Analysis

```{r eval=FALSE, include=FALSE, echo=TRUE}
knitr::opts_chunk$set(
	eval = FALSE,
	message = FALSE,
	warning = FALSE,
	include = FALSE,
	tidy = TRUE
)
```

## Articles per journals per country

**Aim:**

In the [previous analysis](https://sbalci.github.io/pubmed/CountryBasedComparison.html) we have observed that Japanese researchers have much more articles than German and Turkish researchers.

Here we will look at the distribution of articles per journals per country. 


**Methods:**

```{r , eval=FALSE, include=FALSE, echo=TRUE}
# load required packages
library(tidyverse)
library(RISmed)
```

Pathology Journal ISSN List was retrieved from [In Cites Clarivate](https://jcr.incites.thomsonreuters.com/), and Journal Data Filtered as follows: `JCR Year: 2016 Selected Editions: SCIE,SSCI Selected Categories: 'PATHOLOGY' Selected Category Scheme: WoS`

```{r Get ISSN List from data downloaded from WoS, eval=FALSE, include=FALSE, echo=TRUE}
# Get ISSN List from data downloaded from WoS
ISSNList <- JournalHomeGrid <- read_csv("data/JournalHomeGrid.csv", 
                                        skip = 1) %>% 
    select(ISSN) %>% 
    filter(!is.na(ISSN)) %>% 
    t() %>% 
    paste("OR ", collapse = "") # add OR between ISSN List

ISSNList <- gsub(" OR $","" ,ISSNList) # to remove last OR
```

Data is retrieved from PubMed via RISmed package.
PubMed collection from National Library of Medicine (https://www.ncbi.nlm.nih.gov/pubmed/), has the most comprehensive information about peer reviewed articles in medicine.
The API (https://dataguide.nlm.nih.gov/), and R packages are available for getting and fetching data from the server.

The search formula for PubMed is generated as "ISSN List AND Country[Affiliation]" like done in [advanced search of PubMed](https://www.ncbi.nlm.nih.gov/pubmed/advanced).

```{r Generate Search Formula For Pathology Journals AND Countries, eval=FALSE, include=FALSE, echo=TRUE}
# Generate Search Formula For Pathology Journals AND Countries
searchformulaTR <- paste("'",ISSNList,"'", " AND ", "Turkey[Affiliation]")
searchformulaDE <- paste("'",ISSNList,"'", " AND ", "Germany[Affiliation]")
searchformulaJP <- paste("'",ISSNList,"'", " AND ", "Japan[Affiliation]")
```

Articles from Japan, German and Turkey are retrieved limiting the search with pathology journals, affiliation and last 10 years.

```{r Search PubMed, eval=FALSE, include=FALSE, echo=TRUE}
# Search PubMed, Get and Fetch
TurkeyArticles <- EUtilsSummary(searchformulaTR, type = 'esearch', db = 'pubmed', mindate = 2007, maxdate = 2017, retmax = 10000)
fetchTurkey <- EUtilsGet(TurkeyArticles)

GermanyArticles <- EUtilsSummary(searchformulaDE, type = 'esearch', db = 'pubmed', mindate = 2007, maxdate = 2017, retmax = 10000)
fetchGermany <- EUtilsGet(GermanyArticles)

JapanArticles <- EUtilsSummary(searchformulaJP, type = 'esearch', db = 'pubmed', mindate = 2007, maxdate = 2017, retmax = 10000)
fetchJapan <- EUtilsGet(JapanArticles)
```

The retrieved information was compiled in a table.

```{r eval=FALSE, include=FALSE, echo=TRUE}

ISSNTR <- table(ISSN(fetchTurkey)) %>% 
    as_tibble() %>% 
    rename(Turkey = n, Journal = Var1)

ISSNDE <- table(ISSN(fetchGermany)) %>% 
    as_tibble() %>% 
    rename(Germany = n, Journal = Var1)

ISSNJP <- table(ISSN(fetchJapan)) %>% 
    as_tibble() %>% 
    rename(Japan = n, Journal = Var1)

articles_per_journal <- list(
    ISSNTR,
    ISSNDE,
    ISSNJP
) %>%
    reduce(left_join, by = "Journal", .id = "id") %>% 
    gather(Country, n, 2:4)

articles_per_journal$Country <- factor(articles_per_journal$Country,
                                       levels =c("Japan", "Germany", "Turkey"))

```



**Result:**

In this graph x-axis is the list of journals with decreasing impact factor, and y-axis is the number of articles published in that journal. The colors and shapes are showing the country of affiliation. We see that in one journal articles from Japan is more than 800.  

```{r eval=FALSE, include=FALSE, echo=TRUE}
ggplot(data = articles_per_journal, aes(x = Journal, y = n, group = Country,
                                     colour = Country, shape = Country,
                                     levels = Country
)) +
    geom_point() +
    labs(x = "Journals with decreasing impact factor", y = "Number of Articles") +
    ggtitle("Pathology Articles Per Journal") + 
    theme(plot.title = element_text(hjust = 0.5),
          axis.text.x=element_blank())

```



**Comment:**

It is seen that one of the journals [ISSN: 1440-1827](https://onlinelibrary.wiley.com/page/journal/14401827/homepage/productinformation.html) has more than 800 articles from Japan. This journal is also from Japan. Here we wonder if there is an editorial preference for articles from their home country. 

We sometimes observe this situation if there is a conference in that country, and the conference abstracts are indexed. 

This may also be a clue that if a country has a journal listed in indexes, than it is more easy for the researchers in that country to publish their results.


**Future Work:**

Whether this observation is a unique situation, or there is a tendency in the journals to publish article from their country of origin, merits further investigation. 



---


# Feedback

[Serdar Balcı, MD, Pathologist](https://github.com/sbalci) would like to hear your feedback: https://goo.gl/forms/YjGZ5DHgtPlR1RnB3

This document will be continiously updated and the last update was on `{r #  Sys.Date()`.

---

# Back to Main Menu

[Main Page for Bibliographic Analysis](https://sbalci.github.io/pubmed/BibliographicStudies.html)

<!--chapter:end:12-ArticlesPerJournalsPerCountry.Rmd-->

---
title: "Bibliographic Studies"
subtitle: "Country Based Comparison"
author: "Serdar Balcı, MD, Pathologist"
date: '`{r #  format(Sys.Date())`'
---

```
output: 
  html_notebook: 
    code_folding: hide
    fig_caption: yes
    highlight: kate
    number_sections: yes
    theme: cerulean
    toc: yes
    toc_float: yes
  html_document: 
    code_folding: hide
    df_print: kable
    keep_md: yes
    number_sections: yes
    theme: cerulean
    toc: yes
    toc_float: yes
    highlight: kate
```


# Analysis

```{r eval=FALSE, include=FALSE, echo=TRUE}
knitr::opts_chunk$set(
	eval = FALSE,
	message = FALSE,
	warning = FALSE,
	include = FALSE,
	tidy = TRUE
)
```


## PubMed Indexed Peer Reviewed Articles in Pathology Journals: A country based comparison

**Aim:**

Here, we are going to compare 3 countries (German, Japan and Turkey), in terms of number of articles in pathology journals during the last decade.

**Methods:**

If you want to see the code used in the analysis please click the code button on the right upper corner or throughout the page.

```{r , eval=FALSE, include=FALSE, echo=TRUE}
# load required packages
library(tidyverse)
library(RISmed)
```

Pathology Journal ISSN List was retrieved from [In Cites Clarivate](https://jcr.incites.thomsonreuters.com/), and Journal Data Filtered as follows: `JCR Year: 2016 Selected Editions: SCIE,SSCI Selected Categories: 'PATHOLOGY' Selected Category Scheme: WoS`

```{r Get ISSN List from data downloaded from WoS 2, eval=FALSE, include=FALSE, echo=TRUE}
# Get ISSN List from data downloaded from WoS
ISSNList <- JournalHomeGrid <- read_csv("data/JournalHomeGrid.csv", 
                                        skip = 1) %>% 
    select(ISSN) %>% 
    filter(!is.na(ISSN)) %>% 
    t() %>% 
    paste("OR ", collapse = "") # add OR between ISSN List

ISSNList <- gsub(" OR $","" ,ISSNList) # to remove last OR
```

Data is retrieved from PubMed via RISmed package.
PubMed collection from National Library of Medicine (https://www.ncbi.nlm.nih.gov/pubmed/), has the most comprehensive information about peer reviewed articles in medicine.
The API (https://dataguide.nlm.nih.gov/), and R packages are available for getting and fetching data from the server.

The search formula for PubMed is generated as "ISSN List AND Country[Affiliation]" like done in [advanced search of PubMed](https://www.ncbi.nlm.nih.gov/pubmed/advanced).

```{r Generate Search Formula For Pathology Journals AND Countries 2, eval=FALSE, include=FALSE, echo=TRUE}
# Generate Search Formula For Pathology Journals AND Countries
searchformulaTR <- paste("'",ISSNList,"'", " AND ", "Turkey[Affiliation]")
searchformulaDE <- paste("'",ISSNList,"'", " AND ", "Germany[Affiliation]")
searchformulaJP <- paste("'",ISSNList,"'", " AND ", "Japan[Affiliation]")
```

```{r Search PubMed 2, eval=FALSE, include=FALSE, echo=TRUE}
# Search PubMed, Get and Fetch
TurkeyArticles <- EUtilsSummary(searchformulaTR, type = 'esearch', db = 'pubmed', mindate = 2007, maxdate = 2017, retmax = 10000)
fetchTurkey <- EUtilsGet(TurkeyArticles)

GermanyArticles <- EUtilsSummary(searchformulaDE, type = 'esearch', db = 'pubmed', mindate = 2007, maxdate = 2017, retmax = 10000)
fetchGermany <- EUtilsGet(GermanyArticles)

JapanArticles <- EUtilsSummary(searchformulaJP, type = 'esearch', db = 'pubmed', mindate = 2007, maxdate = 2017, retmax = 10000)
fetchJapan <- EUtilsGet(JapanArticles)
```

From the fetched data the year of articles are grouped and counted by country.

```{r Articles per countries per year 2, eval=FALSE, include=FALSE, echo=TRUE}
# Articles per countries per year
tableTR <- table(YearPubmed(fetchTurkey)) %>% 
    as_tibble() %>% 
    rename(Turkey = n, Year = Var1)

tableDE <- table(YearPubmed(fetchGermany)) %>% 
    as_tibble() %>% 
    rename(Germany = n, Year = Var1)

tableJP <- table(YearPubmed(fetchJapan)) %>% 
    as_tibble() %>% 
    rename(Japan = n, Year = Var1)

# Join Tables
articles_per_year_table <- list(
    tableTR,
    tableDE,
    tableJP
    ) %>%
    reduce(left_join, by = "Year", .id = "id")

```


```{r Prepare table for output 2, eval=FALSE, include=FALSE, echo=TRUE}
# Prepare table for output
articles_per_year <- articles_per_year_table %>% 
    gather(Country, n, 2:4)

articles_per_year$Country <- factor(articles_per_year$Country,
                                       levels =c("Japan", "Germany", "Turkey"))
```


**Result:**

In the below table we see the number of articles per country in the last decade.

```{r Print the Table of Articles per year per country 2, eval=FALSE, include=FALSE, echo=TRUE}
# Print the Table of Articles per year, per country
knitr::kable(articles_per_year_table, caption = "Table of Articles per year, per country")
```


And the figure below shows this data in a line graph. 

```{r Graph of Table of Articles per year per country 2, eval=FALSE, fig.align="center", include=FALSE}
ggplot(data = articles_per_year, aes(x = Year, y = n, group = Country,
                                     colour = Country, shape = Country,
                                     levels = Country
                                     )) +
    geom_line() +
    geom_point() +
    labs(x = "Year", y = "Number of Articles") +
    ggtitle("Pathology Articles Per Year") +
    theme(plot.title = element_text(hjust = 0.4), 
          text = element_text(size = 9))
```


**Comment:**

We see that Japan has much more articles than German and Turkey.
Turkey has a small increase in number of articles.

**Future Work:**


* Indentify why Japan has too much articles.
* Compare Japan with other countries.
* Compare Turkey with neighbours, EU, OECD & Middle East countries.
* Analyse multinational studies.
* Analyse adding journal impact as a factor. 

---




# Feedback

[Serdar Balcı, MD, Pathologist](https://github.com/sbalci) would like to hear your feedback: https://goo.gl/forms/YjGZ5DHgtPlR1RnB3

This document will be continiously updated and the last update was on `{r #  Sys.Date()`.

---

# Back to Main Menu

[Main Page for Bibliographic Analysis](https://sbalci.github.io/pubmed/BibliographicStudies.html)

<!--chapter:end:13-CountryBasedComparison.Rmd-->

---
title: "PBPath Journal Watch"
subtitle: "Recent Articles from PubMed"
author: "Serdar Balcı, MD, Pathologist"
date: '`{r #  format(Sys.Date())`'
---

```
output: 
  html_notebook: 
    code_folding: hide
    highlight: kate
    number_sections: yes
    theme: cerulean
    toc: yes
    toc_float: yes
    fig_caption: yes
  html_document: 
    df_print: kable
    number_sections: yes
    toc: yes
```


# Analysis of Recent Pancreas Related Articles

Pancreas Journals
https://www.ncbi.nlm.nih.gov/nlmcatalog/?term=pancreas

Pathology Journals

Member List

DOI Link
PubMed Link
Journal Link
Altmetric API
Dimensions API


USCAP abstracts vs publication

Member list vs worldmap




```{r , eval=FALSE, include=FALSE, echo=TRUE}
# load required packages
library(tidyverse)
library(knitr)
library(rstudioapi)
```

    
```{r eval=FALSE, include=FALSE, echo=TRUE}
knitr::opts_chunk$set(
	eval = FALSE,
	message = FALSE,
	warning = FALSE,
	include = FALSE,
	tidy = TRUE
)
```



```{r eval=FALSE, include=FALSE, echo=TRUE}
myTerm <- rstudioapi::terminalCreate(show = FALSE)
rstudioapi::terminalSend(myTerm, "esearch -db pubmed -query 'pancreas[Title/Abstract]) AND pathology' -datetype EDAT -min 2018/05/01 -max 3000 | \
efetch -format xml | \
xtract -pattern PubmedArticle -element MedlineCitation/PMID \
-block ArticleId -if ArticleId@IdType -equals doi -element ArticleId &> myquery.txt")
Sys.sleep(1)
repeat{
    Sys.sleep(0.1)
    if(rstudioapi::terminalBusy(myTerm) == FALSE){
        print("Code Executed")
        break
    }
}
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
readLines("myquery.txt")
```



Pathology Journal ISSN List was retrieved from [In Cites Clarivate](https://jcr.incites.thomsonreuters.com/), and Journal Data Filtered as follows: `JCR Year: 2016 Selected Editions: SCIE,SSCI Selected Categories: 'PATHOLOGY' Selected Category Scheme: WoS`

```{r Get ISSN List from data downloaded from WoS 1, eval=FALSE, include=FALSE, echo=TRUE}
# Get ISSN List from data downloaded from WoS
ISSNList <- JournalHomeGrid <- read_csv("data/JournalHomeGrid.csv",
                                        skip = 1) %>%
    select(ISSN) %>%
    filter(!is.na(ISSN)) %>%
    t() %>%
    paste("OR ", collapse = "") # add OR between ISSN List

ISSNList <- gsub(" OR $","" ,ISSNList) # to remove last OR
```

Data is retrieved from PubMed via E-direct.

PubMed collection from National Library of Medicine (https://www.ncbi.nlm.nih.gov/pubmed/), has the most comprehensive information about peer reviewed articles in medicine.
The API (https://dataguide.nlm.nih.gov/) is available for getting and fetching data from the server.

The query for PubMed is generated as "ISSN List AND keywords" like done in [advanced search of PubMed](https://www.ncbi.nlm.nih.gov/pubmed/advanced).


```{r Generate Search Formula For Pathology Journals AND Countries 1, eval=FALSE, include=FALSE, echo=TRUE}
# Generate Search Formula For Pathology Journals AND Countries
searchformulaTR <- paste("'",ISSNList,"'", " AND ", "Turkey[Affiliation]")
searchformulaDE <- paste("'",ISSNList,"'", " AND ", "Germany[Affiliation]")
searchformulaJP <- paste("'",ISSNList,"'", " AND ", "Japan[Affiliation]")
```



From the fetched data articles are grouped by country and keywords.

```{r Articles per countries per year 1, eval=FALSE, include=FALSE, echo=TRUE}
# Articles per countries per year
tableTR <- table(YearPubmed(fetchTurkey)) %>%
    as_tibble() %>%
    rename(Turkey = n, Year = Var1)

tableDE <- table(YearPubmed(fetchGermany)) %>%
    as_tibble() %>%
    rename(Germany = n, Year = Var1)

tableJP <- table(YearPubmed(fetchJapan)) %>%
    as_tibble() %>%
    rename(Japan = n, Year = Var1)

# Join Tables
articles_per_year_table <- list(
    tableTR,
    tableDE,
    tableJP
) %>%
    reduce(left_join, by = "Year", .id = "id")

```


```{r Prepare table for output 1, eval=FALSE, include=FALSE, echo=TRUE}
# Prepare table for output
articles_per_year <- articles_per_year_table %>%
    gather(Country, n, 2:4)

articles_per_year$Country <- factor(articles_per_year$Country,
                                    levels =c("Japan", "Germany", "Turkey"))
```


**Result:**


```{r Print the Table of Articles per year per country 1, eval=FALSE, include=FALSE, echo=TRUE}
# Print the Table of Articles per year, per country
knitr::kable(articles_per_year_table, caption = "Table of Articles per year, per country")
```

mapgraph



And the figure below shows this data in a line graph.


---




# Feedback

[Serdar Balcı, MD, Pathologist](https://github.com/sbalci) would like to hear your feedback: https://goo.gl/forms/YjGZ5DHgtPlR1RnB3

This document will be continiously updated and the last update was on `{r #  Sys.Date()`.

---

# Back to Main Menu

[Main Page for Bibliographic Analysis](https://sbalci.github.io/pubmed/BibliographicStudies.html)


<!--chapter:end:14-JournalWatchPBPath.Rmd-->

---
title: "Bibliographic Studies"
subtitle: "MeSH Terms In Pathology Articles From Turkey"
author: "Serdar Balcı, MD, Pathologist"
date: '`{r #  format(Sys.Date())`'
---

```
output: 
  html_notebook: 
    code_folding: hide
    fig_caption: yes
    highlight: kate
    number_sections: yes
    theme: cerulean
    toc: yes
    toc_float: yes
  html_document: 
    code_folding: hide
    df_print: kable
    keep_md: yes
    number_sections: yes
    theme: cerulean
    toc: yes
    toc_float: yes
    highlight: kate
```

If you want to see the code used in the analysis please click the code button on the right upper corner or throughout the page.

---

# Analysis

```{r eval=FALSE, include=FALSE, echo=TRUE}
knitr::opts_chunk$set(
	eval = FALSE,
	message = FALSE,
	warning = FALSE,
	include = FALSE,
	tidy = TRUE
)
```

## MeSH Terms In Pathology Articles From Turkey

**Background**

PubMed collection from [National Library of Medicine](https://www.ncbi.nlm.nih.gov/pubmed/), has the most comprehensive information about peer reviewed articles in medicine.

[MeSH Terms](https://www.nlm.nih.gov/pubs/factsheets/mesh.html) is a controlled vocabulary that is used to label PubMed articles according to their content. It is done by experts in National Library of Medicine. Keywords are lables that are given by authors of the article. Both are included in a PubMed record of an article.

**Aim:**

In this analysis we aimed to identify the common research topics Turkish pathologists are interested. We extracted most common MeSH terms and keywords from PubMed articles using [EDirect](https://dataguide.nlm.nih.gov/edirect/overview.html):
[MeSH Terms Pathology Articles From Turkey](https://sbalci.github.io/pubmed/MeSH_Terms_Pathology_Articles_From_Turkey.html)

**Methods:**

Packages used for analysis. Tidyverse is used for data manipulation, and [rstudioapi to run e-utilities commands from RStudio](https://github.com/rstudio/rstudio/issues/2193).

```{r load -if not present install- required packages 3, eval=FALSE, include=FALSE, echo=TRUE}
usePackage <- function(p) 
{
    if (!is.element(p, installed.packages()[,1]))
        install.packages(p, dep = TRUE)
    require(p, character.only = TRUE)
}

usePackage("tidyverse")
usePackage("rstudioapi")
```


Pathology Journal ISSN List was retrieved from [In Cites Clarivate](https://jcr.incites.thomsonreuters.com/), and Journal Data Filtered as follows: 

    JCR Year: 2016 Selected Editions: SCIE,SSCI Selected Categories: 'PATHOLOGY' Selected Category Scheme: WoS

```{r Get ISSN List from data downloaded from WoS 3, eval=FALSE, include=FALSE, echo=TRUE}
# Get ISSN List from data downloaded from WoS
ISSNList <- JournalHomeGrid <- read_csv("data/JournalHomeGrid.csv", 
                                        skip = 1) %>% 
    select(ISSN) %>% 
    filter(!is.na(ISSN)) %>% 
    t() %>% 
    paste("OR ", collapse = "") # add OR between ISSN List

ISSNList <- gsub(" OR $","" ,ISSNList) # to remove last OR
```

Data is retrieved from PubMed via [e-Utilities](https://dataguide.nlm.nih.gov/).

The search formula for PubMed is generated as "ISSN List AND Country[Affiliation]" like done in [advanced search of PubMed](https://www.ncbi.nlm.nih.gov/pubmed/advanced).

```{r Generate Search Formula For Pathology Journals AND Countries 3, eval=FALSE, include=FALSE, echo=TRUE}
# Generate Search Formula For Pathology Journals AND Countries
searchformula <- paste("'",ISSNList,"'", " AND ", "Turkey[Affiliation]")
write(searchformula, "data/searchformula.txt")
```

Articles are downloaded as xml.

```{r Search PubMed 3, eval=FALSE, include=FALSE, echo=TRUE}
myTerm <- rstudioapi::terminalCreate(show = FALSE)
rstudioapi::terminalSend(myTerm, "esearch -db pubmed -query \"$(cat data/searchformula.txt)\" -datetype PDAT -mindate 1900 -maxdate 3000 | efetch -format xml > data/PathologyTurkey.xml \n")
Sys.sleep(1)
repeat{
    Sys.sleep(0.1)
    if(rstudioapi::terminalBusy(myTerm) == FALSE){
        print("Code Executed")
        break
    }
}
```

MeSH terms are extracted from xml. [Common terms](https://www.nlm.nih.gov/bsd/indexing/training/CHK_010.html) are excluded and [major topics](https://www.nlm.nih.gov/bsd/disted/meshtutorial/principlesofmedlinesubjectindexing/majortopics/) are selected.

```{r extract major MeSH topics -excluding common tags- from xml 3, eval=FALSE, include=FALSE, echo=TRUE}
myTerm <- rstudioapi::terminalCreate(show = FALSE)
rstudioapi::terminalSend(myTerm, "xtract -input data/PathologyTurkey.xml
-pattern MeshHeading -if DescriptorName@MajorTopicYN -equals Y
-or QualifierName@MajorTopicYN -equals Y -element DescriptorName| 
grep -vxf data/checktags.txt | sort-uniq-count-rank > data/PathologyTurkeyMeSH.txt \n")
Sys.sleep(1)
repeat{
    Sys.sleep(0.1)
    if(rstudioapi::terminalBusy(myTerm) == FALSE){
        print("Code Executed")
        break
    }
}
```

Keywords are extracted from xml.

```{r extract author keywords from xml 3, eval=FALSE, include=FALSE, echo=TRUE}
myTerm <- rstudioapi::terminalCreate(show = FALSE)
rstudioapi::terminalSend(myTerm, "xtract -input data/PathologyTurkey.xml -pattern Keyword -element Keyword | sort-uniq-count-rank > authorkeywords.txt \n")
Sys.sleep(1)
repeat{
    Sys.sleep(0.1)
    if(rstudioapi::terminalBusy(myTerm) == FALSE){
        print("Code Executed")
        break
    }
}
```



**Result:**

The retrieved information was compiled in a table.

```{r display results as table 3, eval=FALSE, include=FALSE, echo=TRUE}

my_tbl <- tibble::tribble(
  ~Col_1, ~Col_2, ~Col_3,
      NA,     NA,     NA,
      NA,     NA,     NA,
      NA,     NA,     NA,
      NA,     NA,     NA
  )

require(rhandsontable)
rhandsontable(my_tbl, rowHeaders = NULL,
               digits = 3, useTypes = FALSE, search = FALSE,
               width = NULL, height = NULL)



```














**Comment:**




**Future Work:**





---


# Feedback

[Serdar Balcı, MD, Pathologist](https://github.com/sbalci) would like to hear your feedback: https://goo.gl/forms/YjGZ5DHgtPlR1RnB3

This document will be continiously updated and the last update was on `{r #  Sys.Date()`.

---

# Back to Main Menu

[Main Page for Bibliographic Analysis](https://sbalci.github.io/pubmed/BibliographicStudies.html)

<!--chapter:end:15-MeSH_Terms_Pathology_Articles_From_Turkey.Rmd-->

---
title: "Table options"
---

Several packages support making beautiful tables with R, such as

* [xtable](https://cran.r-project.org/web/packages/xtable/)
* [stargazer](https://cran.r-project.org/web/packages/stargazer/)
* [pander](http://rapporter.github.io/pander/)
* [tables](https://cran.r-project.org/web/packages/tables/)
* [ascii](http://eusebe.github.io/ascii/)
* etc.

It is also very easy to make tables with knitr's `kable` function:


```{r eval=FALSE, include=FALSE, echo=TRUE}
knitr::kable(head(iris), caption = "Title of the table")

```


<!-- ```{r eval=FALSE, include=FALSE, echo=TRUE} -->
<!-- tables::knit_print.tabular(mtcars) -->
<!-- ``` -->




```{r eval=FALSE, include=FALSE, echo=TRUE}
pander::pander(mtcars)
```



```{r eval=FALSE, include=FALSE, echo=TRUE}
stargazer::stargazer(mtcars)
```




```{r echo = TRUE, results = 'asis'}
library(knitr)
kable(mtcars[1:5, ], caption = "A knitr kable.")
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
library(rhandsontable)
rhandsontable(mtcars)
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
xtable::xtable(mtcars)
```


<!--chapter:end:16-6-tables.Rmd-->

---
title: "R Notebook"
---

- Analysing the HIV pandemic

https://rviews.rstudio.com/2019/04/30/analysing-hiv-pandemic-part-1/









<!--chapter:end:AnalysingtheHIVpandemic.Rmd-->

---
title: "arsenal"
---

# arsenal

## The compare function

https://cran.r-project.org/web/packages/arsenal/vignettes/compare.html


```{r eval=FALSE, include=FALSE, echo=TRUE}
library(arsenal)
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
df1 <- data.frame(id = paste0("person", 1:3),
                  a = c("a", "b", "c"),
                  b = c(1, 3, 4),
                  c = c("f", "e", "d"),
                  row.names = paste0("rn", 1:3),
                  stringsAsFactors = FALSE)
df2 <- data.frame(id = paste0("person", 3:1),
                  a = c("c", "b", "a"),
                  b = c(1, 3, 4),
                  d = paste0("rn", 1:3),
                  row.names = paste0("rn", c(1,3,2)),
                  stringsAsFactors = FALSE)
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
compare(df1, df2)
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
summary(compare(df1, df2))
```




```{r eval=FALSE, include=FALSE, echo=TRUE}
summary(compare(df1, df2, by = "id"))
```



```{r eval=FALSE, include=FALSE, echo=TRUE}
data(mockstudy)
mockstudy2 <- muck_up_mockstudy()
```




```{r eval=FALSE, include=FALSE, echo=TRUE}
summary(compare(mockstudy, mockstudy2, by = "case"))
```











```
Summary of data.frames
version	arg	ncol	nrow
x	mockstudy	14	1499
y	mockstudy2	13	1495
Variables not shared
version	variable	position	class
x	age	2	integer
x	arm	3	character
x	fu.time	6	integer
x	fu.stat	7	integer
y	fu_time	11	integer
y	fu stat	12	integer
y	Arm	13	character
Other variables not compared
var.x	pos.x	class.x	var.y	pos.y	class.y
race	5	character	race	3	factor
ast	12	integer	ast	8	numeric
Observations not shared
version	case	observation
x	88989	9
x	90158	8
x	99508	7
x	112263	5
Differences detected by variable
var.x	var.y	n	NAs
sex	sex	1495	0
ps	ps	1	1
hgb	hgb	266	266
bmi	bmi	0	0
alk.phos	alk.phos	0	0
mdquality.s	mdquality.s	0	0
age.ord	age.ord	0	0
First 10 differences detected per variable (1741 differences not shown)
var.x	var.y	case	values.x	values.y	row.x	row.y
sex	sex	76170	Male	Male	26	20
sex	sex	76240	Male	Male	27	21
sex	sex	76431	Female	Female	28	22
sex	sex	76712	Male	Male	29	23
sex	sex	76780	Female	Female	30	24
sex	sex	77066	Female	Female	31	25
sex	sex	77316	Male	Male	32	26
sex	sex	77355	Male	Male	33	27
sex	sex	77591	Male	Male	34	28
sex	sex	77851	Male	Male	35	29
ps	ps	86205	0	NA	6	3
hgb	hgb	88714	NA	-9	192	186
hgb	hgb	88955	NA	-9	204	198
hgb	hgb	89549	NA	-9	229	223
hgb	hgb	89563	NA	-9	231	225
hgb	hgb	89584	NA	-9	237	231
hgb	hgb	89591	NA	-9	238	232
hgb	hgb	89595	NA	-9	239	233
hgb	hgb	89647	NA	-9	243	237
hgb	hgb	89665	NA	-9	244	238
hgb	hgb	89827	NA	-9	255	249
Non-identical attributes
var.x	var.y	name
sex	sex	label
sex	sex	levels
race	race	class
race	race	label
race	race	levels
bmi	bmi	label
Column name comparison options
It is possible to change which column names are considered “the same variable”.

Ignoring case
For example, to ignore case in variable names (so that Arm and arm are considered the same), pass tol.vars = "case".

You can do this using comparison.control()

summary(compare(mockstudy, mockstudy2, by = "case", control = comparison.control(tol.vars = "case")))
or pass it through the ... arguments.

summary(compare(mockstudy, mockstudy2, by = "case", tol.vars = "case"))
Summary of data.frames
version	arg	ncol	nrow
x	mockstudy	14	1499
y	mockstudy2	13	1495
Variables not shared
version	variable	position	class
x	age	2	integer
x	fu.time	6	integer
x	fu.stat	7	integer
y	fu_time	11	integer
y	fu stat	12	integer
Other variables not compared
var.x	pos.x	class.x	var.y	pos.y	class.y
race	5	character	race	3	factor
ast	12	integer	ast	8	numeric
Observations not shared
version	case	observation
x	88989	9
x	90158	8
x	99508	7
x	112263	5
Differences detected by variable
var.x	var.y	n	NAs
arm	Arm	0	0
sex	sex	1495	0
ps	ps	1	1
hgb	hgb	266	266
bmi	bmi	0	0
alk.phos	alk.phos	0	0
mdquality.s	mdquality.s	0	0
age.ord	age.ord	0	0
First 10 differences detected per variable (1741 differences not shown)
var.x	var.y	case	values.x	values.y	row.x	row.y
sex	sex	76170	Male	Male	26	20
sex	sex	76240	Male	Male	27	21
sex	sex	76431	Female	Female	28	22
sex	sex	76712	Male	Male	29	23
sex	sex	76780	Female	Female	30	24
sex	sex	77066	Female	Female	31	25
sex	sex	77316	Male	Male	32	26
sex	sex	77355	Male	Male	33	27
sex	sex	77591	Male	Male	34	28
sex	sex	77851	Male	Male	35	29
ps	ps	86205	0	NA	6	3
hgb	hgb	88714	NA	-9	192	186
hgb	hgb	88955	NA	-9	204	198
hgb	hgb	89549	NA	-9	229	223
hgb	hgb	89563	NA	-9	231	225
hgb	hgb	89584	NA	-9	237	231
hgb	hgb	89591	NA	-9	238	232
hgb	hgb	89595	NA	-9	239	233
hgb	hgb	89647	NA	-9	243	237
hgb	hgb	89665	NA	-9	244	238
hgb	hgb	89827	NA	-9	255	249
Non-identical attributes
var.x	var.y	name
arm	Arm	label
sex	sex	label
sex	sex	levels
race	race	class
race	race	label
race	race	levels
bmi	bmi	label
Treating dots and underscores the same (equivalence classes)
It is possible to treat certain characters or sets of characters as the same by passing a character vector of equivalence classes to the tol.vars= argument.

In short, each string in the vector is split into single characters, and the resulting set of characters is replaced by the first character in the string. For example, passing c("._") would replace all underscores with dots in the column names of both datasets. Similarly, passing c("aA", "BbCc") would replace all instances of "A" with "a" and all instances of "b", "C", or "c" with "B". This is one way to ignore case for certain letters. Otherwise, it’s possible to combine the equivalence classes with ignoring case, by passing (e.g.) c("._", "case").

Passing a single character as an element this vector will replace that character with the empty string. For example, passing c(" “,”.“) would remove all spaces and dots from the column names.

For mockstudy, let’s treat dots, underscores, and spaces as the same, and ignore case:

summary(compare(mockstudy, mockstudy2, by = "case",
                tol.vars = c("._ ", "case") # dots=underscores=spaces, ignore case
))
Summary of data.frames
version	arg	ncol	nrow
x	mockstudy	14	1499
y	mockstudy2	13	1495
Variables not shared
version	variable	position	class
x	age	2	integer
Other variables not compared
var.x	pos.x	class.x	var.y	pos.y	class.y
race	5	character	race	3	factor
ast	12	integer	ast	8	numeric
Observations not shared
version	case	observation
x	88989	9
x	90158	8
x	99508	7
x	112263	5
Differences detected by variable
var.x	var.y	n	NAs
arm	Arm	0	0
sex	sex	1495	0
fu.time	fu_time	0	0
fu.stat	fu stat	0	0
ps	ps	1	1
hgb	hgb	266	266
bmi	bmi	0	0
alk.phos	alk.phos	0	0
mdquality.s	mdquality.s	0	0
age.ord	age.ord	0	0
First 10 differences detected per variable (1741 differences not shown)
var.x	var.y	case	values.x	values.y	row.x	row.y
sex	sex	76170	Male	Male	26	20
sex	sex	76240	Male	Male	27	21
sex	sex	76431	Female	Female	28	22
sex	sex	76712	Male	Male	29	23
sex	sex	76780	Female	Female	30	24
sex	sex	77066	Female	Female	31	25
sex	sex	77316	Male	Male	32	26
sex	sex	77355	Male	Male	33	27
sex	sex	77591	Male	Male	34	28
sex	sex	77851	Male	Male	35	29
ps	ps	86205	0	NA	6	3
hgb	hgb	88714	NA	-9	192	186
hgb	hgb	88955	NA	-9	204	198
hgb	hgb	89549	NA	-9	229	223
hgb	hgb	89563	NA	-9	231	225
hgb	hgb	89584	NA	-9	237	231
hgb	hgb	89591	NA	-9	238	232
hgb	hgb	89595	NA	-9	239	233
hgb	hgb	89647	NA	-9	243	237
hgb	hgb	89665	NA	-9	244	238
hgb	hgb	89827	NA	-9	255	249
Non-identical attributes
var.x	var.y	name
arm	Arm	label
sex	sex	label
sex	sex	levels
race	race	class
race	race	label
race	race	levels
bmi	bmi	label
Column comparison options
Logical tolerance
Use the tol.logical= argument to change how logicals are compared. By default, they’re expected to be equal to each other.

Numeric tolerance
To allow numeric differences of a certain tolerance, use the tol.num= and tol.num.val= options. tol.num.val= determines the maximum (unsigned) difference tolerated if tol.num="absolute" (default), and determines the maximum (unsigned) percent difference tolerated if tol.num="percent".

Also note the option int.as.num=, which determines whether integers and numerics should be compared despite their class difference. If TRUE, the integers are coerced to numeric. Note that mockstudy$ast is integer, while mockstudy2$ast is numeric:

summary(compare(mockstudy, mockstudy2, by = "case",
                tol.vars = c("._ ", "case"), # dots=underscores=spaces, ignore case
                int.as.num = TRUE            # compare integers and numerics
))
Summary of data.frames
version	arg	ncol	nrow
x	mockstudy	14	1499
y	mockstudy2	13	1495
Variables not shared
version	variable	position	class
x	age	2	integer
Other variables not compared
var.x	pos.x	class.x	var.y	pos.y	class.y
race	5	character	race	3	factor
Observations not shared
version	case	observation
x	88989	9
x	90158	8
x	99508	7
x	112263	5
Differences detected by variable
var.x	var.y	n	NAs
arm	Arm	0	0
sex	sex	1495	0
fu.time	fu_time	0	0
fu.stat	fu stat	0	0
ps	ps	1	1
hgb	hgb	266	266
bmi	bmi	0	0
alk.phos	alk.phos	0	0
ast	ast	3	0
mdquality.s	mdquality.s	0	0
age.ord	age.ord	0	0
First 10 differences detected per variable (1741 differences not shown)
var.x	var.y	case	values.x	values.y	row.x	row.y
sex	sex	76170	Male	Male	26	20
sex	sex	76240	Male	Male	27	21
sex	sex	76431	Female	Female	28	22
sex	sex	76712	Male	Male	29	23
sex	sex	76780	Female	Female	30	24
sex	sex	77066	Female	Female	31	25
sex	sex	77316	Male	Male	32	26
sex	sex	77355	Male	Male	33	27
sex	sex	77591	Male	Male	34	28
sex	sex	77851	Male	Male	35	29
ps	ps	86205	0	NA	6	3
hgb	hgb	88714	NA	-9	192	186
hgb	hgb	88955	NA	-9	204	198
hgb	hgb	89549	NA	-9	229	223
hgb	hgb	89563	NA	-9	231	225
hgb	hgb	89584	NA	-9	237	231
hgb	hgb	89591	NA	-9	238	232
hgb	hgb	89595	NA	-9	239	233
hgb	hgb	89647	NA	-9	243	237
hgb	hgb	89665	NA	-9	244	238
hgb	hgb	89827	NA	-9	255	249
ast	ast	86205	27	36	6	3
ast	ast	105271	100	36	3	2
ast	ast	110754	35	36	1	1
Non-identical attributes
var.x	var.y	name
arm	Arm	label
sex	sex	label
sex	sex	levels
race	race	class
race	race	label
race	race	levels
bmi	bmi	label
Suppose a tolerance of up to 10 is allowed for ast:

summary(compare(mockstudy, mockstudy2, by = "case",
                tol.vars = c("._ ", "case"), # dots=underscores=spaces, ignore case
                int.as.num = TRUE,           # compare integers and numerics
                tol.num.val = 10             # allow absolute differences <= 10
))
Summary of data.frames
version	arg	ncol	nrow
x	mockstudy	14	1499
y	mockstudy2	13	1495
Variables not shared
version	variable	position	class
x	age	2	integer
Other variables not compared
var.x	pos.x	class.x	var.y	pos.y	class.y
race	5	character	race	3	factor
Observations not shared
version	case	observation
x	88989	9
x	90158	8
x	99508	7
x	112263	5
Differences detected by variable
var.x	var.y	n	NAs
arm	Arm	0	0
sex	sex	1495	0
fu.time	fu_time	0	0
fu.stat	fu stat	0	0
ps	ps	1	1
hgb	hgb	266	266
bmi	bmi	0	0
alk.phos	alk.phos	0	0
ast	ast	1	0
mdquality.s	mdquality.s	0	0
age.ord	age.ord	0	0
First 10 differences detected per variable (1741 differences not shown)
var.x	var.y	case	values.x	values.y	row.x	row.y
sex	sex	76170	Male	Male	26	20
sex	sex	76240	Male	Male	27	21
sex	sex	76431	Female	Female	28	22
sex	sex	76712	Male	Male	29	23
sex	sex	76780	Female	Female	30	24
sex	sex	77066	Female	Female	31	25
sex	sex	77316	Male	Male	32	26
sex	sex	77355	Male	Male	33	27
sex	sex	77591	Male	Male	34	28
sex	sex	77851	Male	Male	35	29
ps	ps	86205	0	NA	6	3
hgb	hgb	88714	NA	-9	192	186
hgb	hgb	88955	NA	-9	204	198
hgb	hgb	89549	NA	-9	229	223
hgb	hgb	89563	NA	-9	231	225
hgb	hgb	89584	NA	-9	237	231
hgb	hgb	89591	NA	-9	238	232
hgb	hgb	89595	NA	-9	239	233
hgb	hgb	89647	NA	-9	243	237
hgb	hgb	89665	NA	-9	244	238
hgb	hgb	89827	NA	-9	255	249
ast	ast	105271	100	36	3	2
Non-identical attributes
var.x	var.y	name
arm	Arm	label
sex	sex	label
sex	sex	levels
race	race	class
race	race	label
race	race	levels
bmi	bmi	label
Factor tolerance
By default, factors are compared to each other based on both the labels and the underlying numeric levels. Set tol.factor="levels" to match only the numeric levels, or set tol.factor="labels" to match only the labels.

summary(compare(mockstudy, mockstudy2, by = "case",
                tol.vars = c("._ ", "case"), # dots=underscores=spaces, ignore case
                int.as.num = TRUE,           # compare integers and numerics
                tol.num.val = 10,            # allow absolute differences <= 10
                tol.factor = "labels"        # match only factor labels
))
Summary of data.frames
version	arg	ncol	nrow
x	mockstudy	14	1499
y	mockstudy2	13	1495
Variables not shared
version	variable	position	class
x	age	2	integer
Other variables not compared
var.x	pos.x	class.x	var.y	pos.y	class.y
race	5	character	race	3	factor
Observations not shared
version	case	observation
x	88989	9
x	90158	8
x	99508	7
x	112263	5
Differences detected by variable
var.x	var.y	n	NAs
arm	Arm	0	0
sex	sex	0	0
fu.time	fu_time	0	0
fu.stat	fu stat	0	0
ps	ps	1	1
hgb	hgb	266	266
bmi	bmi	0	0
alk.phos	alk.phos	0	0
ast	ast	1	0
mdquality.s	mdquality.s	0	0
age.ord	age.ord	0	0
First 10 differences detected per variable (256 differences not shown)
var.x	var.y	case	values.x	values.y	row.x	row.y
ps	ps	86205	0	NA	6	3
hgb	hgb	88714	NA	-9	192	186
hgb	hgb	88955	NA	-9	204	198
hgb	hgb	89549	NA	-9	229	223
hgb	hgb	89563	NA	-9	231	225
hgb	hgb	89584	NA	-9	237	231
hgb	hgb	89591	NA	-9	238	232
hgb	hgb	89595	NA	-9	239	233
hgb	hgb	89647	NA	-9	243	237
hgb	hgb	89665	NA	-9	244	238
hgb	hgb	89827	NA	-9	255	249
ast	ast	105271	100	36	3	2
Non-identical attributes
var.x	var.y	name
arm	Arm	label
sex	sex	label
sex	sex	levels
race	race	class
race	race	label
race	race	levels
bmi	bmi	label
Also note the option factor.as.char=, which determines whether factors and characters should be compared despite their class difference. If TRUE, the factors are coerced to characters. Note that mockstudy$race is a character, while mockstudy2$race is a factor:

summary(compare(mockstudy, mockstudy2, by = "case",
                tol.vars = c("._ ", "case"), # dots=underscores=spaces, ignore case
                int.as.num = TRUE,           # compare integers and numerics
                tol.num.val = 10,            # allow absolute differences <= 10
                tol.factor = "labels",       # match only factor labels
                factor.as.char = TRUE        # compare factors and characters
))
Summary of data.frames
version	arg	ncol	nrow
x	mockstudy	14	1499
y	mockstudy2	13	1495
Variables not shared
version	variable	position	class
x	age	2	integer
Other variables not compared
No other variables not compared
Observations not shared
version	case	observation
x	88989	9
x	90158	8
x	99508	7
x	112263	5
Differences detected by variable
var.x	var.y	n	NAs
arm	Arm	0	0
sex	sex	0	0
race	race	1285	0
fu.time	fu_time	0	0
fu.stat	fu stat	0	0
ps	ps	1	1
hgb	hgb	266	266
bmi	bmi	0	0
alk.phos	alk.phos	0	0
ast	ast	1	0
mdquality.s	mdquality.s	0	0
age.ord	age.ord	0	0
First 10 differences detected per variable (1531 differences not shown)
var.x	var.y	case	values.x	values.y	row.x	row.y
race	race	76170	Caucasian	caucasian	26	20
race	race	76240	Caucasian	caucasian	27	21
race	race	76431	Caucasian	caucasian	28	22
race	race	76712	Caucasian	caucasian	29	23
race	race	76780	Caucasian	caucasian	30	24
race	race	77066	Caucasian	caucasian	31	25
race	race	77316	Caucasian	caucasian	32	26
race	race	77591	Caucasian	caucasian	34	28
race	race	77851	Caucasian	caucasian	35	29
race	race	77956	Caucasian	caucasian	36	30
ps	ps	86205	0	NA	6	3
hgb	hgb	88714	NA	-9	192	186
hgb	hgb	88955	NA	-9	204	198
hgb	hgb	89549	NA	-9	229	223
hgb	hgb	89563	NA	-9	231	225
hgb	hgb	89584	NA	-9	237	231
hgb	hgb	89591	NA	-9	238	232
hgb	hgb	89595	NA	-9	239	233
hgb	hgb	89647	NA	-9	243	237
hgb	hgb	89665	NA	-9	244	238
hgb	hgb	89827	NA	-9	255	249
ast	ast	105271	100	36	3	2
Non-identical attributes
var.x	var.y	name
arm	Arm	label
sex	sex	label
sex	sex	levels
race	race	class
race	race	label
race	race	levels
bmi	bmi	label
Character tolerance
Use the tol.char= argument to change how character variables are compared. By default, they are compared as-is, but they can be compared after ignoring case or trimming whitespace or both.

summary(compare(mockstudy, mockstudy2, by = "case",
                tol.vars = c("._ ", "case"), # dots=underscores=spaces, ignore case
                int.as.num = TRUE,           # compare integers and numerics
                tol.num.val = 10,            # allow absolute differences <= 10
                tol.factor = "labels",       # match only factor labels
                factor.as.char = TRUE,       # compare factors and characters
                tol.char = "case"            # ignore case in character vectors
))
Summary of data.frames
version	arg	ncol	nrow
x	mockstudy	14	1499
y	mockstudy2	13	1495
Variables not shared
version	variable	position	class
x	age	2	integer
Other variables not compared
No other variables not compared
Observations not shared
version	case	observation
x	88989	9
x	90158	8
x	99508	7
x	112263	5
Differences detected by variable
var.x	var.y	n	NAs
arm	Arm	0	0
sex	sex	0	0
race	race	0	0
fu.time	fu_time	0	0
fu.stat	fu stat	0	0
ps	ps	1	1
hgb	hgb	266	266
bmi	bmi	0	0
alk.phos	alk.phos	0	0
ast	ast	1	0
mdquality.s	mdquality.s	0	0
age.ord	age.ord	0	0
First 10 differences detected per variable (256 differences not shown)
var.x	var.y	case	values.x	values.y	row.x	row.y
ps	ps	86205	0	NA	6	3
hgb	hgb	88714	NA	-9	192	186
hgb	hgb	88955	NA	-9	204	198
hgb	hgb	89549	NA	-9	229	223
hgb	hgb	89563	NA	-9	231	225
hgb	hgb	89584	NA	-9	237	231
hgb	hgb	89591	NA	-9	238	232
hgb	hgb	89595	NA	-9	239	233
hgb	hgb	89647	NA	-9	243	237
hgb	hgb	89665	NA	-9	244	238
hgb	hgb	89827	NA	-9	255	249
ast	ast	105271	100	36	3	2
Non-identical attributes
var.x	var.y	name
arm	Arm	label
sex	sex	label
sex	sex	levels
race	race	class
race	race	label
race	race	levels
bmi	bmi	label
Date tolerance
Use the tol.date= argument to change how dates are compared. By default, they’re expected to be equal to each other.

Other data type tolerances
Use the tol.other= argument to change how other objects are compared. By default, they’re expected to be identical().

User-defined tolerance functions
Details
The comparison.control() function accepts functions for any of the tolerance arguments in addition to the short-hand character strings. This allows the user to create custom tolerance functions to suit his/her needs.

Any custom tolerance function must accept two vectors as arguments and return a logical vector of the same length. The TRUEs in the results should correspond to elements which are deemed “different”. Note that the numeric and date tolerance functions should also include a third argument for tolerance size (even if it’s not used).

CAUTION: the results should not include NAs, since the logical vector is used to subset the input data.frames. The tol.NA() function is useful for considering any NAs in the two vectors (but not both) as differences, in addition to other criteria.

tol.NA
function (x, y, idx) 
{
    (is.na(x) & !is.na(y)) | (is.na(y) & !is.na(x)) | (!is.na(x) & 
        !is.na(y) & idx)
}
<environment: namespace:arsenal>
The tol.NA() function is used in all default tolerance functions to help handle NAs.

Example 1
Suppose we want to ignore any dates which are later in the second dataset than the first. We define a custom tolerance function.

my.tol <- function(x, y, tol)
{
  tol.NA(x, y, x > y)
}

date.df1 <- data.frame(dt = as.Date(c("2017-09-07", "2017-08-08", "2017-07-09", NA)))
date.df2 <- data.frame(dt = as.Date(c("2017-10-01", "2017-08-08", "2017-07-10", "2017-01-01")))
n.diffs(compare(date.df1, date.df2)) # default finds any differences
[1] 3
n.diffs(compare(date.df1, date.df2, tol.date = my.tol)) # our function identifies only the NA as different...
[1] 1
n.diffs(compare(date.df2, date.df1, tol.date = my.tol)) # ... until we change the argument order
[1] 3
Example 2
(Continuing our mockstudy example)

Suppose we’re okay with NAs getting replaced by -9.

tol.minus9 <- function(x, y, tol)
{
  idx1 <- is.na(x) & !is.na(y) & y == -9
  idx2 <- tol.num.absolute(x, y, tol) # find other absolute differences
  return(!idx1 & idx2)
}

summary(compare(mockstudy, mockstudy2, by = "case",
                tol.vars = c("._ ", "case"), # dots=underscores=spaces, ignore case
                int.as.num = TRUE,           # compare integers and numerics
                tol.num.val = 10,            # allow absolute differences <= 10
                tol.factor = "labels",       # match only factor labels
                factor.as.char = TRUE,       # compare factors and characters
                tol.char = "case",           # ignore case in character vectors
                tol.num = tol.minus9         # ignore NA -> -9 changes
))
Summary of data.frames
version	arg	ncol	nrow
x	mockstudy	14	1499
y	mockstudy2	13	1495
Variables not shared
version	variable	position	class
x	age	2	integer
Other variables not compared
No other variables not compared
Observations not shared
version	case	observation
x	88989	9
x	90158	8
x	99508	7
x	112263	5
Differences detected by variable
var.x	var.y	n	NAs
arm	Arm	0	0
sex	sex	0	0
race	race	0	0
fu.time	fu_time	0	0
fu.stat	fu stat	0	0
ps	ps	1	1
hgb	hgb	0	0
bmi	bmi	0	0
alk.phos	alk.phos	0	0
ast	ast	1	0
mdquality.s	mdquality.s	0	0
age.ord	age.ord	0	0
First 10 differences detected per variable
var.x	var.y	case	values.x	values.y	row.x	row.y
ps	ps	86205	0	NA	6	3
ast	ast	105271	100	36	3	2
Non-identical attributes
var.x	var.y	name
arm	Arm	label
sex	sex	label
sex	sex	levels
race	race	class
race	race	label
race	race	levels
bmi	bmi	label
Extract Differences
Differences can be easily extracted using the diffs() function. If you only want to determine how many differences were found, use the n.diffs() function.

cmp <- compare(mockstudy, mockstudy2, by = "case", tol.vars = c("._ ", "case"), int.as.num = TRUE)
n.diffs(cmp)
[1] 1765
head(diffs(cmp))
  var.x var.y  case values.x values.y row.x row.y
1   sex   sex 76170     Male     Male    26    20
2   sex   sex 76240     Male     Male    27    21
3   sex   sex 76431   Female   Female    28    22
4   sex   sex 76712     Male     Male    29    23
5   sex   sex 76780   Female   Female    30    24
6   sex   sex 77066   Female   Female    31    25
Differences can also be summarized by variable.

diffs(cmp, by.var = TRUE)
         var.x       var.y    n NAs
1          arm         Arm    0   0
2          sex         sex 1495   0
3      fu.time     fu_time    0   0
4      fu.stat     fu stat    0   0
5           ps          ps    1   1
6          hgb         hgb  266 266
7          bmi         bmi    0   0
8     alk.phos    alk.phos    0   0
9          ast         ast    3   0
10 mdquality.s mdquality.s    0   0
11     age.ord     age.ord    0   0
To report differences from only a few variables, one can pass a list of variable names to diffs().

diffs(cmp, vars = c("ps", "ast"), by.var = TRUE)
  var.x var.y n NAs
5    ps    ps 1   1
9   ast   ast 3   0
diffs(cmp, vars = c("ps", "ast"))
     var.x var.y   case values.x values.y row.x row.y
1496    ps    ps  86205        0       NA     6     3
1763   ast   ast  86205       27       36     6     3
1764   ast   ast 105271      100       36     3     2
1765   ast   ast 110754       35       36     1     1
Appendix
Stucture of the Object
(This section is just as much for my use as for yours!)

obj <- compare(mockstudy, mockstudy2, by = "case")
There are two main objects in the "compare.data.frame" object, each with its own print method.

The frame.summary contains:

the substituted-deparsed arguments

information about the number of columns and rows in each dataset

the by-variables for each dataset (which may not be the same)

the attributes for each dataset (which get counted in the print method)

a data.frame of by-variables and row numbers of observations not shared between datasets

the number of shared observations

print(obj$frame.summary)
  version        arg ncol nrow   by        attrs       unique n.shared
1       x  mockstudy   14 1499 case 3 attributes 4 unique obs     1495
2       y mockstudy2   13 1495 case 3 attributes 0 unique obs     1495
The vars.summary contains:

variable name, column number, and class vector (with possibly more than one element) for each x and y. These are all NA if there isn’t a match in both datasets.

values, a list-column of the text string "by-variable" for the by-variables, NULL for columns that aren’t compared, or a data.frame containing:

The by-variables for differences found

The values which are different for x and y

The row numbers for differences found

attrs, a list-column of NULL if there are no attributes, or a data.frame containing:

The name of the attributes

The attributes for x and y, set to NA if non-existant

The actual attributes (if show.attr=TRUE).

print(obj$vars.summary)
         var.x pos.x         class.x       var.y pos.y         class.y           values        attrs
8         case     1         integer        case     1         integer      by-variable 0 attributes
17         sex     4          factor         sex     2          factor 1495 differences 2 attributes
16        race     5       character        race     3          factor     Not compared 3 attributes
15          ps     8         integer          ps     4         integer    1 differences 0 attributes
13         hgb     9         numeric         hgb     5         numeric  266 differences 0 attributes
7          bmi    10         numeric         bmi     6         numeric    0 differences 1 attributes
4     alk.phos    11         integer    alk.phos     7         integer    0 differences 0 attributes
6          ast    12         integer         ast     8         numeric     Not compared 0 attributes
14 mdquality.s    13         integer mdquality.s     9         integer    0 differences 0 attributes
3      age.ord    14 ordered, factor     age.ord    10 ordered, factor    0 differences 0 attributes
2          age     2         integer        <NA>    NA              NA     Not compared 0 attributes
5          arm     3       character        <NA>    NA              NA     Not compared 0 attributes
11     fu.time     6         integer        <NA>    NA              NA     Not compared 0 attributes
10     fu.stat     7         integer        <NA>    NA              NA     Not compared 0 attributes
12        <NA>    NA              NA     fu_time    11         integer     Not compared 0 attributes
9         <NA>    NA              NA     fu stat    12         integer     Not compared 0 attributes
1         <NA>    NA              NA         Arm    13       character     Not compared 0 attributes


---

## The freqlist function


https://cran.r-project.org/web/packages/arsenal/vignettes/freqlist.html

The freqlist function
Tina Gunderson and Ethan Heinzen
09 November, 2018
Overview
Sample dataset
The freqlist object
Basic output using summary()
Using a formula with freqlist
Rounding percentage digits or changing variable names for printing
Additional examples
Including combinations with frequencies of zero
Options for NA handling
Frequency counts and percentages subset by factor levels
Change labels on the fly
Using xtable() to format and print freqlist() results
Use freqlist in bookdown
Appendix: Notes regarding table options in R
NAs
Table dimname names (dnn)
Overview
freqlist() is a function meant to produce output similar to SAS’s PROC FREQ procedure when using the /list option of the TABLE statement. freqlist() provides options for handling missing or sparse data and can provide cumulative counts and percentages based on subgroups. It depends on the knitr package for printing.

require(arsenal)
Sample dataset
For our examples, we’ll load the mockstudy data included with this package and use it to create a basic table. Because they have fewer levels, for brevity, we’ll use the variables arm, sex, and mdquality.s to create the example table. We’ll retain NAs in the table creation. See the appendix for notes regarding default NA handling and other useful information regarding tables in R.

# load the data
data(mockstudy)

# retain NAs when creating the table using the useNA argument
tab.ex <- table(mockstudy[, c("arm", "sex", "mdquality.s")], useNA = "ifany")
The freqlist object
The freqlist() function returns an object of class "freqlist", which has three parts: freqlist, byVar, and labels.

freqlist is a single data frame containing all contingency tables with calculated frequencies, cumulative frequencies, percentages, and cumulative percentages.

byVar and labels are used in the summary method for subgroups and variable names, which will be covered in later examples.

Note that freqlist() is an S3 generic, with methods for tables and formulas.

noby <- freqlist(tab.ex)

str(noby)
List of 3
 $ freqlist:'data.frame':   18 obs. of  7 variables:
  ..$ arm        : Factor w/ 3 levels "A: IFL","F: FOLFOX",..: 1 1 1 1 1 1 2 2 2 2 ...
  ..$ sex        : Factor w/ 2 levels "Male","Female": 1 1 1 2 2 2 1 1 1 2 ...
  ..$ mdquality.s: Factor w/ 2 levels "0","1": 1 2 NA 1 2 NA 1 2 NA 1 ...
  ..$ Freq       : int [1:18] 29 214 34 12 118 21 31 285 95 21 ...
  ..$ cumFreq    : int [1:18] 29 243 277 289 407 428 459 744 839 860 ...
  ..$ freqPercent: num [1:18] 1.93 14.28 2.27 0.8 7.87 ...
  ..$ cumPercent : num [1:18] 1.93 16.21 18.48 19.28 27.15 ...
 $ byVar   : NULL
 $ labels  : NULL
 - attr(*, "class")= chr "freqlist"
# view the data frame portion of freqlist output
head(noby[["freqlist"]])  ## or use as.data.frame(noby)
     arm    sex mdquality.s Freq cumFreq freqPercent cumPercent
1 A: IFL   Male           0   29      29        1.93       1.93
2 A: IFL   Male           1  214     243       14.28      16.21
3 A: IFL   Male        <NA>   34     277        2.27      18.48
4 A: IFL Female           0   12     289        0.80      19.28
5 A: IFL Female           1  118     407        7.87      27.15
6 A: IFL Female        <NA>   21     428        1.40      28.55
Basic output using summary()
The summary method for freqlist() relies on the kable() function (in the knitr package) for printing. knitr::kable() converts the output to markdown which can be printed in the console or easily rendered in Word, PDF, or HTML documents.

Note that you must supply results="asis" to properly format the markdown output.

summary(noby)
arm	sex	mdquality.s	Freq	cumFreq	freqPercent	cumPercent
A: IFL	Male	0	29	29	1.93	1.93
1	214	243	14.28	16.21
NA	34	277	2.27	18.48
Female	0	12	289	0.80	19.28
1	118	407	7.87	27.15
NA	21	428	1.40	28.55
F: FOLFOX	Male	0	31	459	2.07	30.62
1	285	744	19.01	49.63
NA	95	839	6.34	55.97
Female	0	21	860	1.40	57.37
1	198	1058	13.21	70.58
NA	61	1119	4.07	74.65
G: IROX	Male	0	17	1136	1.13	75.78
1	187	1323	12.47	88.26
NA	24	1347	1.60	89.86
Female	0	14	1361	0.93	90.79
1	121	1482	8.07	98.87
NA	17	1499	1.13	100.00
You can print a title for the table using the title= argument.

summary(noby, title = "Basic freqlist output")
Basic freqlist output
arm	sex	mdquality.s	Freq	cumFreq	freqPercent	cumPercent
A: IFL	Male	0	29	29	1.93	1.93
1	214	243	14.28	16.21
NA	34	277	2.27	18.48
Female	0	12	289	0.80	19.28
1	118	407	7.87	27.15
NA	21	428	1.40	28.55
F: FOLFOX	Male	0	31	459	2.07	30.62
1	285	744	19.01	49.63
NA	95	839	6.34	55.97
Female	0	21	860	1.40	57.37
1	198	1058	13.21	70.58
NA	61	1119	4.07	74.65
G: IROX	Male	0	17	1136	1.13	75.78
1	187	1323	12.47	88.26
NA	24	1347	1.60	89.86
Female	0	14	1361	0.93	90.79
1	121	1482	8.07	98.87
NA	17	1499	1.13	100.00
You can also easily pull out the freqlist data frame for more complicated formatting or manipulation (e.g. with another function such as xtable() or pander()) using as.data.frame():

head(as.data.frame(noby))
     arm    sex mdquality.s Freq cumFreq freqPercent cumPercent
1 A: IFL   Male           0   29      29        1.93       1.93
2 A: IFL   Male           1  214     243       14.28      16.21
3 A: IFL   Male        <NA>   34     277        2.27      18.48
4 A: IFL Female           0   12     289        0.80      19.28
5 A: IFL Female           1  118     407        7.87      27.15
6 A: IFL Female        <NA>   21     428        1.40      28.55
Using a formula with freqlist
Instead of passing a pre-computed table to freqlist(), you can instead pass a formula, which will be in turn passed to the xtabs() function. Additional freqlist() arguments are passed through the ... to the freqlist() table method.

Note that the addNA= argument was added to xtabs() in R 3.4.0. In previous versions, NAs have to be added to relevant columns using addNA().

### this works in R >= 3.4.0 summary(freqlist(~ arm + sex + mdquality.s, data =
### mockstudy, addNA = TRUE))

### This one is backwards-compatible
summary(freqlist(~arm + sex + addNA(mdquality.s), data = mockstudy))


|arm       |sex    |addNA.mdquality.s. | Freq| cumFreq| freqPercent| cumPercent|
|:---------|:------|:------------------|----:|-------:|-----------:|----------:|
|A: IFL    |Male   |0                  |   29|      29|        1.93|       1.93|
|          |       |1                  |  214|     243|       14.28|      16.21|
|          |       |NA                 |   34|     277|        2.27|      18.48|
|          |Female |0                  |   12|     289|        0.80|      19.28|
|          |       |1                  |  118|     407|        7.87|      27.15|
|          |       |NA                 |   21|     428|        1.40|      28.55|
|F: FOLFOX |Male   |0                  |   31|     459|        2.07|      30.62|
|          |       |1                  |  285|     744|       19.01|      49.63|
|          |       |NA                 |   95|     839|        6.34|      55.97|
|          |Female |0                  |   21|     860|        1.40|      57.37|
|          |       |1                  |  198|    1058|       13.21|      70.58|
|          |       |NA                 |   61|    1119|        4.07|      74.65|
|G: IROX   |Male   |0                  |   17|    1136|        1.13|      75.78|
|          |       |1                  |  187|    1323|       12.47|      88.26|
|          |       |NA                 |   24|    1347|        1.60|      89.86|
|          |Female |0                  |   14|    1361|        0.93|      90.79|
|          |       |1                  |  121|    1482|        8.07|      98.87|
|          |       |NA                 |   17|    1499|        1.13|     100.00|
One can also set NAs to an explicit value using includeNA().

summary(freqlist(~arm + sex + includeNA(mdquality.s, "Missing"), data = mockstudy))


|arm       |sex    |includeNA.mdquality.s...Missing.. | Freq| cumFreq| freqPercent| cumPercent|
|:---------|:------|:---------------------------------|----:|-------:|-----------:|----------:|
|A: IFL    |Male   |0                                 |   29|      29|        1.93|       1.93|
|          |       |1                                 |  214|     243|       14.28|      16.21|
|          |       |Missing                           |   34|     277|        2.27|      18.48|
|          |Female |0                                 |   12|     289|        0.80|      19.28|
|          |       |1                                 |  118|     407|        7.87|      27.15|
|          |       |Missing                           |   21|     428|        1.40|      28.55|
|F: FOLFOX |Male   |0                                 |   31|     459|        2.07|      30.62|
|          |       |1                                 |  285|     744|       19.01|      49.63|
|          |       |Missing                           |   95|     839|        6.34|      55.97|
|          |Female |0                                 |   21|     860|        1.40|      57.37|
|          |       |1                                 |  198|    1058|       13.21|      70.58|
|          |       |Missing                           |   61|    1119|        4.07|      74.65|
|G: IROX   |Male   |0                                 |   17|    1136|        1.13|      75.78|
|          |       |1                                 |  187|    1323|       12.47|      88.26|
|          |       |Missing                           |   24|    1347|        1.60|      89.86|
|          |Female |0                                 |   14|    1361|        0.93|      90.79|
|          |       |1                                 |  121|    1482|        8.07|      98.87|
|          |       |Missing                           |   17|    1499|        1.13|     100.00|
Rounding percentage digits or changing variable names for printing
The digits= argument takes a single numeric value and controls the rounding of percentages in the output. The  labelTranslations= argument is a character vector or list whose length must be equal to the number of factors used in the table. Note: this does not change the names of the data frame in the freqlist object, only those used in printing. Both options are applied in the following example.

withnames <- freqlist(tab.ex, labelTranslations = c("Treatment Arm", "Gender", "LASA QOL"), 
    digits = 0)
summary(withnames)
Treatment Arm	Gender	LASA QOL	Freq	cumFreq	freqPercent	cumPercent
A: IFL	Male	0	29	29	2	2
1	214	243	14	16
NA	34	277	2	18
Female	0	12	289	1	19
1	118	407	8	27
NA	21	428	1	29
F: FOLFOX	Male	0	31	459	2	31
1	285	744	19	50
NA	95	839	6	56
Female	0	21	860	1	57
1	198	1058	13	71
NA	61	1119	4	75
G: IROX	Male	0	17	1136	1	76
1	187	1323	12	88
NA	24	1347	2	90
Female	0	14	1361	1	91
1	121	1482	8	99
NA	17	1499	1	100
Additional examples
Including combinations with frequencies of zero
The sparse= argument takes a single logical value as input. The default option is FALSE. If set to TRUE, the sparse option will include combinations with frequencies of zero in the list of results. As our initial table did not have any such levels, we create a second table to use in our example.

summary(freqlist(~race + sex + arm, data = mockstudy, sparse = TRUE, digits = 1))
race	sex	arm	Freq	cumFreq	freqPercent	cumPercent
African-Am	Male	A: IFL	25	25	1.7	1.7
F: FOLFOX	24	49	1.6	3.3
G: IROX	16	65	1.1	4.4
Female	A: IFL	14	79	0.9	5.3
F: FOLFOX	25	104	1.7	7.0
G: IROX	11	115	0.7	7.7
Asian	Male	A: IFL	0	115	0.0	7.7
F: FOLFOX	10	125	0.7	8.4
G: IROX	1	126	0.1	8.4
Female	A: IFL	1	127	0.1	8.5
F: FOLFOX	4	131	0.3	8.8
G: IROX	2	133	0.1	8.9
Caucasian	Male	A: IFL	240	373	16.1	25.0
F: FOLFOX	352	725	23.6	48.6
G: IROX	195	920	13.1	61.7
Female	A: IFL	131	1051	8.8	70.4
F: FOLFOX	234	1285	15.7	86.1
G: IROX	136	1421	9.1	95.2
Hawaii/Pacific	Male	A: IFL	1	1422	0.1	95.3
F: FOLFOX	1	1423	0.1	95.4
G: IROX	0	1423	0.0	95.4
Female	A: IFL	0	1423	0.0	95.4
F: FOLFOX	2	1425	0.1	95.5
G: IROX	1	1426	0.1	95.6
Hispanic	Male	A: IFL	8	1434	0.5	96.1
F: FOLFOX	17	1451	1.1	97.3
G: IROX	12	1463	0.8	98.1
Female	A: IFL	4	1467	0.3	98.3
F: FOLFOX	11	1478	0.7	99.1
G: IROX	2	1480	0.1	99.2
Native-Am/Alaska	Male	A: IFL	1	1481	0.1	99.3
F: FOLFOX	0	1481	0.0	99.3
G: IROX	2	1483	0.1	99.4
Female	A: IFL	1	1484	0.1	99.5
F: FOLFOX	1	1485	0.1	99.5
G: IROX	0	1485	0.0	99.5
Other	Male	A: IFL	2	1487	0.1	99.7
F: FOLFOX	2	1489	0.1	99.8
G: IROX	1	1490	0.1	99.9
Female	A: IFL	0	1490	0.0	99.9
F: FOLFOX	2	1492	0.1	100.0
G: IROX	0	1492	0.0	100.0
Options for NA handling
The various na.options= allow you to include or exclude data with missing values for one or more factor levels in the counts and percentages, as well as show the missing data but exclude it from the cumulative counts and percentages. The default option is to include all combinations with missing values.

summary(freqlist(tab.ex, na.options = "include"))
arm	sex	mdquality.s	Freq	cumFreq	freqPercent	cumPercent
A: IFL	Male	0	29	29	1.93	1.93
1	214	243	14.28	16.21
NA	34	277	2.27	18.48
Female	0	12	289	0.80	19.28
1	118	407	7.87	27.15
NA	21	428	1.40	28.55
F: FOLFOX	Male	0	31	459	2.07	30.62
1	285	744	19.01	49.63
NA	95	839	6.34	55.97
Female	0	21	860	1.40	57.37
1	198	1058	13.21	70.58
NA	61	1119	4.07	74.65
G: IROX	Male	0	17	1136	1.13	75.78
1	187	1323	12.47	88.26
NA	24	1347	1.60	89.86
Female	0	14	1361	0.93	90.79
1	121	1482	8.07	98.87
NA	17	1499	1.13	100.00
summary(freqlist(tab.ex, na.options = "showexclude"))
arm	sex	mdquality.s	Freq	cumFreq	freqPercent	cumPercent
A: IFL	Male	0	29	29	2.33	2.33
1	214	243	17.16	19.49
NA	34	NA	NA	NA
Female	0	12	255	0.96	20.45
1	118	373	9.46	29.91
NA	21	NA	NA	NA
F: FOLFOX	Male	0	31	404	2.49	32.40
1	285	689	22.85	55.25
NA	95	NA	NA	NA
Female	0	21	710	1.68	56.94
1	198	908	15.88	72.81
NA	61	NA	NA	NA
G: IROX	Male	0	17	925	1.36	74.18
1	187	1112	15.00	89.17
NA	24	NA	NA	NA
Female	0	14	1126	1.12	90.30
1	121	1247	9.70	100.00
NA	17	NA	NA	NA
summary(freqlist(tab.ex, na.options = "remove"))
arm	sex	mdquality.s	Freq	cumFreq	freqPercent	cumPercent
A: IFL	Male	0	29	29	2.33	2.33
1	214	243	17.16	19.49
Female	0	12	255	0.96	20.45
1	118	373	9.46	29.91
F: FOLFOX	Male	0	31	404	2.49	32.40
1	285	689	22.85	55.25
Female	0	21	710	1.68	56.94
1	198	908	15.88	72.81
G: IROX	Male	0	17	925	1.36	74.18
1	187	1112	15.00	89.17
Female	0	14	1126	1.12	90.30
1	121	1247	9.70	100.00
Frequency counts and percentages subset by factor levels
The groupBy= argument internally subsets the data by the specified factor prior to calculating cumulative counts and percentages. By default, when used each subset will print in a separate table. Using the single = TRUE option when printing will collapse the subsetted result into a single table.

withby <- freqlist(tab.ex, groupBy = c("arm", "sex"))
summary(withby)
arm	sex	mdquality.s	Freq	cumFreq	freqPercent	cumPercent
A: IFL	Male	0	29	29	10.47	10.47
1	214	243	77.26	87.73
NA	34	277	12.27	100.00
arm	sex	mdquality.s	Freq	cumFreq	freqPercent	cumPercent
A: IFL	Female	0	12	12	7.95	7.95
1	118	130	78.15	86.09
NA	21	151	13.91	100.00
arm	sex	mdquality.s	Freq	cumFreq	freqPercent	cumPercent
F: FOLFOX	Male	0	31	31	7.54	7.54
1	285	316	69.34	76.89
NA	95	411	23.11	100.00
arm	sex	mdquality.s	Freq	cumFreq	freqPercent	cumPercent
F: FOLFOX	Female	0	21	21	7.50	7.50
1	198	219	70.71	78.21
NA	61	280	21.79	100.00
arm	sex	mdquality.s	Freq	cumFreq	freqPercent	cumPercent
G: IROX	Male	0	17	17	7.46	7.46
1	187	204	82.02	89.47
NA	24	228	10.53	100.00
arm	sex	mdquality.s	Freq	cumFreq	freqPercent	cumPercent
G: IROX	Female	0	14	14	9.21	9.21
1	121	135	79.61	88.82
NA	17	152	11.18	100.00
# using the single = TRUE argument will collapse results into a single table for
# printing
summary(withby, single = TRUE)
arm	sex	mdquality.s	Freq	cumFreq	freqPercent	cumPercent
A: IFL	Male	0	29	29	10.47	10.47
1	214	243	77.26	87.73
NA	34	277	12.27	100.00
Female	0	12	12	7.95	7.95
1	118	130	78.15	86.09
NA	21	151	13.91	100.00
F: FOLFOX	Male	0	31	31	7.54	7.54
1	285	316	69.34	76.89
NA	95	411	23.11	100.00
Female	0	21	21	7.50	7.50
1	198	219	70.71	78.21
NA	61	280	21.79	100.00
G: IROX	Male	0	17	17	7.46	7.46
1	187	204	82.02	89.47
NA	24	228	10.53	100.00
Female	0	14	14	9.21	9.21
1	121	135	79.61	88.82
NA	17	152	11.18	100.00
Change labels on the fly
At this time, the labels can be changed just for the variables (e.g. not the frequency columns).

labels(noby) <- c("Arm", "Sex", "QOL")
summary(noby)
Arm	Sex	QOL	Freq	cumFreq	freqPercent	cumPercent
A: IFL	Male	0	29	29	1.93	1.93
1	214	243	14.28	16.21
NA	34	277	2.27	18.48
Female	0	12	289	0.80	19.28
1	118	407	7.87	27.15
NA	21	428	1.40	28.55
F: FOLFOX	Male	0	31	459	2.07	30.62
1	285	744	19.01	49.63
NA	95	839	6.34	55.97
Female	0	21	860	1.40	57.37
1	198	1058	13.21	70.58
NA	61	1119	4.07	74.65
G: IROX	Male	0	17	1136	1.13	75.78
1	187	1323	12.47	88.26
NA	24	1347	1.60	89.86
Female	0	14	1361	0.93	90.79
1	121	1482	8.07	98.87
NA	17	1499	1.13	100.00
You can also supply labelTranslations= to summary().

summary(noby, labelTranslations = c("Arm", "Sex", "QOL"))
Arm	Sex	QOL	Freq	cumFreq	freqPercent	cumPercent
A: IFL	Male	0	29	29	1.93	1.93
1	214	243	14.28	16.21
NA	34	277	2.27	18.48
Female	0	12	289	0.80	19.28
1	118	407	7.87	27.15
NA	21	428	1.40	28.55
F: FOLFOX	Male	0	31	459	2.07	30.62
1	285	744	19.01	49.63
NA	95	839	6.34	55.97
Female	0	21	860	1.40	57.37
1	198	1058	13.21	70.58
NA	61	1119	4.07	74.65
G: IROX	Male	0	17	1136	1.13	75.78
1	187	1323	12.47	88.26
NA	24	1347	1.60	89.86
Female	0	14	1361	0.93	90.79
1	121	1482	8.07	98.87
NA	17	1499	1.13	100.00
Using xtable() to format and print freqlist() results
Fair warning: xtable() has kind of a steep learning curve. These examples are given without explanation, for more advanced users.

require(xtable)
Loading required package: xtable
# set up custom function for xtable text
italic <- function(x) {
    paste0("<i>", x, "</i>")
}
xftbl <- xtable(noby[["freqlist"]], caption = "xtable formatted output of freqlist data frame", 
    align = "|r|r|r|r|c|c|c|r|")

# change the column names
names(xftbl)[1:3] <- c("Arm", "Gender", "LASA QOL")

print(xftbl, sanitize.colnames.function = italic, include.rownames = FALSE, type = "html", 
    comment = FALSE)
xtable formatted output of freqlist data frame
Arm	Gender	LASA QOL	Freq	cumFreq	freqPercent	cumPercent
A: IFL	Male	0	29	29	1.93	1.93
A: IFL	Male	1	214	243	14.28	16.21
A: IFL	Male		34	277	2.27	18.48
A: IFL	Female	0	12	289	0.80	19.28
A: IFL	Female	1	118	407	7.87	27.15
A: IFL	Female		21	428	1.40	28.55
F: FOLFOX	Male	0	31	459	2.07	30.62
F: FOLFOX	Male	1	285	744	19.01	49.63
F: FOLFOX	Male		95	839	6.34	55.97
F: FOLFOX	Female	0	21	860	1.40	57.37
F: FOLFOX	Female	1	198	1058	13.21	70.58
F: FOLFOX	Female		61	1119	4.07	74.65
G: IROX	Male	0	17	1136	1.13	75.78
G: IROX	Male	1	187	1323	12.47	88.26
G: IROX	Male		24	1347	1.60	89.86
G: IROX	Female	0	14	1361	0.93	90.79
G: IROX	Female	1	121	1482	8.07	98.87
G: IROX	Female		17	1499	1.13	100.00
Use freqlist in bookdown
Since the backbone of freqlist() is knitr::kable(), tables still render well in bookdown. However, print.summary.freqlist() doesn’t use the caption= argument of kable(), so some tables may not have a properly numbered caption. To fix this, use the method described on the bookdown site to give the table a tag/ID.

summary(freqlist(~sex + age, data = mockstudy), title = "(\\#tab:mytableby) Caption here")
Appendix: Notes regarding table options in R
NAs
There are several widely used options for basic tables in R. The table() function in base R is probably the most common; by default it excludes NA values. You can change NA handling in base::table() using the useNA= or exclude= arguments.

# base table default removes NAs
tab.d1 <- base::table(mockstudy[, c("arm", "sex", "mdquality.s")], useNA = "ifany")
tab.d1
, , mdquality.s = 0

           sex
arm         Male Female
  A: IFL      29     12
  F: FOLFOX   31     21
  G: IROX     17     14

, , mdquality.s = 1

           sex
arm         Male Female
  A: IFL     214    118
  F: FOLFOX  285    198
  G: IROX    187    121

, , mdquality.s = NA

           sex
arm         Male Female
  A: IFL      34     21
  F: FOLFOX   95     61
  G: IROX     24     17
xtabs() is similar to table(), but uses a formula-based syntax. However, there is not an option for retaining NAs in the xtabs() function; instead, NAs must be added to each level of the factor where present using the addNA() function, or (in R >= 3.4.0) using the argument addNA = TRUE.

# without specifying addNA
tab.d2 <- xtabs(formula = ~arm + sex + mdquality.s, data = mockstudy)
tab.d2
, , mdquality.s = 0

           sex
arm         Male Female
  A: IFL      29     12
  F: FOLFOX   31     21
  G: IROX     17     14

, , mdquality.s = 1

           sex
arm         Male Female
  A: IFL     214    118
  F: FOLFOX  285    198
  G: IROX    187    121
# now with addNA
tab.d3 <- xtabs(~arm + sex + addNA(mdquality.s), data = mockstudy)
tab.d3
, , addNA(mdquality.s) = 0

           sex
arm         Male Female
  A: IFL      29     12
  F: FOLFOX   31     21
  G: IROX     17     14

, , addNA(mdquality.s) = 1

           sex
arm         Male Female
  A: IFL     214    118
  F: FOLFOX  285    198
  G: IROX    187    121

, , addNA(mdquality.s) = NA

           sex
arm         Male Female
  A: IFL      34     21
  F: FOLFOX   95     61
  G: IROX     24     17
Since the formula method of freqlist() uses xtabs(), NAs should be treated in the same way. includeNA() can also be helpful here for setting explicit NA values.

Table dimname names (dnn)
Supplying a data.frame to the table() function without giving columns individually will create a contingency table using all variables in the data.frame.

However, if the columns of a data.frame or matrix are supplied separately (i.e., as vectors), column names will not be preserved.

# providing variables separately (as vectors) drops column names
tab.d4 <- base::table(mockstudy$arm, mockstudy$sex, mockstudy$mdquality.s)
tab.d4
, ,  = 0

           
            Male Female
  A: IFL      29     12
  F: FOLFOX   31     21
  G: IROX     17     14

, ,  = 1

           
            Male Female
  A: IFL     214    118
  F: FOLFOX  285    198
  G: IROX    187    121
If desired, you can use the dnn= argument to pass variable names.

# add the column name labels back using dnn option in base::table
tab.dnn <- base::table(mockstudy$arm, mockstudy$sex, mockstudy$mdquality.s, dnn = c("Arm", 
    "Sex", "QOL"))
tab.dnn
, , QOL = 0

           Sex
Arm         Male Female
  A: IFL      29     12
  F: FOLFOX   31     21
  G: IROX     17     14

, , QOL = 1

           Sex
Arm         Male Female
  A: IFL     214    118
  F: FOLFOX  285    198
  G: IROX    187    121
If using freqlist(), you can provide the labels directly to freqlist() or to summary() using labelTranslations=.



---

## A Few Notes on Labels


https://cran.r-project.org/web/packages/arsenal/vignettes/labels.html

A Few Notes on Labels
Ethan Heinzen
09 November, 2018
Introduction
Examples
Set labels in the function call
Modify labels after the fact
Add labels to a data.frame
Introduction
The arsenal package relies somewhat heavily on variable labels to make output more “pretty”. A label here is understood to be a single character string with “pretty” text (i.e., not an “ugly” variable name). Three of the main  arsenal function use labels in their summary() output. There are several ways to set these labels.

We’ll use the mockstudy dataset for all examples here:

library(arsenal)
data(mockstudy)
library(magrittr)

# for 'freqlist' examples
tab.ex <- table(mockstudy[, c("arm", "sex", "mdquality.s")], useNA="ifany")
Examples
Set labels in the function call
The summary() method for tableby(), modelsum(), and freqlist() objects contains a labelTranslations = argument to specify labels in the function call. Note that the freqlist() function matches labels in order, whereas the other two match labels by name. The labels can be input as a list or a character vector.

summary(freqlist(tab.ex),
        labelTranslations = c("Treatment Arm", "Gender", "LASA QOL"))
Treatment Arm	Gender	LASA QOL	Freq	cumFreq	freqPercent	cumPercent
A: IFL	Male	0	29	29	1.93	1.93
1	214	243	14.28	16.21
NA	34	277	2.27	18.48
Female	0	12	289	0.80	19.28
1	118	407	7.87	27.15
NA	21	428	1.40	28.55
F: FOLFOX	Male	0	31	459	2.07	30.62
1	285	744	19.01	49.63
NA	95	839	6.34	55.97
Female	0	21	860	1.40	57.37
1	198	1058	13.21	70.58
NA	61	1119	4.07	74.65
G: IROX	Male	0	17	1136	1.13	75.78
1	187	1323	12.47	88.26
NA	24	1347	1.60	89.86
Female	0	14	1361	0.93	90.79
1	121	1482	8.07	98.87
NA	17	1499	1.13	100.00
summary(tableby(arm ~ sex + age, data = mockstudy),
        labelTranslations = c(sex = "SEX", age = "Age, yrs"))
A: IFL (N=428)	F: FOLFOX (N=691)	G: IROX (N=380)	Total (N=1499)	p value
SEX					0.190
   Male	277 (64.7%)	411 (59.5%)	228 (60.0%)	916 (61.1%)	
   Female	151 (35.3%)	280 (40.5%)	152 (40.0%)	583 (38.9%)	
Age, yrs					0.614
   Mean (SD)	59.673 (11.365)	60.301 (11.632)	59.763 (11.499)	59.985 (11.519)	
   Range	27.000 - 88.000	19.000 - 88.000	26.000 - 85.000	19.000 - 88.000	
summary(modelsum(bmi ~ age, adjust = ~sex, data = mockstudy),
        labelTranslations = list(sexFemale = "Female", age = "Age, yrs"))
estimate	std.error	p.value	adj.r.squared
(Intercept)	26.793	0.766	< 0.001	0.004
Age, yrs	0.012	0.012	0.348	
Female	-0.718	0.291	0.014	
Modify labels after the fact
Another option is to add labels after you have created the object. To do this, you can use the form labels(x) <- value or use the pipe-able version, set_labels().

# the non-pipe version; somewhat clunky
tmp <- freqlist(tab.ex)
labels(tmp) <- c("Treatment Arm", "Gender", "LASA QOL")
summary(tmp)
Treatment Arm	Gender	LASA QOL	Freq	cumFreq	freqPercent	cumPercent
A: IFL	Male	0	29	29	1.93	1.93
1	214	243	14.28	16.21
NA	34	277	2.27	18.48
Female	0	12	289	0.80	19.28
1	118	407	7.87	27.15
NA	21	428	1.40	28.55
F: FOLFOX	Male	0	31	459	2.07	30.62
1	285	744	19.01	49.63
NA	95	839	6.34	55.97
Female	0	21	860	1.40	57.37
1	198	1058	13.21	70.58
NA	61	1119	4.07	74.65
G: IROX	Male	0	17	1136	1.13	75.78
1	187	1323	12.47	88.26
NA	24	1347	1.60	89.86
Female	0	14	1361	0.93	90.79
1	121	1482	8.07	98.87
NA	17	1499	1.13	100.00
# piped--much cleaner
mockstudy %>% 
  tableby(arm ~ sex + age, data = .) %>% 
  set_labels(c(sex = "SEX", age = "Age, yrs")) %>% 
  summary()
A: IFL (N=428)	F: FOLFOX (N=691)	G: IROX (N=380)	Total (N=1499)	p value
SEX					0.190
   Male	277 (64.7%)	411 (59.5%)	228 (60.0%)	916 (61.1%)	
   Female	151 (35.3%)	280 (40.5%)	152 (40.0%)	583 (38.9%)	
Age, yrs					0.614
   Mean (SD)	59.673 (11.365)	60.301 (11.632)	59.763 (11.499)	59.985 (11.519)	
   Range	27.000 - 88.000	19.000 - 88.000	26.000 - 85.000	19.000 - 88.000	
mockstudy %>% 
  modelsum(bmi ~ age, adjust = ~ sex, data = .) %>% 
  set_labels(list(sexFemale = "Female", age = "Age, yrs")) %>% 
  summary()
estimate	std.error	p.value	adj.r.squared
(Intercept)	26.793	0.766	< 0.001	0.004
Age, yrs	0.012	0.012	0.348	
Female	-0.718	0.291	0.014	
Add labels to a data.frame
tableby() and modelsum() also allow you to have label attributes on the data. Note that by default these attributes usually get dropped upon subsetting, but tableby() and modelsum() use the keep.labels() function to retain them.

mockstudy.lab <- keep.labels(mockstudy)
You can set attributes one at a time in two ways:

attr(mockstudy.lab$sex, "label") <- "Sex"
labels(mockstudy.lab$age) <- "Age, yrs"
…or all at once:

labels(mockstudy.lab) <- list(sex = "Sex", age = "Age, yrs")
summary(tableby(arm ~ sex + age, data = mockstudy.lab))
A: IFL (N=428)	F: FOLFOX (N=691)	G: IROX (N=380)	Total (N=1499)	p value
Sex					0.190
   Male	277 (64.7%)	411 (59.5%)	228 (60.0%)	916 (61.1%)	
   Female	151 (35.3%)	280 (40.5%)	152 (40.0%)	583 (38.9%)	
Age, yrs					0.614
   Mean (SD)	59.673 (11.365)	60.301 (11.632)	59.763 (11.499)	59.985 (11.519)	
   Range	27.000 - 88.000	19.000 - 88.000	26.000 - 85.000	19.000 - 88.000	
You can pipe this, too.

mockstudy %>% 
  set_labels(list(sex = "SEX", age = "Age, yrs")) %>% 
  modelsum(bmi ~ age, adjust = ~ sex, data = .) %>% 
  summary()
estimate	std.error	p.value	adj.r.squared
(Intercept)	26.793	0.766	< 0.001	0.004
Age, yrs	0.012	0.012	0.348	
SEX Female	-0.718	0.291	0.014	
To extract labels from a data.frame, simply use the labels() function:

labels(mockstudy.lab)
## $case
## NULL
## 
## $age
## [1] "Age, yrs"
## 
## $arm
## [1] "Treatment Arm"
## 
## $sex
## [1] "Sex"
## 
## $race
## [1] "Race"
## 
## $fu.time
## NULL
## 
## $fu.stat
## NULL
## 
## $ps
## NULL
## 
## $hgb
## NULL
## 
## $bmi
## [1] "Body Mass Index (kg/m^2)"
## 
## $alk.phos
## NULL
## 
## $ast
## NULL
## 
## $mdquality.s
## NULL
## 
## $age.ord
## NULL


---

## The modelsum function

https://cran.r-project.org/web/packages/arsenal/vignettes/modelsum.html

The modelsum function
Beth Atkinson, Ethan Heinzen, Pat Votruba, Jason Sinnwell, Shannon McDonnell and Greg Dougherty
09 November, 2018
Introduction
Simple Example
Pretty text version of table
Pretty Rmarkdown version of table
Data frame version of table
Add an adjustor to the model
Models for each endpoint type
Gaussian
Fit and summarize linear regression model
Extract data using the broom package
Create a summary table using modelsum
Binomial
Fit and summarize logistic regression model
Extract data using broom package
Create a summary table using modelsum
Survival
Fit and summarize a Cox regression model
Extract data using broom package
Create a summary table using modelsum
Poisson
Example 1: fit and summarize a Poisson regression model
Extract data using broom package
Create a summary table using modelsum
Example 2: fit and summarize a Poisson regression model
Extract data using broom package
Create a summary table using modelsum
Additional Examples
1. Change summary statistics globally
2. Add labels to independent variables
3. Don’t show intercept values
4. Don’t show results for adjustment variables
5. Summarize multiple variables without typing them out
6. Subset the dataset used in the analysis
7. Create combinations of variables on the fly
8. Transform variables on the fly
9. Change the ordering of the variables or delete a variable
10. Merge two modelsum objects together
11. Add a title to the table
12. Modify how missing values are treated
13. Modify the number of digits used
14. Use case-weights in the models
15. Use modelsum within an Sweave document
16. Export modelsum results to a .CSV file
17. Write modelsum object to a separate Word or HTML file
18. Use modelsum in R Shiny
23. Use modelsum in bookdown
Available Function Options
Summary statistics
modelsum.control settings
summary.modelsum settings
Introduction
Very often we are asked to summarize model results from multiple fits into a nice table. The endpoint might be of different types (e.g., survival, case/control, continuous) and there may be several independent variables that we want to examine univariately or adjusted for certain variables such as age and sex. Locally at Mayo, the SAS macros %modelsum, %glmuniv, and %logisuni were written to create such summary tables. With the increasing interest in R, we have developed the function modelsum to create similar tables within the R environment.

In developing the modelsum function, the goal was to bring the best features of these macros into an R function. However, the task was not simply to duplicate all the functionality, but rather to make use of R’s strengths (modeling, method dispersion, flexibility in function definition and output format) and make a tool that fits the needs of R users. Additionally, the results needed to fit within the general reproducible research framework so the tables could be displayed within an R markdown report.

This report provides step-by-step directions for using the functions associated with modelsum. All functions presented here are available within the arsenal package. An assumption is made that users are somewhat familiar with R markdown documents. For those who are new to the topic, a good initial resource is available at rmarkdown.rstudio.com.

Simple Example
The first step when using the modelsum function is to load the arsenal package. All the examples in this report use a dataset called mockstudy made available by Paul Novotny which includes a variety of types of variables (character, numeric, factor, ordered factor, survival) to use as examples.

> require(arsenal)
> data(mockstudy) # load data
> dim(mockstudy)  # look at how many subjects and variables are in the dataset 
[1] 1499   14
> # help(mockstudy) # learn more about the dataset and variables
> str(mockstudy) # quick look at the data
'data.frame':   1499 obs. of  14 variables:
 $ case       : int  110754 99706 105271 105001 112263 86205 99508 90158 88989 90515 ...
 $ age        : atomic  67 74 50 71 69 56 50 57 51 63 ...
  ..- attr(*, "label")= chr "Age in Years"
 $ arm        : atomic  F: FOLFOX A: IFL A: IFL G: IROX ...
  ..- attr(*, "label")= chr "Treatment Arm"
 $ sex        : Factor w/ 2 levels "Male","Female": 1 2 2 2 2 1 1 1 2 1 ...
 $ race       : atomic  Caucasian Caucasian Caucasian Caucasian ...
  ..- attr(*, "label")= chr "Race"
 $ fu.time    : int  922 270 175 128 233 120 369 421 387 363 ...
 $ fu.stat    : int  2 2 2 2 2 2 2 2 2 2 ...
 $ ps         : int  0 1 1 1 0 0 0 0 1 1 ...
 $ hgb        : num  11.5 10.7 11.1 12.6 13 10.2 13.3 12.1 13.8 12.1 ...
 $ bmi        : atomic  25.1 19.5 NA 29.4 26.4 ...
  ..- attr(*, "label")= chr "Body Mass Index (kg/m^2)"
 $ alk.phos   : int  160 290 700 771 350 569 162 152 231 492 ...
 $ ast        : int  35 52 100 68 35 27 16 12 25 18 ...
 $ mdquality.s: int  NA 1 1 1 NA 1 1 1 1 1 ...
 $ age.ord    : Ord.factor w/ 8 levels "10-19"<"20-29"<..: 6 7 4 7 6 5 4 5 5 6 ...
To create a simple linear regression table (the default), use a formula statement to specify the variables that you want summarized. The example below predicts BMI with the variables sex and age.

> tab1 <- modelsum(bmi ~ sex + age, data=mockstudy)
If you want to take a quick look at the table, you can use summary on your modelsum object and the table will print out as text in your R console window. If you use summary without any options you will see a number of &nbsp; statements which translates to “space” in HTML.

Pretty text version of table
If you want a nicer version in your console window then adding the text=TRUE option.

> summary(tab1, text=TRUE)


|             |estimate |std.error |p.value |adj.r.squared |
|:------------|:--------|:---------|:-------|:-------------|
|(Intercept)  |27.491   |0.181     |< 0.001 |0.004         |
|sex Female   |-0.731   |0.290     |0.012   |              |
|(Intercept)  |26.424   |0.752     |< 0.001 |0.000         |
|Age in Years |0.013    |0.012     |0.290   |              |
Pretty Rmarkdown version of table
In order for the report to look nice within an R markdown (knitr) report, you just need to specify results="asis" when creating the r chunk. This changes the layout slightly (compresses it) and bolds the variable names.

> summary(tab1)
estimate	std.error	p.value	adj.r.squared
(Intercept)	27.491	0.181	< 0.001	0.004
sex Female	-0.731	0.290	0.012	
(Intercept)	26.424	0.752	< 0.001	0.000
Age in Years	0.013	0.012	0.290	
Data frame version of table
If you want a data.frame version, simply use as.data.frame.

> as.data.frame(tab1)
  model        term        label term.type    estimate  std.error
1     1 (Intercept)  (Intercept) Intercept 27.49147713 0.18134740
2     1   sexFemale   sex Female      Term -0.73105055 0.29032223
3     2 (Intercept)  (Intercept) Intercept 26.42372272 0.75211474
4     2         age Age in Years      Term  0.01304859 0.01231653
        p.value adj.r.squared
1  0.000000e+00  3.632258e-03
2  1.190605e-02  3.632258e-03
3 1.279109e-196  8.354809e-05
4  2.895753e-01  8.354809e-05
Add an adjustor to the model
The argument adjust allows the user to indicate that all the variables should be adjusted for these terms.

> tab2 <- modelsum(alk.phos ~ arm + ps + hgb, adjust= ~age + sex, data=mockstudy)
> summary(tab2)
estimate	std.error	p.value	adj.r.squared	Nmiss
(Intercept)	175.548	20.587	< 0.001	-0.001	0
Treatment Arm F: FOLFOX	-13.701	8.730	0.117		
Treatment Arm G: IROX	-2.245	9.860	0.820		
Age in Years	-0.017	0.319	0.956		
sex Female	3.016	7.521	0.688		
(Intercept)	148.391	19.585	< 0.001	0.045	266
ps	46.721	5.987	< 0.001		
Age in Years	-0.084	0.311	0.787		
sex Female	1.169	7.343	0.874		
(Intercept)	336.554	32.239	< 0.001	0.031	266
hgb	-13.845	2.137	< 0.001		
Age in Years	0.095	0.314	0.763		
sex Female	-5.980	7.516	0.426		
Models for each endpoint type
To make sure the correct model is run you need to specify “family”. The options available right now are : gaussian, binomial, survival, and poisson. If there is enough interest, additional models can be added.

Gaussian
Fit and summarize linear regression model
Look at whether there is any evidence that AlkPhos values vary by study arm after adjusting for sex and age (assuming a linear age relationship).

> fit <- lm(alk.phos ~ arm + age + sex, data=mockstudy)
> summary(fit)

Call:
lm(formula = alk.phos ~ arm + age + sex, data = mockstudy)

Residuals:
    Min      1Q  Median      3Q     Max 
-168.80  -81.45  -47.17   37.39  853.56 

Coefficients:
              Estimate Std. Error t value Pr(>|t|)    
(Intercept)  175.54808   20.58665   8.527   <2e-16 ***
armF: FOLFOX -13.70062    8.72963  -1.569    0.117    
armG: IROX    -2.24498    9.86004  -0.228    0.820    
age           -0.01741    0.31878  -0.055    0.956    
sexFemale      3.01598    7.52097   0.401    0.688    
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 128.5 on 1228 degrees of freedom
  (266 observations deleted due to missingness)
Multiple R-squared:  0.002552,  Adjusted R-squared:  -0.0006969 
F-statistic: 0.7855 on 4 and 1228 DF,  p-value: 0.5346
> plot(fit)

The results suggest that the endpoint may need to be transformed. Calculating the Box-Cox transformation suggests a log transformation.

> require(MASS)
> boxcox(fit)

> fit2 <- lm(log(alk.phos) ~ arm + age + sex, data=mockstudy)
> summary(fit2)

Call:
lm(formula = log(alk.phos) ~ arm + age + sex, data = mockstudy)

Residuals:
    Min      1Q  Median      3Q     Max 
-3.0098 -0.4470 -0.1065  0.4205  2.0620 

Coefficients:
               Estimate Std. Error t value Pr(>|t|)    
(Intercept)   4.9692474  0.1025239  48.469   <2e-16 ***
armF: FOLFOX -0.0766798  0.0434746  -1.764    0.078 .  
armG: IROX   -0.0192828  0.0491041  -0.393    0.695    
age          -0.0004058  0.0015876  -0.256    0.798    
sexFemale     0.0179253  0.0374553   0.479    0.632    
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 0.6401 on 1228 degrees of freedom
  (266 observations deleted due to missingness)
Multiple R-squared:  0.003121,  Adjusted R-squared:  -0.0001258 
F-statistic: 0.9613 on 4 and 1228 DF,  p-value: 0.4278
> plot(fit2)

Finally, look to see whether there there is a non-linear relationship with age.

> require(gam)
> fit3 <- lm(log(alk.phos) ~ arm + ns(age, df=2) + sex, data=mockstudy)
> 
> # test whether there is a difference between models 
> stats::anova(fit2,fit3)
Analysis of Variance Table

Model 1: log(alk.phos) ~ arm + age + sex
Model 2: log(alk.phos) ~ arm + ns(age, df = 2) + sex
  Res.Df    RSS Df Sum of Sq      F  Pr(>F)  
1   1228 503.19                              
2   1227 502.07  1    1.1137 2.7218 0.09924 .
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
> 
> # look at functional form of age
> termplot(fit3, term=2, se=T, rug=T)

In this instance it looks like there isn’t enough evidence to say that the relationship is non-linear.

Extract data using the broom package
The broom package makes it easy to extract information from the fit.

> tmp <- tidy(fit3) # coefficients, p-values
> class(tmp)
[1] "tbl_df"     "tbl"        "data.frame"
> tmp
# A tibble: 6 x 5
  term             estimate std.error statistic   p.value
  <chr>               <dbl>     <dbl>     <dbl>     <dbl>
1 (Intercept)        4.76      0.141     33.8   1.93e-177
2 armF: FOLFOX      -0.0767    0.0434    -1.77  7.78e-  2
3 armG: IROX        -0.0195    0.0491    -0.396 6.92e-  1
4 ns(age, df = 2)1   0.330     0.260      1.27  2.04e-  1
5 ns(age, df = 2)2  -0.101     0.0935    -1.08  2.82e-  1
6 sexFemale          0.0183    0.0374     0.489 6.25e-  1
> 
> glance(fit3)
# A tibble: 1 x 11
  r.squared adj.r.squared sigma statistic p.value    df logLik   AIC   BIC
*     <dbl>         <dbl> <dbl>     <dbl>   <dbl> <int>  <dbl> <dbl> <dbl>
1   0.00533       0.00127 0.640      1.31   0.255     6 -1196. 2405. 2441.
# ... with 2 more variables: deviance <dbl>, df.residual <int>
Create a summary table using modelsum
> ms.logy <- modelsum(log(alk.phos) ~ arm + ps + hgb, data=mockstudy, adjust= ~age + sex, 
+                     family=gaussian,  
+                     gaussian.stats=c("estimate","CI.lower.estimate","CI.upper.estimate","p.value"))
> summary(ms.logy)
estimate	CI.lower.estimate	CI.upper.estimate	p.value
(Intercept)	4.969	4.768	5.170	< 0.001
Treatment Arm F: FOLFOX	-0.077	-0.162	0.009	0.078
Treatment Arm G: IROX	-0.019	-0.116	0.077	0.695
Age in Years	-0.000	-0.004	0.003	0.798
sex Female	0.018	-0.056	0.091	0.632
(Intercept)	4.832	4.640	5.023	< 0.001
ps	0.226	0.167	0.284	< 0.001
Age in Years	-0.001	-0.004	0.002	0.636
sex Female	0.009	-0.063	0.081	0.814
(Intercept)	5.765	5.450	6.080	< 0.001
hgb	-0.069	-0.090	-0.048	< 0.001
Age in Years	0.000	-0.003	0.003	0.925
sex Female	-0.027	-0.101	0.046	0.468
Binomial
Fit and summarize logistic regression model
> boxplot(age ~ mdquality.s, data=mockstudy, ylab=attr(mockstudy$age,'label'), xlab='mdquality.s')

> 
> fit <- glm(mdquality.s ~ age + sex, data=mockstudy, family=binomial)
> summary(fit)

Call:
glm(formula = mdquality.s ~ age + sex, family = binomial, data = mockstudy)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-2.1832   0.4500   0.4569   0.4626   0.4756  

Coefficients:
             Estimate Std. Error z value Pr(>|z|)    
(Intercept)  2.329442   0.514684   4.526 6.01e-06 ***
age         -0.002353   0.008256  -0.285    0.776    
sexFemale    0.039227   0.195330   0.201    0.841    
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 807.68  on 1246  degrees of freedom
Residual deviance: 807.55  on 1244  degrees of freedom
  (252 observations deleted due to missingness)
AIC: 813.55

Number of Fisher Scoring iterations: 4
> 
> # create Odd's ratio w/ confidence intervals
> tmp <- data.frame(summary(fit)$coef)
> tmp
                Estimate  Std..Error    z.value     Pr...z..
(Intercept)  2.329441734 0.514683688  4.5259677 6.011977e-06
age         -0.002353404 0.008255814 -0.2850602 7.755980e-01
sexFemale    0.039227292 0.195330166  0.2008256 8.408350e-01
> 
> tmp$OR <- round(exp(tmp[,1]),2)
> tmp$lower.CI <- round(exp(tmp[,1] - 1.96* tmp[,2]),2)
> tmp$upper.CI <- round(exp(tmp[,1] + 1.96* tmp[,2]),2)
> names(tmp)[4] <- 'P-value'
> 
> kable(tmp[,c('OR','lower.CI','upper.CI','P-value')])
OR	lower.CI	upper.CI	P-value
(Intercept)	10.27	3.75	28.17	0.000006
age	1.00	0.98	1.01	0.775598
sexFemale	1.04	0.71	1.53	0.840835
> 
> # Assess the predictive ability of the model
> 
> # code using the pROC package
> require(pROC)
> pred <- predict(fit, type='response')
> tmp <- pROC::roc(mockstudy$mdquality.s[!is.na(mockstudy$mdquality.s)]~ pred, plot=TRUE, percent=TRUE)

> tmp$auc
Area under the curve: 50.69%
Extract data using broom package
The broom package makes it easy to extract information from the fit.

> tidy(fit, exp=T, conf.int=T) # coefficients, p-values, conf.intervals
# A tibble: 3 x 7
  term        estimate std.error statistic    p.value conf.low conf.high
  <chr>          <dbl>     <dbl>     <dbl>      <dbl>    <dbl>     <dbl>
1 (Intercept)   10.3     0.515       4.53  0.00000601    3.83      28.9 
2 age            0.998   0.00826    -0.285 0.776         0.981      1.01
3 sexFemale      1.04    0.195       0.201 0.841         0.712      1.53
> 
> glance(fit) # model summary statistics
# A tibble: 1 x 7
  null.deviance df.null logLik   AIC   BIC deviance df.residual
          <dbl>   <int>  <dbl> <dbl> <dbl>    <dbl>       <int>
1          808.    1246  -404.  814.  829.     808.        1244
Create a summary table using modelsum
> summary(modelsum(mdquality.s ~ age + bmi, data=mockstudy, adjust=~sex, family=binomial))
OR	CI.lower.OR	CI.upper.OR	p.value	concordance	Nmiss
(Intercept)	10.272	3.831	28.876	< 0.001	0.507	0
Age in Years	0.998	0.981	1.014	0.776		
sex Female	1.040	0.712	1.534	0.841		
(Intercept)	4.814	1.709	13.221	0.003	0.550	33
Body Mass Index (kg/m^2)	1.023	0.987	1.063	0.220		
sex Female	1.053	0.717	1.561	0.794		
> 
> fitall <- modelsum(mdquality.s ~ age, data=mockstudy, family=binomial,
+                    binomial.stats=c("Nmiss2","OR","p.value"))
> summary(fitall)
OR	p.value	Nmiss2
(Intercept)	10.493	< 0.001	0
Age in Years	0.998	0.766	
Survival
Fit and summarize a Cox regression model
> require(survival)
Loading required package: survival

Attaching package: 'survival'
The following object is masked from 'package:rpart':

    solder
> 
> # multivariable model with all 3 terms
> fit  <- coxph(Surv(fu.time, fu.stat) ~ age + sex + arm, data=mockstudy)
> summary(fit)
Call:
coxph(formula = Surv(fu.time, fu.stat) ~ age + sex + arm, data = mockstudy)

  n= 1499, number of events= 1356 

                  coef exp(coef)  se(coef)      z Pr(>|z|)    
age           0.004600  1.004611  0.002501  1.839   0.0659 .  
sexFemale     0.039893  1.040699  0.056039  0.712   0.4765    
armF: FOLFOX -0.454650  0.634670  0.064878 -7.008 2.42e-12 ***
armG: IROX   -0.140785  0.868676  0.072760 -1.935   0.0530 .  
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

             exp(coef) exp(-coef) lower .95 upper .95
age             1.0046     0.9954    0.9997    1.0095
sexFemale       1.0407     0.9609    0.9324    1.1615
armF: FOLFOX    0.6347     1.5756    0.5589    0.7207
armG: IROX      0.8687     1.1512    0.7532    1.0018

Concordance= 0.563  (se = 0.009 )
Rsquare= 0.037   (max possible= 1 )
Likelihood ratio test= 56.21  on 4 df,   p=2e-11
Wald test            = 56.26  on 4 df,   p=2e-11
Score (logrank) test = 56.96  on 4 df,   p=1e-11
> 
> # check proportional hazards assumption
> fit.z <- cox.zph(fit)
> fit.z
                 rho chisq     p
age          -0.0311  1.46 0.226
sexFemale    -0.0325  1.44 0.230
armF: FOLFOX  0.0343  1.61 0.205
armG: IROX    0.0337  1.54 0.214
GLOBAL            NA  4.59 0.332
> plot(fit.z[1], resid=FALSE) # makes for a cleaner picture in this case
> abline(h=coef(fit)[1], col='red')

> 
> # check functional form for age using pspline (penalized spline)
> # results are returned for the linear and non-linear components
> fit2 <- coxph(Surv(fu.time, fu.stat) ~ pspline(age) + sex + arm, data=mockstudy)
> fit2
Call:
coxph(formula = Surv(fu.time, fu.stat) ~ pspline(age) + sex + 
    arm, data = mockstudy)

                         coef se(coef)      se2    Chisq   DF       p
pspline(age), linear  0.00443  0.00237  0.00237  3.48989 1.00  0.0617
pspline(age), nonlin                            13.11270 3.08  0.0047
sexFemale             0.03993  0.05610  0.05607  0.50663 1.00  0.4766
armF: FOLFOX         -0.46240  0.06494  0.06493 50.69608 1.00 1.1e-12
armG: IROX           -0.15243  0.07301  0.07299  4.35876 1.00  0.0368

Iterations: 6 outer, 16 Newton-Raphson
     Theta= 0.954 
Degrees of freedom for terms= 4.1 1.0 2.0 
Likelihood ratio test=70.1  on 7.08 df, p=2e-12
n= 1499, number of events= 1356 
> 
> # plot smoothed age to visualize why significant
> termplot(fit2, se=T, terms=1)
> abline(h=0)

> 
> # The c-statistic comes out in the summary of the fit
> summary(fit2)$concordance
        C     se(C) 
0.5684325 0.5684325 
> 
> # It can also be calculated using the survConcordance function
> survConcordance(Surv(fu.time, fu.stat) ~ predict(fit2), data=mockstudy)
Call:
survConcordance(formula = Surv(fu.time, fu.stat) ~ predict(fit2), 
    data = mockstudy)

  n= 1499 
Concordance= 0.5684325 se= 0.008779125
concordant discordant  tied.risk  tied.time   std(c-d) 
 620221.00  470282.00    5021.00     766.00   19235.49 
Extract data using broom package
The broom package makes it easy to extract information from the fit.

> tidy(fit) # coefficients, p-values
# A tibble: 4 x 7
  term         estimate std.error statistic  p.value  conf.low conf.high
  <chr>           <dbl>     <dbl>     <dbl>    <dbl>     <dbl>     <dbl>
1 age           0.00460   0.00250     1.84  6.59e- 2 -0.000302   0.00950
2 sexFemale     0.0399    0.0560      0.712 4.77e- 1 -0.0699     0.150  
3 armF: FOLFOX -0.455     0.0649     -7.01  2.42e-12 -0.582     -0.327  
4 armG: IROX   -0.141     0.0728     -1.93  5.30e- 2 -0.283      0.00182
> 
> glance(fit) # model summary statistics
# A tibble: 1 x 15
      n nevent statistic.log p.value.log statistic.sc p.value.sc
  <int>  <dbl>         <dbl>       <dbl>        <dbl>      <dbl>
1  1499   1356          56.2    1.81e-11         57.0   1.26e-11
# ... with 9 more variables: statistic.wald <dbl>, p.value.wald <dbl>,
#   r.squared <dbl>, r.squared.max <dbl>, concordance <dbl>,
#   std.error.concordance <dbl>, logLik <dbl>, AIC <dbl>, BIC <dbl>
Create a summary table using modelsum
> ##Note: You must use quotes when specifying family="survival" 
> ##      family=survival will not work
> summary(modelsum(Surv(fu.time, fu.stat) ~ arm, 
+                  adjust=~age + sex, data=mockstudy, family="survival"))
HR	CI.lower.HR	CI.upper.HR	p.value	concordance
Treatment Arm F: FOLFOX	0.635	0.559	0.721	< 0.001	0.563
Treatment Arm G: IROX	0.869	0.753	1.002	0.053	
Age in Years	1.005	1.000	1.010	0.066	
sex Female	1.041	0.932	1.162	0.477	
> 
> ##Note: the pspline term is not working yet
> #summary(modelsum(Surv(fu.time, fu.stat) ~ arm, 
> #                adjust=~pspline(age) + sex, data=mockstudy, family='survival'))
Poisson
Poisson regression is useful when predicting an outcome variable representing counts. It can also be useful when looking at survival data. Cox models and Poisson models are very closely related and survival data can be summarized using Poisson regression. If you have overdispersion (see if the residual deviance is much larger than degrees of freedom), you may want to use quasipoisson() instead of poisson(). Some of these diagnostics need to be done outside of modelsum.

Example 1: fit and summarize a Poisson regression model
For the first example, use the solder dataset available in the rpart package. The endpoint skips has a definite Poisson look.

> require(rpart) ##just to get access to solder dataset
> data(solder)
> hist(solder$skips)

> 
> fit <- glm(skips ~ Opening + Solder + Mask , data=solder, family=poisson)
> stats::anova(fit, test='Chi')
Analysis of Deviance Table

Model: poisson, link: log

Response: skips

Terms added sequentially (first to last)

        Df Deviance Resid. Df Resid. Dev  Pr(>Chi)    
NULL                      899     8788.2              
Opening  2   2920.5       897     5867.7 < 2.2e-16 ***
Solder   1   1168.4       896     4699.3 < 2.2e-16 ***
Mask     4   2015.7       892     2683.7 < 2.2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
> summary(fit)

Call:
glm(formula = skips ~ Opening + Solder + Mask, family = poisson, 
    data = solder)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-6.1251  -1.4720  -0.7826   0.5986   6.6031  

Coefficients:
            Estimate Std. Error z value Pr(>|z|)    
(Intercept) -1.12220    0.07742  -14.50  < 2e-16 ***
OpeningM     0.57161    0.05707   10.02  < 2e-16 ***
OpeningS     1.81475    0.05044   35.98  < 2e-16 ***
SolderThin   0.84682    0.03327   25.45  < 2e-16 ***
MaskA3       0.51315    0.07098    7.23 4.83e-13 ***
MaskA6       1.81103    0.06609   27.40  < 2e-16 ***
MaskB3       1.20225    0.06697   17.95  < 2e-16 ***
MaskB6       1.86648    0.06310   29.58  < 2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

(Dispersion parameter for poisson family taken to be 1)

    Null deviance: 8788.2  on 899  degrees of freedom
Residual deviance: 2683.7  on 892  degrees of freedom
AIC: 4802.2

Number of Fisher Scoring iterations: 5
Overdispersion is when the Residual deviance is larger than the degrees of freedom. This can be tested, approximately using the following code. The goal is to have a p-value that is >0.05.

> 1-pchisq(fit$deviance, fit$df.residual)
[1] 0
One possible solution is to use the quasipoisson family instead of the poisson family. This adjusts for the overdispersion.

> fit2 <- glm(skips ~ Opening + Solder + Mask, data=solder, family=quasipoisson)
> summary(fit2)

Call:
glm(formula = skips ~ Opening + Solder + Mask, family = quasipoisson, 
    data = solder)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-6.1251  -1.4720  -0.7826   0.5986   6.6031  

Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept) -1.12220    0.13483  -8.323 3.19e-16 ***
OpeningM     0.57161    0.09939   5.751 1.22e-08 ***
OpeningS     1.81475    0.08784  20.660  < 2e-16 ***
SolderThin   0.84682    0.05794  14.615  < 2e-16 ***
MaskA3       0.51315    0.12361   4.151 3.62e-05 ***
MaskA6       1.81103    0.11510  15.735  < 2e-16 ***
MaskB3       1.20225    0.11663  10.308  < 2e-16 ***
MaskB6       1.86648    0.10989  16.984  < 2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

(Dispersion parameter for quasipoisson family taken to be 3.033198)

    Null deviance: 8788.2  on 899  degrees of freedom
Residual deviance: 2683.7  on 892  degrees of freedom
AIC: NA

Number of Fisher Scoring iterations: 5
Extract data using broom package
The broom package makes it easy to extract information from the fit.

> tidy(fit) # coefficients, p-values
# A tibble: 8 x 5
  term        estimate std.error statistic   p.value
  <chr>          <dbl>     <dbl>     <dbl>     <dbl>
1 (Intercept)   -1.12     0.0774    -14.5  1.29e- 47
2 OpeningM       0.572    0.0571     10.0  1.29e- 23
3 OpeningS       1.81     0.0504     36.0  1.66e-283
4 SolderThin     0.847    0.0333     25.5  6.47e-143
5 MaskA3         0.513    0.0710      7.23 4.83e- 13
6 MaskA6         1.81     0.0661     27.4  2.45e-165
7 MaskB3         1.20     0.0670     18.0  4.55e- 72
8 MaskB6         1.87     0.0631     29.6  2.71e-192
> 
> glance(fit) # model summary statistics
# A tibble: 1 x 7
  null.deviance df.null logLik   AIC   BIC deviance df.residual
          <dbl>   <int>  <dbl> <dbl> <dbl>    <dbl>       <int>
1         8788.     899 -2393. 4802. 4841.    2684.         892
Create a summary table using modelsum
> summary(modelsum(skips~Opening + Solder + Mask, data=solder, family="quasipoisson"))
RR	CI.lower.RR	CI.upper.RR	p.value
(Intercept)	1.533	1.179	1.952	< 0.001
Opening M	2.328	1.733	3.167	< 0.001
Opening S	7.491	5.780	9.888	< 0.001
(Intercept)	2.904	2.423	3.446	< 0.001
Solder Thin	2.808	2.295	3.458	< 0.001
(Intercept)	1.611	1.135	2.204	0.005
Mask A3	1.469	0.995	2.214	0.059
Mask A6	8.331	5.839	12.222	< 0.001
Mask B3	3.328	2.309	4.920	< 0.001
Mask B6	6.466	4.598	9.378	< 0.001
> summary(modelsum(skips~Opening + Solder + Mask, data=solder, family="poisson"))
RR	CI.lower.RR	CI.upper.RR	p.value
(Intercept)	1.533	1.397	1.678	< 0.001
Opening M	2.328	2.089	2.599	< 0.001
Opening S	7.491	6.805	8.267	< 0.001
(Intercept)	2.904	2.750	3.065	< 0.001
Solder Thin	2.808	2.637	2.992	< 0.001
(Intercept)	1.611	1.433	1.804	< 0.001
Mask A3	1.469	1.280	1.690	< 0.001
Mask A6	8.331	7.341	9.487	< 0.001
Mask B3	3.328	2.923	3.800	< 0.001
Mask B6	6.466	5.724	7.331	< 0.001
Example 2: fit and summarize a Poisson regression model
This second example uses the survival endpoint available in the mockstudy dataset. There is a close relationship between survival and Poisson models, and often it is easier to fit the model using Poisson regression, especially if you want to present absolute risk.

> # add .01 to the follow-up time (.01*1 day) in order to keep everyone in the analysis
> fit <- glm(fu.stat ~ offset(log(fu.time+.01)) + age + sex + arm, data=mockstudy, family=poisson)
> summary(fit)

Call:
glm(formula = fu.stat ~ offset(log(fu.time + 0.01)) + age + sex + 
    arm, family = poisson, data = mockstudy)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-3.1188  -0.4041   0.3242   0.9727   4.3588  

Coefficients:
              Estimate Std. Error z value Pr(>|z|)    
(Intercept)  -5.875627   0.108984 -53.913  < 2e-16 ***
age           0.003724   0.001705   2.184   0.0290 *  
sexFemale     0.027321   0.038575   0.708   0.4788    
armF: FOLFOX -0.335141   0.044600  -7.514 5.72e-14 ***
armG: IROX   -0.107776   0.050643  -2.128   0.0333 *  
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

(Dispersion parameter for poisson family taken to be 1)

    Null deviance: 2113.5  on 1498  degrees of freedom
Residual deviance: 2048.0  on 1494  degrees of freedom
AIC: 5888.2

Number of Fisher Scoring iterations: 5
> 1-pchisq(fit$deviance, fit$df.residual)
[1] 0
> 
> coef(coxph(Surv(fu.time,fu.stat) ~ age + sex + arm, data=mockstudy))
         age    sexFemale armF: FOLFOX   armG: IROX 
 0.004600011  0.039892735 -0.454650445 -0.140784996 
> coef(fit)[-1]
         age    sexFemale armF: FOLFOX   armG: IROX 
 0.003723763  0.027320917 -0.335141090 -0.107775577 
> 
> # results from the Poisson model can then be described as risk ratios (similar to the hazard ratio)
> exp(coef(fit)[-1])
         age    sexFemale armF: FOLFOX   armG: IROX 
   1.0037307    1.0276976    0.7152372    0.8978291 
> 
> # As before, we can model the dispersion which alters the standard error
> fit2 <- glm(fu.stat ~ offset(log(fu.time+.01)) + age + sex + arm, 
+             data=mockstudy, family=quasipoisson)
> summary(fit2)

Call:
glm(formula = fu.stat ~ offset(log(fu.time + 0.01)) + age + sex + 
    arm, family = quasipoisson, data = mockstudy)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-3.1188  -0.4041   0.3242   0.9727   4.3588  

Coefficients:
              Estimate Std. Error t value Pr(>|t|)    
(Intercept)  -5.875627   0.566666 -10.369   <2e-16 ***
age           0.003724   0.008867   0.420    0.675    
sexFemale     0.027321   0.200572   0.136    0.892    
armF: FOLFOX -0.335141   0.231899  -1.445    0.149    
armG: IROX   -0.107776   0.263318  -0.409    0.682    
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

(Dispersion parameter for quasipoisson family taken to be 27.03493)

    Null deviance: 2113.5  on 1498  degrees of freedom
Residual deviance: 2048.0  on 1494  degrees of freedom
AIC: NA

Number of Fisher Scoring iterations: 5
Extract data using broom package
The broom package makes it easy to extract information from the fit.

> tidy(fit) ##coefficients, p-values
# A tibble: 5 x 5
  term         estimate std.error statistic  p.value
  <chr>           <dbl>     <dbl>     <dbl>    <dbl>
1 (Intercept)  -5.88      0.109     -53.9   0.      
2 age           0.00372   0.00171     2.18  2.90e- 2
3 sexFemale     0.0273    0.0386      0.708 4.79e- 1
4 armF: FOLFOX -0.335     0.0446     -7.51  5.72e-14
5 armG: IROX   -0.108     0.0506     -2.13  3.33e- 2
> 
> glance(fit) ##model summary statistics
# A tibble: 1 x 7
  null.deviance df.null logLik   AIC   BIC deviance df.residual
          <dbl>   <int>  <dbl> <dbl> <dbl>    <dbl>       <int>
1         2114.    1498 -2939. 5888. 5915.    2048.        1494
Create a summary table using modelsum
Remember that the result from modelsum is different from the fit above. The modelsum summary shows the results for age + offset(log(fu.time+.01)) then sex + offset(log(fu.time+.01)) instead of age + sex + arm + offset(log(fu.time+.01)).

> summary(modelsum(fu.stat ~ age, adjust=~offset(log(fu.time+.01))+ sex + arm, 
+                  data=mockstudy, family=poisson))
RR	CI.lower.RR	CI.upper.RR	p.value
(Intercept)	0.003	0.002	0.003	< 0.001
Age in Years	1.004	1.000	1.007	0.029
sexFemale	1.028	0.953	1.108	0.479
armF: FOLFOX	0.715	0.656	0.781	< 0.001
armG: IROX	0.898	0.813	0.991	0.033
Additional Examples
Here are multiple examples showing how to use some of the different options.

1. Change summary statistics globally
There are standard settings for each type of model regarding what information is summarized in the table. This behavior can be modified using the modelsum.control function. In fact, you can save your standard settings and use that for future tables.

> mycontrols  <- modelsum.control(gaussian.stats=c("estimate","std.error","adj.r.squared","Nmiss"),
+                                 show.adjust=FALSE, show.intercept=FALSE)                            
> tab2 <- modelsum(bmi ~ age, adjust=~sex, data=mockstudy, control=mycontrols)
> summary(tab2)
estimate	std.error	adj.r.squared
Age in Years	0.012	0.012	0.004
You can also change these settings directly in the modelsum call.

> tab3 <- modelsum(bmi ~  age, adjust=~sex, data=mockstudy,
+                  gaussian.stats=c("estimate","std.error","adj.r.squared","Nmiss"), 
+                  show.intercept=FALSE, show.adjust=FALSE)
> summary(tab3)
estimate	std.error	adj.r.squared
Age in Years	0.012	0.012	0.004
2. Add labels to independent variables
In the above example, age is shown with a label (Age in Years), but sex is listed “as is”. This is because the data was created in SAS and in the SAS dataset, age had a label but sex did not. The label is stored as an attribute within R.

> ## Look at one variable's label
> attr(mockstudy$age,'label')
[1] "Age in Years"
> 
> ## See all the variables with a label
> unlist(lapply(mockstudy,'attr','label'))
                       age                        arm 
            "Age in Years"            "Treatment Arm" 
                      race                        bmi 
                    "Race" "Body Mass Index (kg/m^2)" 
> 
> ## or
> cbind(sapply(mockstudy,attr,'label'))
            [,1]                      
case        NULL                      
age         "Age in Years"            
arm         "Treatment Arm"           
sex         NULL                      
race        "Race"                    
fu.time     NULL                      
fu.stat     NULL                      
ps          NULL                      
hgb         NULL                      
bmi         "Body Mass Index (kg/m^2)"
alk.phos    NULL                      
ast         NULL                      
mdquality.s NULL                      
age.ord     NULL                      
If you want to add labels to other variables, there are a couple of options. First, you could add labels to the variables in your dataset.

> attr(mockstudy$age,'label')  <- 'Age, yrs'
> 
> tab1 <- modelsum(bmi ~  age, adjust=~sex, data=mockstudy)
> summary(tab1)
estimate	std.error	p.value	adj.r.squared
(Intercept)	26.793	0.766	< 0.001	0.004
Age, yrs	0.012	0.012	0.348	
sex Female	-0.718	0.291	0.014	
You can also use the built-in data.frame method for labels<-:

> labels(mockstudy)  <- c(age = 'Age, yrs')
> 
> tab1 <- modelsum(bmi ~  age, adjust=~sex, data=mockstudy)
> summary(tab1)
estimate	std.error	p.value	adj.r.squared
(Intercept)	26.793	0.766	< 0.001	0.004
Age, yrs	0.012	0.012	0.348	
sex Female	-0.718	0.291	0.014	
Another option is to add labels after you have created the table

> mylabels <- list(sexFemale = "Female", age ="Age, yrs")
> summary(tab1, labelTranslations = mylabels)
estimate	std.error	p.value	adj.r.squared
(Intercept)	26.793	0.766	< 0.001	0.004
Age, yrs	0.012	0.012	0.348	
Female	-0.718	0.291	0.014	
Alternatively, you can check the variable labels and manipulate them with a function called labels, which works on the modelsum object.

> labels(tab1)
                       bmi                        age 
"Body Mass Index (kg/m^2)"                 "Age, yrs" 
                 sexFemale 
              "sex Female" 
> labels(tab1) <- c(sexFemale="Female", age="Baseline Age (yrs)")
> labels(tab1)
                       bmi                        age 
"Body Mass Index (kg/m^2)"       "Baseline Age (yrs)" 
                 sexFemale 
                  "Female" 
> summary(tab1)
estimate	std.error	p.value	adj.r.squared
(Intercept)	26.793	0.766	< 0.001	0.004
Baseline Age (yrs)	0.012	0.012	0.348	
Female	-0.718	0.291	0.014	
3. Don’t show intercept values
> summary(modelsum(age~mdquality.s+sex, data=mockstudy), show.intercept=FALSE)
estimate	std.error	p.value	adj.r.squared	Nmiss
mdquality.s	-0.326	1.093	0.766	-0.001	252
sex Female	-1.208	0.610	0.048	0.002	0
4. Don’t show results for adjustment variables
> summary(modelsum(mdquality.s ~ age + bmi, data=mockstudy, adjust=~sex, family=binomial),
+         show.adjust=FALSE)  
OR	CI.lower.OR	CI.upper.OR	p.value	concordance	Nmiss
(Intercept)	10.272	3.831	28.876	< 0.001	0.507	0
Age, yrs	0.998	0.981	1.014	0.776		
(Intercept)	4.814	1.709	13.221	0.003	0.550	33
Body Mass Index (kg/m^2)	1.023	0.987	1.063	0.220		
5. Summarize multiple variables without typing them out
Often one wants to summarize a number of variables. Instead of typing by hand each individual variable, an alternative approach is to create a formula using the paste command with the collapse="+" option.

> # create a vector specifying the variable names
> myvars <- names(mockstudy)
> 
> # select the 8th through the 12th
> # paste them together, separated by the + sign
> RHS <- paste(myvars[8:12], collapse="+")
> RHS
[1] “ps+hgb+bmi+alk.phos+ast”

> 
> # create a formula using the as.formula function
> as.formula(paste('mdquality.s ~ ', RHS))
mdquality.s ~ ps + hgb + bmi + alk.phos + ast

> 
> # use the formula in the modelsum function
> summary(modelsum(as.formula(paste('mdquality.s ~', RHS)), family=binomial, data=mockstudy))
OR	CI.lower.OR	CI.upper.OR	p.value	concordance	Nmiss
(Intercept)	14.628	10.755	20.399	< 0.001	0.620	266
ps	0.461	0.332	0.639	< 0.001		
(Intercept)	1.236	0.272	5.560	0.783	0.573	266
hgb	1.176	1.040	1.334	0.011		
(Intercept)	4.963	1.818	13.292	0.002	0.549	33
Body Mass Index (kg/m^2)	1.023	0.987	1.062	0.225		
(Intercept)	10.622	7.687	14.794	< 0.001	0.552	266
alk.phos	0.999	0.998	1.000	0.159		
(Intercept)	10.936	7.912	15.232	< 0.001	0.545	266
ast	0.995	0.988	1.001	0.099		
These steps can also be done using the formulize function.

> ## The formulize function does the paste and as.formula steps
> tmp <- formulize('mdquality.s',myvars[8:10])
> tmp
mdquality.s ~ ps + hgb + bmi

> 
> ## More complex formulas could also be written using formulize
> tmp2 <- formulize('mdquality.s',c('ps','hgb','sqrt(bmi)'))
> 
> ## use the formula in the modelsum function
> summary(modelsum(tmp, data=mockstudy, family=binomial))
OR	CI.lower.OR	CI.upper.OR	p.value	concordance	Nmiss
(Intercept)	14.628	10.755	20.399	< 0.001	0.620	266
ps	0.461	0.332	0.639	< 0.001		
(Intercept)	1.236	0.272	5.560	0.783	0.573	266
hgb	1.176	1.040	1.334	0.011		
(Intercept)	4.963	1.818	13.292	0.002	0.549	33
Body Mass Index (kg/m^2)	1.023	0.987	1.062	0.225		
6. Subset the dataset used in the analysis
Here are two ways to get the same result (limit the analysis to subjects age>50 and in the F: FOLFOX treatment group).

The first approach uses the subset function applied to the dataset mockstudy. This example also selects a subset of variables. The modelsum function is then applied to this subsetted data.
> newdata <- subset(mockstudy, subset=age>50 & arm=='F: FOLFOX', select = c(age,sex, bmi:alk.phos))
> dim(mockstudy)
[1] 1499   14
> table(mockstudy$arm)

   A: IFL F: FOLFOX   G: IROX 
      428       691       380 
> dim(newdata)
[1] 557   4
> names(newdata)
[1] "age"      "sex"      "bmi"      "alk.phos"
> summary(modelsum(alk.phos ~ ., data=newdata))
estimate	std.error	p.value	adj.r.squared	Nmiss
(Intercept)	122.577	46.924	0.009	-0.001	0
age	0.619	0.719	0.390		
(Intercept)	164.814	7.673	< 0.001	-0.002	0
sex Female	-5.497	12.118	0.650		
(Intercept)	238.658	33.705	< 0.001	0.010	15
bmi	-2.776	1.207	0.022		
The second approach does the same analysis but uses the subset argument within modelsum to subset the data.
> summary(modelsum(log(alk.phos) ~ sex + ps + bmi, subset=age>50 & arm=="F: FOLFOX", data=mockstudy))
estimate	std.error	p.value	adj.r.squared	Nmiss
(Intercept)	4.872	0.039	< 0.001	-0.002	0
sex Female	-0.005	0.062	0.931		
(Intercept)	4.770	0.040	< 0.001	0.027	108
ps	0.183	0.050	< 0.001		
(Intercept)	5.207	0.172	< 0.001	0.007	15
Body Mass Index (kg/m^2)	-0.012	0.006	0.044		
> summary(modelsum(alk.phos ~ ps + bmi, adjust=~sex, subset = age>50 & bmi<24, data=mockstudy))
estimate	std.error	p.value	adj.r.squared	Nmiss
(Intercept)	178.812	14.550	< 0.001	0.007	77
ps	20.834	13.440	0.122		
sex Female	-17.542	16.656	0.293		
(Intercept)	373.008	104.272	< 0.001	0.009	24
Body Mass Index (kg/m^2)	-8.239	4.727	0.083		
sex Female	-24.058	16.855	0.155		
> summary(modelsum(alk.phos ~ ps + bmi, adjust=~sex, subset=1:30, data=mockstudy))
estimate	std.error	p.value	adj.r.squared	Nmiss
(Intercept)	169.112	57.013	0.006	0.294	0
ps	254.901	68.100	< 0.001		
sex Female	49.566	67.643	0.470		
(Intercept)	453.070	200.651	0.033	-0.049	1
Body Mass Index (kg/m^2)	-5.993	7.408	0.426		
sex Female	-22.308	79.776	0.782		
7. Create combinations of variables on the fly
> ## create a variable combining the levels of mdquality.s and sex
> with(mockstudy, table(interaction(mdquality.s,sex)))

  0.Male   1.Male 0.Female 1.Female 
      77      686       47      437 
> summary(modelsum(age ~ interaction(mdquality.s,sex), data=mockstudy))
estimate	std.error	p.value	adj.r.squared	Nmiss
(Intercept)	59.714	1.314	< 0.001	0.003	252
interaction(mdquality.s, sex) 1.Male	0.730	1.385	0.598		
interaction(mdquality.s, sex) 0.Female	0.988	2.134	0.643		
interaction(mdquality.s, sex) 1.Female	-1.021	1.425	0.474		
8. Transform variables on the fly
Certain transformations need to be surrounded by I() so that R knows to treat it as a variable transformation and not some special model feature. If the transformation includes any of the symbols / - + ^ * then surround the new variable by I().

> summary(modelsum(arm=="F: FOLFOX" ~ I(age/10) + log(bmi) + mdquality.s,
+                  data=mockstudy, family=binomial))
OR	CI.lower.OR	CI.upper.OR	p.value	concordance	Nmiss
(Intercept)	0.656	0.382	1.124	0.126	0.514	0
Age, yrs	1.045	0.957	1.142	0.326		
(Intercept)	0.633	0.108	3.698	0.611	0.508	33
Body Mass Index (kg/m^2)	1.092	0.638	1.867	0.748		
(Intercept)	0.722	0.503	1.029	0.074	0.502	252
mdquality.s	1.045	0.719	1.527	0.819		
9. Change the ordering of the variables or delete a variable
> mytab <- modelsum(bmi ~ sex + alk.phos + age, data=mockstudy)
> mytab2 <- mytab[c('age','sex','alk.phos')]
> summary(mytab2)
estimate	std.error	p.value	adj.r.squared	Nmiss
(Intercept)	26.424	0.752	< 0.001	0.000	0
Age, yrs	0.013	0.012	0.290		
(Intercept)	27.491	0.181	< 0.001	0.004	0
sex Female	-0.731	0.290	0.012		
(Intercept)	27.944	0.253	< 0.001	0.011	266
alk.phos	-0.005	0.001	< 0.001		
> summary(mytab[c('age','sex')])
estimate	std.error	p.value	adj.r.squared
(Intercept)	26.424	0.752	< 0.001	0.000
Age, yrs	0.013	0.012	0.290	
(Intercept)	27.491	0.181	< 0.001	0.004
sex Female	-0.731	0.290	0.012	
> summary(mytab[c(3,1)])
estimate	std.error	p.value	adj.r.squared
(Intercept)	26.424	0.752	< 0.001	0.000
Age, yrs	0.013	0.012	0.290	
(Intercept)	27.491	0.181	< 0.001	0.004
sex Female	-0.731	0.290	0.012	
10. Merge two modelsum objects together
It is possible to combine two modelsum objects so that they print out together, however you need to pay attention to the columns that are being displayed. It is easier to combine two models of the same family (such as two sets of linear models). If you want to combine linear and logistic model results then you would want to display the beta coefficients for the logistic model.

> ## demographics
> tab1 <- modelsum(bmi ~ sex + age, data=mockstudy)
> ## lab data
> tab2 <- modelsum(mdquality.s ~ hgb + alk.phos, data=mockstudy, family=binomial)
>                 
> tab12 <- merge(tab1,tab2)
> class(tab12)
[1] “modelsumList”

> 
> ##ERROR: The merge works, but not the summary
> #summary(tab12)
11. Add a title to the table
When creating a pdf the tables are automatically numbered and the title appears below the table. In Word and HTML, the titles appear un-numbered and above the table.

> t1 <- modelsum(bmi ~ sex + age, data=mockstudy)
> summary(t1, title='Demographics')
Demographics
estimate	std.error	p.value	adj.r.squared
(Intercept)	27.491	0.181	< 0.001	0.004
sex Female	-0.731	0.290	0.012	
(Intercept)	26.424	0.752	< 0.001	0.000
Age, yrs	0.013	0.012	0.290	
12. Modify how missing values are treated
Depending on the report you are writing you have the following options:

Use all values available for each variable

Use only those subjects who have measurements available for all the variables

> ## look at how many missing values there are for each variable
> apply(is.na(mockstudy),2,sum)
       case         age         arm         sex        race     fu.time 
          0           0           0           0           7           0 
    fu.stat          ps         hgb         bmi    alk.phos         ast 
          0         266         266          33         266         266 
mdquality.s     age.ord 
        252           0 
> ## Show how many subjects have each variable (non-missing)
> summary(modelsum(bmi ~ ast + age, data=mockstudy,
+                 control=modelsum.control(gaussian.stats=c("N","estimate"))))
estimate	N
(Intercept)	27.331	1233
ast	-0.005	
(Intercept)	26.424	1499
Age, yrs	0.013	
> 
> ## Always list the number of missing values
> summary(modelsum(bmi ~ ast + age, data=mockstudy,
+                 control=modelsum.control(gaussian.stats=c("Nmiss2","estimate"))))
estimate	Nmiss2
(Intercept)	27.331	266
ast	-0.005	
(Intercept)	26.424	0
Age, yrs	0.013	
> 
> ## Only show the missing values if there are some (default)
> summary(modelsum(bmi ~ ast + age, data=mockstudy, 
+                 control=modelsum.control(gaussian.stats=c("Nmiss","estimate"))))
estimate	Nmiss
(Intercept)	27.331	266
ast	-0.005	
(Intercept)	26.424	0
Age, yrs	0.013	
> 
> ## Don't show N at all
> summary(modelsum(bmi ~ ast + age, data=mockstudy, 
+                 control=modelsum.control(gaussian.stats=c("estimate"))))
estimate
(Intercept)	27.331
ast	-0.005
(Intercept)	26.424
Age, yrs	0.013
13. Modify the number of digits used
Within modelsum.control function there are 3 options for controlling the number of significant digits shown.

digits: controls the number of digits after the decimal point for continuous values

digits.ratio: controls the number of digits after the decimal point for continuous values

digits.p: controls the number of digits after the decimal point for continuous values

> summary(modelsum(bmi ~ sex + age + fu.time, data=mockstudy), digits=4, digits.test=2)
Warning: Using 'digits.test = ' is deprecated. Use 'digits.p = ' instead.
estimate	std.error	p.value	adj.r.squared
(Intercept)	27.4915	0.1813	< 0.001	0.0036
sex Female	-0.7311	0.2903	0.012	
(Intercept)	26.4237	0.7521	< 0.001	0.0001
Age, yrs	0.0130	0.0123	0.290	
(Intercept)	26.4937	0.2447	< 0.001	0.0079
fu.time	0.0011	0.0003	< 0.001	
14. Use case-weights in the models
Occasionally it is of interest to fit models using case weights. The modelsum function allows you to pass on the weights to the models and it will do the appropriate fit.

> mockstudy$agegp <- cut(mockstudy$age, breaks=c(18,50,60,70,90), right=FALSE)
> 
> ## create weights based on agegp and sex distribution
> tab1 <- with(mockstudy,table(agegp, sex))
> tab1
         sex
agegp     Male Female
  [18,50)  152    110
  [50,60)  258    178
  [60,70)  295    173
  [70,90)  211    122
> tab2 <- with(mockstudy, table(agegp, sex, arm))
> gpwts <- rep(tab1, length(unique(mockstudy$arm)))/tab2
> 
> ## apply weights to subjects
> index <- with(mockstudy, cbind(as.numeric(agegp), as.numeric(sex), as.numeric(as.factor(arm)))) 
> mockstudy$wts <- gpwts[index]
> 
> ## show weights by treatment arm group
> tapply(mockstudy$wts,mockstudy$arm, summary)
$`A: IFL`
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
  2.923   3.225   3.548   3.502   3.844   4.045 

$`F: FOLFOX`
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
  2.033   2.070   2.201   2.169   2.263   2.303 

$`G: IROX`
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
  3.667   3.734   4.023   3.945   4.031   4.471 
> mockstudy$newvarA <- as.numeric(mockstudy$arm=='A: IFL')
> tab1 <- modelsum(newvarA ~ ast + bmi + hgb, data=mockstudy, subset=(arm !='G: IROX'), 
+                  family=binomial)
> summary(tab1, title='No Case Weights used')
No Case Weights used
OR	CI.lower.OR	CI.upper.OR	p.value	concordance	Nmiss
(Intercept)	0.590	0.473	0.735	< 0.001	0.550	210
ast	1.003	0.998	1.008	0.258		
(Intercept)	0.578	0.306	1.093	0.091	0.500	29
Body Mass Index (kg/m^2)	1.003	0.980	1.026	0.808		
(Intercept)	1.006	0.386	2.631	0.990	0.514	210
hgb	0.965	0.894	1.043	0.372		
> 
> suppressWarnings({
+ tab2 <- modelsum(newvarA ~ ast + bmi + hgb, data=mockstudy, subset=(arm !='G: IROX'), 
+                  weights=wts, family=binomial)
+ summary(tab2, title='Case Weights used')
+ })
Case Weights used
OR	CI.lower.OR	CI.upper.OR	p.value	concordance	Nmiss
(Intercept)	0.956	0.837	1.091	0.504	0.550	210
ast	1.003	1.000	1.006	0.068		
(Intercept)	0.957	0.658	1.393	0.820	0.500	29
Body Mass Index (kg/m^2)	1.002	0.988	1.016	0.780		
(Intercept)	1.829	1.031	3.248	0.039	0.514	210
hgb	0.956	0.913	1.001	0.058		
15. Use modelsum within an Sweave document
For those users who wish to create tables within an Sweave document, the following code seems to work.

\documentclass{article}

\usepackage{longtable}
\usepackage{pdfpages}

\begin{document}

\section{Read in Data}
<<echo=TRUE>>=
require(arsenal)
require(knitr)
require(rmarkdown)
data(mockstudy)

tab1 <- modelsum(bmi~sex+age, data=mockstudy)
@

\section{Convert Summary.modelsum to LaTeX}
<<echo=TRUE, results='hide', message=FALSE>>=
capture.output(summary(tab1), file="Test.md")

## Convert R Markdown Table to LaTeX
render("Test.md", pdf_document(keep_tex=TRUE))
@ 

\includepdf{Test.pdf}

\end{document}
16. Export modelsum results to a .CSV file
When looking at multiple variables it is sometimes useful to export the results to a csv file. The as.data.frame function creates a data frame object that can be exported or further manipulated within R.

> summary(tab2, text=T)


|                         |OR    |CI.lower.OR |CI.upper.OR |p.value |concordance |Nmiss |
|:------------------------|:-----|:-----------|:-----------|:-------|:-----------|:-----|
|(Intercept)              |0.956 |0.837       |1.091       |0.504   |0.550       |210   |
|ast                      |1.003 |1.000       |1.006       |0.068   |            |      |
|(Intercept)              |0.957 |0.658       |1.393       |0.820   |0.500       |29    |
|Body Mass Index (kg/m^2) |1.002 |0.988       |1.016       |0.780   |            |      |
|(Intercept)              |1.829 |1.031       |3.248       |0.039   |0.514       |210   |
|hgb                      |0.956 |0.913       |1.001       |0.058   |            |      |
> tmp <- as.data.frame(tab2)
> tmp
  model        term                    label term.type        OR
1     1 (Intercept)              (Intercept) Intercept 0.9559704
2     1         ast                      ast      Term 1.0027311
3     2 (Intercept)              (Intercept) Intercept 0.9573694
4     2         bmi Body Mass Index (kg/m^2)      Term 1.0019251
5     3 (Intercept)              (Intercept) Intercept 1.8287083
6     3         hgb                      hgb      Term 0.9563507
  CI.lower.OR CI.upper.OR    p.value concordance Nmiss
1   0.8373522    1.090904 0.50443340   0.5499494   210
2   0.9998110    1.005696 0.06813456   0.5499494   210
3   0.6579225    1.392859 0.81981779   0.5002561    29
4   0.9884804    1.015561 0.78019163   0.5002561    29
5   1.0311954    3.247941 0.03911088   0.5138162   210
6   0.9132041    1.001419 0.05770821   0.5138162   210
> # write.csv(tmp, '/my/path/here/mymodel.csv')
17. Write modelsum object to a separate Word or HTML file
> ## write to an HTML document
> write2html(tab2, "~/ibm/trash.html")
> 
> ## write to a Word document
> write2word(tab2, "~/ibm/trash.doc", title="My table in Word")
18. Use modelsum in R Shiny
The easiest way to output a modelsum() object in an R Shiny app is to use the tableOutput() UI in combination with the renderTable() server function and as.data.frame(summary(modelsum())):

> # A standalone shiny app
> library(shiny)
> library(arsenal)
> data(mockstudy)
> 
> shinyApp(
+   ui = fluidPage(tableOutput("table")),
+   server = function(input, output) {
+     output$table <- renderTable({
+       as.data.frame(summary(modelsum(age ~ sex, data = mockstudy), text = "html"))
+     }, sanitize.text.function = function(x) x)
+   }
+ )
This can be especially powerful if you feed the selections from a selectInput(multiple = TRUE) into formulize() to make the table dynamic!

23. Use modelsum in bookdown
Since the backbone of modelsum() is knitr::kable(), tables still render well in bookdown. However, print.summary.modelsum() doesn’t use the caption= argument of kable(), so some tables may not have a properly numbered caption. To fix this, use the method described on the bookdown site to give the table a tag/ID.

> summary(modelsum(age ~ sex, data = mockstudy), title="(\\#tab:mytableby) Caption here")
Available Function Options
Summary statistics
The available summary statistics, by varible type, are:

ordinal: Ordinal logistic regression models
default: Nmiss, OR, CI.lower.OR, CI.upper.OR, p.value
optional: estimate, CI.OR, CI.estimate, CI.lower.estimate, CI.upper.estimate, N, Nmiss2, endpoint, std.error, statistic, logLik, AIC, BIC, edf, deviance, df.residual
binomial,quasibinomial: Logistic regression models
default: OR, CI.lower.OR, CI.upper.OR, p.value, concordance, Nmiss
optional: estimate, CI.OR, CI.estimate, CI.lower.estimate, CI.upper.estimate, N, Nmiss2, endpoint, std.error, statistic, logLik, AIC, BIC, null.deviance, deviance, df.residual, df.null
gaussian: Linear regression models
default: estimate, std.error, p.value, adj.r.squared, Nmiss
optional: CI.estimate, CI.lower.estimate, CI.upper.estimate, N, Nmiss2, statistic, standard.estimate, endpoint, r.squared, AIC, BIC, logLik, statistic.F, p.value.F
poisson, quasipoisson: Poisson regression models
default: RR, CI.lower.RR, CI.upper.RR, p.value, Nmiss
optional: CI.RR, CI.estimate, CI.lower.estimate, CI.upper.estimate, CI.RR, Nmiss2, std.error, estimate, statistic, endpoint, AIC, BIC, logLik, dispersion, null.deviance, deviance, df.residual, df.null
negbin: Negative binomial regression models
default: RR, CI.lower.RR, CI.upper.RR, p.value, Nmiss
optional: CI.RR, CI.estimate, CI.lower.estimate, CI.upper.estimate, CI.RR, Nmiss2, std.error, estimate, statistic, endpoint, AIC, BIC, logLik, dispersion, null.deviance, deviance, df.residual, df.null, theta, SE.theta
survival: Cox models
default: HR, CI.lower.HR, CI.upper.HR, p.value, concordance, Nmiss
optional: CI.HR, CI.estimate, CI.lower.estimate, CI.upper.estimate, N, Nmiss2, estimate, std.error, endpoint, Nevents, statistic, r.squared, logLik, AIC, BIC, statistic.sc, p.value.sc, p.value.log, p.value.wald, N, std.error.concordance
The full description of these parameters that can be shown for models include:

N: a count of the number of observations used in the analysis
Nmiss: only show the count of the number of missing values if there are some missing values
Nmiss2: always show a count of the number of missing values for a model
endpoint: dependent variable used in the model
std.err: print the standard error
statistic: test statistic
statistic.F: test statistic (F test)
p.value: print the p-value
r.squared: print the model R-square
adj.r.squared: print the model adjusted R-square
r.squared: print the model R-square
concordance: print the model C statistic (which is the AUC for logistic models)
logLik: print the loglikelihood value
p.value.log: print the p-value for the overall model likelihood test
p.value.wald: print the p-value for the overall model wald test
p.value.sc: print the p-value for overall model score test
AIC: print the Akaike information criterion
BIC: print the Bayesian information criterion
null.deviance: null deviance
deviance: model deviance
df.residual: degrees of freedom for the residual
df.null: degrees of freedom for the null model
dispersion: This is used in Poisson models and is defined as the deviance/df.residual
statistic.sc: overall model score statistic
std.error.concordance: standard error for the C statistic
HR: print the hazard ratio (for survival models), i.e. exp(beta)
CI.lower.HR, CI.upper.HR: print the confidence interval for the HR
OR: print the odd’s ratio (for logistic models), i.e. exp(beta)
CI.lower.OR, CI.upper.OR: print the confidence interval for the OR
RR: print the risk ratio (for poisson models), i.e. exp(beta)
CI.lower.RR, CI.upper.RR: print the confidence interval for the RR
estimate: print beta coefficient
standardized.estimate: print the standardized beta coefficient
CI.lower.estimate, CI.upper.estimate: print the confidence interval for the beta coefficient
edf: print the effective degrees of freedom.
theta: print the estimate of theta.
SE.theta: print the estimate of theta’s standard error.
modelsum.control settings
A quick way to see what arguments are possible to utilize in a function is to use the args() command. Settings involving the number of digits can be set in modelsum.control or in summary.modelsum.

> args(modelsum.control)
function (digits = 3L, digits.ratio = 3L, digits.p = 3L, format.p = TRUE, 
    show.adjust = TRUE, show.intercept = TRUE, conf.level = 0.95, 
    ordinal.stats = c("OR", "CI.lower.OR", "CI.upper.OR", "p.value", 
        "Nmiss"), binomial.stats = c("OR", "CI.lower.OR", "CI.upper.OR", 
        "p.value", "concordance", "Nmiss"), gaussian.stats = c("estimate", 
        "std.error", "p.value", "adj.r.squared", "Nmiss"), poisson.stats = c("RR", 
        "CI.lower.RR", "CI.upper.RR", "p.value", "Nmiss"), negbin.stats = c("RR", 
        "CI.lower.RR", "CI.upper.RR", "p.value", "Nmiss"), survival.stats = c("HR", 
        "CI.lower.HR", "CI.upper.HR", "p.value", "concordance", 
        "Nmiss"), stat.labels = list(), ...) 
NULL
summary.modelsum settings
The summary.modelsum function has options that modify how the table appears (such as adding a title or modifying labels).

> args(arsenal:::summary.modelsum)
function (object, ..., labelTranslations = NULL, text = FALSE, 
    title = NULL, term.name = "") 
NULL


---

## The paired function


https://cran.r-project.org/web/packages/arsenal/vignettes/paired.html

The paired function
Ethan Heinzen, Beth Atkinson, Jason Sinnwell
09 November, 2018
Introduction
Simple Example
NAs
Available Function Options
Testing options
paired.control settings
summary.tableby settings
Introduction
Another one of the most common tables in medical literature includes summary statistics for a set of variables paired across two time points. Locally at Mayo, the SAS macro %paired was written to create summary tables with a single call. With the increasing interest in R, we have developed the function paired() to create similar tables within the R environment.

This vignette is light on purpose; paired() piggybacks off of tableby, so most documentation there applies here, too.

Simple Example
The first step when using the paired() function is to load the arsenal package. We can’t use mockstudy here because we need a dataset with paired observations, so we’ll create our own dataset.

library(arsenal)
dat <- data.frame(
  tp = paste0("Time Point ", c(1, 2, 1, 2, 1, 2, 1, 2, 1, 2)),
  id = c(1, 1, 2, 2, 3, 3, 4, 4, 5, 6),
  Cat = c("A", "A", "A", "B", "B", "B", "B", "A", NA, "B"),
  Fac = factor(c("A", "B", "C", "A", "B", "C", "A", "B", "C", "A")),
  Num = c(1, 2, 3, 4, 4, 3, 3, 4, 0, NA),
  Ord = ordered(c("I", "II", "II", "III", "III", "III", "I", "III", "II", "I")),
  Lgl = c(TRUE, TRUE, FALSE, TRUE, FALSE, TRUE, TRUE, FALSE, FALSE, FALSE),
  Dat = as.Date("2018-05-01") + c(1, 1, 2, 2, 3, 4, 5, 6, 3, 4),
  stringsAsFactors = FALSE
)
To create a simple table stratified by time point, use a formula= statement to specify the variables that you want summarized and the id= argument to specify the paired observations.

p <- paired(tp ~ Cat + Fac + Num + Ord + Lgl + Dat, data = dat, id = id, signed.rank.exact = FALSE)
summary(p)
Time Point 1 (N=4)	Time Point 2 (N=4)	Difference (N=4)	p value
Cat				1.000
   A	2 (50.0%)	2 (50.0%)	1 (50.0%)	
   B	2 (50.0%)	2 (50.0%)	1 (50.0%)	
Fac				0.261
   A	2 (50.0%)	1 (25.0%)	2 (100.0%)	
   B	1 (25.0%)	2 (50.0%)	1 (100.0%)	
   C	1 (25.0%)	1 (25.0%)	1 (100.0%)	
Num				0.391
   Mean (SD)	2.750 (1.258)	3.250 (0.957)	0.500 (1.000)	
   Range	1.000 - 4.000	2.000 - 4.000	-1.000 - 1.000	
Ord				0.174
   I	2 (50.0%)	0 (0.0%)	2 (100.0%)	
   II	1 (25.0%)	1 (25.0%)	1 (100.0%)	
   III	1 (25.0%)	3 (75.0%)	0 (0.0%)	
Lgl				1.000
   FALSE	2 (50.0%)	1 (25.0%)	2 (100.0%)	
   TRUE	2 (50.0%)	3 (75.0%)	1 (50.0%)	
Dat				0.182
   median	2018-05-03	2018-05-04	0.500	
   Range	2018-05-02 - 2018-05-06	2018-05-02 - 2018-05-07	0.000 - 1.000	
The third column shows the difference between time point 1 and time point 2. For categorical variables, it reports the percent of observations from time point 1 which changed in time point 2.

NAs
Note that by default, observations which do not have both timepoints are removed. This is easily changed using the na.action = na.paired("<arg>") argument. For example:

p <- paired(tp ~ Cat + Fac + Num + Ord + Lgl + Dat, data = dat, id = id,
            signed.rank.exact = FALSE, na.action = na.paired("fill"))
summary(p)
Time Point 1 (N=6)	Time Point 2 (N=6)	Difference (N=6)	p value
Cat				1.000
   N-Miss	2	1	2	
   A	2 (50.0%)	2 (40.0%)	1 (50.0%)	
   B	2 (50.0%)	3 (60.0%)	1 (50.0%)	
Fac				0.261
   N-Miss	1	1	2	
   A	2 (40.0%)	2 (40.0%)	2 (100.0%)	
   B	1 (20.0%)	2 (40.0%)	1 (100.0%)	
   C	2 (40.0%)	1 (20.0%)	1 (100.0%)	
Num				0.391
   N-Miss	1	2	2	
   Mean (SD)	2.200 (1.643)	3.250 (0.957)	0.500 (1.000)	
   Range	0.000 - 4.000	2.000 - 4.000	-1.000 - 1.000	
Ord				0.174
   N-Miss	1	1	2	
   I	2 (40.0%)	1 (20.0%)	2 (100.0%)	
   II	2 (40.0%)	1 (20.0%)	1 (100.0%)	
   III	1 (20.0%)	3 (60.0%)	0 (0.0%)	
Lgl				1.000
   N-Miss	1	1	2	
   FALSE	3 (60.0%)	2 (40.0%)	2 (100.0%)	
   TRUE	2 (40.0%)	3 (60.0%)	1 (50.0%)	
Dat				0.182
   N-Miss	1	1	2	
   median	2018-05-04	2018-05-05	0.500	
   Range	2018-05-02 - 2018-05-06	2018-05-02 - 2018-05-07	0.000 - 1.000	
For more details, see the help page for na.paired().

Available Function Options
Testing options
The tests used to calculate p-values differ by the variable type, but can be specified explicitly in the formula statement or in the control function.

The following tests are accepted:

paired.t: A paired t-test.

mcnemar: McNemar’s test.

signed.rank: the signed-rank test.

sign.test: the sign test.

notest: Don’t perform a test.

paired.control settings
A quick way to see what arguments are possible to utilize in a function is to use the args() command. Settings involving the number of digits can be set in paired.control or in summary.tableby.

args(paired.control)
## function (test = TRUE, diff = TRUE, test.pname = NULL, numeric.test = "paired.t", 
##     cat.test = "mcnemar", ordered.test = "signed.rank", date.test = "paired.t", 
##     numeric.stats = c("Nmiss", "meansd", "range"), cat.stats = c("Nmiss", 
##         "countpct"), ordered.stats = c("Nmiss", "countpct"), 
##     date.stats = c("Nmiss", "median", "range"), stats.labels = list(Nmiss = "N-Miss", 
##         Nmiss2 = "N-Miss", meansd = "Mean (SD)", medianq1q3 = "Median (Q1, Q3)", 
##         q1q3 = "Q1, Q3", range = "Range", countpct = "Count (Pct)"), 
##     digits = 3L, digits.count = 0L, digits.p = 3L, format.p = TRUE, 
##     conf.level = 0.95, mcnemar.correct = TRUE, signed.rank.exact = NULL, 
##     signed.rank.correct = TRUE, ...) 
## NULL
summary.tableby settings
Since the “paired” object inherits “tableby”, the summary.tableby function is what’s actually used to format and print the table.

args(arsenal:::summary.tableby)
## function (object, ..., labelTranslations = NULL, text = FALSE, 
##     title = NULL, pfootnote = FALSE, term.name = "") 
## NULL


---

## The tableby function


https://cran.r-project.org/web/packages/arsenal/vignettes/tableby.html

The tableby function
Beth Atkinson, Ethan Heinzen, Jason Sinnwell, Shannon McDonnell and Greg Dougherty
09 November, 2018
Introduction
Simple Example
Pretty text version of table
Pretty Rmarkdown version of table
Data frame version of table
Summaries using standard R code
Modifying Output
Add labels
Change summary statistics globally
Change summary statistics within the formula
Controlling Options for Categorical Tests (Chisq and Fisher’s)
Modifying the look & feel in Word documents
Additional Examples
1. Summarize without a group/by variable
2. Display footnotes indicating which “test” was used
3. Summarize an ordered factor
4. Summarize a survival variable
5. Summarize date variables
6. Summarize multiple variables without typing them out
7. Subset the dataset used in the analysis
8. Create combinations of variables on the fly
9. Transform variables on the fly
10. Subsetting (change the ordering of the variables, delete a variable, sort by p-value, filter by p-value)
11. Merge two tableby objects together
12. Add a title to the table
13. Modify how missing values are displayed
14. Modify the number of digits used
15. Create a user-defined summary statistic
16. Use case-weights for creating summary statistics
17. Create your own p-value and add it to the table
18. For two-level categorical variables or one-line numeric variables, simplify the output.
19. Use tableby within an Sweave document
20. Export tableby object to a .CSV file
21. Write tableby object to a separate Word or HTML file
22. Use tableby in R Shiny
23. Use tableby in bookdown
24. Adjust tableby for multiple p-values
Available Function Options
Summary statistics
Testing options
tableby.control settings
summary.tableby settings
Introduction
One of the most common tables in medical literature includes summary statistics for a set of variables, often stratified by some group (e.g. treatment arm). Locally at Mayo, the SAS macros %table and %summary were written to create summary tables with a single call. With the increasing interest in R, we have developed the function tableby to create similar tables within the R environment.

In developing the tableby() function, the goal was to bring the best features of these macros into an R function. However, the task was not simply to duplicate all the functionality, but rather to make use of R’s strengths (modeling, method dispersion, flexibility in function definition and output format) and make a tool that fits the needs of R users. Additionally, the results needed to fit within the general reproducible research framework so the tables could be displayed within an R markdown report.

This report provides step-by-step directions for using the functions associated with tableby(). All functions presented here are available within the arsenal package. An assumption is made that users are somewhat familiar with R Markdown documents. For those who are new to the topic, a good initial resource is available at rmarkdown.rstudio.com.

Simple Example
The first step when using the tableby function is to load the arsenal package. All the examples in this report use a dataset called mockstudy made available by Paul Novotny which includes a variety of types of variables (character, numeric, factor, ordered factor, survival) to use as examples.

require(arsenal)
require(knitr)
require(survival)
data(mockstudy) ##load data
dim(mockstudy)  ##look at how many subjects and variables are in the dataset 
## [1] 1499   14
# help(mockstudy) ##learn more about the dataset and variables
str(mockstudy) ##quick look at the data
## 'data.frame':    1499 obs. of  14 variables:
##  $ case       : int  110754 99706 105271 105001 112263 86205 99508 90158 88989 90515 ...
##  $ age        : atomic  67 74 50 71 69 56 50 57 51 63 ...
##   ..- attr(*, "label")= chr "Age in Years"
##  $ arm        : atomic  F: FOLFOX A: IFL A: IFL G: IROX ...
##   ..- attr(*, "label")= chr "Treatment Arm"
##  $ sex        : Factor w/ 2 levels "Male","Female": 1 2 2 2 2 1 1 1 2 1 ...
##  $ race       : atomic  Caucasian Caucasian Caucasian Caucasian ...
##   ..- attr(*, "label")= chr "Race"
##  $ fu.time    : int  922 270 175 128 233 120 369 421 387 363 ...
##  $ fu.stat    : int  2 2 2 2 2 2 2 2 2 2 ...
##  $ ps         : int  0 1 1 1 0 0 0 0 1 1 ...
##  $ hgb        : num  11.5 10.7 11.1 12.6 13 10.2 13.3 12.1 13.8 12.1 ...
##  $ bmi        : atomic  25.1 19.5 NA 29.4 26.4 ...
##   ..- attr(*, "label")= chr "Body Mass Index (kg/m^2)"
##  $ alk.phos   : int  160 290 700 771 350 569 162 152 231 492 ...
##  $ ast        : int  35 52 100 68 35 27 16 12 25 18 ...
##  $ mdquality.s: int  NA 1 1 1 NA 1 1 1 1 1 ...
##  $ age.ord    : Ord.factor w/ 8 levels "10-19"<"20-29"<..: 6 7 4 7 6 5 4 5 5 6 ...
To create a simple table stratified by treament arm, use a formula statement to specify the variables that you want summarized. The example below uses age (a continuous variable) and sex (a factor).

tab1 <- tableby(arm ~ sex + age, data=mockstudy)
If you want to take a quick look at the table, you can use summary() on your tableby object and the table will print out as text in your R console window. If you use summary() without any options you will see a number of &nbsp; statements which translates to “space” in HTML.

Pretty text version of table
If you want a nicer version in your console window then add the text=TRUE option.

summary(tab1, text=TRUE)
## 
## 
## |             | A: IFL (N=428)  | F: FOLFOX (N=691) | G: IROX (N=380) | Total (N=1499)  | p value|
## |:------------|:---------------:|:-----------------:|:---------------:|:---------------:|-------:|
## |sex          |                 |                   |                 |                 |   0.190|
## |-  Male      |   277 (64.7%)   |    411 (59.5%)    |   228 (60.0%)   |   916 (61.1%)   |        |
## |-  Female    |   151 (35.3%)   |    280 (40.5%)    |   152 (40.0%)   |   583 (38.9%)   |        |
## |Age in Years |                 |                   |                 |                 |   0.614|
## |-  Mean (SD) | 59.673 (11.365) |  60.301 (11.632)  | 59.763 (11.499) | 59.985 (11.519) |        |
## |-  Range     | 27.000 - 88.000 |  19.000 - 88.000  | 26.000 - 85.000 | 19.000 - 88.000 |        |
Pretty Rmarkdown version of table
In order for the report to look nice within an R markdown (knitr) report, you just need to specify results="asis" when creating the r chunk. This changes the layout slightly (compresses it) and bolds the variable names.

summary(tab1)
A: IFL (N=428)	F: FOLFOX (N=691)	G: IROX (N=380)	Total (N=1499)	p value
sex					0.190
   Male	277 (64.7%)	411 (59.5%)	228 (60.0%)	916 (61.1%)	
   Female	151 (35.3%)	280 (40.5%)	152 (40.0%)	583 (38.9%)	
Age in Years					0.614
   Mean (SD)	59.673 (11.365)	60.301 (11.632)	59.763 (11.499)	59.985 (11.519)	
   Range	27.000 - 88.000	19.000 - 88.000	26.000 - 85.000	19.000 - 88.000	
Data frame version of table
If you want a data.frame version, simply use as.data.frame.

as.data.frame(tab1)
##   variable     term        label variable.type              A: IFL           F: FOLFOX
## 1      sex      sex          sex   categorical                                        
## 2      sex countpct         Male   categorical 277.00000, 64.71963 411.00000, 59.47902
## 3      sex countpct       Female   categorical 151.00000, 35.28037 280.00000, 40.52098
## 4      age      age Age in Years       numeric                                        
## 5      age   meansd    Mean (SD)       numeric  59.67290, 11.36454  60.30101, 11.63225
## 6      age    range        Range       numeric              27, 88              19, 88
##              G: IROX              Total                       test   p.value
## 1                                       Pearson's Chi-squared test 0.1904388
## 2            228, 60  916.0000, 61.1074 Pearson's Chi-squared test 0.1904388
## 3            152, 40  583.0000, 38.8926 Pearson's Chi-squared test 0.1904388
## 4                                               Linear Model ANOVA 0.6143859
## 5 59.76316, 11.49930 59.98532, 11.51877         Linear Model ANOVA 0.6143859
## 6             26, 85             19, 88         Linear Model ANOVA 0.6143859
Summaries using standard R code
## base R frequency example
tmp <- table(Gender=mockstudy$sex, "Study Arm"=mockstudy$arm)
tmp
##         Study Arm
## Gender   A: IFL F: FOLFOX G: IROX
##   Male      277       411     228
##   Female    151       280     152
# Note: The continuity correction is applied by default in R (not used in %table)
chisq.test(tmp)
## 
##  Pearson's Chi-squared test
## 
## data:  tmp
## X-squared = 3.3168, df = 2, p-value = 0.1904
## base R numeric summary example
tapply(mockstudy$age, mockstudy$arm, summary)
## $`A: IFL`
##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##   27.00   53.00   61.00   59.67   68.00   88.00 
## 
## $`F: FOLFOX`
##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##    19.0    52.0    61.0    60.3    69.0    88.0 
## 
## $`G: IROX`
##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##   26.00   52.00   61.00   59.76   68.00   85.00
summary(aov(age ~ arm, data=mockstudy))
##               Df Sum Sq Mean Sq F value Pr(>F)
## arm            2    129    64.7   0.487  0.614
## Residuals   1496 198628   132.8
Modifying Output
Add labels
In the above example, age is shown with a label (Age in Years), but sex is listed “as is” with lower case letters. This is because the data was created in SAS and in the SAS dataset, age had a label but sex did not. The label is stored as an attribute within R.

## Look at one variable's label
attr(mockstudy$age,'label')
## [1] "Age in Years"
## See all the variables with a label
unlist(lapply(mockstudy,'attr','label'))
##                        age                        arm                       race 
##             "Age in Years"            "Treatment Arm"                     "Race" 
##                        bmi 
## "Body Mass Index (kg/m^2)"
# Can also use labels(mockstudy)
If you want to add labels to other variables, there are a couple of options. First, you could add labels to the variables in your dataset.

attr(mockstudy$sex,'label')  <- 'Gender'

tab1 <- tableby(arm ~ sex + age, data=mockstudy)
summary(tab1)
A: IFL (N=428)	F: FOLFOX (N=691)	G: IROX (N=380)	Total (N=1499)	p value
Gender					0.190
   Male	277 (64.7%)	411 (59.5%)	228 (60.0%)	916 (61.1%)	
   Female	151 (35.3%)	280 (40.5%)	152 (40.0%)	583 (38.9%)	
Age in Years					0.614
   Mean (SD)	59.673 (11.365)	60.301 (11.632)	59.763 (11.499)	59.985 (11.519)	
   Range	27.000 - 88.000	19.000 - 88.000	26.000 - 85.000	19.000 - 88.000	
You can also use the built-in data.frame method for labels<-:

labels(mockstudy)  <- c(age = 'Age, yrs', sex = "Gender")

tab1 <- tableby(arm ~ sex + age, data=mockstudy)
summary(tab1)
A: IFL (N=428)	F: FOLFOX (N=691)	G: IROX (N=380)	Total (N=1499)	p value
Gender					0.190
   Male	277 (64.7%)	411 (59.5%)	228 (60.0%)	916 (61.1%)	
   Female	151 (35.3%)	280 (40.5%)	152 (40.0%)	583 (38.9%)	
Age, yrs					0.614
   Mean (SD)	59.673 (11.365)	60.301 (11.632)	59.763 (11.499)	59.985 (11.519)	
   Range	27.000 - 88.000	19.000 - 88.000	26.000 - 85.000	19.000 - 88.000	
Another option is to add labels after you have created the table

mylabels <- list(sex = "SEX", age = "Age, yrs")
summary(tab1, labelTranslations = mylabels)
A: IFL (N=428)	F: FOLFOX (N=691)	G: IROX (N=380)	Total (N=1499)	p value
SEX					0.190
   Male	277 (64.7%)	411 (59.5%)	228 (60.0%)	916 (61.1%)	
   Female	151 (35.3%)	280 (40.5%)	152 (40.0%)	583 (38.9%)	
Age, yrs					0.614
   Mean (SD)	59.673 (11.365)	60.301 (11.632)	59.763 (11.499)	59.985 (11.519)	
   Range	27.000 - 88.000	19.000 - 88.000	26.000 - 85.000	19.000 - 88.000	
Alternatively, you can check the variable labels and manipulate them with a function called labels, which works on the tableby object.

labels(tab1)
##        arm        sex        age 
##      "arm"   "Gender" "Age, yrs"
labels(tab1) <- c(arm="Treatment Assignment", age="Baseline Age (yrs)")
labels(tab1)
##                    arm                    sex                    age 
## "Treatment Assignment"               "Gender"   "Baseline Age (yrs)"
summary(tab1)
A: IFL (N=428)	F: FOLFOX (N=691)	G: IROX (N=380)	Total (N=1499)	p value
Gender					0.190
   Male	277 (64.7%)	411 (59.5%)	228 (60.0%)	916 (61.1%)	
   Female	151 (35.3%)	280 (40.5%)	152 (40.0%)	583 (38.9%)	
Baseline Age (yrs)					0.614
   Mean (SD)	59.673 (11.365)	60.301 (11.632)	59.763 (11.499)	59.985 (11.519)	
   Range	27.000 - 88.000	19.000 - 88.000	26.000 - 85.000	19.000 - 88.000	
Change summary statistics globally
Currently the default behavior is to summarize continuous variables with: Number of missing values, Mean (SD), 25th - 75th quantiles, and Minimum-Maximum values with an ANOVA (t-test with equal variances) p-value. For categorical variables the default is to show: Number of missing values and count (column percent) with a chi-square p-value. This behavior can be modified using the tableby.control function. In fact, you can save your standard settings and use that for future tables. Note that test=FALSE and total=FALSE results in the total column and p-value column not being printed.

mycontrols  <- tableby.control(test=FALSE, total=FALSE,
                               numeric.test="kwt", cat.test="chisq",
                               numeric.stats=c("N", "median", "q1q3"),
                               cat.stats=c("countpct"),
                               stats.labels=list(N='Count', median='Median', q1q3='Q1,Q3'))
tab2 <- tableby(arm ~ sex + age, data=mockstudy, control=mycontrols)
summary(tab2)
A: IFL (N=428)	F: FOLFOX (N=691)	G: IROX (N=380)
Gender			
   Male	277 (64.7%)	411 (59.5%)	228 (60.0%)
   Female	151 (35.3%)	280 (40.5%)	152 (40.0%)
Age, yrs			
   Count	428	691	380
   Median	61.000	61.000	61.000
   Q1,Q3	53.000, 68.000	52.000, 69.000	52.000, 68.000
You can also change these settings directly in the tableby call.

tab3 <- tableby(arm ~ sex + age, data=mockstudy, test=FALSE, total=FALSE, 
                numeric.stats=c("median","q1q3"), numeric.test="kwt")
summary(tab3)
A: IFL (N=428)	F: FOLFOX (N=691)	G: IROX (N=380)
Gender			
   Male	277 (64.7%)	411 (59.5%)	228 (60.0%)
   Female	151 (35.3%)	280 (40.5%)	152 (40.0%)
Age, yrs			
   Median	61.000	61.000	61.000
   Q1, Q3	53.000, 68.000	52.000, 69.000	52.000, 68.000
Change summary statistics within the formula
In addition to modifying summary options globally, it is possible to modify the test and summary statistics for specific variables within the formula statement. For example, both the kwt (Kruskal-Wallis rank-based) and anova (asymptotic analysis of variance) tests apply to numeric variables, and we can use one for the variable “age”, another for the variable “bmi”, and no test for the variable “ast”. A list of all the options is shown at the end of the vignette.

The tests function can do a quick check on what tests were performed on each variable in tableby.

tab.test <- tableby(arm ~ kwt(age) + anova(bmi) + notest(ast), data=mockstudy)
tests(tab.test)
##                     Variable   p.value                       Method
## age                 Age, yrs 0.6390614 Kruskal-Wallis rank sum test
## bmi Body Mass Index (kg/m^2) 0.8916552           Linear Model ANOVA
## ast                      ast        NA                      No test
summary(tab.test)
A: IFL (N=428)	F: FOLFOX (N=691)	G: IROX (N=380)	Total (N=1499)	p value
Age, yrs					0.639
   Mean (SD)	59.673 (11.365)	60.301 (11.632)	59.763 (11.499)	59.985 (11.519)	
   Range	27.000 - 88.000	19.000 - 88.000	26.000 - 85.000	19.000 - 88.000	
Body Mass Index (kg/m^2)					0.892
   N-Miss	9	20	4	33	
   Mean (SD)	27.290 (5.552)	27.210 (5.173)	27.106 (5.751)	27.206 (5.432)	
   Range	14.053 - 53.008	16.649 - 49.130	15.430 - 60.243	14.053 - 60.243	
ast					
   N-Miss	69	141	56	266	
   Mean (SD)	37.292 (28.036)	35.202 (26.659)	35.670 (25.807)	35.933 (26.843)	
   Range	10.000 - 205.000	7.000 - 174.000	5.000 - 176.000	5.000 - 205.000	
Summary statistics for any individual variable can also be modified, but it must be done as secondary arguments to the test function. The function names must be strings that are functions already written for tableby, built-in R functions like mean and range, or user-defined functions.

tab.test <- tableby(arm ~ kwt(ast, "Nmiss2","median") + anova(age, "N","mean") +
                    notest(bmi, "Nmiss","median"), data=mockstudy)
summary(tab.test)
A: IFL (N=428)	F: FOLFOX (N=691)	G: IROX (N=380)	Total (N=1499)	p value
ast					0.039
   N-Miss	69	141	56	266	
   Median	29.000	25.500	27.000	27.000	
Age, yrs					0.614
   N	428	691	380	1499	
   mean	59.7	60.3	59.8	60	
Body Mass Index (kg/m^2)					
   N-Miss	9	20	4	33	
   Median	26.234	26.525	25.978	26.325	
Controlling Options for Categorical Tests (Chisq and Fisher’s)
The formal tests for categorical variables against the levels of the by variable, chisq and fe, have options to simulate p-values. We show how to turn on the simulations for these with 500 replicates for the Fisher’s test (fe).

set.seed(100)
tab.catsim <- tableby(arm ~ sex + race, cat.test="fe", simulate.p.value=TRUE, B=500, data=mockstudy)
tests(tab.catsim)
 Variable   p.value
sex Gender 0.2195609 race Race 0.3093812 Method sex Fisher’s Exact Test for Count Data with simulated p-value(based on 500 replicates) race Fisher’s Exact Test for Count Data with simulated p-value(based on 500 replicates)

The chis-square test on 2x2 tables applies Yates’ continuity correction by default, so we provide an option to turn off the correction. We show the results with and without the correction that is applied to treatment arm by sex, if we use subset to ignore one of the three treatment arms.

cat.correct <- tableby(arm ~ sex + race, cat.test="chisq", subset = !grepl("^F", arm), data=mockstudy)
tests(cat.correct)
 Variable   p.value                     Method
sex Gender 0.1666280 Pearson’s Chi-squared test race Race 0.8108543 Pearson’s Chi-squared test

cat.nocorrect <- tableby(arm ~ sex + race, cat.test="chisq", subset = !grepl("^F", arm),
     chisq.correct=FALSE, data=mockstudy)
tests(cat.nocorrect)
 Variable   p.value                     Method
sex Gender 0.1666280 Pearson’s Chi-squared test race Race 0.8108543 Pearson’s Chi-squared test

Modifying the look & feel in Word documents
You can easily create Word versions of tableby output via an Rmarkdown report and the default options will give you a reasonable table in Word - just select the “Knit Word” option in RStudio.

The functionality listed in this next paragraph is coming soon but needs an upgraded version of RStudio If you want to modify fonts used for the table, then you’ll need to add an extra line to your header at the beginning of your file. You can take the WordStylesReference01.docx file and modify the fonts (storing the format preferences in your project directory). To see how this works, run your report once using WordStylesReference01.docx and then WordStylesReference02.docx.

output: word_document
  reference_docx: /projects/bsi/gentools/R/lib320/arsenal/doc/WordStylesReference01.docx 
For more informating on changing the look/feel of your Word document, see the Rmarkdown documentation website.

Additional Examples
Here are multiple examples showing how to use some of the different options.

1. Summarize without a group/by variable
tab.noby <- tableby(~ bmi + sex + age, data=mockstudy)
summary(tab.noby)
Overall (N=1499)
Body Mass Index (kg/m^2)	
   N-Miss	33
   Mean (SD)	27.206 (5.432)
   Range	14.053 - 60.243
Gender	
   Male	916 (61.1%)
   Female	583 (38.9%)
Age, yrs	
   Mean (SD)	59.985 (11.519)
   Range	19.000 - 88.000
2. Display footnotes indicating which “test” was used
summary(tab.test) #, pfootnote=TRUE)
A: IFL (N=428)	F: FOLFOX (N=691)	G: IROX (N=380)	Total (N=1499)	p value
ast					0.039
   N-Miss	69	141	56	266	
   Median	29.000	25.500	27.000	27.000	
Age, yrs					0.614
   N	428	691	380	1499	
   mean	59.7	60.3	59.8	60	
Body Mass Index (kg/m^2)					
   N-Miss	9	20	4	33	
   Median	26.234	26.525	25.978	26.325	
3. Summarize an ordered factor
When comparing groups of ordered data there are a couple of options. The default uses a general independence test available from the coin package. For two-group comparisons, this is essentially the Armitage trend test. The other option is to specify the Kruskal Wallis test. The example below shows both options.

mockstudy$age.ordnew <- ordered(c("a",NA,as.character(mockstudy$age.ord[-(1:2)])))
table(mockstudy$age.ord, mockstudy$sex)
##        
##         Male Female
##   10-19    1      0
##   20-29    8     11
##   30-39   37     30
##   40-49  127     83
##   50-59  257    179
##   60-69  298    170
##   70-79  168    101
##   80-89   20      9
table(mockstudy$age.ordnew, mockstudy$sex)
##        
##         Male Female
##   10-19    1      0
##   20-29    8     11
##   30-39   37     30
##   40-49  127     83
##   50-59  257    179
##   60-69  297    170
##   70-79  168    100
##   80-89   20      9
##   a        1      0
class(mockstudy$age.ord)
## [1] "ordered" "factor"
summary(tableby(sex ~ age.ordnew, data = mockstudy)) #, pfootnote = TRUE)
Male (N=916)	Female (N=583)	Total (N=1499)	p value
age.ordnew				0.040
   N-Miss	0	1	1	
   10-19	1 (0.1%)	0 (0.0%)	1 (0.1%)	
   20-29	8 (0.9%)	11 (1.9%)	19 (1.3%)	
   30-39	37 (4.0%)	30 (5.2%)	67 (4.5%)	
   40-49	127 (13.9%)	83 (14.3%)	210 (14.0%)	
   50-59	257 (28.1%)	179 (30.8%)	436 (29.1%)	
   60-69	297 (32.4%)	170 (29.2%)	467 (31.2%)	
   70-79	168 (18.3%)	100 (17.2%)	268 (17.9%)	
   80-89	20 (2.2%)	9 (1.5%)	29 (1.9%)	
   a	1 (0.1%)	0 (0.0%)	1 (0.1%)	
summary(tableby(sex ~ kwt(age.ord), data = mockstudy)) #) #, pfootnote = TRUE)
Male (N=916)	Female (N=583)	Total (N=1499)	p value
age.ord				0.067
   10-19	1 (0.1%)	0 (0.0%)	1 (0.1%)	
   20-29	8 (0.9%)	11 (1.9%)	19 (1.3%)	
   30-39	37 (4.0%)	30 (5.1%)	67 (4.5%)	
   40-49	127 (13.9%)	83 (14.2%)	210 (14.0%)	
   50-59	257 (28.1%)	179 (30.7%)	436 (29.1%)	
   60-69	298 (32.5%)	170 (29.2%)	468 (31.2%)	
   70-79	168 (18.3%)	101 (17.3%)	269 (17.9%)	
   80-89	20 (2.2%)	9 (1.5%)	29 (1.9%)	
4. Summarize a survival variable
First look at the information that is presented by the survfit() function, then see how the same results can be seen with tableby. The default is to show the median survival (time at which the probability of survival = 50%).

survfit(Surv(fu.time, fu.stat)~sex, data=mockstudy)
## Call: survfit(formula = Surv(fu.time, fu.stat) ~ sex, data = mockstudy)
## 
##              n events median 0.95LCL 0.95UCL
## sex=Male   916    829    550     515     590
## sex=Female 583    527    543     511     575
survdiff(Surv(fu.time, fu.stat)~sex, data=mockstudy)
## Call:
## survdiff(formula = Surv(fu.time, fu.stat) ~ sex, data = mockstudy)
## 
##              N Observed Expected (O-E)^2/E (O-E)^2/V
## sex=Male   916      829      830  0.000370  0.000956
## sex=Female 583      527      526  0.000583  0.000956
## 
##  Chisq= 0  on 1 degrees of freedom, p= 1
summary(tableby(sex ~ Surv(fu.time, fu.stat), data=mockstudy))
Male (N=916)	Female (N=583)	Total (N=1499)	p value
Surv(fu.time, fu.stat)				0.975
   Events	829	527	1356	
   Median Survival	550.000	543.000	546.000	
It is also possible to obtain summaries of the % survival at certain time points (say the probability of surviving 1-year).

summary(survfit(Surv(fu.time/365.25, fu.stat)~sex, data=mockstudy), times=1:5)
## Call: survfit(formula = Surv(fu.time/365.25, fu.stat) ~ sex, data = mockstudy)
## 
##                 sex=Male 
##  time n.risk n.event survival std.err lower 95% CI upper 95% CI
##     1    626     286   0.6870  0.0153       0.6576       0.7177
##     2    309     311   0.3437  0.0158       0.3142       0.3761
##     3    152     151   0.1748  0.0127       0.1516       0.2015
##     4     57      61   0.0941  0.0104       0.0759       0.1168
##     5     24      16   0.0628  0.0095       0.0467       0.0844
## 
##                 sex=Female 
##  time n.risk n.event survival std.err lower 95% CI upper 95% CI
##     1    380     202   0.6531  0.0197       0.6155        0.693
##     2    190     189   0.3277  0.0195       0.2917        0.368
##     3     95      90   0.1701  0.0157       0.1420        0.204
##     4     51      32   0.1093  0.0133       0.0861        0.139
##     5     18      12   0.0745  0.0126       0.0534        0.104
summary(tableby(sex ~ Surv(fu.time/365.25, fu.stat), data=mockstudy, times=1:5, surv.stats=c("NeventsSurv","NriskSurv")))
Male (N=916)	Female (N=583)	Total (N=1499)	p value
Surv(fu.time/365.25, fu.stat)				0.975
   time = 1	286 (68.7)	202 (65.3)	488 (67.4)	
   time = 2	597 (34.4)	391 (32.8)	988 (33.7)	
   time = 3	748 (17.5)	481 (17.0)	1229 (17.3)	
   time = 4	809 (9.4)	513 (10.9)	1322 (10.1)	
   time = 5	825 (6.3)	525 (7.4)	1350 (6.8)	
   time = 1	626	380	1006	
   time = 2	309	190	499	
   time = 3	152	95	247	
   time = 4	57	51	108	
   time = 5	24	18	42	
5. Summarize date variables
Date variables by default are summarized with the number of missing values, the median, and the range. For example purposes we’ve created a random date. Missing values are introduced for impossible February dates.

set.seed(100)
N <- nrow(mockstudy)
mockstudy$dtentry <- mdy.Date(month=sample(1:12,N,replace=T), day=sample(1:29,N,replace=T), 
                              year=sample(2005:2009,N,replace=T))
summary(tableby(sex ~ dtentry, data=mockstudy))
Male (N=916)	Female (N=583)	Total (N=1499)	p value
dtentry				0.554
   N-Miss	3	2	5	
   Median	2007-06-16	2007-06-15	2007-06-15	
   Range	2005-01-03 - 2009-12-27	2005-01-01 - 2009-12-28	2005-01-01 - 2009-12-28	
6. Summarize multiple variables without typing them out
Often one wants to summarize a number of variables. Instead of typing by hand each individual variable, an alternative approach is to create a formula using the paste command with the collapse="+" option.

## create a vector specifying the variable names
myvars <- names(mockstudy)

## select the 8th through the last variables
## paste them together, separated by the + sign
RHS <- paste(myvars[8:10], collapse="+")
RHS
[1] “ps+hgb+bmi”

## create a formula using the as.formula function
as.formula(paste('arm ~ ', RHS))
arm ~ ps + hgb + bmi

## use the formula in the tableby function
summary(tableby(as.formula(paste('arm ~', RHS)), data=mockstudy))
A: IFL (N=428)	F: FOLFOX (N=691)	G: IROX (N=380)	Total (N=1499)	p value
ps					0.903
   N-Miss	69	141	56	266	
   Mean (SD)	0.529 (0.597)	0.547 (0.595)	0.537 (0.606)	0.539 (0.598)	
   Range	0.000 - 2.000	0.000 - 2.000	0.000 - 2.000	0.000 - 2.000	
hgb					0.639
   N-Miss	69	141	56	266	
   Mean (SD)	12.276 (1.686)	12.381 (1.763)	12.373 (1.680)	12.348 (1.719)	
   Range	9.060 - 17.300	9.000 - 18.200	9.000 - 17.000	9.000 - 18.200	
Body Mass Index (kg/m^2)					0.892
   N-Miss	9	20	4	33	
   Mean (SD)	27.290 (5.552)	27.210 (5.173)	27.106 (5.751)	27.206 (5.432)	
   Range	14.053 - 53.008	16.649 - 49.130	15.430 - 60.243	14.053 - 60.243	
These steps can also be done using the formulize function.

## The formulize function does the paste and as.formula steps
tmp <- formulize('arm',myvars[8:10])
tmp
arm ~ ps + hgb + bmi

## More complex formulas could also be written using formulize
tmp2 <- formulize('arm',c('ps','hgb^2','bmi'))

## use the formula in the tableby function
summary(tableby(tmp, data=mockstudy))
A: IFL (N=428)	F: FOLFOX (N=691)	G: IROX (N=380)	Total (N=1499)	p value
ps					0.903
   N-Miss	69	141	56	266	
   Mean (SD)	0.529 (0.597)	0.547 (0.595)	0.537 (0.606)	0.539 (0.598)	
   Range	0.000 - 2.000	0.000 - 2.000	0.000 - 2.000	0.000 - 2.000	
hgb					0.639
   N-Miss	69	141	56	266	
   Mean (SD)	12.276 (1.686)	12.381 (1.763)	12.373 (1.680)	12.348 (1.719)	
   Range	9.060 - 17.300	9.000 - 18.200	9.000 - 17.000	9.000 - 18.200	
Body Mass Index (kg/m^2)					0.892
   N-Miss	9	20	4	33	
   Mean (SD)	27.290 (5.552)	27.210 (5.173)	27.106 (5.751)	27.206 (5.432)	
   Range	14.053 - 53.008	16.649 - 49.130	15.430 - 60.243	14.053 - 60.243	
7. Subset the dataset used in the analysis
Here are two ways to get the same result (limit the analysis to subjects age>5 and in the F: FOLFOX treatment group).

The first approach uses the subset function applied to the dataset mockstudy. This example also selects a subset of variables. The tableby function is then applied to this subsetted data.
newdata <- subset(mockstudy, subset=age>50 & arm=='F: FOLFOX', select = c(sex,ps:bmi))
dim(mockstudy)
## [1] 1499   16
table(mockstudy$arm)
## 
##    A: IFL F: FOLFOX   G: IROX 
##       428       691       380
dim(newdata)
## [1] 557   4
names(newdata)
## [1] "sex" "ps"  "hgb" "bmi"
summary(tableby(sex ~ ., data=newdata))
Male (N=333)	Female (N=224)	Total (N=557)	p value
ps				0.652
   N-Miss	64	44	108	
   Mean (SD)	0.554 (0.600)	0.528 (0.602)	0.543 (0.600)	
   Range	0.000 - 2.000	0.000 - 2.000	0.000 - 2.000	
hgb				< 0.001
   N-Miss	64	44	108	
   Mean (SD)	12.720 (1.925)	12.063 (1.395)	12.457 (1.760)	
   Range	9.000 - 18.200	9.100 - 15.900	9.000 - 18.200	
bmi				0.650
   N-Miss	9	6	15	
   Mean (SD)	27.539 (4.780)	27.337 (5.508)	27.458 (5.081)	
   Range	17.927 - 47.458	16.649 - 49.130	16.649 - 49.130	
The second approach does the same analysis but uses the subset argument within tableby to subset the data.
summary(tableby(sex ~ ps + hgb + bmi, subset=age>50 & arm=="F: FOLFOX", data=mockstudy))
Male (N=333)	Female (N=224)	Total (N=557)	p value
ps				0.652
   N-Miss	64	44	108	
   Mean (SD)	0.554 (0.600)	0.528 (0.602)	0.543 (0.600)	
   Range	0.000 - 2.000	0.000 - 2.000	0.000 - 2.000	
hgb				< 0.001
   N-Miss	64	44	108	
   Mean (SD)	12.720 (1.925)	12.063 (1.395)	12.457 (1.760)	
   Range	9.000 - 18.200	9.100 - 15.900	9.000 - 18.200	
Body Mass Index (kg/m^2)				0.650
   N-Miss	9	6	15	
   Mean (SD)	27.539 (4.780)	27.337 (5.508)	27.458 (5.081)	
   Range	17.927 - 47.458	16.649 - 49.130	16.649 - 49.130	
8. Create combinations of variables on the fly
## create a variable combining the levels of mdquality.s and sex
with(mockstudy, table(interaction(mdquality.s,sex)))
## 
##   0.Male   1.Male 0.Female 1.Female 
##       77      686       47      437
summary(tableby(arm ~ interaction(mdquality.s,sex), data=mockstudy))
A: IFL (N=428)	F: FOLFOX (N=691)	G: IROX (N=380)	Total (N=1499)	p value
interaction(mdquality.s, sex)					0.493
   N-Miss	55	156	41	252	
   0.Male	29 (7.8%)	31 (5.8%)	17 (5.0%)	77 (6.2%)	
   1.Male	214 (57.4%)	285 (53.3%)	187 (55.2%)	686 (55.0%)	
   0.Female	12 (3.2%)	21 (3.9%)	14 (4.1%)	47 (3.8%)	
   1.Female	118 (31.6%)	198 (37.0%)	121 (35.7%)	437 (35.0%)	
## create a new grouping variable with combined levels of arm and sex
summary(tableby(interaction(mdquality.s, sex) ~  age + bmi, data=mockstudy, subset=arm=="F: FOLFOX"))
0.Male (N=31)	1.Male (N=285)	0.Female (N=21)	1.Female (N=198)	Total (N=535)	p value
Age, yrs						0.190
   Mean (SD)	63.065 (11.702)	60.653 (11.833)	60.810 (10.103)	58.924 (11.366)	60.159 (11.612)	
   Range	41.000 - 82.000	19.000 - 88.000	42.000 - 81.000	29.000 - 83.000	19.000 - 88.000	
Body Mass Index (kg/m^2)						0.894
   N-Miss	0	6	1	5	12	
   Mean (SD)	26.633 (5.094)	27.387 (4.704)	27.359 (4.899)	27.294 (5.671)	27.307 (5.100)	
   Range	20.177 - 41.766	17.927 - 47.458	19.801 - 39.369	16.799 - 44.841	16.799 - 47.458	
9. Transform variables on the fly
Certain transformations need to be surrounded by I() so that R knows to treat it as a variable transformation and not some special model feature. If the transformation includes any of the symbols / - + ^ * then surround the new variable by I().

trans <- tableby(arm ~ I(age/10) + log(bmi) + factor(mdquality.s, levels=0:1, labels=c('N','Y')),
                 data=mockstudy)
summary(trans)
A: IFL (N=428)	F: FOLFOX (N=691)	G: IROX (N=380)	Total (N=1499)	p value
Age, yrs					0.614
   Mean (SD)	5.967 (1.136)	6.030 (1.163)	5.976 (1.150)	5.999 (1.152)	
   Range	2.700 - 8.800	1.900 - 8.800	2.600 - 8.500	1.900 - 8.800	
Body Mass Index (kg/m^2)					0.811
   N-Miss	9	20	4	33	
   Mean (SD)	3.287 (0.197)	3.286 (0.183)	3.279 (0.200)	3.285 (0.192)	
   Range	2.643 - 3.970	2.812 - 3.894	2.736 - 4.098	2.643 - 4.098	
factor(mdquality.s, levels = 0:1, labels = c(“N”, “Y”))					0.694
   N-Miss	55	156	41	252	
   N	41 (11.0%)	52 (9.7%)	31 (9.1%)	124 (9.9%)	
   Y	332 (89.0%)	483 (90.3%)	308 (90.9%)	1123 (90.1%)	
The labels for these variables isn’t exactly what we’d like so we can change modify those after the fact. Instead of typing out the very long variable names you can modify specific labels by position.

labels(trans)
##                                                           arm 
##                                                         "arm" 
##                                                     I(age/10) 
##                                                    "Age, yrs" 
##                                                      log(bmi) 
##                                    "Body Mass Index (kg/m^2)" 
##       factor(mdquality.s, levels = 0:1, labels = c("N", "Y")) 
## "factor(mdquality.s, levels = 0:1, labels = c(\"N\", \"Y\"))"
labels(trans)[2:4] <- c('Age per 10 yrs', 'log(BMI)', 'MD Quality')
labels(trans)
##                                                     arm 
##                                                   "arm" 
##                                               I(age/10) 
##                                        "Age per 10 yrs" 
##                                                log(bmi) 
##                                              "log(BMI)" 
## factor(mdquality.s, levels = 0:1, labels = c("N", "Y")) 
##                                            "MD Quality"
summary(trans)
A: IFL (N=428)	F: FOLFOX (N=691)	G: IROX (N=380)	Total (N=1499)	p value
Age per 10 yrs					0.614
   Mean (SD)	5.967 (1.136)	6.030 (1.163)	5.976 (1.150)	5.999 (1.152)	
   Range	2.700 - 8.800	1.900 - 8.800	2.600 - 8.500	1.900 - 8.800	
log(BMI)					0.811
   N-Miss	9	20	4	33	
   Mean (SD)	3.287 (0.197)	3.286 (0.183)	3.279 (0.200)	3.285 (0.192)	
   Range	2.643 - 3.970	2.812 - 3.894	2.736 - 4.098	2.643 - 4.098	
MD Quality					0.694
   N-Miss	55	156	41	252	
   N	41 (11.0%)	52 (9.7%)	31 (9.1%)	124 (9.9%)	
   Y	332 (89.0%)	483 (90.3%)	308 (90.9%)	1123 (90.1%)	
Note that if we had not changed mdquality.s to a factor, it would have been summarized as though it were a continuous variable.

class(mockstudy$mdquality.s)
[1] “integer”

summary(tableby(arm~mdquality.s, data=mockstudy))
A: IFL (N=428)	F: FOLFOX (N=691)	G: IROX (N=380)	Total (N=1499)	p value
mdquality.s					0.695
   N-Miss	55	156	41	252	
   Mean (SD)	0.890 (0.313)	0.903 (0.297)	0.909 (0.289)	0.901 (0.299)	
   Range	0.000 - 1.000	0.000 - 1.000	0.000 - 1.000	0.000 - 1.000	
Another option would be to specify the test and summary statistics. In fact, if I had a set of variables coded 0/1 and that was all I was summarizing, then I could change the global option for continuous variables to use the chi-square test and show countpct.

summary(tableby(arm ~ chisq(mdquality.s, "Nmiss","countpct"), data=mockstudy))
A: IFL (N=428)	F: FOLFOX (N=691)	G: IROX (N=380)	Total (N=1499)	p value
mdquality.s					0.694
   N-Miss	55	156	41	252	
   0	41 (11.0%)	52 (9.7%)	31 (9.1%)	124 (9.9%)	
   1	332 (89.0%)	483 (90.3%)	308 (90.9%)	1123 (90.1%)	
10. Subsetting (change the ordering of the variables, delete a variable, sort by p-value, filter by p-value)
mytab <- tableby(arm ~ sex + alk.phos + age, data=mockstudy)
mytab2 <- mytab[c('age','sex','alk.phos')]
summary(mytab2)
A: IFL (N=428)	F: FOLFOX (N=691)	G: IROX (N=380)	Total (N=1499)	p value
Age, yrs					0.614
   Mean (SD)	59.673 (11.365)	60.301 (11.632)	59.763 (11.499)	59.985 (11.519)	
   Range	27.000 - 88.000	19.000 - 88.000	26.000 - 85.000	19.000 - 88.000	
Gender					0.190
   Male	277 (64.7%)	411 (59.5%)	228 (60.0%)	916 (61.1%)	
   Female	151 (35.3%)	280 (40.5%)	152 (40.0%)	583 (38.9%)	
alk.phos					0.226
   N-Miss	69	141	56	266	
   Mean (SD)	175.577 (128.608)	161.984 (121.978)	173.506 (138.564)	168.969 (128.492)	
   Range	11.000 - 858.000	10.000 - 1014.000	7.000 - 982.000	7.000 - 1014.000	
summary(mytab[c('age','sex')], digits = 2)
A: IFL (N=428)	F: FOLFOX (N=691)	G: IROX (N=380)	Total (N=1499)	p value
Age, yrs					0.614
   Mean (SD)	59.67 (11.36)	60.30 (11.63)	59.76 (11.50)	59.99 (11.52)	
   Range	27.00 - 88.00	19.00 - 88.00	26.00 - 85.00	19.00 - 88.00	
Gender					0.190
   Male	277 (64.7%)	411 (59.5%)	228 (60.0%)	916 (61.1%)	
   Female	151 (35.3%)	280 (40.5%)	152 (40.0%)	583 (38.9%)	
summary(mytab[c(3,1)], digits = 3)
A: IFL (N=428)	F: FOLFOX (N=691)	G: IROX (N=380)	Total (N=1499)	p value
Age, yrs					0.614
   Mean (SD)	59.673 (11.365)	60.301 (11.632)	59.763 (11.499)	59.985 (11.519)	
   Range	27.000 - 88.000	19.000 - 88.000	26.000 - 85.000	19.000 - 88.000	
Gender					0.190
   Male	277 (64.7%)	411 (59.5%)	228 (60.0%)	916 (61.1%)	
   Female	151 (35.3%)	280 (40.5%)	152 (40.0%)	583 (38.9%)	
summary(sort(mytab, decreasing = TRUE))
A: IFL (N=428)	F: FOLFOX (N=691)	G: IROX (N=380)	Total (N=1499)	p value
Age, yrs					0.614
   Mean (SD)	59.673 (11.365)	60.301 (11.632)	59.763 (11.499)	59.985 (11.519)	
   Range	27.000 - 88.000	19.000 - 88.000	26.000 - 85.000	19.000 - 88.000	
alk.phos					0.226
   N-Miss	69	141	56	266	
   Mean (SD)	175.577 (128.608)	161.984 (121.978)	173.506 (138.564)	168.969 (128.492)	
   Range	11.000 - 858.000	10.000 - 1014.000	7.000 - 982.000	7.000 - 1014.000	
Gender					0.190
   Male	277 (64.7%)	411 (59.5%)	228 (60.0%)	916 (61.1%)	
   Female	151 (35.3%)	280 (40.5%)	152 (40.0%)	583 (38.9%)	
summary(mytab[mytab < 0.5])
A: IFL (N=428)	F: FOLFOX (N=691)	G: IROX (N=380)	Total (N=1499)	p value
Gender					0.190
   Male	277 (64.7%)	411 (59.5%)	228 (60.0%)	916 (61.1%)	
   Female	151 (35.3%)	280 (40.5%)	152 (40.0%)	583 (38.9%)	
alk.phos					0.226
   N-Miss	69	141	56	266	
   Mean (SD)	175.577 (128.608)	161.984 (121.978)	173.506 (138.564)	168.969 (128.492)	
   Range	11.000 - 858.000	10.000 - 1014.000	7.000 - 982.000	7.000 - 1014.000	
head(mytab, 1) # can also use tail()
Tableby Object

Function Call: tableby(formula = arm ~ sex + alk.phos + age, data = mockstudy)

y variable: [1] “arm” x variables: [1] “sex”

11. Merge two tableby objects together
It is possible to combine two tableby objects so that they print out together.

## demographics
tab1 <- tableby(arm ~ sex + age, data=mockstudy,
                control=tableby.control(numeric.stats=c("Nmiss","meansd"), total=FALSE))
## lab data
tab2 <- tableby(arm ~ hgb + alk.phos, data=mockstudy,
                control=tableby.control(numeric.stats=c("Nmiss","median","q1q3"),
                                        numeric.test="kwt", total=FALSE))
names(tab1$x)
[1] “sex” “age”

names(tab2$x)
[1] “hgb” “alk.phos”

tab12 <- merge(tab1,tab2)
class(tab12)
[1] “tableby”

names(tab12$x)
[1] “sex” “age” “hgb” “alk.phos”

summary(tab12) #, pfootnote=TRUE)
A: IFL (N=428)	F: FOLFOX (N=691)	G: IROX (N=380)	p value
Gender				0.190
   Male	277 (64.7%)	411 (59.5%)	228 (60.0%)	
   Female	151 (35.3%)	280 (40.5%)	152 (40.0%)	
Age, yrs				0.614
   Mean (SD)	59.673 (11.365)	60.301 (11.632)	59.763 (11.499)	
hgb				0.570
   N-Miss	69	141	56	
   Median	12.100	12.200	12.400	
   Q1, Q3	11.000, 13.450	11.100, 13.600	11.175, 13.625	
alk.phos				0.104
   N-Miss	69	141	56	
   Median	133.000	116.000	122.000	
   Q1, Q3	89.000, 217.000	85.000, 194.750	87.750, 210.250	
12. Add a title to the table
When creating a pdf the tables are automatically numbered and the title appears below the table. In Word and HTML, the titles appear un-numbered and above the table.

t1 <- tableby(arm ~ sex + age, data=mockstudy)
summary(t1, title='Demographics')
Demographics
A: IFL (N=428)	F: FOLFOX (N=691)	G: IROX (N=380)	Total (N=1499)	p value
Gender					0.190
   Male	277 (64.7%)	411 (59.5%)	228 (60.0%)	916 (61.1%)	
   Female	151 (35.3%)	280 (40.5%)	152 (40.0%)	583 (38.9%)	
Age, yrs					0.614
   Mean (SD)	59.673 (11.365)	60.301 (11.632)	59.763 (11.499)	59.985 (11.519)	
   Range	27.000 - 88.000	19.000 - 88.000	26.000 - 85.000	19.000 - 88.000	
13. Modify how missing values are displayed
Depending on the report you are writing you have the following options:

Show how many subjects have each variable

Show how many subjects are missing each variable

Show how many subjects are missing each variable only if there are any missing values

Don’t indicate missing values at all

## look at how many missing values there are for each variable
apply(is.na(mockstudy),2,sum)
##        case         age         arm         sex        race     fu.time     fu.stat          ps 
##           0           0           0           0           7           0           0         266 
##         hgb         bmi    alk.phos         ast mdquality.s     age.ord  age.ordnew     dtentry 
##         266          33         266         266         252           0           1           5
## Show how many subjects have each variable (non-missing)
summary(tableby(sex ~ ast + age, data=mockstudy,
                control=tableby.control(numeric.stats=c("N","median"), total=FALSE)))
Male (N=916)	Female (N=583)	p value
ast			0.921
   N	754	479	
   Median	27.000	27.000	
Age, yrs			0.048
   N	916	583	
   Median	61.000	60.000	
## Always list the number of missing values
summary(tableby(sex ~ ast + age, data=mockstudy,
                control=tableby.control(numeric.stats=c("Nmiss2","median"), total=FALSE)))
Male (N=916)	Female (N=583)	p value
ast			0.921
   N-Miss	162	104	
   Median	27.000	27.000	
Age, yrs			0.048
   N-Miss	0	0	
   Median	61.000	60.000	
## Only show the missing values if there are some (default)
summary(tableby(sex ~ ast + age, data=mockstudy, 
                control=tableby.control(numeric.stats=c("Nmiss","mean"),total=FALSE)))
Male (N=916)	Female (N=583)	p value
ast			0.921
   N-Miss	162	104	
   mean	35.9	36	
Age, yrs			0.048
   mean	60.5	59.2	
## Don't show N at all
summary(tableby(sex ~ ast + age, data=mockstudy, 
                control=tableby.control(numeric.stats=c("mean"),total=FALSE)))
Male (N=916)	Female (N=583)	p value
ast			0.921
   mean	35.9	36	
Age, yrs			0.048
   mean	60.5	59.2	
One might also consider the use of includeNA() to include NAs in the counts and percents for categorical variables.

mockstudy$ps.cat <- factor(mockstudy$ps)
attr(mockstudy$ps.cat, "label") <- "ps"
summary(tableby(sex ~ includeNA(ps.cat), data = mockstudy, cat.stats = "countpct"))
Male (N=916)	Female (N=583)	Total (N=1499)	p value
ps				0.354
   0	391 (42.7%)	244 (41.9%)	635 (42.4%)	
   1	329 (35.9%)	202 (34.6%)	531 (35.4%)	
   2	34 (3.7%)	33 (5.7%)	67 (4.5%)	
   (Missing)	162 (17.7%)	104 (17.8%)	266 (17.7%)	
14. Modify the number of digits used
Within tableby.control function there are 4 options for controlling the number of significant digits shown.

digits: controls the number of digits after the decimal place for continuous values

digits.count: controls the number of digits after the decimal point for counts

digits.pct: controls the number of digits after the decimal point for percents

digits.p: controls the number of digits after the decimal point for p-values

summary(tableby(arm ~ sex + age + fu.time, data=mockstudy), digits=4, digits.p=2, digits.pct=1)
A: IFL (N=428)	F: FOLFOX (N=691)	G: IROX (N=380)	Total (N=1499)	p value
Gender					0.19
   Male	277 (64.7%)	411 (59.5%)	228 (60.0%)	916 (61.1%)	
   Female	151 (35.3%)	280 (40.5%)	152 (40.0%)	583 (38.9%)	
Age, yrs					0.61
   Mean (SD)	59.6729 (11.3645)	60.3010 (11.6323)	59.7632 (11.4993)	59.9853 (11.5188)	
   Range	27.0000 - 88.0000	19.0000 - 88.0000	26.0000 - 85.0000	19.0000 - 88.0000	
fu.time					< 0.01
   Mean (SD)	553.5841 (419.6065)	731.2460 (487.7443)	607.2421 (435.5092)	649.0841 (462.5109)	
   Range	9.0000 - 2170.0000	0.0000 - 2472.0000	17.0000 - 2118.0000	0.0000 - 2472.0000	
With the exception of digits.p, all of these can be specified on a per-variable basis using the in-formula functions that specify which tests are run:

summary(tableby(arm ~ chisq(sex, digits.pct=1) + anova(age, digits=4) +
                  anova(fu.time, digits = 1), data=mockstudy))
A: IFL (N=428)	F: FOLFOX (N=691)	G: IROX (N=380)	Total (N=1499)	p value
Gender					0.190
   Male	277 (64.7%)	411 (59.5%)	228 (60.0%)	916 (61.1%)	
   Female	151 (35.3%)	280 (40.5%)	152 (40.0%)	583 (38.9%)	
Age, yrs					0.614
   Mean (SD)	59.6729 (11.3645)	60.3010 (11.6323)	59.7632 (11.4993)	59.9853 (11.5188)	
   Range	27.0000 - 88.0000	19.0000 - 88.0000	26.0000 - 85.0000	19.0000 - 88.0000	
fu.time					< 0.001
   Mean (SD)	553.6 (419.6)	731.2 (487.7)	607.2 (435.5)	649.1 (462.5)	
   Range	9.0 - 2170.0	0.0 - 2472.0	17.0 - 2118.0	0.0 - 2472.0	
15. Create a user-defined summary statistic
For purposes of this example, the code below creates a trimmed mean function (trims 10%) and use that to summarize the data. Note the use of the ... which tells R to pass extra arguments on - this is required for user-defined functions. In this case, na.rm=T is passed to myfunc. The weights argument is also required, even though it isn’t passed on to the internal function in this particular example.

myfunc <- function(x, weights=rep(1,length(x)), ...){
  mean(x, trim=.1, ...)
}

summary(tableby(sex ~ hgb, data=mockstudy, 
                control=tableby.control(numeric.stats=c("Nmiss","myfunc"), numeric.test="kwt",
                    stats.labels=list(Nmiss='Missing values', myfunc="Trimmed Mean, 10%"))))
Male (N=916)	Female (N=583)	Total (N=1499)	p value
hgb				< 0.001
   Missing values	162	104	266	
   Trimmed Mean, 10%	12.6	11.9	NA	
16. Use case-weights for creating summary statistics
When comparing groups, they are often unbalanced when it comes to nuisances such as age and sex. The tableby function allows you to create weighted summary statistics. If this option us used then p-values are not calculated (test=FALSE).

##create fake group that is not balanced by age/sex 
set.seed(200)
mockstudy$fake_arm <- ifelse(mockstudy$age>60 & mockstudy$sex=='Female',sample(c('A','B'),replace=T, prob=c(.2,.8)),
                            sample(c('A','B'),replace=T, prob=c(.8,.4)))

mockstudy$agegp <- cut(mockstudy$age, breaks=c(18,50,60,70,90), right=FALSE)

## create weights based on agegp and sex distribution
tab1 <- with(mockstudy,table(agegp, sex))
tab2 <- with(mockstudy, table(agegp, sex, fake_arm))
tab2
## , , fake_arm = A
## 
##          sex
## agegp     Male Female
##   [18,50)   73     62
##   [50,60)  128     94
##   [60,70)  139      7
##   [70,90)  102      0
## 
## , , fake_arm = B
## 
##          sex
## agegp     Male Female
##   [18,50)   79     48
##   [50,60)  130     84
##   [60,70)  156    166
##   [70,90)  109    122
gpwts <- rep(tab1, length(unique(mockstudy$fake_arm)))/tab2
gpwts[gpwts>50] <- 30

## apply weights to subjects
index <- with(mockstudy, cbind(as.numeric(agegp), as.numeric(sex), as.numeric(as.factor(fake_arm)))) 
mockstudy$wts <- gpwts[index]

## show weights by treatment arm group
tapply(mockstudy$wts,mockstudy$fake_arm, summary)
## $A
##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##   1.774   1.894   2.069   2.276   2.082  24.714 
## 
## $B
##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##   1.000   1.042   1.924   1.677   1.985   2.292
orig <- tableby(fake_arm ~ age + sex + Surv(fu.time/365, fu.stat), data=mockstudy, test=FALSE)
summary(orig, title='No Case Weights used')
No Case Weights used
A (N=605)	B (N=894)	Total (N=1499)
Age, yrs			
   Mean (SD)	57.413 (11.618)	61.726 (11.125)	59.985 (11.519)
   Range	22.000 - 85.000	19.000 - 88.000	19.000 - 88.000
Gender			
   Male	442 (73.1%)	474 (53.0%)	916 (61.1%)
   Female	163 (26.9%)	420 (47.0%)	583 (38.9%)
Surv(fu.time/365, fu.stat)			
   Events	554	802	1356
   Median Survival	1.504	1.493	1.496
tab1 <- tableby(fake_arm ~ age + sex + Surv(fu.time/365, fu.stat), data=mockstudy, weights=wts)
summary(tab1, title='Case Weights used')
Case Weights used
A (N=605)	B (N=894)	Total (N=1499)
Age, yrs			
   Mean (SD)	58.009 (10.925)	60.151 (11.428)	59.126 (11.235)
   Range	22.000 - 85.000	19.000 - 88.000	19.000 - 88.000
Gender			
   Male	916 (66.5%)	916 (61.1%)	1832 (63.7%)
   Female	461 (33.5%)	583 (38.9%)	1044 (36.3%)
Surv(fu.time/365, fu.stat)			
   Events	1252	1348	2599
   Median Survival	1.534	1.496	1.532
17. Create your own p-value and add it to the table
When using weighted summary statistics, it is often desirable to then show a p-value from a model that corresponds to the weighted analysis. It is possible to add your own p-value and modify the column title for that new p-value. Another use for this would be to add standardized differences or confidence intervals instead of a p-value.

To add the p-value you simply need to create a data frame and use the function modpval.tableby. The first 2 columns in the dataframe are required and are the variable name and the new p-value. The third column can be used to indicate what method was used to calculate the p-value. If you specify use.pname=TRUE then the column name indicating the p-value will be also be used in the tableby summary.

mypval <- data.frame(variable=c('age','sex','Surv(fu.time/365, fu.stat)'), 
                     adj.pvalue=c(.953,.811,.01), 
                     method=c('Age/Sex adjusted model results'))
tab2 <- modpval.tableby(tab1, mypval, use.pname=TRUE)
summary(tab2, title='Case Weights used, p-values added') #, pfootnote=TRUE)
Case Weights used, p-values added
A (N=605)	B (N=894)	Total (N=1499)	adj.pvalue
Age, yrs				0.953
   Mean (SD)	58.009 (10.925)	60.151 (11.428)	59.126 (11.235)	
   Range	22.000 - 85.000	19.000 - 88.000	19.000 - 88.000	
Gender				0.811
   Male	916 (66.5%)	916 (61.1%)	1832 (63.7%)	
   Female	461 (33.5%)	583 (38.9%)	1044 (36.3%)	
Surv(fu.time/365, fu.stat)				0.010
   Events	1252	1348	2599	
   Median Survival	1.534	1.496	1.532	
18. For two-level categorical variables or one-line numeric variables, simplify the output.
If the cat.simplify option is set to TRUE, then only the second level of two-level categorical varialbes is shown. In the example below, sex has two levels, and “Female” is the second level, hence only the counts and percents for Female are shown. Similarly, “mdquality.s” was turned to a factor, and “1” is the second level, but since there are missings, the table ignores cat.simplify and displays all levels (since the output can no longer be displayed on one line).

table2 <- tableby(arm~sex + factor(mdquality.s), data=mockstudy, cat.simplify=TRUE)
summary(table2, labelTranslations=c(sex="Female", "factor(mdquality.s)"="MD Quality"))
A: IFL (N=428)	F: FOLFOX (N=691)	G: IROX (N=380)	Total (N=1499)	p value
Female	151 (35.3%)	280 (40.5%)	152 (40.0%)	583 (38.9%)	0.190
MD Quality					0.694
   N-Miss	55	156	41	252	
   0	41 (11.0%)	52 (9.7%)	31 (9.1%)	124 (9.9%)	
   1	332 (89.0%)	483 (90.3%)	308 (90.9%)	1123 (90.1%)	
Similarly, if numeric.simplify is set to TRUE, then any numerics which only have one row of summary statistics are simplified into a single row. Note again that ast has missing values and so is not simplified to a single row.

summary(tableby(arm ~ age + ast, data = mockstudy,
                numeric.simplify=TRUE, numeric.stats=c("Nmiss", "meansd")))
A: IFL (N=428)	F: FOLFOX (N=691)	G: IROX (N=380)	Total (N=1499)	p value
Age, yrs	59.673 (11.365)	60.301 (11.632)	59.763 (11.499)	59.985 (11.519)	0.614
ast					0.507
   N-Miss	69	141	56	266	
   Mean (SD)	37.292 (28.036)	35.202 (26.659)	35.670 (25.807)	35.933 (26.843)	
The in-formula functions to change which tests are run can also be used to specify these options for each variable at a time.

summary(tableby(arm ~ anova(age, "meansd", numeric.simplify=TRUE) +
                  chisq(sex, cat.simplify=TRUE), data = mockstudy))
## 
## 
## |             | A: IFL (N=428)  | F: FOLFOX (N=691) | G: IROX (N=380) | Total (N=1499)  | p value|
## |:------------|:---------------:|:-----------------:|:---------------:|:---------------:|-------:|
## |**Age, yrs** | 59.673 (11.365) |  60.301 (11.632)  | 59.763 (11.499) | 59.985 (11.519) |   0.614|
## |**Gender**   |   151 (35.3%)   |    280 (40.5%)    |   152 (40.0%)   |   583 (38.9%)   |   0.190|
19. Use tableby within an Sweave document
For those users who wish to create tables within an Sweave document, the following code seems to work.

\documentclass{article}

\usepackage{longtable}
\usepackage{pdfpages}

\begin{document}

\section{Read in Data}
<<echo=TRUE>>=
require(arsenal)
require(knitr)
require(rmarkdown)
data(mockstudy)

tab1 <- tableby(arm~sex+age, data=mockstudy)
@

\section{Convert Summary.Tableby to LaTeX}
<<echo=TRUE, results='hide', message=FALSE>>=
capture.output(summary(tab1), file="Test.md")

## Convert R Markdown Table to LaTeX
render("Test.md", pdf_document(keep_tex=TRUE))
@ 

\includepdf{Test.pdf}

\end{document}
20. Export tableby object to a .CSV file
When looking at multiple variables it is sometimes useful to export the results to a csv file. The as.data.frame function creates a data frame object that can be exported or further manipulated within R.

tab1 <- tableby(arm~sex+age, data=mockstudy)
as.data.frame(tab1)
##   variable     term     label variable.type              A: IFL           F: FOLFOX
## 1      sex      sex    Gender   categorical                                        
## 2      sex countpct      Male   categorical 277.00000, 64.71963 411.00000, 59.47902
## 3      sex countpct    Female   categorical 151.00000, 35.28037 280.00000, 40.52098
## 4      age      age  Age, yrs       numeric                                        
## 5      age   meansd Mean (SD)       numeric  59.67290, 11.36454  60.30101, 11.63225
## 6      age    range     Range       numeric              27, 88              19, 88
##              G: IROX              Total                       test   p.value
## 1                                       Pearson's Chi-squared test 0.1904388
## 2            228, 60  916.0000, 61.1074 Pearson's Chi-squared test 0.1904388
## 3            152, 40  583.0000, 38.8926 Pearson's Chi-squared test 0.1904388
## 4                                               Linear Model ANOVA 0.6143859
## 5 59.76316, 11.49930 59.98532, 11.51877         Linear Model ANOVA 0.6143859
## 6             26, 85             19, 88         Linear Model ANOVA 0.6143859
# write.csv(tmp, '/my/path/here/mymodel.csv')
21. Write tableby object to a separate Word or HTML file
## write to an HTML document
tab1 <- tableby(arm ~ sex + age, data=mockstudy)
write2html(tab1, "~/trash.html")

## write to a Word document
write2word(tab1, "~/trash.doc", title="My table in Word")
22. Use tableby in R Shiny
The easiest way to output a tableby() object in an R Shiny app is to use the tableOutput() UI in combination with the renderTable() server function and as.data.frame(summary(tableby())):

# A standalone shiny app
library(shiny)
library(arsenal)
data(mockstudy)

shinyApp(
  ui = fluidPage(tableOutput("table")),
  server = function(input, output) {
    output$table <- renderTable({
      as.data.frame(summary(tableby(sex ~ age, data = mockstudy), text = "html"))
    }, sanitize.text.function = function(x) x)
  }
)
This can be especially powerful if you feed the selections from a selectInput(multiple = TRUE) into formulize() to make the table dynamic!

23. Use tableby in bookdown
Since the backbone of tableby() is knitr::kable(), tables still render well in bookdown. However, print.summary.tableby() doesn’t use the caption= argument of kable(), so some tables may not have a properly numbered caption. To fix this, use the method described on the bookdown site to give the table a tag/ID.

summary(tableby(sex ~ age, data = mockstudy), title="(\\#tab:mytableby) Caption here")
24. Adjust tableby for multiple p-values
The padjust() function is a new S3 generic piggybacking off of p.adjust(). It works on both tableby and summary.tableby objects:

tab <- summary(tableby(sex ~ age + fu.time + bmi + mdquality.s, data = mockstudy))
tab
## 
## 
## |                             |   Male (N=916)    |  Female (N=583)   |  Total (N=1499)   | p value|
## |:----------------------------|:-----------------:|:-----------------:|:-----------------:|-------:|
## |**Age, yrs**                 |                   |                   |                   |   0.048|
## |&nbsp;&nbsp;&nbsp;Mean (SD)  |  60.455 (11.369)  |  59.247 (11.722)  |  59.985 (11.519)  |        |
## |&nbsp;&nbsp;&nbsp;Range      |  19.000 - 88.000  |  22.000 - 88.000  |  19.000 - 88.000  |        |
## |**fu.time**                  |                   |                   |                   |   0.978|
## |&nbsp;&nbsp;&nbsp;Mean (SD)  | 649.345 (454.332) | 648.674 (475.472) | 649.084 (462.511) |        |
## |&nbsp;&nbsp;&nbsp;Range      | 0.000 - 2472.000  | 9.000 - 2441.000  | 0.000 - 2472.000  |        |
## |**Body Mass Index (kg/m^2)** |                   |                   |                   |   0.012|
## |&nbsp;&nbsp;&nbsp;N-Miss     |        22         |        11         |        33         |        |
## |&nbsp;&nbsp;&nbsp;Mean (SD)  |  27.491 (5.030)   |  26.760 (5.984)   |  27.206 (5.432)   |        |
## |&nbsp;&nbsp;&nbsp;Range      |  14.053 - 60.243  |  15.430 - 53.008  |  14.053 - 60.243  |        |
## |**mdquality.s**              |                   |                   |                   |   0.827|
## |&nbsp;&nbsp;&nbsp;N-Miss     |        153        |        99         |        252        |        |
## |&nbsp;&nbsp;&nbsp;Mean (SD)  |   0.899 (0.301)   |   0.903 (0.296)   |   0.901 (0.299)   |        |
## |&nbsp;&nbsp;&nbsp;Range      |   0.000 - 1.000   |   0.000 - 1.000   |   0.000 - 1.000   |        |
padjust(tab, method = "bonferroni")
## 
## 
## |                             |   Male (N=916)    |  Female (N=583)   |  Total (N=1499)   | p value|
## |:----------------------------|:-----------------:|:-----------------:|:-----------------:|-------:|
## |**Age, yrs**                 |                   |                   |                   |   0.191|
## |&nbsp;&nbsp;&nbsp;Mean (SD)  |  60.455 (11.369)  |  59.247 (11.722)  |  59.985 (11.519)  |        |
## |&nbsp;&nbsp;&nbsp;Range      |  19.000 - 88.000  |  22.000 - 88.000  |  19.000 - 88.000  |        |
## |**fu.time**                  |                   |                   |                   |   1.000|
## |&nbsp;&nbsp;&nbsp;Mean (SD)  | 649.345 (454.332) | 648.674 (475.472) | 649.084 (462.511) |        |
## |&nbsp;&nbsp;&nbsp;Range      | 0.000 - 2472.000  | 9.000 - 2441.000  | 0.000 - 2472.000  |        |
## |**Body Mass Index (kg/m^2)** |                   |                   |                   |   0.048|
## |&nbsp;&nbsp;&nbsp;N-Miss     |        22         |        11         |        33         |        |
## |&nbsp;&nbsp;&nbsp;Mean (SD)  |  27.491 (5.030)   |  26.760 (5.984)   |  27.206 (5.432)   |        |
## |&nbsp;&nbsp;&nbsp;Range      |  14.053 - 60.243  |  15.430 - 53.008  |  14.053 - 60.243  |        |
## |**mdquality.s**              |                   |                   |                   |   1.000|
## |&nbsp;&nbsp;&nbsp;N-Miss     |        153        |        99         |        252        |        |
## |&nbsp;&nbsp;&nbsp;Mean (SD)  |   0.899 (0.301)   |   0.903 (0.296)   |   0.901 (0.299)   |        |
## |&nbsp;&nbsp;&nbsp;Range      |   0.000 - 1.000   |   0.000 - 1.000   |   0.000 - 1.000   |        |
Available Function Options
Summary statistics
The default summary statistics, by varible type, are:

numeric.stats: Continuous variables will show by default Nmiss, meansd, range
cat.stats: Categorical and factor variables will show by default Nmiss, countpct
ordered.stats: Ordered factors will show by default Nmiss, countpct
surv.stats: Survival variables will show by default Nmiss, Nevents, medsurv
date.stats: Date variables will show by default Nmiss, median, range
Any summary statistics standardly defined in R (e.g. mean, median, sd, med, range) can be specified, however there are a number of extra functions defined specifically for the tableby function.

N: a count of the number of observations for a particular group
Nmiss: only show the count of the number of missing values if there are some missing values
Nmiss2: always show a count of the number of missing values for a variable within each group
meansd: print the mean and standard deviation in the format mean(sd)
countpct: print the number of values in a category plus the column-percentage in the format N (%)
countrowpct: print the number of values in a category plus the row-percentage in the format N (%)
countcellpct: print the number of values in a category plus the cell-percentage in the format N (%)
binomCI: print the proportion in a category plus a binomial confidence interval.
rowbinomCI: print the row proportion in a category plus a binomial confidence interval.
medianq1q3: print the median, 25th, and 75th quantiles median (Q1, Q3)
q1q3: print the 25th and 75th quantiles Q1, Q3
iqr: print the inter-quartile range.
medianrange: print the median, minimum and maximum values median (minimum, maximum)
Nevents: print number of events for a survival object within each grouping level
medsurv: print the median survival
NeventsSurv: print number of events and survival at given times
NriskSurv: print the number still at risk at given times
medTime: print the median follow-up time
Testing options
The tests used to calculate p-values differ by the variable type, but can be specified explicitly in the formula statement or in the control function.

The following tests are accepted:

anova: analysis of variance test; the default test for continuous variables. When the grouping variable has two levels, it is equivalent to the two-sample t-test with equal variance.

kwt: Kruskal-Wallis test, optional test for continuous variables. When the grouping variable has two levels, it is equivalent to the Wilcoxon Rank Sum test.

chisq: chi-square goodness of fit test for equal counts of a categorical variable across categories; the default for categorical or factor variables

fe: Fisher’s exact test for categorical variables; optional

logrank: log-rank test, the default test for time-to-event variables

trend: The independence_test function from the coin is used to test for trends. Whenthe grouping variable has two levels, it is equivalent to the Armitage trend test. This is the default for ordered factors

notest: Don’t perform a test.

tableby.control settings
A quick way to see what arguments are possible to utilize in a function is to use the args() command. Settings involving the number of digits can be set in tableby.control or in summary.tableby.

args(tableby.control)
## function (test = TRUE, total = TRUE, test.pname = NULL, cat.simplify = FALSE, 
##     numeric.simplify = FALSE, numeric.test = "anova", cat.test = "chisq", 
##     ordered.test = "trend", surv.test = "logrank", date.test = "kwt", 
##     numeric.stats = c("Nmiss", "meansd", "range"), cat.stats = c("Nmiss", 
##         "countpct"), ordered.stats = c("Nmiss", "countpct"), 
##     surv.stats = c("Nevents", "medSurv"), date.stats = c("Nmiss", 
##         "median", "range"), stats.labels = list(Nmiss = "N-Miss", 
##         Nmiss2 = "N-Miss", meansd = "Mean (SD)", medianrange = "Median (Range)", 
##         median = "Median", medianq1q3 = "Median (Q1, Q3)", q1q3 = "Q1, Q3", 
##         iqr = "IQR", range = "Range", countpct = "Count (Pct)", 
##         Nevents = "Events", medSurv = "Median Survival", medTime = "Median Follow-Up"), 
##     digits = 3L, digits.count = 0L, digits.pct = 1L, digits.p = 3L, 
##     format.p = TRUE, conf.level = 0.95, chisq.correct = FALSE, 
##     simulate.p.value = FALSE, B = 2000, ...) 
## NULL
summary.tableby settings
The summary.tableby function has options that modify how the table appears (such as adding a title or modifying labels).

args(arsenal:::summary.tableby)
## function (object, ..., labelTranslations = NULL, text = FALSE, 
##     title = NULL, pfootnote = FALSE, term.name = "") 
## NULL


---


## The write2 function

https://cran.r-project.org/web/packages/arsenal/vignettes/write2.html

The write2 function
Ethan Heinzen
09 November, 2018
Introduction
A note on piping
Examples Using arsenal Objects
tableby
modelsum
freqlist
compare
Examples Using Other Objects
knitr::kable()
xtable::xtable()
pander::pander_return()
Output Multiple Tables to One Document
Output Other Objects Monospaced (as if in a terminal)
Add a YAML Header to the Output
FAQs
How do I suppress the note about my document getting rendered?
How do I look at the temporary .md file?
How do I prevent my document from being rendered?
How do I output headers, raw HTML/LaTeX, paragraphs, etc.?
How do I tweak the default format from write2word(), write2html(), or write2pdf()?
How do I output to a file format other than word, HTML, and PDF?
How do I avoid prefixes on my table captions in PDF?
How do I output multiple tables with different titles?
Introduction
The write2*() functions were designed as an alternative to SAS’s ODS procedure for useRs who want to save R Markdown tables to separate Word, HTML, or PDF files without needing separate R Markdown programs.

There are three shortcut functions for the most common output types: HTML, PDF, and Word. Each of these three functions calls write2(), an S3 function which accepts many file output types (see the help pages for rmarkdown::render()). Methods have been implemented for tableby(), modelsum(), and freqlist(), but also knitr::kable(), xtable::xtable(), and pander::pander_return().

The two most important things to recognize with write2() are the following:

Which function is being used to output the object. Sometimes the write2 functions use summary(), while other times they will use print(). The details for each object specifically are described below.

How the ... arguments are passed. To change the options for the summary-like or print-like function, you can pass named arguments which will in turn get passed to the appropriate function. Details for each object specifically are described below.

A note on piping
arsenal is piping-compatible!

The write2*() functions are probably the most useful place to take advantage of the magrittr package’s piping framework, since commands are often nested several functions deep in the context of write2*(). Piping also allows the arsenal package to become a part of more standard analysis pipelines; instead of needing to write separate R Markdown programs, intermediate analysis tables and output can be easily incorporated into piped statements.

This vignette will sprinkle the foward pipe (%>%) throughout as a hint at the power and flexibility of arsenal and piping.

Examples Using arsenal Objects
library(arsenal)
library(magrittr)
data(mockstudy)
tmpdir <- tempdir()
tableby
For tableby objects, the output function in write2() is summary(). For summary.tableby objects, the output function is print(). For available arguments, see the help pages for summary.tableby(). Don’t use the option text = TRUE with the write2 functions.

mylabels <- list(sex = "SEX", age ="Age, yrs")
tab1 <- tableby(arm ~ sex + age, data=mockstudy)

write2html(
  tab1, paste0(tmpdir, "/test.tableby.html"), quiet = TRUE,
  title = "My test table",      # passed to summary.tableby
  labelTranslations = mylabels, # passed to summary.tableby
  total = FALSE                 # passed to summary.tableby
)
modelsum
For modelsum objects, the output function in write2() is summary(). For summary.modelsum objects, the output function is print(). For available arguments, see the help pages for summary.modelsum(). Don’t use the option text = TRUE with the write2 functions.

tab2 <- modelsum(alk.phos ~ arm + ps + hgb, adjust= ~ age + sex, family = "gaussian", data = mockstudy)

write2pdf(
  tab2, paste0(tmpdir, "/test.modelsum.pdf"), quiet = TRUE,
  title = "My test table", # passed to summary.modelsum
  show.intercept = FALSE,  # passed to summary.modelsum
  digits = 5               # passed to summary.modelsum
)
freqlist
For freqlist objects, the output function in write2() is summary(). For summary.freqlist objects, the output function is print(). For available arguments, see the help pages for summary.freqlist().

mockstudy[, c("arm", "sex", "mdquality.s")] %>% 
  table(useNA = "ifany") %>% 
  freqlist(groupBy = c("arm", "sex")) %>% 
  write2word(
    paste0(tmpdir, "/test.freqlist.doc"), quiet = TRUE,
    single = FALSE,         # passed to summary.freqlist
    title = "My cool title" # passed to summary.freqlist
  )
compare
For compare.data.frame objects, the output function in write2() is summary(). For summary.compare.data.frame objects, the output function is print().

Examples Using Other Objects
knitr::kable()
For objects resulting from a call to kable(), the output function in write2() is print(). There aren’t any arguments to the print.knitr_kable() function.

mockstudy %>% 
  head() %>% 
  knitr::kable() %>% 
  write2html(paste0(tmpdir, "/test.kable.html"), quiet = TRUE)
xtable::xtable()
For xtable objects, the output function in write2() is print(). For available arguments, see the help pages for print.xtable().

mockstudy %>% 
  head() %>% 
  xtable::xtable(caption = "My xtable") %>% 
  write2pdf(
    paste0(tmpdir, "/test.xtable.pdf"), quiet = TRUE,
    comment = FALSE, # passed to print.xtable to turn off the default message about xtable version
    include.rownames = FALSE, # passed to print.xtable
    caption.placement = "top" # passed to print.xtable
  )
To make an HTML document, use the print.xtable() option type = "html".

mockstudy %>% 
  head() %>% 
  xtable::xtable(caption = "My xtable") %>% 
  write2html(
    paste0(tmpdir, "/test.xtable.html"), quiet = TRUE,
    type = "html",            # passed to print.xtable
    comment = FALSE, # passed to print.xtable to turn off the default message about xtable version
    include.rownames = FALSE, # passed to print.xtable
    caption.placement = "top" # passed to print.xtable
  )
User beware! xtable() is not compatible with write2word().

pander::pander_return()
Pander is a little bit more tricky. Since pander::pander() doesn’t return an object, the useR should instead use pander::pander_return(). For this (and for all character vectors), the the output function in write2() is cat(sep = '\n').

write2word(pander::pander_return(head(mockstudy)), file = paste0(tmpdir, "/test.pander.doc"), quiet = TRUE)
Output Multiple Tables to One Document
To output multiple tables into a document, simply make a list of them and call the same function as before.

mylist <- list(
  tableby(sex ~ age, data = mockstudy),
  freqlist(table(mockstudy[, c("sex", "arm")])),
  knitr::kable(head(mockstudy))
)

write2pdf(mylist, paste0(tmpdir, "/test.mylist.pdf"), quiet = TRUE)
One neat side-effect of this function is that you can output text and headers, etc. The possibilities are endless!

mylist2 <- list(
  "# Header 1",
  "This is a small paragraph introducing tableby.",
  tableby(sex ~ age, data = mockstudy),
  "<hr>",
  "# Header 2",
  "<font color='red'>I can change color of my text!</font>"
)
write2html(mylist2, paste0(tmpdir, "/test.mylist2.html"), quiet = TRUE)
In fact, you can even recurse on the lists!

write2pdf(list(mylist2, mylist), paste0(tmpdir, "/test.mylists.pdf"), quiet = TRUE)
Output Other Objects Monospaced (as if in a terminal)
It may be useful at times to write output that would normally be copied from the terminal. The default method for write2() does this automatically. To output the results of summary.lm(), for example:

lm(age ~ sex, data = mockstudy) %>% 
  summary() %>% 
  write2pdf(paste0(tmpdir, "/test.lm.pdf"), quiet = TRUE)
The verbatim() function is another option to explicitly alert write2() to do this. This becomes particularly helpful to overrule existing S3 methods.

For example, suppose you wanted to just print a tableby object (as if it were to print in the terminal):

tab4 <- tableby(arm ~ sex + age, data=mockstudy)
write2html(verbatim(tab4), paste0(tmpdir, "/test.print.tableby.html"), quiet = TRUE)
Or suppose you wanted to print a character vector (as if it were to print in the terminal):

chr <- paste0("MyVector", 1:10)
write2pdf(verbatim(chr), paste0(tmpdir, "/test.character.pdf"), quiet = TRUE)
Add a YAML Header to the Output
You can add a YAML header to write2() output using the yaml() function.

mylist3 <- list(
  yaml(title = "Test YAML Title", author = "My cool author name"),
  "# Header 1",
  "This is a small paragraph introducing tableby.",
  tableby(sex ~ age, data = mockstudy)
)
write2html(mylist3, paste0(tmpdir, "/test.yaml.html"), quiet = TRUE)
In fact, all detected YAML pieces will be moved as the first output, so that the above code chunk gives the same output as this one:

mylist4 <- list(
  "# Header 1",
  "This is a small paragraph introducing tableby.",
  yaml(title = "Test YAML Title"),
  tableby(sex ~ age, data = mockstudy),
  yaml(author = "My cool author name")
)
write2html(mylist3, paste0(tmpdir, "/test.yaml2.html"), quiet = TRUE)
FAQs
How do I suppress the note about my document getting rendered?
This is easily accomplished by using the argument quiet = TRUE (passed to the rmarkdown::render() function).

write2html(
  knitr::kable(head(mockstudy)), paste0(tmpdir, "/test.kable.quiet.html"),
  quiet = TRUE # passed to rmarkdown::render
)
How do I look at the temporary .md file?
This is easily accomplished by using the option keep.md = TRUE.

write2html(
  knitr::kable(head(mockstudy)), paste0(tmpdir, "/test.kable.keep.md.html"),
  quiet = TRUE, # passed to rmarkdown::render
  keep.md = TRUE
)
How do I prevent my document from being rendered?
This is easily accomplished by using the option render. = FALSE. Note that this will then default to keep.md = TRUE.

write2html(
  knitr::kable(head(mockstudy)), paste0(tmpdir, "/test.kable.dont.render.html"),
  render. = FALSE
)
How do I output headers, raw HTML/LaTeX, paragraphs, etc.?
One can simply abuse the list S3 method for write2()!

mylist2 <- list(
  "# Header 1",
  "This is a small paragraph introducing tableby.",
  tableby(sex ~ age, data = mockstudy),
  "<hr>",
  "# Header 2",
  "<font color='red'>I can change color of my text!</font>"
)
write2html(mylist2, paste0(tmpdir, "/test.mylist2.html"), quiet = TRUE)
How do I tweak the default format from write2word(), write2html(), or write2pdf()?
You can pass arguments to the format functions used behind the scenes.

write2html(
  knitr::kable(head(mockstudy)), paste0(tmpdir, "/test.kable.theme.html"),
  quiet = TRUE,  # passed to rmarkdown::render
  theme = "yeti" # passed to rmarkdown::html_document
)
See the help pages for rmarkdown::word_document(), rmarkdown::html_document(), and rmarkdown::pdf_document().

How do I output to a file format other than word, HTML, and PDF?
This can be done using the generic write2() function. The last argument in the function can be another format specification. For details on the acceptable inputs, see the help page for write2().

write2(
  knitr::kable(head(mockstudy[, 1:4])), paste0(tmpdir, "/test.kable.rtf"),
  quiet = TRUE,  # passed to rmarkdown::render
  output_format = rmarkdown::rtf_document
)
How do I avoid prefixes on my table captions in PDF?
You can do this pretty easily with the yaml() function:

mylist5 <- list(
  yaml("header-includes" = list("\\usepackage[labelformat=empty]{caption}")),
  "# Header 1",
  "This is a small paragraph introducing tableby.",
  tableby(sex ~ age, data = mockstudy)
)
write2pdf(mylist5, paste0(tmpdir, "/test.noprefixes.pdf"), title = "My tableby")
How do I output multiple tables with different titles?
There are now write2() methods for the summary objects of arsenal functions. This allows you to specify a title for each table:

mylist6 <- list(
  summary(tableby(sex ~ age, data = mockstudy), title = "A Title for tableby"),
  summary(modelsum(age ~ sex, data = mockstudy), title = "A Title for modelsum"),
  summary(freqlist(~ sex, data = mockstudy), title = "A Title for freqlist")
)
write2pdf(mylist6, paste0(tmpdir, "/test.multiple.titles.pdf"))
```





<!--chapter:end:arsenal.Rmd-->

---
title: "Dashboard visualizations in R: Deviation"
---

```
author: "Kristian Larsen"
output: 
  flexdashboard::flex_dashboard:
    orientation: rows
    vertical_layout: scroll
```


```
from: https://datascienceplus.com/automated-dashboard-visualizations-with-deviation-in-r/?fbclid=IwAR2JcAMQ4eNRMrEBPGL79HDbS818vGZX0evs-ateBX0d9SRFIilY7U44Szw
```



```{r eval=FALSE, include=FALSE, echo=TRUE}
library(flexdashboard)
library(ggplot2)
library(plotly)
theme_set(theme_bw())  

# Data Prep
data("mtcars")  # load data
mtcars$`car name` <- rownames(mtcars)  # create new column for car names
mtcars$mpg_z <- round((mtcars$mpg - mean(mtcars$mpg))/sd(mtcars$mpg), 2)  # compute normalized mpg
mtcars$mpg_type <- ifelse(mtcars$mpg_z < 0, "below", "above")  # above / below avg flag
mtcars <- mtcars[order(mtcars$mpg_z), ]  # sort
mtcars$`car name` <- factor(mtcars$`car name`, levels = mtcars$`car name`)  # convert to factor to retain sorted order in plot.
```

Row
-----------------------------------------------------------------------

### Chart A: Diverging Barcharts

```{r eval=FALSE, include=FALSE, echo=TRUE}
ggplot(mtcars, aes(x=`car name`, y=mpg_z, label=mpg_z)) + 
  geom_bar(stat='identity', aes(fill=mpg_type), width=.5)  +
  scale_fill_manual(name="Mileage", 
                    labels = c("Above Average", "Below Average"), 
                    values = c("above"="#00ba38", "below"="#f8766d")) + 
  labs(subtitle="Normalised mileage from 'mtcars'", 
       title= "Diverging Bars") + 
  coord_flip()
ggplotly(p = ggplot2::last_plot())
```


### Chart B: Diverging Lollipop Chart

```{r eval=FALSE, include=FALSE, echo=TRUE}
library(ggplot2)
theme_set(theme_bw())

ggplot(mtcars, aes(x=`car name`, y=mpg_z, label=mpg_z)) + 
  geom_point(stat='identity', fill="black", size=6)  +
  geom_segment(aes(y = 0, 
                   x = `car name`, 
                   yend = mpg_z, 
                   xend = `car name`), 
               color = "black") +
  geom_text(color="white", size=2) +
  labs(title="Diverging Lollipop Chart", 
       subtitle="Normalized mileage from 'mtcars': Lollipop") + 
  ylim(-2.5, 2.5) +
  coord_flip()
ggplotly(p = ggplot2::last_plot())
```

Row
-----------------------------------------------------------------------

### Cart C: Diverging Dot Plot



```{r eval=FALSE, include=FALSE, echo=TRUE}
library(ggplot2)
theme_set(theme_bw())

# Plot
ggplot(mtcars, aes(x=`car name`, y=mpg_z, label=mpg_z)) + 
  geom_point(stat='identity', aes(col=mpg_type), size=6)  +
  scale_color_manual(name="Mileage", 
                     labels = c("Above Average", "Below Average"), 
                     values = c("above"="#00ba38", "below"="#f8766d")) + 
  geom_text(color="white", size=2) +
  labs(title="Diverging Dot Plot", 
       subtitle="Normalized mileage from 'mtcars': Dotplot") + 
  ylim(-2.5, 2.5) +
  coord_flip()
ggplotly(p = ggplot2::last_plot())
```

<!--chapter:end:AutomatedDashboardDeviation.Rmd-->

---
title: "R Notebook"
---



```
print(paste0("Git Update Started at: ", Sys.time()))
CommitMessage <- paste("updated on: ", Sys.time(), sep = "")
wd <- "~/serdarbalci"
setorigin <- "git remote set-url origin git@github.com:sbalci/MyJournalWatch.git \n"
gitCommand <- paste("cd ", wd, " \n git add . \n git commit --message '", CommitMessage, "' \n", setorigin, "git push origin master \n",  sep = "")
system(command = paste(gitCommand, "\n") , intern = TRUE, wait = TRUE)
Sys.sleep(5)
print(paste0("Git Update Ended at: ", Sys.time()))
```




---

## Describe results of analysis

Copy/paste t-tests Directly to Manuscripts: https://neuropsychology.github.io/psycho.R//2018/06/19/analyze_ttest.html

https://github.com/neuropsychology/psycho.R

    
```{r eval=FALSE, include=FALSE, echo=TRUE}
# Load packages
library(tidyverse)

# devtools::install_github("neuropsychology/psycho.R")  # Install the latest psycho version
library(psycho)

```

```{r eval=FALSE, include=FALSE, echo=TRUE}
df <- psycho::affective  # Load the data

df
```

```{r eval=FALSE, include=FALSE, echo=TRUE}

results <- t.test(df$Age ~ df$Sex)  # Perform a simple t-test
results
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
psycho::analyze(results)

```


```{r eval=FALSE, include=FALSE, echo=TRUE}
t.test(df$Adjusting ~ df$Sex,
       var.equal=TRUE, 
       conf.level = .90) %>% 
  psycho::analyze()
```

```{r eval=FALSE, include=FALSE, echo=TRUE}

t.test(df$Adjusting, 
       mu = 0,
       conf.level = .90) %>% 
      psycho::analyze()

```

```{r eval=FALSE, include=FALSE, echo=TRUE}

t.test(df$Adjusting ~ df$Sex) %>% 
  psycho::analyze() %>% 
  summary()

```

# citation

```{r PubMed references, eval=FALSE, include=FALSE, echo=TRUE}
PMID_25783680 <- RefManageR::ReadPubMed("25783680", database = "PubMed")
cit_25783680 <- paste0(PMID_25783680$title, " ", PMID_25783680$journal, " ", "PMID: https://www.ncbi.nlm.nih.gov/pubmed/?term=", PMID_25783680$eprint, " ", "doi: https://doi.org/", PMID_25783680$doi)
```

My next citation is here^["r cit_25783680"].


```{r dimension badge, eval=FALSE, include=FALSE, echo=TRUE}
PMID_25783680 <- RefManageR::ReadPubMed("25783680", database = "PubMed")
dimensionBadge <- paste0(
    "<script async='' charset='utf-8' src='https://badge.dimensions.ai/badge.js'></script>
<span class='__dimensions_badge_embed__' data-doi='",
PMID_25783680$doi,
"' data-style='small_circle'></span>"
)
```

"r dimensionBadge"


```{r eval=FALSE, include=FALSE, echo=TRUE}
PMID_25783680 <- RefManageR::ReadPubMed("25783680", database = "PubMed")
altmetricBadge <- paste0(
    "<script type='text/javascript' src='https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js'></script>
<span class='altmetric-embed' data-badge-popover='right' data-badge-type='donut' data-doi='",
    PMID_25783680$doi,
    "'></span>"
)

```


"r altmetricBadge"



<!--chapter:end:Autoreport.Rmd-->

---
title: "bbplot"
---


# BBC Visual and Data Journalism cookbook for R graphics

https://bbc.github.io/rcookbook/


```{r eval=FALSE, include=FALSE, echo=TRUE}
# devtools::install_github('bbc/bbplot')
```



```{r eval=FALSE, include=FALSE, echo=TRUE}
#This line of code installs the pacman page if you do not have it installed - if you do, it simply loads the package
if(!require(pacman))install.packages("pacman")

pacman::p_load('dplyr', 'tidyr', 'gapminder',
               'ggplot2',  'ggalt',
               'forcats', 'R.utils', 'png', 
               'grid', 'ggpubr', 'scales',
               'bbplot')
```




<!--chapter:end:bbplot.Rmd-->

---
title: "Bibliography"
---


A brief introduction to bibliometrix


https://cran.r-project.org/web/packages/bibliometrix/vignettes/bibliometrix-vignette.html

 Bibliographic Network Visualization for Academic Literature Reviews 
 
 http://www.mburnamfink.com/blog/bibliographic-network-visualization-for-academic-literature-reviews
 
 
 https://embed.kumu.io/0b991b02bb20975fde904f4bf7433333#jpsp-top-50?s=%23doi-101037-0022-35147451252
 
  More Than Words? Computer-Aided Text Analysis in Organizational Behavior and Psychology Research
  
  https://www.annualreviews.org/doi/10.1146/annurev-orgpsych-032117-104622
  
  
 
 
 https://www.kumu.io/nicholasjkelley/jpsp-top-50
 
 
# knitcitations

https://github.com/cboettig/knitcitations


```{r eval=FALSE, include=FALSE, echo=TRUE}
# library(devtools)
# install_github("cboettig/knitcitations")
install.packages("knitcitations")
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
library("knitcitations")
cleanbib()
options("citation_format" = "pandoc")
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
knitcitations::citep("10.1890/11-0011.1")
```


```
citation "r citep("10.1890/11-0011.1")" in text
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
knitcitations::citet("10.1098/rspb.2013.1372")
```



```
citation "r citet("10.1098/rspb.2013.1372")" in text
```

```{r eval=FALSE, include=FALSE, echo=TRUE}
knitcitations::citep("http://knowledgeblog.org/greycite")
```


```
write.bibtex(file="references.bib")
```


# rcrossref

https://github.com/ropensci/rcrossref



# rorcid tutorial

https://ropensci.org/tutorials/rorcid_tutorial/



# rentrez tutorial

https://ropensci.org/tutorials/rentrez_tutorial/  

# WebSciCorpus

https://www.clarehooper.net/WebSciCorpus/

# WEB OF SCIENCE (WOS) CORPUS | PARSING SCRIPT

https://docs.cortext.net/question/web-of-science-wos-corpus-parsing-script-2/

# T-LAB PLUS 2019

https://tlab.it/en/allegati/help_en_online/mmappe2.htm

# Tools for bibliometric analyses

https://ju.se/library/research--teaching-support/bibliometrics/tools-for-bibliometric-analyses.html


# evidencepartners

https://www.evidencepartners.com/

# R script for creating a cross-citation network

https://www.researchgate.net/publication/327790285_R_script_for_creating_a_cross-citation_network

Repository: https://github.com/arsiders/citation-network



```
# RCitation - Quick Citation Network 
# Fall 2018 
# A.R. Siders (siders@alumni.stanford.edu)

# Creates a network of the citations among a set of academic papers. 
# Rationale: If full title of Article 2 is present in text of Article 1, Article 1 cites Article 2. 
# NOTE: Will only work in fields where full, unabbreviated titles are used in reference/bibliography citation format. 
# NOTE: Will have high error rate if titles are very short or comprised of common words (e.g., paper "Vulnerability" produced many false positives). Some errors result from authors using a shortened version of a title (e.g., only text before a colon) or incorrect citations or typos. Citation networks produced are therefore approximate and to be used primarily for exploration of the data.
# NOTE: Error rate may be reduced by using only reference sections of the articles of interest, rather than full texts, but this will increase work required to prepare articles. 


# ==> FIVE STEPS TO CITATION NETWORK

# STEP 1. FORMAT INPUT
# a. Papers: Folder of papers in txt format (UTF-8) organized *in SAME ORDER* as Titles 
# b. Titles: Column of paper titles in csv spreadsheet (Column #1) *in SAME ORDER* as documents in Papers folder. Need a header cell or top title will be removed.
# Recommend naming all texts in Papers folder using author last name listed alphabetically. Organize Titles using same order.


# STEP 2.  PREP
# set working directory
setwd("C:\[name of working space]") # make sure \ not / in name
setwd("C:/Users/User/OneDrive/Adaptive Capacity Text Mining/Citation Network Test/CitationNetwork Test Data")
# load packages
install.packages(c("tm","plyr"))
library(tm)
library(plyr)


# STEP 3. LOAD INPUTS
# a. Papers 
papers<-Corpus(DirSource("[name of folder where papers located]"))
papers<-Corpus(DirSource("Papers"))
# b. Titles
titletable<-read.csv("[name of titles file].csv") #make sure column has a header
titletable<-read.csv("TestTitles.csv")
titles<-as.vector(titletable[,1])
# load functions at bottom of this script (below Step 5)

length(papers)
length(titles)

# STEP 4. RUN FUNCTION 

CitationNetwork<-CreateCitationNetwork(papers,titles)
# add date
currentDate <- Sys.Date()
csvFileName <- paste("CitationEdges",currentDate,".csv",sep="")
# save results
write.csv(CitationNetwork, file=csvFileName) 

  
# STEP 5. VISUALIZE NETWORK

# Install Gephi or other network visualization software and load CitationEdges.csv 
# Load list of titles or other spreadsheet as nodes to visualize network 
# Gephi available at https://gephi.org/


# ===> FUNCTIONS TO LOAD 

CreateCitationNetwork<-function(papers,titles){
  # prep papers corpus
  papers<-tm_map(papers, content_transformer(tolower))
  papers<-tm_map(papers, removePunctuation)
  papers<-tm_map(papers, removeNumbers)
  papers<-tm_map(papers, stripWhitespace)
  # prep titles 
  titles<-removePunctuation(titles)
  titles<-stripWhitespace(titles)
  titles<-tolower(titles)
  # create citation true/false matrix
  Cites.TF<-CiteMatrix(titles, papers)
  # format matrix into edges file 
  CitationEdges<-EdgesFormat(Cites.TF, titles)
  return(CitationEdges)
}  

# format true/false matrix into edges file 
EdgesFormat<-function(Cites.TF, titles){
  #create an empty object to put information in
  edges<-data.frame(matrix(NA), nrow=NA, ncol=NA)
  colnames(edges)<- c("Source","Target","Weight")
  for (i in 1:length(Cites.TF)){
  #for each document, run through all titles accross columns
    for (j in 1:ncol(Cites.TF)){
      # for each title, see if document [row] cited that title [column]
      if (Cites.TF[i,j]==TRUE){  #if document is cited
        temp<-data.frame(matrix(NA), nrow=NA, ncol=NA)
        colnames(temp)<- c("Source","Target","Weight")
        # first column <- document doing the citing 
        temp[1,1]<-titles[i]
        # second column <- document being cited
        temp[1,2]<-titles[j]
        # third column the yes/no [weight]
        temp[1,3]<-1  
        temp[1,4]<-"Directed"
        edges<-rbind(edges,temp)    
      } 
    }
  }  
  return(edges[-1,]) #-1 removes initial row of null values
}

# Citation true/false matrix 
CiteMatrix<-function(search.vector, Ref.corpus){
  # Creates a csv matrix with True/False for citation patterns 
  citations<-data.frame(matrix(NA, nrow = length(Ref.corpus), ncol=length(search.vector)))
  #Columns are the document being cited
  colnames(citations)<-search.vector
  #Rows are the document doing the citing 
  rownames(citations)<-search.vector
  for (i in 1:length(search.vector)){
    searchi<-search.vector[i]
    papercite<-grepl(searchi, Ref.corpus$content, fixed=TRUE)
    citations[,i]<-papercite
  }
  return(citations)
}
```

---

- The application of methods of social network analysis in bibliometrics and webometrics. Measures and tools

https://www.researchgate.net/publication/327817518_The_application_of_methods_of_social_network_analysis_in_bibliometrics_and_webometrics_Measures_and_tools

---

# ScientoMiner ICR

https://zenodo.org/record/1432557#.XItjfxO2k1J

---

# onodo

https://onodo.org/dashboard

https://onodo.org/tutorials

---


# BibExcel

https://homepage.univie.ac.at/juan.gorraiz/bibexcel/

---

# Scientometric Portal

https://sites.google.com/site/hjamali/scientometric-portal

---

# leydesdorff

https://www.leydesdorff.net/software.htm


---

# Publish or Perish

https://harzing.com/resources/publish-or-perish

---

# Pajek: analysis and visualization of large networks

http://mrvar.fdv.uni-lj.si/pajek/

---





<!--chapter:end:Bibliography.Rmd-->

---
title: "R Bioconductor"
---


- https://www.bioconductor.org/

```
## try http:// if https:// URLs are not supported
source("https://bioconductor.org/biocLite.R")
biocLite()
```

- The Bioconductor 2018 Workshop Compilation
https://bioconductor.github.io/BiocWorkshops/index.html

https://github.com/Bioconductor/BiocWorkshops


```{r eval=FALSE, include=FALSE, echo=TRUE}
fname <- file.choose()
fname
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
file.exists(fname)
```


https://raw.githubusercontent.com/Bioconductor/BiocWorkshops/master/100_Morgan_RBiocForAll/ALL-phenoData.csv

```{r eval=FALSE, include=FALSE, echo=TRUE}
pdata <- read.csv(fname)
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
pdata
```



```{r eval=FALSE, include=FALSE, echo=TRUE}
dim(pdata)
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
head(pdata)
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
tail(pdata)
```



```{r eval=FALSE, include=FALSE, echo=TRUE}
summary(pdata)
```



```{r eval=FALSE, include=FALSE, echo=TRUE}
class(fname)
class(pdata)
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
pdata <- read.csv(
    fname,
    colClasses = c("character", "factor", "integer", "factor")
)
summary(pdata)
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
pdata[1:5, c("sex", "mol.biol")]
```

```{r eval=FALSE, include=FALSE, echo=TRUE}
pdata[1:5, c(2, 3)]
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
pdata[1:5, ]
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
pdata$age
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
pdata[["age"]]
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
class(pdata$age)
```



```{r eval=FALSE, include=FALSE, echo=TRUE}
table(pdata$mol.biol)
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
table(is.na(pdata$age))

```


```{r eval=FALSE, include=FALSE, echo=TRUE}
levels(pdata$sex)

```

```{r eval=FALSE, include=FALSE, echo=TRUE}
pdata$sex == "F"

```

```{r eval=FALSE, include=FALSE, echo=TRUE}
(pdata$sex == "F") & (pdata$age > 50)

```


```{r eval=FALSE, include=FALSE, echo=TRUE}
table( pdata$mol.biol )

```


```{r eval=FALSE, include=FALSE, echo=TRUE}
pdata$mol.biol %in% c("BCR/ABL", "NEG")

```


```{r eval=FALSE, include=FALSE, echo=TRUE}
subset(pdata, sex == "F" & age > 50)
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
bcrabl <- subset(pdata, mol.biol %in% c("BCR/ABL", "NEG"))
dim( bcrabl )
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
table(bcrabl$mol.biol)
```

```{r eval=FALSE, include=FALSE, echo=TRUE}
str(bcrabl$mol.biol)
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
factor(bcrabl$mol.biol)
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
bcrabl$mol.biol <- factor(bcrabl$mol.biol)
table(bcrabl$mol.biol)
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
str(bcrabl$mol.biol)
```

```{r eval=FALSE, include=FALSE, echo=TRUE}
boxplot(age ~ mol.biol, bcrabl)
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
t.test(age ~ mol.biol, bcrabl)
```



```{r eval=FALSE, include=FALSE, echo=TRUE}
library(ggplot2)
ggplot(bcrabl, aes(x = mol.biol, y = age))
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
ggplot(bcrabl, aes(x = mol.biol, y = age)) + geom_boxplot()
```





```{r eval=FALSE, include=FALSE, echo=TRUE}
if (!"BiocManager" %in% rownames(installed.packages()))
    install.packages("BiocManager", repos="https://cran.r-project.org")
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
BiocManager::install(c("rtracklayer", "GenomicRanges"))
```



```{r eval=FALSE, include=FALSE, echo=TRUE}
BiocManager::valid()
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
BiocManager::available("TxDb.Hsapiens")
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
browseVignettes("simpleSingleCell")
```


https://support.bioconductor.org/


https://bioconductor.org/help/course-materials/


```{r eval=FALSE, include=FALSE, echo=TRUE}
library("rtracklayer")
library("GenomicRanges")
```


https://genome.ucsc.edu/cgi-bin/hgTables?hgsid=578954849_wF1QP81SIHdfr8b0kmZUOcsZcHYr&clade=mammal&org=Human&db=hg38&hgta_group=regulation&hgta_track=knownGene&hgta_table=0&hgta_regionType=genome&position=chr9%3A133252000-133280861&hgta_outputType=primaryTable&hgta_outFileName=

```{r eval=FALSE, include=FALSE, echo=TRUE}
fname <- file.choose()
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
cpg <- rtracklayer::import(fname)
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
file.exists(fname)
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
cpg
```



https://bioconductor.github.io/BiocWorkshops/r-and-bioconductor-for-everyone-an-introduction.html


---

- **Introduction to Bioconductor**

https://www.datacamp.com/community/tutorials/intro-bioconductor



```{r eval=FALSE, include=FALSE, echo=TRUE}
source("https://bioconductor.org/biocLite.R")
biocLite()
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
source("https://bioconductor.org/biocLite.R")
biocLite(c( "Biostrings", "GenomicRanges", "IMMAN"))
```

```{r eval=FALSE, include=FALSE, echo=TRUE}
library(Biostrings)

dnaSequence <- DNAStringSet( c("AAACTG", "CCCAACCA") )
dnaSequence

```


```{r eval=FALSE, include=FALSE, echo=TRUE}
complement(dnaSequence)

```


Important packages:  
- DNAStringSet  
- Biostrings  
- GenomicRanges  

```{r eval=FALSE, include=FALSE, echo=TRUE}
library(GenomicRanges)

grangeObj <-
  GRanges(seqnames =
            Rle(c("chr1", "chr2", "chr1", "chr3"), c(1, 3, 2, 4)),
          ranges =
            IRanges(1:10, end = 7:16, names = head(letters, 10)),
          strand =
            Rle(strand(c("-", "+", "*", "+", "-")),
                c(1, 2, 2, 3, 2)),
          score = 1:10,
          GC = seq(1, 0, length=10))


grangeObj
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
seqnames(grangeObj)

```


```{r eval=FALSE, include=FALSE, echo=TRUE}
ranges(grangeObj)

```



```{r eval=FALSE, include=FALSE, echo=TRUE}
strand(grangeObj)

```


```{r eval=FALSE, include=FALSE, echo=TRUE}
library("clusterProfiler")

```


```{r eval=FALSE, include=FALSE, echo=TRUE}
library("DOSE")

```


```{r eval=FALSE, include=FALSE, echo=TRUE}
library("org.Hs.eg.db")

```



```{r eval=FALSE, include=FALSE, echo=TRUE}
data(geneList, package="DOSE")

gene <- names(geneList)[abs(geneList) > 2]
```



```{r eval=FALSE, include=FALSE, echo=TRUE}
ego <- enrichGO(gene          = gene,
                universe      = names(geneList),
                OrgDb         = org.Hs.eg.db,
                ont           = "CC",
                pAdjustMethod = "BH",
                pvalueCutoff  = 0.01,
                qvalueCutoff  = 0.05,
                readable      = TRUE)

head(ego)
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
emapplot(ego)

```



```{r eval=FALSE, include=FALSE, echo=TRUE}
class(dnaSequence)
```



```{r eval=FALSE, include=FALSE, echo=TRUE}
methods(class = "DNAStringSet")

```



---

https://bioconductor.org/packages


https://support.bioconductor.org/


http://bioconductor.org/help/course-materials/


---













<!--chapter:end:bioconductor.Rmd-->

---
title: "Biyoinformatik"
---

- DESeq results to pathways in 60 Seconds with the fgsea package

https://stephenturner.github.io/deseq-to-fgsea/

# Bioconductor

https://www.youtube.com/user/bioconductor


## Courses & Conferences

https://www.bioconductor.org/help/course-materials/


# Neuroconductor Tutorials

https://neuroconductor.org/tutorials


# Neuroconductor Courses

https://neuroconductor.org/courses








<!--chapter:end:Biyoinformatik.Rmd-->

---
title: "CancerInSilico"
---

An R interface for computational modeling of tumor progression

https://bioconductor.org/packages/release/bioc/html/CancerInSilico.html



```{r eval=FALSE, include=FALSE, echo=TRUE}
if (!requireNamespace("BiocManager"))
    install.packages("BiocManager")
BiocManager::install()

if (!requireNamespace("BiocManager", quietly = TRUE))
    install.packages("BiocManager")
BiocManager::install("CancerInSilico", version = "3.8")
library(CancerInSilico)
```



```{r eval=FALSE, include=FALSE, echo=TRUE}
browseVignettes("CancerInSilico")
```


https://bioconductor.org/packages/release/bioc/vignettes/CancerInSilico/inst/doc/CancerInSilico.html

# Running a Cell Simulation



## Run Simple Simulation

```{r eval=FALSE, include=FALSE, echo=TRUE}
simple_mod <- suppressMessages(inSilicoCellModel(initialNum=30, runTime=72,
    density=0.1, outputIncrement=24, randSeed=123))
```

## Plot CellModel Object



```{r eval=FALSE, include=FALSE, echo=TRUE}
plotCells(simple_mod, time=0)
```

```{r eval=FALSE, include=FALSE, echo=TRUE}
plotCells(simple_mod, time=36)
```

```{r eval=FALSE, include=FALSE, echo=TRUE}
plotCells(simple_mod, time=72)
```


## Query Cell Information




```{r eval=FALSE, include=FALSE, echo=TRUE}
# hours in simulation
times <- 0:simple_mod@runTime

# plot number of cells over time
nCells <- sapply(times, getNumberOfCells, model=simple_mod)
plot(times, nCells, type="l", xlab="hour", ylab="number of cells")
```

```{r eval=FALSE, include=FALSE, echo=TRUE}
# plot population density over time
den <- sapply(times, getDensity, model=simple_mod)
plot(times, den, type="l", xlab="hour", ylab="population density")
```

# Drugs

```{r eval=FALSE, include=FALSE, echo=TRUE}
drug <- new("Drug", name="Drug_A", timeAdded=24,
    cycleLengthEffect=function(type, length) length * 2)
drug_mod <- suppressMessages(inSilicoCellModel(initialNum=30, runTime=72,
    density=0.1, drugs=c(drug), outputIncrement=24, randSeed=123))
```



```{r eval=FALSE, include=FALSE, echo=TRUE}
# hours in simulation
times <- 0:simple_mod@runTime

# plot number of cells over time
nCells <- sapply(times, getNumberOfCells, model=simple_mod)
nCells_drug <- sapply(times, getNumberOfCells, model=drug_mod)
plot(times, nCells, type="l", xlab="hour", ylab="number of cells")
lines(times, nCells_drug, type="l", xlab="hour", ylab="number of cells",
    col="red")
```


# Cell Types

## Adding a Single Cell Type



```{r eval=FALSE, include=FALSE, echo=TRUE}
type_A <- new("CellType", name="A", minCycle=16, cycleLength=function() 16)
fast_cells_mod <- suppressMessages(inSilicoCellModel(initialNum=30, runTime=72,
    density=0.1, cellTypes=c(type_A), outputIncrement=24, randSeed=123))
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
# hours in simulation
times <- 0:fast_cells_mod@runTime

# plot number of cells over time
nCells <- sapply(times, getNumberOfCells, model=simple_mod)
nCells_fast <- sapply(times, getNumberOfCells, model=fast_cells_mod)
plot(times, nCells, type="l", xlab="hour", ylab="number of cells")
lines(times, nCells_fast, type="l", xlab="hour", ylab="number of cells",
    col="red")
```


## Adding Multiple Cell Types



```{r eval=FALSE, include=FALSE, echo=TRUE}
type_B <- new("CellType", name="B", size=1, minCycle=16,
    cycleLength=function() 16 + rexp(1,1/4))
type_C <- new("CellType", name="C", size=1, minCycle=32,
    cycleLength=function() 32 + rexp(1,1/4))
two_types_mod <- suppressMessages(inSilicoCellModel(initialNum=30, runTime=72,
    density=0.1, cellTypes=c(type_B, type_C), cellTypeInitFreq=c(0.4,0.6),
    outputIncrement=24, randSeed=123))
```


## Getting Cell Type



```{r eval=FALSE, include=FALSE, echo=TRUE}
getTypeBProportion <- function(time)
{
    N <- getNumberOfCells(two_types_mod, time)
    sum(sapply(1:N, function(i) getCellType(two_types_mod, time, i) == 1)) / N
}
times <- 0:two_types_mod@runTime
Bprop <- sapply(times, getTypeBProportion)
plot(times, Bprop, type="l", xlab="hour", ylab="type B proportion")
```


# Pathways


```{r eval=FALSE, include=FALSE, echo=TRUE}
mitosisGeneNames <- paste("m_", letters[1:20], sep="")
mitosisExpression <- function(model, cell, time)
{
    ifelse(getCellPhase(model, time, cell) == "M", 1, 0)
}

pwyMitosis <- new("Pathway", genes=mitosisGeneNames,
    expressionScale=mitosisExpression)
```



```{r eval=FALSE, include=FALSE, echo=TRUE}
contactInhibitionGeneNames <- paste("ci_", letters[1:15], sep="")
contactInhibitionExpression <- function(model, cell, time)
{
    getLocalDensity(model, time, cell, 3.3)
}
pwyContactInhibition <- new("Pathway", genes=contactInhibitionGeneNames,
    expressionScale=contactInhibitionExpression)
```


## Calibrate Gene Expression Range

```{r eval=FALSE, include=FALSE, echo=TRUE}
# create simulated data set
allGenes <- c(mitosisGeneNames, contactInhibitionGeneNames)
geneMeans <- 2 + rexp(length(allGenes), 1/20)
data <- t(pmax(sapply(geneMeans, rnorm, n=25, sd=2), 0))
rownames(data) <- allGenes

# calibrate pathways
pwyMitosis <- calibratePathway(pwyMitosis, data)
pwyContactInhibition <- calibratePathway(pwyContactInhibition, data)
```


## Generate Pathway Activity

```{r eval=FALSE, include=FALSE, echo=TRUE}
params <- new("GeneExpressionParams")
params@randSeed <- 123 # control this for reporducibility
params@nCells <- 30 # sample 30 cells at each time point to measure activity
params@sampleFreq <- 6 # measure activity every 6 hours

pwys <- c(pwyMitosis, pwyContactInhibition)
pwyActivity <- inSilicoGeneExpression(simple_mod, pwys, params)$pathways
```


## Visualize Pathway Activity

```{r eval=FALSE, include=FALSE, echo=TRUE}
# mitosis
plot(seq(0,72,6), pwyActivity[[1]], type="l", col="orange", ylim=c(0,1))
# contact inhibition
lines(seq(0,72,6), pwyActivity[[2]], col="blue")
```


## Accounting for Model Effects




```{r eval=FALSE, include=FALSE, echo=TRUE}
pwyMitosis@expressionScale = function(model, cell, time)
{
    window <- c(max(time - 2, 0), min(time + 2, model@runTime))
    a1 <- getAxisLength(model, window[1], cell)
    a2 <- getAxisLength(model, window[2], cell)
    if (is.na(a1)) a1 <- 0 # in case cell was just born
    return(ifelse(a2 < a1, 1, 0))
}
pwys <- c(pwyMitosis, pwyContactInhibition)
pwyActivity <- inSilicoGeneExpression(simple_mod, pwys, params)$pathways
# mitosis
plot(seq(0,72,6), pwyActivity[[1]], type="l", col="orange", ylim=c(0,1))
# contact inhibition
lines(seq(0,72,6), pwyActivity[[2]], col="blue")
```



## Normalize Pathway Activity


```{r eval=FALSE, include=FALSE, echo=TRUE}
pwyMitosis@transformMidpoint = 0.1  
pwyMitosis@transformSlope = 5 / 0.1
pwys <- c(pwyMitosis, pwyContactInhibition)
pwyActivity <- inSilicoGeneExpression(simple_mod, pwys, params)$pathways
# mitosis
plot(seq(0,72,6), pwyActivity[[1]], type="l", col="orange", ylim=c(0,1))
# contact inhibition
lines(seq(0,72,6), pwyActivity[[2]], col="blue")
```


# Simulating Bulk Gene Expression Data

## Simulating Microarray Data

```{r eval=FALSE, include=FALSE, echo=TRUE}
params@RNAseq <- FALSE # generate microarray data
params@singleCell <- FALSE # generate bulk data
params@perError <- 0.1 # parameter for simulated noise

pwys <- c(pwyMitosis, pwyContactInhibition)
ge <- inSilicoGeneExpression(simple_mod, pwys, params)$expression
```

## Visualize Bulk Gene Expression Data

```{r eval=FALSE, include=FALSE, echo=TRUE}
ndx <- apply(ge, 1, var) == 0 # remove zero variance rows
gplots::heatmap.2(ge[!ndx,], 
    col = "greenred", scale="row",
    trace="none", hclust=function(x) hclust(x,method = "complete"),
    distfun=function(x) as.dist((1-cor(t(x)))/2), 
    Colv=FALSE, dendrogram="row",
    RowSideColors = ifelse(rownames(ge[!ndx,]) %in%
        mitosisGeneNames, "orange", "blue"),
    labRow = FALSE, labCol = seq(0,72,6),
    main="Bulk Gene Expression from Simple Cell Simulation")
```

# Simulating Single Cell Gene Expression Data

## Cell Type Pathways


```{r eval=FALSE, include=FALSE, echo=TRUE}
# gene names
B_genes <- paste("b.", letters[1:20], sep="")
C_genes <- paste("c.", letters[1:20], sep="")

# pathway behavior
pwy_B <- new("Pathway", genes=B_genes, expressionScale=
    function(model, cell, time) ifelse(getCellType(model, time, cell)==1, 1, 0))
pwy_C <- new("Pathway", genes=C_genes, expressionScale=
    function(model, cell, time) ifelse(getCellType(model, time, cell)==2, 1, 0))

# calibrate pathways
geneMeans <- 2 + rexp(length(c(B_genes, C_genes)), 1/20)
data <- t(pmax(sapply(geneMeans, rnorm, n=25, sd=2), 0))
rownames(data) <- c(B_genes, C_genes)
pwy_B <- calibratePathway(pwy_B, data)
pwy_C <- calibratePathway(pwy_C, data)
```


## Simulating Single Cell RNA-seq

```{r eval=FALSE, include=FALSE, echo=TRUE}
params@RNAseq <- TRUE
params@singleCell <- TRUE
params@dropoutPresent <- TRUE
ge <- inSilicoGeneExpression(two_types_mod, c(pwy_B, pwy_C), params)$expression
```


## Visualize Single Cell Data

```{r eval=FALSE, include=FALSE, echo=TRUE}
cells <- unname(sapply(colnames(ge), function(x) strsplit(x,"_")[[1]][1]))
cells <- as.numeric(gsub("c", "", cells))
type <- sapply(cells, getCellType, model=two_types_mod,
    time=two_types_mod@runTime)
type[type==1] <- "red"
type[type==2] <- "blue"

pca <- prcomp(ge, center=FALSE, scale.=FALSE)
plot(pca$rotation[,c(1,2)], col=type)
```









<!--chapter:end:CancerInSilico.Rmd-->

---
title: "Cancer Packages"
---

# BCRA

https://cran.r-project.org/web/packages/BCRA/index.html


# cgdsr

cgdsr: R-Based API for Accessing the MSKCC Cancer Genomics Data Server (CGDS)

https://cran.r-project.org/web/packages/cgdsr/index.html


# TCGAbiolinksGUI

```{r eval=FALSE, include=FALSE, echo=TRUE}
if (!requireNamespace("BiocManager", quietly = TRUE))
    install.packages("BiocManager")
BiocManager::install("TCGAbiolinksGUI", version = "3.8")
```

```{r eval=FALSE, include=FALSE, echo=TRUE}
browseVignettes("TCGAbiolinksGUI")
```

https://bioconductor.org/packages/release/bioc/html/TCGAbiolinksGUI.html

```{r eval=FALSE, include=FALSE, echo=TRUE}
library(TCGAbiolinksGUI)
TCGAbiolinksGUI()
```



# RTCGA


```{r eval=FALSE, include=FALSE, echo=TRUE}
if (!requireNamespace("BiocManager", quietly = TRUE))
    install.packages("BiocManager")
BiocManager::install("RTCGA", version = "3.8")
```

```{r eval=FALSE, include=FALSE, echo=TRUE}
browseVignettes("RTCGA")
```

```{r eval=FALSE, include=FALSE, echo=TRUE}
RTCGA::infoTCGA()
```


# CancerSubtypes


```{r eval=FALSE, include=FALSE, echo=TRUE}
if (!requireNamespace("BiocManager", quietly = TRUE))
    install.packages("BiocManager")
BiocManager::install("CancerSubtypes", version = "3.8")
```



```{r eval=FALSE, include=FALSE, echo=TRUE}
browseVignettes("CancerSubtypes")
```



# CancerMutationAnalysis


# cancerclass


# canceR

```{r eval=FALSE, include=FALSE, echo=TRUE}
if (!requireNamespace("BiocManager", quietly = TRUE))
    install.packages("BiocManager")
BiocManager::install("canceR", version = "3.8")
```

```{r eval=FALSE, include=FALSE, echo=TRUE}
browseVignettes("canceR")
```

```{r eval=FALSE, include=FALSE, echo=TRUE}
canceR::canceR()
```


# bioCancer

```{r eval=FALSE, include=FALSE, echo=TRUE}
if (!requireNamespace("BiocManager", quietly = TRUE))
    install.packages("BiocManager")
BiocManager::install("bioCancer", version = "3.8")
```

```{r eval=FALSE, include=FALSE, echo=TRUE}
browseVignettes("bioCancer")
```

```{r eval=FALSE, include=FALSE, echo=TRUE}
bioCancer::bioCancer()
```



# TCGAretriever

TCGAretriever: Retrieve Genomic and Clinical Data from TCGA


https://cran.r-project.org/web/packages/TCGAretriever/index.html

# TCGA2STAT

https://cran.r-project.org/web/packages/TCGA2STAT/vignettes/TCGA2STAT.html


# TCIApathfinder

TCIApathfinder: Client for the Cancer Imaging Archive REST API


https://cran.r-project.org/web/packages/TCIApathfinder/index.html


# MILC

MILC: MIcrosimulation Lung Cancer (MILC) model

https://cran.r-project.org/web/packages/MILC/index.html

# InfiniumPurify


InfiniumPurify: Estimate and Account for Tumor Purity in Cancer Methylation Data Analysis

https://cran.r-project.org/web/packages/InfiniumPurify/index.html



<!--chapter:end:CancerPackages.Rmd-->

---
title: "Using Cloud for Research"
---

# rclone

https://rclone.org/drive/

# rmdrive

https://github.com/ekothe/rmdrive











<!--chapter:end:CloudForResearch.Rmd-->

---
title: "My R Codes For Data Analysis"
subtitle: "In this repository I am going to collect `R codes` for data analysis. Codes are from various resources and I try to give original link as much as possible."
author: "[Serdar Balcı, MD, Pathologist](https://www.serdarbalci.com/)"
date: '`{r #  format(Sys.Date())`'
---


rstudioapi::selectDirectory()

xaringan:::inf_mr()


**Load required packages**


`Load required packages`


- Load required packages


> Gerekli paketleri yükle



```{r  1, message=FALSE, warning=FALSE}
library(tidyverse)
```




<!-- ```{r eval=FALSE, include=FALSE, echo=TRUE} -->
<!-- library(tidyverse) -->
<!-- library(readxl) -->
<!-- ``` -->


<!-- ```{r eval=FALSE, include=FALSE, echo=TRUE} -->
<!-- path <- "ICPN_RAW.xls" -->
<!-- icpn <- path %>%  -->
<!--     excel_sheets() %>%  -->
<!--     set_names() %>% -->
<!--     map(read_excel, skip = 1, path = path) -->

<!-- icpn <- icpn[-29] -->

<!-- icpn <- icpn %>%  -->
<!--     reduce(left_join, by = "Respondant") -->

<!-- # names(icpn) -->

<!-- namesColumn <- make.names(names(read_excel(path, skip = 1))) -->

<!-- namesColumn <- namesColumn[-1] -->

<!-- # length(names(icpn)) -->
<!-- # length(namesColumn) -->

<!-- names(icpn) <- c("Respondant", paste(namesColumn, rep(1:28, each = 11), sep = "")) -->


<!-- ``` -->







<!-- ```{r eval=FALSE, include=FALSE, echo=TRUE} -->
<!-- # glimpse(icpn) -->

<!-- icpn$country <- icpn$ID1 -->

<!-- icpn <- icpn %>%  -->
<!--     select(-starts_with("X.plastic")) %>%  -->
<!--     select(-starts_with("COMMENTS")) %>%  -->
<!--     select(-starts_with("ID")) -->

<!-- # glimpse(icpn) -->

<!-- ``` -->

<!-- <!-- -->
<!-- ```{r eval=FALSE, include=FALSE, echo=TRUE} -->
<!-- jmv::descriptives(vars = names(select(icpn, starts_with("DX")) -->
<!--                                ), data = icpn, freq = TRUE, n = FALSE, missing = FALSE, mean = FALSE, median = FALSE, min = FALSE, max = FALSE) -->

<!-- ``` -->
<!-- -->


<!-- ```{r eval=FALSE, include=FALSE, echo=TRUE} -->
<!-- icpn %>%  -->
<!--     select(starts_with("y.n")) %>%  -->
<!--     gather() %>%  -->
<!--     select(value) %>%  -->
<!--     unique() -->


<!-- ``` -->

<!-- ```{r eval=FALSE, include=FALSE, echo=TRUE} -->

<!-- # recode invasion N = 0, Y = 1 -->
<!-- namesYN <- icpn %>%  -->
<!--     select(starts_with("y.n")) %>%  -->
<!--     names() -->



<!-- icpn <- icpn %>%  -->
<!--      mutate_at(namesYN, funs(recode(., `N` = 0, `Y` = 1, .default = 999))) %>%  -->
<!--      mutate_at(.vars = namesYN,  -->
<!--             .funs = funs(ifelse(. == 999, NA, .))) -->

<!-- icpn <- icpn %>%  -->
<!--      mutate_at(namesYN, funs(as.character)) %>%  -->
<!--     mutate_at(namesYN, funs(factor(., -->
<!--                                     levels = c(0,1),  -->
<!--                                     labels = c("no", "invasion") -->
<!--                                     ))) -->

<!-- ``` -->


<!-- WHOclass <- icpn %>%  -->
<!--     select(starts_with("WHO..2010.classification")) %>%  -->
<!--     gather() %>%  -->
<!--     select(value) %>%  -->
<!--     unique() -->



<!-- write(lineage$value, "lineage.txt", sep = "\n") -->

<!-- write(paste("'", readLines("lineage.txt"), "'", " =   ,", sep = ""), "lineage2.txt") -->


<!-- icpn <- icpn %>% -->
<!--     select(-starts_with("DX")) -->


<!-- file.choose() -->







---

<!-- Stratified MH Odds Ratios for Confounding -->
<!-- We can also stratify our odds ratio by a third variable to look for confounding -->

<!-- epi.2by2(table(datadepvar,dataindep_var, data$stratificationvariable)) #As above make sure the levels of dep_var and indep_var are set such that the table is ordered in the way that epi.2by2 wants the data -->

<!-- The output gives us the crude odds ratio (identical to the one we got without the stratification variable) and then an adjusted odds ratio. We can look at these two for evidence of confounding. -->

<!-- We can get (if we desire) the stratum specific odds ratios out -->

<!-- stratum <- epi.2by2(table(relevel(scabies$scabies_infestation,"yes"), relevel(scabies$impetigo_active,"yes"),scabies$gender)) -->


<!--  #Note this time I'm saving the 2*2 table -->
<!-- stratum -->
<!-- ##              Outcome +    Outcome -      Total        Inc risk * -->
<!-- ## Exposed +          179          187        366              48.9 -->
<!-- ## Exposed -          444         1098       1542              28.8 -->
<!-- ## Total              623         1285       1908              32.7 -->
<!-- ##                  Odds -->
<!-- ## Exposed +       0.957 -->
<!-- ## Exposed -       0.404 -->
<!-- ## Total           0.485 -->
<!-- ##  -->
<!-- ##  -->
<!-- ## Point estimates and 95 % CIs: -->
<!-- ## ------------------------------------------------------------------- -->
<!-- ## Inc risk ratio (crude)                       1.70 (1.49, 1.94) -->
<!-- ## Inc risk ratio (M-H)                         1.58 (1.39, 1.79) -->
<!-- ## Inc risk ratio (crude:M-H)                   1.08 -->
<!-- ## Odds ratio (crude)                           2.37 (1.88, 2.99) -->
<!-- ## Odds ratio (M-H)                             2.18 (1.72, 2.76) -->
<!-- ## Odds ratio (crude:M-H)                       1.09 -->
<!-- ## Attrib risk (crude) *                        20.11 (14.52, 25.71) -->
<!-- ## Attrib risk (M-H) *                          17.75 (5.52, 29.99) -->
<!-- ## Attrib risk (crude:M-H)                      1.13 -->
<!-- ## ------------------------------------------------------------------- -->
<!-- ##  Test of homogeneity of IRR: X2 test statistic: 8.011 p-value: 0.005 -->
<!-- ##  Test of homogeneity of  OR: X2 test statistic: 0.629 p-value: 0.428 -->
<!-- ##  Wald confidence limits -->
<!-- ##  M-H: Mantel-Haenszel -->
<!-- ##  * Outcomes per 100 population units -->


<!-- summary(stratum)$OR.strata.wald -->


<!-- ##        est    lower   upper -->
<!-- ## 1 2.417620 1.716620 3.40488 -->
<!-- ## 2 1.996753 1.439806 2.76914 -->



<!-- # Plots -->


<!-- ## Histogram -->

<!-- ```{r eval=FALSE, include=FALSE, echo=TRUE} -->
<!-- hist(scabies$age) -->

<!-- ``` -->




<!-- Scatterplot -->
<!-- We could do a basic scatter plot of house size vs number of people in a room -->

<!-- plot(scabies$house_inhabitants,scabies$room_cohabitation) -->


<!-- Colour Scatter -->
<!-- We might make this look nicer by adding some colours representing key variables -->

<!-- scabies$scatter_colour[scabies$scabies_infestation == "yes"] <- "red"  -->
<!-- #If scabies_infestation is 0 then the colour is red -->

<!-- scabies$scatter_colour[scabies$scabies_infestation == "no"] <- "blue" -->
<!-- #If scabies_infestation is 0 then the colour is blue -->

<!-- plot(scabies$house_inhabitants,scabies$room_cohabitation, col = scabies$scatter_colour) -->


<!-- #For each value look in the variable scatter_colours and work out what colour to make each person -->
<!-- Lots of the points overlap so we can always make jitter them -->

<!-- plot(jitter(scabies$house_inhabitants),scabies$room_cohabitation, col = scabies$scatter_colour) -->


<!-- Later we will come back to GGPLOT2 which makes much nicer graphs! -->

<!-- Logistic Regression -->
<!-- Sample Dataset 3 -->
<!-- The examples here use the “scabies dataset” again. For ease I quickly repeat below the code needed to implement the data-cleaning steps demonstrated above on the dataset -->

<!-- scabies <- read.csv(file = "S1-Dataset_CSV.csv", header = TRUE, sep = ",") -->
<!-- #Load dataset -->
<!-- scabies$agegroups <- as.factor(cut(scabies$age, c(0,10,20,Inf), labels = c("0-10","11-20","21+"), include.lowest = TRUE))  -->
<!-- scabies$agegroups <-relevel(scabies$agegroups, ref = "21+") -->
<!-- #Categorise age and set a baseline -->

<!-- scabies$house_cat <- as.factor(cut(scabies$house_inhabitants, c(0,5,10,Inf), labels = c("0-5","6-10","10+"), include.lowest = TRUE)) -->
<!-- scabies$house_cat <- relevel(scabies$house_cat, ref = "0-5") -->
<!-- #Categorise housesize and set a baseline -->
<!-- We can perform logistic regression in a straightforward fashion in. The formula for any logistic regression is -->

<!-- logistic_model <- glm(dependent_variable~indepdent_variable+independent_variable, data = dataframe, family = binomial) -->

<!-- If we want to include a variable as a Factor then we state that by saying > factor(independent_variable) -->

<!-- logistic_model <- glm(dependent_variable~factor(indepdent_variable), data = dataframe, family = binomial) -->

<!-- Unlike in STATA this does not automatically generate an output. As a clinical epidemiologist we want a table with the odds ratio, co-efficients, standard errors, confidence interval and the Wald-test P-Value. -->

<!-- We get the Odds ratios and Confidence intervals by taking the exponential of the output from the logistic regression model -->

<!-- logistic_model_OR <- cbind(OR = exp(coef(logistic_model)), exp(confint(logistic_model))) -->

<!-- Then we need to combine the Odds ratios and CI values with the raw coefficients and p-values into a single table -->

<!-- logistic_model_summary <- summary(logistic_model) logistic_model_summary <- cbind(logistic_model_OR, logistic_model_summary) -->

<!-- We can display then display the complte table of the output -->

<!-- logistic_model_summary -->

<!-- An example of logistic regression -->
<!-- Here we will see if the size of the household is associated with risk of scabies (variable = house_cat), after controlling for age (categorised) and gender. -->

<!-- scabiesrisk <- glm(scabies_infestation~factor(agegroups)+factor(gender)+factor(house_cat),data=scabies,family=binomial()) -->
<!-- scabiesrisk_OR <- exp(cbind(OR= coef(scabiesrisk), confint(scabiesrisk))) -->
<!-- ## Waiting for profiling to be done... -->
<!-- scabiesrisk_summary <- summary(scabiesrisk) -->
<!-- scabiesrisk_summary <- cbind(scabiesrisk_OR, scabiesrisk_summary$coefficients) -->
<!-- scabiesrisk_summary -->
<!-- ##                                OR      2.5 %    97.5 %   Estimate -->
<!-- ## (Intercept)            0.09357141 0.06925691 0.1243857 -2.3690303 -->
<!-- ## factor(agegroups)0-10  2.20016940 1.61679635 3.0237474  0.7885344 -->
<!-- ## factor(agegroups)11-20 2.53291768 1.80434783 3.5769136  0.9293719 -->
<!-- ## factor(gender)male     1.44749159 1.13940858 1.8399368  0.3698321 -->
<!-- ## factor(house_cat)6-10  1.30521927 1.02664372 1.6624397  0.2663710 -->
<!-- ## factor(house_cat)10+   1.17003712 0.65654905 1.9900581  0.1570355 -->
<!-- ##                        Std. Error     z value     Pr(>|z|) -->
<!-- ## (Intercept)             0.1492092 -15.8772359 9.110557e-57 -->
<!-- ## factor(agegroups)0-10   0.1594864   4.9442116 7.645264e-07 -->
<!-- ## factor(agegroups)11-20  0.1743214   5.3313714 9.747386e-08 -->
<!-- ## factor(gender)male      0.1221866   3.0267824 2.471718e-03 -->
<!-- ## factor(house_cat)6-10   0.1228792   2.1677478 3.017788e-02 -->
<!-- ## factor(house_cat)10+    0.2813713   0.5581076 5.767709e-01 -->
<!-- The output gives us the OR associated with each variable, the 95% CI of that OR, and a Wald-Test P-Value for each variable -->

<!-- Trying a logistic regression for yourself -->
<!-- Investigate using logistic regression whether having impetigo (impetigo_odds) is associated with scabies (scabies_infestation) after controlling for age (agegroups) and sex (gender). -->

<!-- ## Waiting for profiling to be done... -->
<!-- ##                               OR     2.5 %    97.5 %   Estimate Std. Error -->
<!-- ## (Intercept)            0.1379817 0.1064948 0.1768205 -1.9806341  0.1292414 -->
<!-- ## scabies_infestationyes 1.9380091 1.5180396 2.4740877  0.6616612  0.1245435 -->
<!-- ## agegroups0-10          3.7264517 2.8550721 4.8992194  1.3154565  0.1376324 -->
<!-- ## agegroups11-20         2.7487745 2.0310065 3.7353394  1.0111552  0.1553069 -->
<!-- ## gendermale             1.7509457 1.4225953 2.1557118  0.5601561  0.1060015 -->
<!-- ## house_cat6-10          0.9548564 0.7741093 1.1776160 -0.0461943  0.1069907 -->
<!-- ## house_cat10+           0.8941528 0.5449317 1.4384328 -0.1118786  0.2468512 -->
<!-- ##                            z value     Pr(>|z|) -->
<!-- ## (Intercept)            -15.3250751 5.199446e-53 -->
<!-- ## scabies_infestationyes   5.3126904 1.080185e-07 -->
<!-- ## agegroups0-10            9.5577530 1.203411e-21 -->
<!-- ## agegroups11-20           6.5106924 7.480520e-11 -->
<!-- ## gendermale               5.2844143 1.261076e-07 -->
<!-- ## house_cat6-10           -0.4317601 6.659158e-01 -->
<!-- ## house_cat10+            -0.4532228 6.503883e-01 -->
<!-- You should get a table showing an Odds Ration of 2 - i.e the odds of impetigo are twice as high in people with scabies. The p-value suggests this is a highly significant finding. -->

<!-- Comparing two models via a Likelihood Ratio Test -->
<!-- We might want to compare two models to see if overall the factor variable house_cat (size of the house) is significant after controlling for other variables (as opposed to the individual Wald tests for specific values of house_cat). -->

<!-- As in STATA we simply run the model without the Variable we are interested in and do a Likelihood Ratio test -->

<!-- scabiesrisk2 <- glm(scabies_infestation~factor(agegroups)+factor(gender),data=scabies,family=binomial()) -->
<!-- #We could have got out all the results of this new model just like we did for the initial Logistic Regression but for the purpose of this demonstration we don't need to -->
<!-- lrtest(scabiesrisk,scabiesrisk2) -->
<!-- ## Likelihood ratio test -->
<!-- ##  -->
<!-- ## Model 1: scabies_infestation ~ factor(agegroups) + factor(gender) + factor(house_cat) -->
<!-- ## Model 2: scabies_infestation ~ factor(agegroups) + factor(gender) -->
<!-- ##   #Df  LogLik Df  Chisq Pr(>Chisq)   -->
<!-- ## 1   6 -900.44                        -->
<!-- ## 2   4 -902.80 -2 4.7328    0.09382 . -->
<!-- ## --- -->
<!-- ## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 -->
<!-- We se that overall there is borderline significance between household size and scabies risk. This makes sense if we look at the Wald-Test for each category as one house size category was associated and one was not. -->

<!-- Fiting an interaction term -->
<!-- We can easily fit an interaction term to our model. The general syntax is -->

<!-- interaction_model <- glm(dependent_variable~indepdent_variable*independent_variable, data = dataframe family = binomial) -->

<!-- scabiesrisk3 <- glm(scabies_infestation~factor(agegroups)*factor(gender),data=scabies,family=binomial()) -->
<!-- scabiesrisk3_OR <- exp(cbind(OR= coef(scabiesrisk3), confint(scabiesrisk3))) -->
<!-- ## Waiting for profiling to be done... -->
<!-- scabiesrisk3_summary <- summary(scabiesrisk3) -->
<!-- scabiesrisk3_summary <- cbind(scabiesrisk3_OR, scabiesrisk3_summary$coefficients) -->
<!-- scabiesrisk3_summary -->
<!-- ##                                                  OR      2.5 %    97.5 % -->
<!-- ## (Intercept)                               0.1018100 0.07391201 0.1366901 -->
<!-- ## factor(agegroups)0-10                     2.6697985 1.81088534 3.9801955 -->
<!-- ## factor(agegroups)11-20                    2.3655612 1.51556214 3.6962701 -->
<!-- ## factor(gender)male                        1.6634407 0.93937388 2.8651848 -->
<!-- ## factor(agegroups)0-10:factor(gender)male  0.7017255 0.37220313 1.3517237 -->
<!-- ## factor(agegroups)11-20:factor(gender)male 1.1363006 0.56352873 2.3357191 -->
<!-- ##                                             Estimate Std. Error -->
<!-- ## (Intercept)                               -2.2846473  0.1564481 -->
<!-- ## factor(agegroups)0-10                      0.9820030  0.2004429 -->
<!-- ## factor(agegroups)11-20                     0.8610153  0.2268275 -->
<!-- ## factor(gender)male                         0.5088882  0.2831246 -->
<!-- ## factor(agegroups)0-10:factor(gender)male  -0.3542130  0.3280584 -->
<!-- ## factor(agegroups)11-20:factor(gender)male  0.1277779  0.3619259 -->
<!-- ##                                               z value     Pr(>|z|) -->
<!-- ## (Intercept)                               -14.6032328 2.678375e-48 -->
<!-- ## factor(agegroups)0-10                       4.8991654 9.624460e-07 -->
<!-- ## factor(agegroups)11-20                      3.7959034 1.471068e-04 -->
<!-- ## factor(gender)male                          1.7974000 7.227214e-02 -->
<!-- ## factor(agegroups)0-10:factor(gender)male   -1.0797255 2.802644e-01 -->
<!-- ## factor(agegroups)11-20:factor(gender)male   0.3530499 7.240510e-01 -->
<!-- The output givs us 1) The Odds Ratio of each factor in the interaction in the baseline group of the other factor: i.e odds ratio for scabies, amongst people aged 0-10 compared to those aged 21+ (the reference group for age) in the baseline group of gender (female) 2) The interaction terms -->

<!-- As in STATA we can then compare the model with and without the interaction term using a likelihood ratio test to see if the interaction is statistically significant. -->

<!-- #Compare our model containing age & gender (scabiesrisk2) without an interaction term to our third model (scabiesrisk3) which does have an interaction term between age and gender -->
<!-- lrtest(scabiesrisk2,scabiesrisk3) -->
<!-- ## Likelihood ratio test -->
<!-- ##  -->
<!-- ## Model 1: scabies_infestation ~ factor(agegroups) + factor(gender) -->
<!-- ## Model 2: scabies_infestation ~ factor(agegroups) * factor(gender) -->
<!-- ##   #Df  LogLik Df  Chisq Pr(>Chisq) -->
<!-- ## 1   4 -902.80                      -->
<!-- ## 2   6 -901.15  2 3.3131     0.1908 -->
<!-- Remember the null-hypothesis = No interaction. So here the P-value suggests no evidence against the null; so we should not fit this interaction term. -->

<!-- Matched Case-Control Data -->
<!-- Conditional Logistic Regression -->
<!-- For individually matched case control studies we use Conditional Logistic Regression not normal logistic regression -->

<!-- The syntax for this is: -->

<!-- conditional_model <- clogit(variable_denoting)or_control~indepdent_variable+independent_variable+strata(matching_var), data = dataframe) -->

<!-- Where ‘matching_var’ is a variable saying which Case is matched to which Control -->

<!-- We get the ORs, 95% CIs and everything else exactly the same as for logistic regression. -->

<!-- > conditional <- clogit(case~independent_variable+strata(matching_var), data = dataframe) -->
<!-- > conditional_OR <- exp(cbind(OR= coef(conditional), confint(conditional))) -->
<!-- > conditional_summary <- summary(conditional) -->
<!-- > conditional_summary <- cbind(conditional_OR, conditional_summary$coefficients) -->
<!-- > conditional_summary -->
<!-- Equally everything else is just like normal logistic regression i.e we can fit multiple models, run Likelihood-Ratio Tests etc -->

<!-- Poisson and Cox Regression -->
<!-- Sample Dataset 4 -->
<!-- For the purpose of Poisson and cox regression we will initially go back to a clean version of our “Ebola dataset”. For ease I quickly repeat below the code needed to implement the data-cleaning steps demonstrated above on the dataset -->

<!-- ##This is the ebola dataset again -->
<!-- ebola <- read.csv(file = "mmc1.txt", header = TRUE, sep = ",") -->

<!-- #Lets set 'person_to_person' route of transmission as baseline -->
<!-- ebola$transmission <- relevel(ebola$transmission, "person_to_person") -->

<!-- #Replace blank values of sex as missing -->
<!-- ebola$sex[ebola$sex ==""] <- is.null(ebola$sex)  -->
<!-- ## Warning in `[<-.factor`(`*tmp*`, ebola$sex == "", value = structure(c(3L, : -->
<!-- ## invalid factor level, NA generated -->
<!-- Setting up our time variables -->
<!-- The package we are using for the Poisson and Cox Regression needs the dates in a particular format to work. -->

<!-- Specifically it needs them in a ‘decimal year’ i.e 1982.145 If our data isn’t already like this then we set it up using the ‘cal.yr’ command. The cal.yr command wants a date as an input - so remember if your dates are stored as strings and not dates in the raw data then you need to convert them to a date and then to a decimal date. You can do this in one step. -->

<!-- class(ebola$disease_onset) -->
<!-- ## [1] "factor" -->
<!-- #We see that the timein variable is currently a factor (i.e just recognisd as a bunch of characters), so we need to convert factor>data>decimal date.  -->
<!-- ebola$disease_onset <- cal.yr(as.Date(ebola$disease_onset, format = "%Y-%m-%d")) -->
<!-- ebola$disease_ended <- cal.yr(as.Date(ebola$disease_ended, format = "%Y-%m-%d")) -->
<!-- Declaring our periods of follow-up: Making a lexis model -->
<!-- We need to declare the way time is modelled when doing this kind of analysis. This is like STSET in STATA. -->

<!-- That is we need to specify 1) Entry point 2) Exit point -->

<!-- We may have multiple time scales - for example both Calendar Time and Chronological age. The general syntax is: -->

<!-- lexis_object <- Lexis(  -->
<!--   entry = list ( name_first_time_scale = variable_setting_that, -->
<!--                  name_second_time_scale = variable_setting_that), -->
<!--   exit = list (name_of_time_scale_we calculate_exit_against = variable_for exit), -->
<!--   exit.status = (where_we_define_the_outcome), -->
<!--   data = data_frame) -->
<!-- For example if we might be interested in time in follow-up and chronological age as time scales. So we might do: -->

<!-- ebola_lexis_model <- Lexis(entry = list("calendar_time"=disease_onset), exit = list("calendar_time" = disease_ended), exit.status = status =="died", data = ebola) -->
<!-- ## NOTE: entry.status has been set to FALSE for all. -->
<!-- ## Warning in Lexis(entry = list(calendar_time = disease_onset), exit = list(calendar_time = disease_ended), : Dropping NA rows with duration of follow up < tol -->
<!-- #We define the beginning of calendar time in the study as the date in the 'disease_onset' variable -->
<!-- #We call time on this scale "calendar_time" -->
<!-- #Time out is on the same timescale as time in - i.e calendar_time -->
<!-- #If people are missing either a time in or time-out so they get dropped -->
<!-- R is also happy if we just have a duration of follow-up as opposed to a Date of Entry and a Date of Exit. In that case we can just say: -->

<!-- lexis_model <- Lexis(exit = list("calendar_time" = duration), exit.status = died, data = dataset) -->
<!-- ##Here we specify the exit time as being the duration of follow-up -->
<!-- ##We haven't stated an entry time so R assumes it to be 0 - i.e people were followed up from time 0 for the duration of time specified as the exit time -->
<!-- Key variables in the Lexis model -->
<!-- When we make this model it creates several key variable we need to reference when running our Cox and Poisson models. -->

<!-- ‘lex.dur’ = duration of follow-up ‘lex.Xst’ = Status of the individual at the end of each time block. For example in the above lexis model this is whether or not the event CHD occured in the any given time period. -->

<!-- We will reference these variables when we run our poisson and cox regression models. -->

<!-- Making a rate table -->
<!-- Once we have a lexis model it is easy to see crude rates and rates stratified by a variable -->

<!-- The general syntax is: > rate_table <- rate(lexis_model, obs = lex.Xst, pyrs = lex.dur, print = stratifying_var) -->

<!-- If we want we can give per 100 or 1,000 years by simply adjusting ‘pyrs’ > rate_table <- rate(lexis_model, obs = lex.Xst, pyrs = lex.dur/1000, print = stratifying_var) -->

<!-- average_rate <- rate(ebola_lexis_model, obs = lex.Xst, pyrs = lex.dur) -->
<!-- #Show the overall rate of cardiovascular outcomes per 1 person years (because people die fast of ebola and follow-up is short) -->
<!-- rate_by_transmission <- rate(ebola_lexis_model, obs = lex.Xst, pyrs = lex.dur, print = transmission) -->
<!-- #Show the rates of cardiovascular outcome by sex per 1 person years (because people die fast of ebola and follow-up is short) -->
<!-- average_rate -->
<!-- ##  -->
<!-- ## Crude rates and 95% confidence intervals: -->
<!-- ##  -->
<!-- ##    lex.Xst  lex.dur     rate  SE.rate  rate.lo  rate.hi -->
<!-- ## 1:     245 5.303217 46.19837 2.951506 40.76096 52.36112 -->
<!-- rate_by_transmission -->
<!-- ##  -->
<!-- ## Crude rates and 95% confidence intervals: -->
<!-- ##  -->
<!-- ##        transmission lex.Xst   lex.dur     rate   SE.rate  rate.lo -->
<!-- ## 1: person_to_person     129 2.8993840 44.49221  3.917321 37.44023 -->
<!-- ## 2:             both       8 0.2491444 32.10989 11.352561 16.05788 -->
<!-- ## 3:            other      10 0.1341547 74.54082 23.571876 40.10658 -->
<!-- ## 4:          syringe      86 1.7686516 48.62461  5.243327 39.36108 -->
<!-- ## 5:          unknown      12 0.2518823 47.64130 13.752860 27.05568 -->
<!-- ##      rate.hi -->
<!-- ## 1:  52.87245 -->
<!-- ## 2:  64.20806 -->
<!-- ## 3: 138.53919 -->
<!-- ## 4:  60.06829 -->
<!-- ## 5:  83.88975 -->
<!-- Running a simple Poisson Regression -->
<!-- We run a poisson regression essentially the same as a logistic regression. We use the same GLM command as before. -->

<!-- Instead of setting ‘family = binomial’ we need to set ‘family = poisson’ -->

<!-- We include an additional variable called offset. This represents the fact that follow-up time is different for each person in the study (i.e it gives you a rate against person-time rather than a number of events / people). This variable is the log of the follow-up time in the lexis-model: ‘log(lex.dur)’ -->

<!-- We set the outcome as whether lex.Xst occured (i.e did the event occur) -->

<!-- The generic formula is > poisson_model <- glm(lex.Xst~offset(log(lex.dur))+indepdent_variable+independent_variable, data = lexis_dataframe, family = poisson) -->

<!-- #Fit a model for HR of death from CHD against grade -->
<!-- ebola_model <- glm(lex.Xst~factor(transmission)+sex+offset(log(lex.dur)), data = ebola_lexis_model, family = poisson) -->
<!-- #The outcome is lex.Xst - whatever we set as the outcome when we made our lexis dataset  -->
<!-- #The offset is the log(lex.dur) - the log of duration of follow-up -->
<!-- #We fit a poisson model  -->

<!-- ebola_ratios <- cbind(HR = exp(coef(ebola_model)), exp(confint(ebola_model))) -->
<!-- ## Waiting for profiling to be done... -->
<!-- #Just like in the Logistic model we take exponentials of the coefficients to get our Hazard Rations -->
<!-- ebola_model_out <- summary(ebola_model)  -->
<!-- ebola_model_out <-cbind (ebola_ratios, ebola_model_out$coefficients) -->
<!-- #We combine this all in to a nice table -->
<!-- ebola_model_out -->
<!-- ##                                     HR      2.5 %    97.5 %     Estimate -->
<!-- ## (Intercept)                 44.5963777 36.2674133 54.231421  3.797652638 -->
<!-- ## factor(transmission)both     0.7230170  0.3237964  1.389407 -0.324322562 -->
<!-- ## factor(transmission)other    1.6052597  0.7574323  2.976279  0.473285546 -->
<!-- ## factor(transmission)syringe  1.0928077  0.8294145  1.432693  0.088750226 -->
<!-- ## factor(transmission)unknown  1.0722117  0.5607743  1.861057  0.069723525 -->
<!-- ## sexmale                      0.9941797  0.7663835  1.284689 -0.005837258 -->
<!-- ##                             Std. Error     z value      Pr(>|z|) -->
<!-- ## (Intercept)                  0.1025784 37.02195634 5.077767e-300 -->
<!-- ## factor(transmission)both     0.3666753 -0.88449514  3.764290e-01 -->
<!-- ## factor(transmission)other    0.3450006  1.37183997  1.701133e-01 -->
<!-- ## factor(transmission)syringe  0.1392193  0.63748492  5.238090e-01 -->
<!-- ## factor(transmission)unknown  0.3033065  0.22987807  8.181865e-01 -->
<!-- ## sexmale                      0.1316124 -0.04435188  9.646239e-01 -->
<!-- Lexis Expansions and time as a confounder. -->
<!-- Time (for example age) is a key confounder so we need to be able to control for this. In STATA we would use STSPLIT to do this. We can do the same in R. -->

<!-- We do this using: > expanded_lexis_model <- splitLexis(lexis_model, “time_scale_to_split_on”, breaks = c(break1,break2,break3)) -->

<!-- Sample Dataset 5 -->
<!-- We need a different dataset for this example. Again the dataset is freely available on the LSHTM Data Repository: “Alcohol outcomes” This is a simulated dataset (a fully-anoynmised, tweaked dataset based on a real-one) looking at the association between age, alcohol and death. -->

<!-- #Load in our new dataset -->
<!-- alcohol <- read.csv(file = "alcohol_outcomes.csv", header = TRUE, sep=",") -->

<!-- #Check how dates are saved -->
<!-- class(alcohol$timein) -->
<!-- ## [1] "factor" -->
<!-- #time is saved as a factor so we need to convert it -->
<!-- alcohol$timein <- cal.yr(as.Date(alcohol$timein, format = "%d/%m/%Y")) -->
<!-- alcohol$timeout <- cal.yr(as.Date(alcohol$timeout, format = "%d/%m/%Y")) -->
<!-- alcohol$timebth <- cal.yr(as.Date(alcohol$timebth, format = "%d/%m/%Y")) -->
<!-- #Setup our lexis model -->
<!-- alcohol_lexis <- Lexis(entry = list("calendar_time"=timein, "age" = timein - timebth), exit = list("calendar_time" = timeout), exit.status = death ==1, data = alcohol) -->
<!-- ## NOTE: entry.status has been set to FALSE for all. -->
<!-- #We define the beginning of calendar time in the study as the date in the 'timein' variable -->
<!-- #We call time on this scale "calendar_time" -->
<!-- #We define the beginning of the age-time scale as the difference between when you joined the study and your date of birth. We call this time scale "age" -->
<!-- #Time out is on the same timescale as time in - i.e calendar_time -->

<!-- #Perform a lexis expansion -->
<!-- split_alcohol_lexis <- splitLexis(alcohol_lexis, breaks = c(50,60,70,80), time.scale = "age") -->
<!-- #We say we want to split our lexis_model -->
<!-- #We want to do this on our defined "age" time scale -->
<!-- #We want the breaks to be at the listed points -->
<!-- Now we need to tell R we want to use those Time-Blocks as a variable in our regression. NB Compare to STATA - In stata we cut the time and make it a new variable in one go with -->

<!-- stplit newvariable, at(timepoint,timepoint,timepoint) -->

<!-- But each time we want to split on a new timescale in STATA we need to stset the data again and do a new ‘stplit’ command. -->

<!-- In R we declare all the timescales initially (in the lexis command), then we split each timeperiod and then make a new variable reflecting each split. The generic syntax is -->

<!-- expanded_model$block_name <- timeBand(expanded_model, “name_of_split”, type = “factor”) -->

<!-- split_alcohol_lexis$age_blocks <- timeBand(split_alcohol_lexis, "age", type = "factor") -->
<!-- #We make a new variable called age_blocks. -->
<!-- #We used the splits we defined on the "age" time-scale for this -->
<!-- #Make each segment of this time-scale an element of a categorical variable -->
<!-- #We then remove any blank 'time-blocks' i.e segments with no observations -->
<!-- #This is done by re-factoring the variable -->
<!-- split_alcohol_lexis$age_blocks <- factor(split_alcohol_lexis$age_blocks) -->
<!-- Running a poisson regression including a time-varying variable -->
<!-- Now we can run the poisson regression with age (in blocks) as a variable just like in STATA. -->

<!-- age_alcohol_model <- glm(lex.Xst~factor(alcohol)+factor(age_blocks)+offset(log(lex.dur)), data = split_alcohol_lexis, family = poisson) -->
<!-- age_alcohol_ratios <- cbind(HR = exp(coef(age_alcohol_model)), exp(confint(age_alcohol_model))) -->
<!-- ## Waiting for profiling to be done... -->
<!-- age_alcohol_out <- summary(age_alcohol_model)  -->
<!-- age_alcohol_out <-cbind (age_alcohol_ratios, age_alcohol_out$coefficients) -->
<!-- age_alcohol_out -->
<!-- ##                                      HR        2.5 %       97.5 % -->
<!-- ## (Intercept)                 0.001758427  0.000698216 3.568313e-03 -->
<!-- ## factor(alcohol)2            1.583354946  1.287840186 1.943563e+00 -->
<!-- ## factor(age_blocks)(50,60]   2.788198316  1.304756320 7.235037e+00 -->
<!-- ## factor(age_blocks)(60,70]   8.358271714  4.051253875 2.125521e+01 -->
<!-- ## factor(age_blocks)(70,80]  18.434422669  8.856542929 4.712319e+01 -->
<!-- ## factor(age_blocks)(80,Inf] 49.721819767 18.829170021 1.449072e+02 -->
<!-- ##                             Estimate Std. Error    z value     Pr(>|z|) -->
<!-- ## (Intercept)                -6.343336  0.4088553 -15.514867 2.752023e-54 -->
<!-- ## factor(alcohol)2            0.459546  0.1049087   4.380437 1.184415e-05 -->
<!-- ## factor(age_blocks)(50,60]   1.025396  0.4292502   2.388806 1.690321e-02 -->
<!-- ## factor(age_blocks)(60,70]   2.123252  0.4154268   5.111013 3.204365e-07 -->
<!-- ## factor(age_blocks)(70,80]   2.914220  0.4189527   6.955964 3.501584e-12 -->
<!-- ## factor(age_blocks)(80,Inf]  3.906444  0.5098479   7.661979 1.830903e-14 -->
<!-- #Now fit a model adjusting for the effect of changing age through follow-up time -->
<!-- Comparing Models -->
<!-- Just like in logistic regression we can compare our two poisson models using ‘lrtest’. Remember the two models need to be fittd on the same number of observations. This means both models need to be fitted AFTER the lexis expansion - so in the current example we would compare: Lexis Expanded Dataset - controlling for Age & Grade Lexis Expanded Dataset - controlling for Age Only The LRTEST would be whether Grade was associated having controlled for Age -->

<!-- age_model <- glm(lex.Xst~age_blocks+offset(log(lex.dur)), data = split_alcohol_lexis, family = poisson) -->
<!-- #Fit a model on the expanded dataset where only Age is an independent variable -->
<!-- age_model_ratios <- cbind(HR = exp(coef(age_model)), exp(confint(age_model))) -->
<!-- ## Waiting for profiling to be done... -->
<!-- age_model_out <- summary(age_model)  -->
<!-- age_model_out <-cbind (age_model_ratios, age_model_out$coefficients) -->
<!-- age_model_out -->
<!-- ##                              HR        2.5 %       97.5 %  Estimate -->
<!-- ## (Intercept)         0.001908232 7.583858e-04   0.00386644 -6.261578 -->
<!-- ## age_blocks(50,60]   2.864554178 1.340670e+00   7.43252992  1.052413 -->
<!-- ## age_blocks(60,70]   9.040064110 4.386843e+00  22.97293591  2.201666 -->
<!-- ## age_blocks(70,80]  21.271550431 1.025650e+01  54.25717095  3.057371 -->
<!-- ## age_blocks(80,Inf] 61.056794442 2.323188e+01 177.24621063  4.111804 -->
<!-- ##                    Std. Error    z value     Pr(>|z|) -->
<!-- ## (Intercept)         0.4082441 -15.337829 4.272431e-53 -->
<!-- ## age_blocks(50,60]   0.4291935   2.452070 1.420369e-02 -->
<!-- ## age_blocks(60,70]   0.4149555   5.305789 1.121869e-07 -->
<!-- ## age_blocks(70,80]   0.4174887   7.323242 2.420515e-13 -->
<!-- ## age_blocks(80,Inf]  0.5075158   8.101825 5.414068e-16 -->
<!-- lrtest(age_model,age_alcohol_model) -->
<!-- ## Likelihood ratio test -->
<!-- ##  -->
<!-- ## Model 1: lex.Xst ~ age_blocks + offset(log(lex.dur)) -->
<!-- ## Model 2: lex.Xst ~ factor(alcohol) + factor(age_blocks) + offset(log(lex.dur)) -->
<!-- ##   #Df  LogLik Df Chisq Pr(>Chisq)     -->
<!-- ## 1   5 -1487.5                         -->
<!-- ## 2   6 -1478.1  1 18.72  1.514e-05 *** -->
<!-- ## --- -->
<!-- ## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 -->
<!-- #Perform a likelihood test of the model with Age&Alcohol compared to the model with only Age -->
<!-- If we wanted to test if age was associated we would compare: Lexis Expanded Dataset - controlling for Age & Alcohol Lexis Expanded Dataset - controlling for Alcohol Only Note even though strictly we don’t need to expand the dataset for the Alcohol model (as its not a time varying factor) we need the two models to have the same number of observations so it does need to be run against the Lexis Expanded dataset -->

<!-- Cox Regression -->
<!-- Cox Regression is done in a very similar fashion to all other models. We declare the outcome and follow-up time slightly differently than from in the poisson but otherwise things look very similar! -->

<!-- The generic formula is: -->

<!-- cox_model <- coxph(Surv(lex.dur,lex.Xst)+indepdent_variable+independent_variable, data = lexis_dataframe) -->

<!-- cox_lexis_model <- coxph(Surv(lex.dur,lex.Xst)~factor(alcohol),data = split_alcohol_lexis) -->
<!-- ##Fit a cox model.  -->
<!-- ##The initial statement Surv(lex.dur,lex.Xst) says that the duration of follow-up is in lex.dur and the outcome variable in lex.Xst (as outlined above) -->

<!-- cox_lexis_ratios <- cbind(HR = exp(coef(cox_lexis_model)), exp(confint(cox_lexis_model))) -->
<!-- #As every other time we exponentiat the coefficients to get hazard ratios -->
<!-- cox_lexis_out <- summary(cox_lexis_model) -->
<!-- cox_lexis_out <- cbind(cox_lexis_ratios,cox_lexis_out$coefficients) -->
<!-- #And we put everything in a nice table -->
<!-- cox_lexis_out -->
<!-- ##                        HR    2.5 %   97.5 %      coef exp(coef)  se(coef) -->
<!-- ## factor(alcohol)2 2.237758 1.830073 2.736263 0.8054745  2.237758 0.1026134 -->
<!-- ##                         z     Pr(>|z|) -->
<!-- ## factor(alcohol)2 7.849601 4.218847e-15 -->
<!-- Note that the poisson and the cox give very similar estimates of the impact of alcohol on the outcome. -->

<!-- As in Poisson Regression we can compare two cox-models using ‘lrtest’ if we need to. Remember as always that both models must be performed on a dataset with the same number of observations - i.e after any lexis-expansion has been performed. -->

<!-- ##Kaplan Meier Plots -->

<!-- We can plot very nice KM plots. As before we have declared out data as a Lexis Model. We input the dataset in a similar fashion to our cox-regression but use the survfit command rather than coxph > km <- survfit(Surv(lex.dur, lex.Xst) ~ var_to_stratify_on, data = dataset) -->

<!-- Then we plot the KM Plot > ggsurvplot(km, data = dataset, risk.table = TRUE / FALSE) -->

<!-- km <- survfit(Surv(lex.dur,lex.Xst)~factor(alcohol),data = alcohol_lexis) -->
<!-- ##Fit the data -->
<!-- ##The initial statement Surv(lex.dur,lex.Xst) says that the duration of follow-up is in lex.dur and the outcome variable in lex.Xst (as outlined above).  -->
<!-- #We want to stratify by alcohol -->

<!-- ggsurvplot(km,data=alcohol_lexis) -->


<!-- #Plot the Kaplan Meier Plot -->
<!-- Correlated Outcomes -->
<!-- We often have data that is correlated. For example 1) Multiple episods of flu in a single individual 2) Clustered-survey data - i.e individuals within households -->

<!-- Commonly we use a random-effects model to adjust for this correlation. -->

<!-- We do this using a very similar model to all previous ones. The general syntax for a clustered Logistic Regression is: -->

<!-- correlated_logistic <- glmer(DEPVAR~INDEPVAR + (1|CLUSTER_VAR),family = “binomial”, data = data) #For example if we have individuals within households then CLUSTER_VAR would be the variable specifying which house they were in -->

<!-- The general syntax for a clustered Poisson Regression (after Lexis setup as before is): -->

<!-- correlated_poisson <- glmer(lex.Xst~INDEPVAR + offset(log(lex.dur))+ (1|CLUSTER_VAR),family = “poisson”, data = data) #For example if we have individuals followed over time with multiple episodes of infection, with one line in our dataframe per period of follow-up, then CLUSTER_VAR would be the variable identifying that person across each block of follow-up -->

<!-- Specifying the Clustering and getting the data out -->
<!-- In both cases the key point in setting up the formula is the addition of: > +(1|CLUSTER_VAR) -->

<!-- If we have nested levels of clustering we can specify this easily: -->

<!-- +(1|first_clustering_level/nested_clustering_level) #For example +(1|province/village) where villages are nested within provinces -->

<!-- When we want to get out our odds ratios and confidence intervbals we have to specify that slightly diffrently than in a non-random-effects model -->

<!-- OR <- cbind(OR = exp(fixef(re_model)),exp(confint(re_model,parm=“beta_”))) ##Rather than the co-efficients we want the Fixed-Effects for our Odds Ratios ##And we want the confidence intervals specifcally for those rather than all the other variables a RE-Model makes -->

<!-- Otherwise everything is the same as all our other models -->

<!-- Saying how many integration points to use -->
<!-- R-Defaults to 1 integration point for RE-Models. We can if needed change this (more accurate vs more time) > model_name <- glmer(DEPVAR~INDEPVAR +(1|CLUSTERVAR), family = “binomial”, nAGQ = X, data = DATA) #Currently you can only set integration points >1 for models with a single level of clustering -->

<!-- Random Effects Logistic Regression -->
<!-- Sample Dataset 6 -->
<!-- To look at a random effects logistic regression we are using a (simulated) dataset about trachoma. This is a clustered dataset with many individuals per household. Again the dataset is freely available on the LSHTM Data Repository: “Trachoma Household Survey” We want to find out if a latrine is protective from getting trachoma. -->

<!-- hh_tf <- read.csv (file = "hh_tf.csv", header = TRUE, sep = ",") -->
<!-- #Read in data -->

<!-- re_hh_tf <- glmer(trachoma~factor(latrine)+ (1 | household),family = "binomial", data = hh_tf) -->
<!-- summary(re_hh_tf) -->
<!-- ## Generalized linear mixed model fit by maximum likelihood (Laplace -->
<!-- ##   Approximation) [glmerMod] -->
<!-- ##  Family: binomial  ( logit ) -->
<!-- ## Formula: trachoma ~ factor(latrine) + (1 | household) -->
<!-- ##    Data: hh_tf -->
<!-- ##  -->
<!-- ##      AIC      BIC   logLik deviance df.resid  -->
<!-- ##   1922.3   1940.9   -958.1   1916.3     3676  -->
<!-- ##  -->
<!-- ## Scaled residuals:  -->
<!-- ##      Min       1Q   Median       3Q      Max  -->
<!-- ## -2.27997 -0.01801 -0.01775 -0.01742  3.06478  -->
<!-- ##  -->
<!-- ## Random effects: -->
<!-- ##  Groups    Name        Variance Std.Dev. -->
<!-- ##  household (Intercept) 74.66    8.641    -->
<!-- ## Number of obs: 3679, groups:  household, 1761 -->
<!-- ##  -->
<!-- ## Fixed effects: -->
<!-- ##                  Estimate Std. Error z value Pr(>|z|)     -->
<!-- ## (Intercept)       -8.0091     0.3434 -23.321   <2e-16 *** -->
<!-- ## factor(latrine)1  -1.2450     0.5981  -2.082   0.0374 *   -->
<!-- ## --- -->
<!-- ## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 -->
<!-- ##  -->
<!-- ## Correlation of Fixed Effects: -->
<!-- ##             (Intr) -->
<!-- ## fctr(ltrn)1 0.017 -->
<!-- #Run a Random Effects model -->
<!-- #Individuals share a household which is designated by the 'household' variable -->
<!-- #This therefore is the unit of clustering -->

<!-- re_hh_tf_OR <- cbind(OR = exp(fixef(re_hh_tf)),exp(confint(re_hh_tf,parm="beta_"))) -->
<!-- ## Computing profile confidence intervals ... -->
<!-- re_hh_tf_out <- summary(re_hh_tf) -->
<!-- #We still want our P-Values and raw data just like before -->

<!-- re_hh_tf_out <- cbind (re_hh_tf_OR, re_hh_tf_out$coefficients) -->
<!-- #Combine it all together -->
<!-- re_hh_tf_out -->
<!-- ##                            OR        2.5 %       97.5 %  Estimate -->
<!-- ## (Intercept)      0.0003324303 0.0001656782 0.0006394189 -8.009080 -->
<!-- ## factor(latrine)1 0.2879269306 0.0828465249 0.8755127240 -1.245049 -->
<!-- ##                  Std. Error    z value      Pr(>|z|) -->
<!-- ## (Intercept)       0.3434317 -23.320735 2.731632e-120 -->
<!-- ## factor(latrine)1  0.5981090  -2.081642  3.737522e-02 -->
<!-- Random Effects Poisson Regression -->
<!-- Sample Dataset 7 -->
<!-- We are using a dataset about trachoma and PCR results - this is taken from a series of surveys in the Gambia. -->

<!-- Again the dataset is freely available on the LSHTM Data Repository: “Trachoma Cohort Study” -->

<!-- For your information: PCR is coded 2 for positive and 1 for negative. Clinical evidence of active trachoma is recorded as RE.Grade == TF -->

<!-- tf_pcr <- read.csv (file = "TF_PCR.csv", header = TRUE, sep = ",") -->
<!-- #Read in data -->

<!-- tf_pcr$timein <- cal.yr(as.Date(tf_pcr$timein, format = "%d-%b-%y")) -->
<!-- tf_pcr$timeout <- cal.yr(as.Date(tf_pcr$timeout, format = "%d-%b-%y")) -->
<!-- #Convert dates in to decimal dates -->

<!-- tf_pcr$RE.Grade <- relevel(tf_pcr$RE.Grade,"N") -->
<!-- #Make a normal clinical examination of the eye the baseline level -->

<!-- tf_pcr_lexis <- Lexis(entry = list("calendar" = timein), exit = list("calendar" = timeout), exit.status = PCR==2, data = tf_pcr) -->
<!-- ## NOTE: entry.status has been set to FALSE for all. -->
<!-- #Setup a lexis model -->

<!-- re_tf_pcr <- glmer(lex.Xst~factor(RE.Grade)+offset(log(lex.dur))+(1|ID), family = "poisson", data = tf_pcr_lexis) -->
<!-- #Multiple readings from the same invdividual -->
<!-- #The ID for the individuals is in the 'ID' variable -->
<!-- #This therefore is the unit of clustering -->

<!-- re_tf_OR <- cbind(HR = exp(fixef(re_tf_pcr)),exp(confint(re_tf_pcr,parm="beta_"))) -->
<!-- ## Computing profile confidence intervals ... -->
<!-- ##Rather than the co-efficients we want the Fixed-Effects for our Odds Ratios -->
<!-- ##And we want the confidence intervals specifcally for those rather than all the other variabls a RE-Model makes -->

<!-- re_tf_pcr_out <- summary(re_tf_pcr) -->
<!-- #We still want our P-Values and raw data just like before -->

<!-- re_tf_pcr_out <- cbind (re_tf_OR, re_tf_pcr_out$coefficients) -->
<!-- #Combine it all together -->
<!-- re_tf_pcr_out -->
<!-- ##                           HR     2.5 %   97.5 %   Estimate Std. Error -->
<!-- ## (Intercept)        1.5609469 1.1207591 2.080427  0.4452926  0.1563783 -->
<!-- ## factor(RE.Grade)TF 2.0846403 1.4417488 2.975819  0.7345963  0.1845991 -->
<!-- ## factor(RE.Grade)TS 0.5650381 0.1418873 1.824576 -0.5708621  0.6397415 -->
<!-- ##                       z value     Pr(>|z|) -->
<!-- ## (Intercept)         2.8475347 4.405929e-03 -->
<!-- ## factor(RE.Grade)TF  3.9794144 6.908522e-05 -->
<!-- ## factor(RE.Grade)TS -0.8923325 3.722148e-01 -->
<!-- #Individuals with clinical evidence of trachoma (TF) are twice as likely to have a positive PCR result. -->
<!-- Confirming there is clustering -->
<!-- When we run a Random-Effects model we also want to see if whether there is or is not evidence of clustering. -->

<!-- STATA does this by looking to see if rho (logistic regression, measure of within cluster variation) or alpha (poisson, measure of within cluster variation) = zero. -->

<!-- It does this STATA runs a likelihood ratio test comparing the model with and without random-effects (where ro/alpha = 0). -->

<!-- We can do the same by fitting a model without clustering and running an LRT comparing it to our RE-model. We can also get the equivalent estimate of rho. -->

<!-- no_re_hh_tf <- glm(trachoma~factor(latrine),family = "binomial", data = hh_tf) -->

<!-- re_hh_tf <- glmer(trachoma~factor(latrine)+ (1 | household),family = "binomial", data = hh_tf) -->

<!-- icc(re_hh_tf) -->
<!-- ##  -->
<!-- ## Generalized linear mixed model -->
<!-- ##  Family: binomial (logit) -->
<!-- ## Formula: trachoma ~ factor(latrine) + (1 | household) -->
<!-- ##  -->
<!-- ##   ICC (household): 0.957795 -->
<!-- #Give estimate of the intra-cluster correlation == rho -->

<!-- lrtest(re_hh_tf,no_re_hh_tf) -->
<!-- ## Warning in modelUpdate(objects[[i - 1]], objects[[i]]): original model was -->
<!-- ## of class "glmerMod", updated model is of class "glm" -->
<!-- ## Likelihood ratio test -->
<!-- ##  -->
<!-- ## Model 1: trachoma ~ factor(latrine) + (1 | household) -->
<!-- ## Model 2: trachoma ~ factor(latrine) -->
<!-- ##   #Df   LogLik Df  Chisq Pr(>Chisq)     -->
<!-- ## 1   3  -958.15                          -->
<!-- ## 2   2 -1188.67 -1 461.05  < 2.2e-16 *** -->
<!-- ## --- -->
<!-- ## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 -->
<!-- #Test the null hypothesis that rho = 0 and that there is therefore no evidence of clustering -->
<!-- The difference between the two models is not Zero and the p-value is highly suggestive that there is evidence of clustering -->

<!-- Introduction to Sample Size Calculations -->
<!-- Suggested by “Chrissy h Roberts” -->

<!-- Below the basics of a few types of sample size calculations - for example comparing proportions, means or to detect an effect size of a certain magnitude are shown. -->

<!-- Comparing proportions -->
<!-- The general syntax is -->

<!-- epi.propsize(treat = X, control = Y, power = Z, n = Z) #Specify either n - to compute power OR power to compute sample size #Specify the other as NA -->

<!-- epi.propsize(treat = 0.1, control = 0.2, power = 0.8, n = NA) -->
<!-- ## $n.total -->
<!-- ## [1] 398 -->
<!-- ##  -->
<!-- ## $n.treat -->
<!-- ## [1] 199 -->
<!-- ##  -->
<!-- ## $n.control -->
<!-- ## [1] 199 -->
<!-- ##  -->
<!-- ## $power -->
<!-- ## [1] 0.8 -->
<!-- ##  -->
<!-- ## $lambda -->
<!-- ## [1] 0.5 -->
<!-- #Calculate sample size needed to have 80% power to show difference of 10% assuming group 1 has proportion of 10% -->

<!-- epi.propsize(treat = 0.1, control = 0.2, power = 0.8, n = NA, sided.test = 1) -->
<!-- ## $n.total -->
<!-- ## [1] 314 -->
<!-- ##  -->
<!-- ## $n.treat -->
<!-- ## [1] 157 -->
<!-- ##  -->
<!-- ## $n.control -->
<!-- ## [1] 157 -->
<!-- ##  -->
<!-- ## $power -->
<!-- ## [1] 0.8 -->
<!-- ##  -->
<!-- ## $lambda -->
<!-- ## [1] 0.5 -->
<!-- #As above but a 1-sided test -->

<!-- epi.propsize(treat = 0.1, control = 0.2, power = NA, n = 300) -->
<!-- ## $n.total -->
<!-- ## [1] 300 -->
<!-- ##  -->
<!-- ## $n.treat -->
<!-- ## [1] 150 -->
<!-- ##  -->
<!-- ## $n.control -->
<!-- ## [1] 150 -->
<!-- ##  -->
<!-- ## $power -->
<!-- ## [1] 0.6808308 -->
<!-- ##  -->
<!-- ## $lambda -->
<!-- ## [1] 0.5 -->
<!-- #Calculate the power to detect a difference between arms given a sample size of 300 -->
<!-- Sample size to detect a difference in proportions -->
<!-- We need to know the 1) Proportion in control group 2) Odds ratio we wish to detect -->

<!-- The general syntax when giving an Odds Ratio to detect is -->

<!-- epi.ccsize(OR = X, p0 = Y, n = Z, power = Z) #Specify either n - to compute power OR power to compute sample size #Specify the other as NA epi.ccsize(OR = X, p0 = Y, n = Z, power = Z, r = ) #Where R is the ratio of controls to cases -->

<!-- epi.ccsize(OR = 2, p0 = 0.2, n = NA, power = 0.8) -->
<!-- ## $n.total -->
<!-- ## [1] 344 -->
<!-- ##  -->
<!-- ## $n.case -->
<!-- ## [1] 172 -->
<!-- ##  -->
<!-- ## $n.control -->
<!-- ## [1] 172 -->
<!-- ##  -->
<!-- ## $power -->
<!-- ## [1] 0.8 -->
<!-- ##  -->
<!-- ## $OR -->
<!-- ## [1] 2 -->
<!-- help("epi.ccsize") -->
<!-- #Calculate sample size required to detect an Odds Ratio of 2 given an expected prevalence of 20% in the control group -->

<!-- epi.ccsize(OR = 4, p0 = 0.3, n = NA, power = 0.8, r =4) -->
<!-- ## $n.total -->
<!-- ## [1] 110 -->
<!-- ##  -->
<!-- ## $n.case -->
<!-- ## [1] 22 -->
<!-- ##  -->
<!-- ## $n.control -->
<!-- ## [1] 88 -->
<!-- ##  -->
<!-- ## $power -->
<!-- ## [1] 0.8 -->
<!-- ##  -->
<!-- ## $OR -->
<!-- ## [1] 4 -->
<!-- #Calculate sample size required to detect an Odds Ratio of 4 given an expected prevalence of 30% in the control group and 4 controls for every case -->
<!-- Sample size for detecting a specific prevalence -->
<!-- The general syntax is > epi.simplesize(Py = prevalence_to_find, epsilon.R = variation_around_prevalence, method = “proportion”) #epsilon.R is specified as a proportion of Py #So if we want to find 10% +/- 2% we would set epsion.R as 0.2 -->

<!-- epi.simplesize(Py = 0.2, epsilon.r = 0.3, method = "proportion") -->
<!-- ## [1] 171 -->
<!-- #Calculate sample size required to detect a true prevalence of 10% +/- 3% -->

<!-- #We can specify a fixed population calculation by adding in 'n ='  -->
<!-- Sample size for comparing means -->
<!-- The general syntax is -->

<!-- power.t.test(delta = DIFF_BETWEEN_MEANS, power = Z, type =“SAMPLE_TYPE”) #Type may be “two.sample”, “one.sample”, “paired” #We can additionally specify the anticiapted standard deviation for example power.t.test(delta = DIFF_BETWEEN_MEANS, power = Z, type =“SAMPLE_TYPE”, sd = 1) -->

<!-- OR alternatively (using same package as other sample size calculations) >epi.meansize(treat = X, control = Y, n = Z, power = Z, sigma = A) #Specify either n - to compute power OR power to compute sample size #Specify the other as NA #Sigma is the anticipated Standard Deviation -->

<!-- power.t.test(delta = 2, power = 0.8, type ="two.sample", sd = 1) -->
<!-- ##  -->
<!-- ##      Two-sample t test power calculation  -->
<!-- ##  -->
<!-- ##               n = 5.090008 -->
<!-- ##           delta = 2 -->
<!-- ##              sd = 1 -->
<!-- ##       sig.level = 0.05 -->
<!-- ##           power = 0.8 -->
<!-- ##     alternative = two.sided -->
<!-- ##  -->
<!-- ## NOTE: n is number in *each* group -->
<!-- #Calculate sample size needed to have 80% power to show difference of 2 in the mean value between two groups -->

<!-- power.t.test(delta = 2, power = 0.8, type ="paired", sd = 1) -->
<!-- ##  -->
<!-- ##      Paired t test power calculation  -->
<!-- ##  -->
<!-- ##               n = 4.220731 -->
<!-- ##           delta = 2 -->
<!-- ##              sd = 1 -->
<!-- ##       sig.level = 0.05 -->
<!-- ##           power = 0.8 -->
<!-- ##     alternative = two.sided -->
<!-- ##  -->
<!-- ## NOTE: n is number of *pairs*, sd is std.dev. of *differences* within pairs -->
<!-- #Calculate sample size needed to have 80% power to show difference of 0.5 in the mean of two paired values - assuming a standard deviation of 1 -->

<!-- epi.meansize(treat = 14 , control = 12, n = NA, power = 0.8, sigma = 1) -->
<!-- ## $n.total -->
<!-- ## [1] 8 -->
<!-- ##  -->
<!-- ## $n.treat -->
<!-- ## [1] 4 -->
<!-- ##  -->
<!-- ## $n.control -->
<!-- ## [1] 4 -->
<!-- ##  -->
<!-- ## $power -->
<!-- ## [1] 0.8 -->
<!-- ##  -->
<!-- ## $delta -->
<!-- ## [1] 2 -->
<!-- #Calculate sample size needed to have 80% power to show difference of 2 between mean assuming a standard deviation of 1 -->
<!-- #Capture Recapture Population Estimates We can calculate the predicted true population based on results of a capture-recapture survey The general syntax is > epi.popsize(people_seen_firs_time,people_seen_second_time,people_seen_both_times) -->

<!-- epi.popsize(1104,1112,920) -->
<!-- ##  -->
<!-- ##  Estimated population size: 1334 (95% CI 1319 - 1350) -->
<!-- ##  Estimated number of untested subjects: 38 (95% CI 23 - 54) -->
<!-- #We saw a 1104 people in our first survey -->
<!-- #We saw 1112 people in our second survey -->
<!-- #920 people were seen both times -->
<!-- #Calculate estimated true population and proportion of individuals missed both times -->
<!-- Multiple Imputation -->
<!-- Suggested by “Tom Yates” -->

<!-- I presume as always that you understand the caveats of multiple imputation. The MICE package uses Multivariate Imputation via Chained Equations which is a common approach to multiple imputation. -->

<!-- Sample Dataset 8 -->
<!-- I deleted some data from our earlier scabies dataset so we can look at how multiple imputation works. -->

<!-- Again the dataset with missing data is available on the LSHTM Data-Repository: “Scabies Missing Data” -->

<!-- scabies_missing <- read.csv(file = "scabies_results_missing.csv", header = TRUE, sep = ",") -->
<!-- Looking at what data is missing -->
<!-- We can see what data is missing using the ‘md.pattern’ command. The general syntax is: -->

<!-- md.pattern(dataset) -->

<!-- md.pattern(scabies_missing) -->
<!-- ##      village impetigo_inactive other_lesions room_cohabitation age -->
<!-- ## 1439       1                 1             1                 1   1 -->
<!-- ##   63       1                 1             1                 1   0 -->
<!-- ##   81       1                 1             1                 1   1 -->
<!-- ##   52       1                 1             1                 1   1 -->
<!-- ##   54       1                 1             1                 0   1 -->
<!-- ##   70       1                 1             1                 1   1 -->
<!-- ##  103       1                 1             1                 1   1 -->
<!-- ##    4       1                 1             1                 1   0 -->
<!-- ##    7       1                 1             1                 1   1 -->
<!-- ##    2       1                 1             1                 0   0 -->
<!-- ##    2       1                 1             1                 0   1 -->
<!-- ##    4       1                 1             1                 0   1 -->
<!-- ##    1       1                 1             1                 1   0 -->
<!-- ##    3       1                 1             1                 1   1 -->
<!-- ##    2       1                 1             1                 1   1 -->
<!-- ##    3       1                 1             1                 0   1 -->
<!-- ##    3       1                 1             1                 1   0 -->
<!-- ##    2       1                 1             1                 1   1 -->
<!-- ##    4       1                 1             1                 1   1 -->
<!-- ##    4       1                 1             1                 0   1 -->
<!-- ##    3       1                 1             1                 1   1 -->
<!-- ##    1       1                 1             1                 0   0 -->
<!-- ##    1       1                 1             1                 0   1 -->
<!-- ##            0                 0             0                71  74 -->
<!-- ##      house_inhabitants scabies_infestation gender impetigo_active     -->
<!-- ## 1439                 1                   1      1               1   0 -->
<!-- ##   63                 1                   1      1               1   1 -->
<!-- ##   81                 1                   1      0               1   1 -->
<!-- ##   52                 0                   1      1               1   1 -->
<!-- ##   54                 1                   1      1               1   1 -->
<!-- ##   70                 1                   0      1               1   1 -->
<!-- ##  103                 1                   1      1               0   1 -->
<!-- ##    4                 0                   1      1               1   2 -->
<!-- ##    7                 0                   1      0               1   2 -->
<!-- ##    2                 1                   1      1               1   2 -->
<!-- ##    2                 1                   1      0               1   2 -->
<!-- ##    4                 0                   1      1               1   2 -->
<!-- ##    1                 1                   0      1               1   2 -->
<!-- ##    3                 1                   0      0               1   2 -->
<!-- ##    2                 0                   0      1               1   2 -->
<!-- ##    3                 1                   0      1               1   2 -->
<!-- ##    3                 1                   1      1               0   2 -->
<!-- ##    2                 1                   1      0               0   2 -->
<!-- ##    4                 0                   1      1               0   2 -->
<!-- ##    4                 1                   1      1               0   2 -->
<!-- ##    3                 1                   0      1               0   2 -->
<!-- ##    1                 1                   1      0               1   3 -->
<!-- ##    1                 0                   0      1               1   3 -->
<!-- ##                     74                  83     96             119 517 -->
<!-- Imputing data using Multivariate Imputation via Chained Equations -->
<!-- We can impute our data using the ‘mice’ command. The general syntax is: -->

<!-- imputed_data <- mice(data = datasets, m = number_imputet_sets_to_make, defaultMethod = “method_for_imputing_values”, maxit = number_iterations_to_impute_values, seed = 500) -->

<!-- This generates ‘m’ datasets each with slightly different imputed data. -->

<!-- NB Seed is just a number which kicks off the random-number generator. So if you alter the seed number the imputed dataset is going to change. -->

<!-- Specifying which imputation method to use -->
<!-- We don’t actually need to specify defaultMethod unless we want to change how R imputes values. -->

<!-- By default ‘mice’ will use the following methods: * Numeric values: pmm, predictive mean matching * Binary data: logreg, logistic regression imputation * Unordered categorical data with >2 levels: polyreg, polytomous regression * Ordered categorical data with >2 levels: polr, proportional odds model -->

<!-- We can specify alternative methods * norm = Bayesian linear regression -->

<!-- For example we could use Bayesian linear regression for Numeric imputation by stating -->

<!-- defaultMethod = c(“norm”,“logreg”, “polyreg”, “polr”), -->

<!-- imputed_scabies_data <- mice(data = scabies_missing, m = 5, maxit = 10, seed = 500) -->
<!-- By default MICE uses all values to impute from and imputes all missing values. These settings can be changed if needed -->

<!-- Checking our imputation -->
<!-- MICE includes a specific graphics package which is set up to work well with imputed data. We can use this to see how reasonable our imputations are. -->

<!-- We can show box and whisker plots to see if our nurmerical imputed data looks reasonable with -->

<!-- bwplot(imputed_dataset, variable) #R automatically colour codes the raw and imputed datasets differently -->

<!-- bwplot(imputed_scabies_data, age) -->


<!-- We can see distributions also with a density plot -->

<!-- densityplot(imputed_dataset) #R automatically colour codes the raw and imputed datasets differently -->

<!-- We can see if association still look reasonable -->

<!-- xyplot(imputer_dataset,variable_a~variable_b) #R automatically colour codes the raw and imputed datasets differently -->

<!-- Seeing an imputed dataset -->
<!-- We can see an individual imputed dataset with -->

<!-- imputed_set_1 <- complete(imputed_data,1) -->

<!-- We can merge the datasets together - for example into a long dataset -->

<!-- imputed_long <- complete(imputed_data, “long”) -->

<!-- If we want to include the original dataset we can do that too -->

<!-- imputed_long <- complete(imputed_data, “long”, include = TRUE) -->

<!-- Manipulating an imputed dataset -->
<!-- If we want to apply a data-manipulation such as converting a continuous to a categorical variable or making a new variable we can do this across our datasets. -->

<!-- First of all we convert the imputed data into a standard-dataset that we can work with: -->

<!-- imputed_long <- mice::complete(imputed_data, “long”, include = TRUE) #We have to specify mice::complete as ‘tidyr’ also has function called complete, so this lets R know which version to use. #Its important we include the Raw data as this lets us put the data back in to an ‘imputed’ dataset for the purposes of analysis #Now we can perform whatever manipulations we want #Then we convert the manipulated dataset back to an ‘imputed’ dataset using “as.mids” -->

<!-- coded_imputed_data <- as.mids(imputed_long) -->

<!-- complete_scabies_data <- mice::complete(imputed_scabies_data,"long", include = TRUE) -->
<!-- #Take out imputed data and make a long data-table. Include the original dataset with missing values. -->


<!-- complete_scabies_data$agegroups <- as.factor(cut(complete_scabies_data$age, c(0,10,20,Inf), include.lowest = TRUE, labels = FALSE))  -->
<!-- #Categorise age into groups across all our imputed datasets -->
<!-- #Labels == False asigns a numeric value to each category (i.e 1,2,3) -->

<!-- complete_scabies_data$agegroups <-relevel(complete_scabies_data$agegroups, ref = 3) -->
<!-- #Set the baseline value -->

<!-- coded_imputed_scabies <- as.mids(complete_scabies_data) -->
<!-- #Convert back to "Imputed" format -->
<!-- Fitting a model across all the imputed datasets -->
<!-- If we want to fit a model across all the different imputed datasets then we can do that! The general syntax is very similar to a normal logistic regression but with a few changes as we are working with multiple imputed datasets and then pooling the results. -->

<!-- imputed_model <- with(imputed_data,glm(attack~ smokes+female+hsgrad, family = binomial())) #We specify the ‘with’ command to say we are modelling across each of our imputed datasets -->

<!-- imputed_model_summary <- (summary(pool(imputed_model))) #We specify the ’pool’command to now pool the estimates across each of our imputed datasets -->

<!-- imputed_model_OR <- exp(cbind(imputed_model_summary[,“est”],imputed_model_summary[,“lo 95”],imputed_model_summary[,“hi 95”])) #exponentiate the coefficient and the lower and upper 95% CI -->

<!-- imputed_model_summary <- (cbind(imputed_model_OR,imputed_model_summary)) #Combine into a single table -->

<!-- imputed_model_summary Show the summary -->

<!-- #Imputed Data -->
<!-- imputed_scabies_model <- with(coded_imputed_scabies,glm(impetigo_active~ factor(scabies_infestation)+factor(gender)+factor(agegroups), family = binomial())) -->
<!-- imputed_scabies_model_summary <- (summary(pool(imputed_scabies_model))) -->
<!-- imputed_scabies_model_OR <- exp(cbind(imputed_scabies_model_summary[,"est"],imputed_scabies_model_summary[,"lo 95"],imputed_scabies_model_summary[,"hi 95"])) -->
<!-- imputed_scabies_model_summary <- (cbind(imputed_scabies_model_OR,imputed_scabies_model_summary)) -->

<!-- #Let us also fit this same model to the full-original dataset -->
<!-- impetigorisk <- glm(impetigo_active~factor(scabies_infestation)+factor(gender)+factor(agegroups),data=scabies,family=binomial()) -->
<!-- impetigorisk_OR <- exp(cbind(OR= coef(impetigorisk), confint(impetigorisk))) -->
<!-- ## Waiting for profiling to be done... -->
<!-- impetigorisk_summary <- summary(impetigorisk) -->
<!-- impetigorisk_summary <- cbind(impetigorisk_OR, impetigorisk_summary$coefficients) -->
<!-- You can see we get very similar estimates of risk factors for impetigo from both our imputed dataset and our original dataset -->

<!-- imputed_scabies_model_summary -->
<!-- ##                                                                    est -->
<!-- ## (Intercept)                  0.1276546 0.09976431 0.1633419 -2.0584271 -->
<!-- ## factor(scabies_infestation)1 1.8930284 1.45590022 2.4614026  0.6381779 -->
<!-- ## factor(gender)male           1.7632437 1.40651386 2.2104499  0.5671551 -->
<!-- ## factor(agegroups)1           3.9041378 2.96900602 5.1338031  1.3620370 -->
<!-- ## factor(agegroups)2           2.9657803 2.16356394 4.0654461  1.0871402 -->
<!-- ##                                     se          t        df     Pr(>|t|) -->
<!-- ## (Intercept)                  0.1255421 -16.396309  649.1702 0.000000e+00 -->
<!-- ## factor(scabies_infestation)1 0.1333104   4.787156  250.2391 2.896359e-06 -->
<!-- ## factor(gender)male           0.1145406   4.951566  176.9970 1.708353e-06 -->
<!-- ## factor(agegroups)1           0.1396040   9.756431 1736.0393 0.000000e+00 -->
<!-- ## factor(agegroups)2           0.1606484   6.767200  736.6786 2.679634e-11 -->
<!-- ##                                   lo 95      hi 95 nmis        fmi -->
<!-- ## (Intercept)                  -2.3049448 -1.8119095   NA 0.06546130 -->
<!-- ## factor(scabies_infestation)1  0.3756244  0.9007314   NA 0.12360737 -->
<!-- ## factor(gender)male            0.3411142  0.7931961   NA 0.15147441 -->
<!-- ## factor(agegroups)1            1.0882272  1.6358467   NA 0.01424917 -->
<!-- ## factor(agegroups)2            0.7717568  1.4025235   NA 0.05911479 -->
<!-- ##                                  lambda -->
<!-- ## (Intercept)                  0.06258655 -->
<!-- ## factor(scabies_infestation)1 0.11663081 -->
<!-- ## factor(gender)male           0.14194025 -->
<!-- ## factor(agegroups)1           0.01311419 -->
<!-- ## factor(agegroups)2           0.05656385 -->
<!-- #Show the result of logistic regression on the imputed dataset -->
<!-- impetigorisk_summary -->
<!-- ##                                       OR     2.5 %    97.5 %   Estimate -->
<!-- ## (Intercept)                    0.1345342 0.1057763 0.1691508 -2.0059369 -->
<!-- ## factor(scabies_infestation)yes 1.9325497 1.5143192 2.4661511  0.6588402 -->
<!-- ## factor(gender)male             1.7577258 1.4287526 2.1630912  0.5640208 -->
<!-- ## factor(agegroups)0-10          3.7025777 2.8401712 4.8620068  1.3090292 -->
<!-- ## factor(agegroups)11-20         2.7347978 2.0226139 3.7127619  1.0060575 -->
<!-- ##                                Std. Error    z value     Pr(>|z|) -->
<!-- ## (Intercept)                     0.1196454 -16.765690 4.349338e-63 -->
<!-- ## factor(scabies_infestation)yes  0.1243502   5.298267 1.169072e-07 -->
<!-- ## factor(gender)male              0.1057719   5.332426 9.690930e-08 -->
<!-- ## factor(agegroups)0-10           0.1370214   9.553466 1.254279e-21 -->
<!-- ## factor(agegroups)11-20          0.1548162   6.498398 8.117993e-11 -->
<!-- #Show the result of logistic regression on the original dataset -->
<!-- Comparing two imputed datasets -->
<!-- When we want to compate two nested models derived from imputed datasets we can do so using ‘pool.compare’ -->

<!-- pool.compare(model_1, model_2, data = NULL, method = “Wald”) #This is similar to performing an LRT on two normal models - but in the circumstance of pooled data this approach should be used. -->

<!-- Intra-class-correlation Co-efficients and Design Effects -->
<!-- Suggested and with help from “Peter MacPherson” and “Daniel Parker” -->

<!-- Statisitcally there are lots of different ways to calculate an Intra-Class-Correlation coefficient. -->

<!-- Below I provide an outline of a few otions. Which one is appropriate may depend on why you need an ICC. -->

<!-- Survey Data and Design Effects -->
<!-- If we have used a complex survey design then we can declare the survey design and then calculate the design effect. -->

<!-- We declare the survey design using > survey_dataset <- svydesign(id=~cluster_var,data=dataset) -->

<!-- We can then calculate the frequency of our outcome (say presence of Trachoma) adjusting for clustering and then get out the design effect of our data compared to SRS with replacement. >svymean(~outcome,survey_dataset,proportion = TRUE, deff = “replace”) -->

<!-- #Lets look at our scabies survey again -->
<!-- scabies <- read.csv(file = "S1-Dataset_CSV.csv", header = TRUE, sep = ",") -->

<!-- #Declare the survey design -->
<!-- survey_scabies <- svydesign(id=~Village, data = scabies) -->
<!-- ## Warning in svydesign.default(id = ~Village, data = scabies): No weights or -->
<!-- ## probabilities supplied, assuming equal probability -->
<!-- svymean(~scabies_infestation,survey_scabies,proportion = TRUE, deff = "replace") -->
<!-- ##                            mean       SE   DEff -->
<!-- ## scabies_infestationno  0.808176 0.010242 1.2903 -->
<!-- ## scabies_infestationyes 0.191824 0.010242 1.2903 -->
<!-- #Work out the DF of scabies adjusting for clustering at the village level -->
<!-- Survey Data and ICC -->
<!-- If we have used a complex survey design then we can also get out an ICC and then given the cluster size calculate the design effect. -->

<!-- The general syntax is > ICCest(group_var,outcome_var,data=dataset) -->

<!-- It expects outcome_var to be coded numerically -->

<!-- scabies$hasscabies <- 0 -->
<!-- scabies$hasscabies[scabies$scabies_infestation == "yes"] <- 1 -->
<!-- #Convert our yes/no variable to 0 / 1 variable as expected by ICCest -->

<!-- ICCest(Village,hasscabies, data = scabies) -->
<!-- ## Warning in ICCest(Village, hasscabies, data = scabies): 'x' has been -->
<!-- ## coerced to a factor -->
<!-- ## $ICC -->
<!-- ## [1] 0.004087404 -->
<!-- ##  -->
<!-- ## $LowerCI -->
<!-- ## [1] -0.0009097985 -->
<!-- ##  -->
<!-- ## $UpperCI -->
<!-- ## [1] 0.02560099 -->
<!-- ##  -->
<!-- ## $N -->
<!-- ## [1] 10 -->
<!-- ##  -->
<!-- ## $k -->
<!-- ## [1] 185.7484 -->
<!-- ##  -->
<!-- ## $varw -->
<!-- ## [1] 0.1545527 -->
<!-- ##  -->
<!-- ## $vara -->
<!-- ## [1] 0.0006343122 -->
<!-- #Gives the ICC for scabies and the mean village size. -->
<!-- #ICC of 0.004087404 -->
<!-- #K (average cluster size) 185 -->
<!-- #So DF = 1 + (185-1)*0.004087404 -->
<!-- 1 + (185-1)*0.004087404 -->
<!-- ## [1] 1.752082 -->
<!-- #Which is broadly similar to the output of the previous estimate -->
<!-- Getting an ICC from a Random-Effects Model -->
<!-- Alternatively in some situations you might want to fit a random-effects model and then calculate the ICC from the model. -->

<!-- The command for this is just: > icc (re_model) -->

<!-- re_village_scabies <- glmer(scabies_infestation~ + (1 | Village),family = "binomial", data = scabies, nAGQ = 10) -->
<!-- #Run a model clustered at household level. Ten intgration points. -->
<!-- icc(re_village_scabies) -->
<!-- ##  -->
<!-- ## Generalized linear mixed model -->
<!-- ##  Family: binomial (logit) -->
<!-- ## Formula: scabies_infestation ~ +(1 | Village) -->
<!-- ##  -->
<!-- ##   ICC (Village): 0.003350 -->
<!-- #Get out our ICC -->
<!-- #You can then use the ICC to calculate the design effect using standard formulas -->
<!-- Meta-analysis -->
<!-- ##Meta-analysis of proportions -->

<!-- The general syntax is > metaprop(Number_Events, Number_Observations) -->

<!-- metaprop(c(267,175,219,117),c(406,329,223,137),method="GLMM") -->
<!-- ##   proportion           95%-CI %W(fixed) %W(random) -->
<!-- ## 1     0.6576 [0.6092; 0.7037]        --         -- -->
<!-- ## 2     0.5319 [0.4764; 0.5869]        --         -- -->
<!-- ## 3     0.9821 [0.9547; 0.9951]        --         -- -->
<!-- ## 4     0.8540 [0.7836; 0.9085]        --         -- -->
<!-- ##  -->
<!-- ## Number of studies combined: k = 4 -->
<!-- ##  -->
<!-- ##                      proportion           95%-CI  z p-value -->
<!-- ## Fixed effect model       0.7105 [0.6829; 0.7366] --      -- -->
<!-- ## Random effects model     0.8330 [0.5400; 0.9550] --      -- -->
<!-- ##  -->
<!-- ## Quantifying heterogeneity: -->
<!-- ## tau^2 = 2.1010; H = 9.04; I^2 = 98.8% -->
<!-- ##  -->
<!-- ## Test of heterogeneity: -->
<!-- ##       Q d.f.  p-value             Test -->
<!-- ##   87.24    3 < 0.0001        Wald-type -->
<!-- ##  187.20    3 < 0.0001 Likelihood-Ratio -->
<!-- ##  -->
<!-- ## Details on meta-analytical method: -->
<!-- ## - Random intercept logistic regression model -->
<!-- ## - Maximum-likelihood estimator for tau^2 -->
<!-- ## - Logit transformation -->
<!-- ## - Clopper-Pearson confidence interval for individual studies -->
<!-- #Calculate the weighted proportion of events across four studies -->
<!-- #Method = GLMM specifies that a random-intercept should be used -->
<!-- meta_data <- metaprop(c(267,175,219,117),c(406,329,223,137),method="GLMM", comb.fixed = FALSE) -->
<!-- #As above but don't give a fixed-effec estimate and store the results so we can use them in our plot -->

<!-- meta_forest <- forest(meta_data) -->


<!-- #Calculate the weighted proportion of events across four studies -->
<!-- #Method = GLMM specifies that a random-intercept should be used -->
<!-- GGPLOT2 -->
<!-- Basic elements of a GGPLOT -->
<!-- The GGPLOT package makes much better graphs than the basic R package. This is a very brief introduction to the syntax of GGPLOT2. -->

<!-- Everything in ggplot combines a similar group of elements to make its graphs. In any plot we specify 1) Dataset 2) A Co-ordinate system 3) How to show the data points (ggplot calls these ‘geoms’) -->

<!-- We add elements on top of one another to build up the plot. The first two elements are given together via the syntax > plot <- ggplot2 (dataset, aes(co-ordinate system)) #Make something called plot. Take data from ‘dataset’ and define the way to show the data as statd by the co-ordinate system. -->

<!-- There are a variety of co-ordinate systems. When our plot involves a single variable (i.e age as a histograme) then we need to specify X = -->

<!-- plot <- ggplot2 (scabies, aes(x = age)) #Look in the scabies dataframe - we are plotting age along the X-Axis -->

<!-- If we want to stratify our plots by a factor we can do so by telling GGPLOT with ‘fill’ -->

<!-- plot <- ggplot2 (scabies, aes(x = age, fill = gender)) #Stratify by Gender - i.e draw two histograms with different colours based on the variable gender -->

<!-- When our plot involves two variables (i.e a scatter plot or a box-plot) then we specify X= and y = -->

<!-- plot <- ggplot2 (scabies, aes(x = house_inhabitants, y = room_cohabitation)) #Look in the scabies dataframe - we are plotting number people in the house along the X-Axis and number of people sharing a room on the Y-Axis -->

<!-- Specifcying the type of graph - ‘geoms’ -->
<!-- We then tell GGPLOT how to render the data - this is the geom. -->

<!-- Some common geoms are: geom_point = scatterplot geom_histogram = histogram geom_boxplot -->

<!-- We can specify specific values for the data-points (“geom”) like colour: > + geom_point(aes(colour = factor(gender))) #Add to our plot by drawing each value as a point i.e make a scatter-plot #Set the colour of each datapoint based on the factor variable gender -->

<!-- Labels, Legends etc -->
<!-- We can add labels using: > + labs (title = “Graph Title”, x = “X-Axis Label”, y = “Y-Axis Label”) #Add to our plot by adding labels for axis etc -->

<!-- Splitting GGPLOT code across multiple lines -->
<!-- Sometimes to make the code easier to follow we split it across several lines. For example -->

<!-- plot <- ggplot2 (scabies, aes(x = house_inhabitants, y = room_cohabitation))+ geom_point(aes(colour = factor(gender))) + labs (title = “House size and Room Cohabitation”, x = “House size”, y = “Room size”) #Note we put a PLUS sign at the end of each line to tell R that the code is continuing -->

<!-- Example Plots with GGPLOT -->
<!-- Here are some examples similar to the graphs we drew with basic R earlier. The aim is just to give a feel for how the plots are constructed. -->

<!-- GGPLOT Scatterplot -->
<!-- gg_scatter <- ggplot (scabies, aes(x = house_inhabitants, y = room_cohabitation)) + -->
<!-- #Two variable House size and room cohabitation -->

<!-- #NB I'm saving the plot under the name gg_scatter by saying gg_scatter <- X,Y,Z -->
<!--   geom_point(aes(colour = factor(gender))) + -->

<!-- #I'm drawing a scatter plot and colouring dots based on gender   -->
<!--   labs(title = "House size vs Number people sharing a room", x ="House Size", y = "People sharing a room") -->

<!-- #I'm giving some labels to the graph -->

<!-- gg_scatter -->


<!-- #Show the gg_scatter graph -->
<!-- GGPLOT Histogram -->
<!-- gg_histo <- ggplot (scabies, aes(x = age)) + -->

<!-- #I have a single variable age -->
<!-- geom_histogram() + -->

<!-- #With which I will draw a scatterplot   -->
<!-- labs(title = "Distribution of age in study", y = "No. participants", x = "Age (yrs)") -->

<!-- #I will give some axis and graph labels -->
<!-- gg_histo -->
<!-- ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. -->


<!-- GGPLOT Boxplot -->
<!-- gg_boxplot <- ggplot (scabies, aes(x = gender, y = age)) + -->

<!-- #I have two variable Gender and Age -->
<!--   geom_boxplot(aes(colour = factor(gender))) + -->

<!-- #I am drawing a box plot and colouring them in by age -->
<!--   labs(title = "Distribution of age by gender", y = "Age", x = "Gender") -->

<!-- #I have some axis labels -->
<!-- gg_boxplot -->


<!-- Examples of powerful plotting with GGPLOT -->
<!-- We can do really fancy things with GGPLOT like make multiple graphs in a single panel. We declare these using one of the “facet” options. For example we could draw age distribution histograms by village. -->

<!-- library(ggplot2) -->
<!-- gg_histo_by_village <- ggplot (scabies, aes(x = age, fill = gender)) + -->

<!-- #I have only one co-ordinate x = age -->
<!-- #Stratifying on = gender; i.e fill in  colour based on  value in gender -->
<!--   geom_histogram(breaks = seq(0,80, by = 2)) + -->

<!-- #I'm plotting a histogram. I've divided the data between 0 to 80 it to blocks 5 years wide. -->
<!--   labs(title = "Distribution of age in study", y = "No. participants", x = "Age (yrs)")+ -->

<!-- #I've given some labels -->
<!--   facet_wrap("Village") -->

<!-- #I've broken the data down in to several facets based on the village -->
<!-- gg_histo_by_village -->


# tips

```{r eval=FALSE, include=FALSE, echo=TRUE}
my_string1 <- "3+4"
my_string2 <- "plot(cars)"
eval(parse(text = my_string1))
eval(parse(text = my_string2))

```


# environment memory

http://r-statistics.co/R-Tutorial.html

As you create new variables, by default they get store in what is called a global environment.

a <- 10
b <- 20
ls()  # list objects in global env
rm(a)  # delete the object 'a'
rm(list = ls())  # caution: delete all objects in .GlobalEnv
gc()  # free system memory

However if you choose, you can create a new environment and store them there.

rm(list=ls())  # remove all objects in work space
env1 <- new.env()  # create a new environment
assign("a", 3, envir = env1)  # store a=3 inside env1
ls()  # returns objects in .GlobalEnv
ls(env1)  # returns objects in env1
get('a', envir=env1)  # retrieve value from env1



sort(vec1)  # ascending sort
sort(vec1, decreasing = TRUE)  # Descending sort 
Sorting can also be achieved using the order() function which returns the indices of elements in ascending order.

vec1[order(vec1)]  # ascending sort
vec1[rev(order(vec1))]  # descending sort



seq(1, 10, by = 2)  # diff between adj elements is 2
seq(1, 10, length=25)  # length of the vector is 25
rep(1, 5)  # repeat 1, five times.
rep(1:3, 5)  # repeat 1:3, 5 times
rep(1:3, each=5)  # repeat 1 to 3, each 5 times.




subset(airquality, Day == 1, select = -Temp)  # select Day=1 and exclude 'Temp'
airquality[which(airquality$Day==1), -c(4)]  # same as above



set.seed(100)
trainIndex <- sample(c(1:nrow(airquality)), size=nrow(airquality)*0.7, replace=F)  # get test sample indices
airquality[trainIndex, ]  # training data
airquality[-trainIndex, ]  # test data



if(checkConditionIfTrue) {
  ....statements..
  ....statements..
} else {   # place the 'else' in same line as '}'
  ....statements..
  ....statements..
} 





for(counterVar in c(1:n)){
  .... statements..
}













































<!--chapter:end:codes.Rmd-->

---
title: "My R Codes For Data Analysis"
subtitle: "In this repository I am going to collect `R codes` for data analysis. Codes are from various resources and I try to give original link as much as possible."
author: "[Serdar Balcı, MD, Pathologist](https://www.serdarbalci.com/)"
date: '`{r #  format(Sys.Date())`'
---


### Compare Means

```{r eval=FALSE, include=FALSE, echo=TRUE}
t.test(scabies$age[scabies$gender=="male"],scabies$age[scabies$gender=="female"])

```

```{r eval=FALSE, include=FALSE, echo=TRUE}
test <- t.test(scabies$age[scabies$gender=="male"],scabies$age[scabies$gender=="female"])
psycho::analyze(test)
```


# infer

Randomization Examples using nycflights13 flights data

https://cran.r-project.org/web/packages/infer/vignettes/flights_examples.html


```{r eval=FALSE, include=FALSE, echo=TRUE}
library(nycflights13)
library(dplyr)
library(ggplot2)
library(stringr)
library(infer)
set.seed(2017)
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
fli_small <- flights %>% 
  na.omit() %>%
  sample_n(size = 500) %>% 
  mutate(season = case_when(
    month %in% c(10:12, 1:3) ~ "winter",
    month %in% c(4:9) ~ "summer"
  )) %>% 
  mutate(day_hour = case_when(
    between(hour, 1, 12) ~ "morning",
    between(hour, 13, 24) ~ "not morning"
  )) %>% 
  select(arr_delay, dep_delay, season, 
         day_hour, origin, carrier)
fli_small
```


Hypothesis tests
One numerical variable (mean)


```{r eval=FALSE, include=FALSE, echo=TRUE}
x_bar <- fli_small %>%
  summarize(mean(dep_delay)) %>%
  pull()
x_bar
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
null_distn <- fli_small %>%
  specify(response = dep_delay) %>%
  hypothesize(null = "point", mu = 10) %>%
  generate(reps = 1000, type = "bootstrap") %>%
  calculate(stat = "mean")
null_distn
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
ggplot(data = null_distn, mapping = aes(x = stat)) +
  geom_density() +
  geom_vline(xintercept = x_bar, color = "red")
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
null_distn %>%
  summarize(p_value = mean(stat >= x_bar) * 2)
```


One numerical variable (median)

```{r eval=FALSE, include=FALSE, echo=TRUE}
x_tilde <- fli_small %>%
  summarize(median(dep_delay)) %>%
  pull()
x_tilde
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
null_distn <- fli_small %>%
  specify(response = dep_delay) %>%
  hypothesize(null = "point", med = -1) %>% 
  generate(reps = 1000, type = "bootstrap") %>% 
  calculate(stat = "median")
null_distn
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
ggplot(null_distn, aes(x = stat)) +
  geom_bar() +
  geom_vline(xintercept = x_tilde, color = "red")
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
null_distn %>%
  summarize(p_value = mean(stat <= x_tilde) * 2)
```



One categorical (one proportion)


```{r eval=FALSE, include=FALSE, echo=TRUE}
p_hat <- fli_small %>%
  summarize(mean(day_hour == "morning")) %>%
  pull()
p_hat
```





```
null_distn <- fli_small %>%
  specify(response = day_hour, success = "morning") %>%
  hypothesize(null = "point", p = .5) %>%
  generate(reps = 1000, type = "simulate") %>%
  calculate(stat = "prop")
ggplot(null_distn, aes(x = stat)) +
  geom_bar() +
  geom_vline(xintercept = p_hat, color = "red")

null_distn %>%
  summarize(p_value = mean(stat <= p_hat) * 2)
p_value
0.132
Logical variables will be coerced to factors:

null_distn <- fli_small %>%
  mutate(day_hour_logical = (day_hour == "morning")) %>%
  specify(response = day_hour_logical, success = "TRUE") %>%
  hypothesize(null = "point", p = .5) %>%
  generate(reps = 1000, type = "simulate") %>%
  calculate(stat = "prop")
Two categorical (2 level) variables
d_hat <- fli_small %>%
  group_by(season) %>%
  summarize(prop = mean(day_hour == "morning")) %>%
  summarize(diff(prop)) %>%
  pull()
null_distn <- fli_small %>%
  specify(day_hour ~ season, success = "morning") %>%
  hypothesize(null = "independence") %>% 
  generate(reps = 1000, type = "permute") %>% 
  calculate(stat = "diff in props", order = c("winter", "summer"))
ggplot(null_distn, aes(x = stat)) +
  geom_density() +
  geom_vline(xintercept = d_hat, color = "red")

null_distn %>%
  summarize(p_value = mean(stat <= d_hat) * 2) %>%
  pull()
## [1] 0.758
One categorical (>2 level) - GoF
Chisq_hat <- fli_small %>%
  specify(response = origin) %>%
  hypothesize(null = "point", 
              p = c("EWR" = .33, "JFK" = .33, "LGA" = .34)) %>% 
  calculate(stat = "Chisq")
null_distn <- fli_small %>%
  specify(response = origin) %>%
  hypothesize(null = "point", 
              p = c("EWR" = .33, "JFK" = .33, "LGA" = .34)) %>% 
  generate(reps = 1000, type = "simulate") %>% 
  calculate(stat = "Chisq")
ggplot(null_distn, aes(x = stat)) +
  geom_density() +
  geom_vline(xintercept = pull(Chisq_hat), color = "red")

null_distn %>%
  summarize(p_value = mean(stat >= pull(Chisq_hat))) %>%
  pull()
## [1] 0.002
Two categorical (>2 level) variables
Chisq_hat <- fli_small %>%
  chisq_stat(formula = day_hour ~ origin)
null_distn <- fli_small %>%
  specify(day_hour ~ origin, success = "morning") %>%
  hypothesize(null = "independence") %>% 
  generate(reps = 1000, type = "permute") %>% 
  calculate(stat = "Chisq")
ggplot(null_distn, aes(x = stat)) +
  geom_density() +
  geom_vline(xintercept = pull(Chisq_hat), color = "red")

null_distn %>%
  summarize(p_value = mean(stat >= pull(Chisq_hat))) %>%
  pull()
## [1] 0.017
One numerical variable, one categorical (2 levels) (diff in means)
d_hat <- fli_small %>% 
  group_by(season) %>% 
  summarize(mean_stat = mean(dep_delay)) %>% 
  # Since summer - winter
  summarize(-diff(mean_stat)) %>% 
  pull()
null_distn <- fli_small %>%
  specify(dep_delay ~ season) %>% # alt: response = dep_delay, 
  # explanatory = season
  hypothesize(null = "independence") %>%
  generate(reps = 1000, type = "permute") %>%
  calculate(stat = "diff in means", order = c("summer", "winter"))
ggplot(null_distn, aes(x = stat)) +
  geom_density() +
  geom_vline(xintercept = d_hat, color = "red")

null_distn %>%
  summarize(p_value = mean(stat <= d_hat) * 2) %>%
  pull()
## [1] 1.574
One numerical variable, one categorical (2 levels) (diff in medians)
d_hat <- fli_small %>% 
  group_by(season) %>% 
  summarize(median_stat = median(dep_delay)) %>% 
  # Since summer - winter
  summarize(-diff(median_stat)) %>% 
  pull()
null_distn <- fli_small %>%
  specify(dep_delay ~ season) %>% # alt: response = dep_delay, 
  # explanatory = season
  hypothesize(null = "independence") %>%
  generate(reps = 1000, type = "permute") %>%
  calculate(stat = "diff in medians", order = c("summer", "winter"))
ggplot(null_distn, aes(x = stat)) +
  geom_bar() +
  geom_vline(xintercept = d_hat, color = "red")

null_distn %>%
  summarize(p_value = mean(stat >= d_hat) * 2) %>%
  pull()
## [1] 0.068
One numerical, one categorical (>2 levels) - ANOVA
F_hat <- anova(
               aov(formula = arr_delay ~ origin, data = fli_small)
               )$`F value`[1]
null_distn <- fli_small %>%
   specify(arr_delay ~ origin) %>% # alt: response = arr_delay, 
   # explanatory = origin
   hypothesize(null = "independence") %>%
   generate(reps = 1000, type = "permute") %>%
   calculate(stat = "F")
ggplot(null_distn, aes(x = stat)) +
  geom_density() +
  geom_vline(xintercept = F_hat, color = "red")  

null_distn %>% 
  summarize(p_value = mean(stat >= F_hat)) %>%
  pull()
## [1] 0.351
Two numerical vars - SLR
slope_hat <- lm(arr_delay ~ dep_delay, data = fli_small) %>% 
  broom::tidy() %>% 
  filter(term == "dep_delay") %>% 
  pull(estimate)
null_distn <- fli_small %>%
   specify(arr_delay ~ dep_delay) %>% 
   hypothesize(null = "independence") %>%
   generate(reps = 1000, type = "permute") %>%
   calculate(stat = "slope")
ggplot(null_distn, aes(x = stat)) +
  geom_density() +
  geom_vline(xintercept = slope_hat, color = "red")  

null_distn %>% 
  summarize(p_value = mean(stat >= slope_hat) * 2) %>%
  pull()
## [1] 0
Confidence intervals
One numerical (one mean)
x_bar <- fli_small %>%
   summarize(mean(arr_delay)) %>%
   pull()
boot <- fli_small %>%
   specify(response = arr_delay) %>%
   generate(reps = 1000, type = "bootstrap") %>%
   calculate(stat = "mean") %>%
   pull()
c(lower = x_bar - 2 * sd(boot),
  upper = x_bar + 2 * sd(boot))
##    lower    upper 
## 1.122209 8.021791
One categorical (one proportion)
p_hat <- fli_small %>%
 summarize(mean(day_hour == "morning")) %>%
 pull()
boot <- fli_small %>%
 specify(response = day_hour, success = "morning") %>%
 generate(reps = 1000, type = "bootstrap") %>%
 calculate(stat = "prop") %>%
 pull()
c(lower = p_hat - 2 * sd(boot),
 upper = p_hat + 2 * sd(boot))
##     lower     upper 
## 0.4194756 0.5125244
One numerical variable, one categorical (2 levels) (diff in means)
d_hat <- fli_small %>% 
  group_by(season) %>% 
  summarize(mean_stat = mean(arr_delay)) %>% 
  # Since summer - winter
  summarize(-diff(mean_stat)) %>% 
  pull()
boot <- fli_small %>%
   specify(arr_delay ~ season) %>%
   generate(reps = 1000, type = "bootstrap") %>%
   calculate(stat = "diff in means", order = c("summer", "winter")) %>% 
   pull()
c(lower = d_hat - 2 * sd(boot), 
  upper = d_hat + 2 * sd(boot))
##     lower     upper 
## -7.704370  6.213971
Two categorical variables (diff in proportions)
d_hat <- fli_small %>%
  group_by(season) %>%
  summarize(prop = mean(day_hour == "morning")) %>%
  # Since summer - winter
  summarize(-diff(prop)) %>%
  pull()
boot <- fli_small %>%
  specify(day_hour ~ season, success = "morning") %>%
  generate(reps = 1000, type = "bootstrap") %>% 
  calculate(stat = "diff in props", order = c("summer", "winter")) %>%
  pull()
c(lower = d_hat - 2 * sd(boot), 
  upper = d_hat + 2 * sd(boot))
##       lower       upper 
## -0.07149487  0.11258550
Two numerical vars - SLR
slope_hat <- lm(arr_delay ~ dep_delay, data = fli_small) %>% 
  broom::tidy() %>% 
  filter(term == "dep_delay") %>% 
  pull(estimate)
boot <- fli_small %>%
   specify(arr_delay ~ dep_delay) %>% 
   generate(reps = 1000, type = "bootstrap") %>%
   calculate(stat = "slope") %>% 
   pull()
c(lower = slope_hat - 2 * sd(boot), 
  upper = slope_hat + 2 * sd(boot))   
##     lower     upper 
## 0.9657595 1.0681384



---


Examples using mtcars data


https://cran.r-project.org/web/packages/infer/vignettes/mtcars_examples.html


Examples using mtcars data
Chester Ismay and Andrew Bray
2018-01-05
Note: The type argument in generate() is automatically filled based on the entries for specify() and hypothesize(). It can be removed throughout the examples that follow. It is left in to reiterate the type of generation process being performed.

Data preparation
library(infer)
library(dplyr)
mtcars <- mtcars %>%
  mutate(cyl = factor(cyl),
         vs = factor(vs),
         am = factor(am),
         gear = factor(gear),
         carb = factor(carb))
# For reproducibility         
set.seed(2018)         
One numerical variable (mean)

mtcars %>%
  specify(response = mpg) %>% # formula alt: mpg ~ NULL
  hypothesize(null = "point", mu = 25) %>% 
  generate(reps = 100, type = "bootstrap") %>% 
  calculate(stat = "mean")
## # A tibble: 100 x 2
##    replicate  stat
##        <int> <dbl>
##  1         1  26.6
##  2         2  25.1
##  3         3  25.2
##  4         4  24.7
##  5         5  24.6
##  6         6  25.8
##  7         7  24.7
##  8         8  25.6
##  9         9  25.0
## 10        10  25.1
## # ... with 90 more rows
One numerical variable (median)

mtcars %>%
  specify(response = mpg) %>% # formula alt: mpg ~ NULL
  hypothesize(null = "point", med = 26) %>% 
  generate(reps = 100, type = "bootstrap") %>% 
  calculate(stat = "median")
## # A tibble: 100 x 2
##    replicate  stat
##        <int> <dbl>
##  1         1  28.2
##  2         2  27.2
##  3         3  26.2
##  4         4  26  
##  5         5  26.5
##  6         6  24.5
##  7         7  26  
##  8         8  28.2
##  9         9  28.2
## 10        10  23.2
## # ... with 90 more rows
One categorical (2 level) variable

mtcars %>%
  specify(response = am, success = "1") %>% # formula alt: am ~ NULL
  hypothesize(null = "point", p = .25) %>% 
  generate(reps = 100, type = "simulate") %>% 
  calculate(stat = "prop")
## # A tibble: 100 x 2
##    replicate   stat
##    <fct>      <dbl>
##  1 1         0.375 
##  2 2         0.0625
##  3 3         0.125 
##  4 4         0.25  
##  5 5         0.188 
##  6 6         0.406 
##  7 7         0.219 
##  8 8         0.375 
##  9 9         0.344 
## 10 10        0.188 
## # ... with 90 more rows
Two categorical (2 level) variables

mtcars %>%
  specify(am ~ vs, success = "1") %>% # alt: response = am, explanatory = vs
  hypothesize(null = "independence") %>%
  generate(reps = 100, type = "permute") %>%
  calculate(stat = "diff in props", order = c("0", "1"))
## # A tibble: 100 x 2
##    replicate    stat
##        <int>   <dbl>
##  1         1 -0.421 
##  2         2 -0.167 
##  3         3 -0.421 
##  4         4 -0.0397
##  5         5  0.0873
##  6         6 -0.0397
##  7         7 -0.0397
##  8         8 -0.0397
##  9         9  0.0873
## 10        10 -0.167 
## # ... with 90 more rows
One categorical (>2 level) - GoF

mtcars %>%
  specify(cyl ~ NULL) %>% # alt: response = cyl
  hypothesize(null = "point", p = c("4" = .5, "6" = .25, "8" = .25)) %>%
  generate(reps = 100, type = "simulate") %>%
  calculate(stat = "Chisq")
## # A tibble: 100 x 2
##    replicate  stat
##    <fct>     <dbl>
##  1 1         6.75 
##  2 2         1.69 
##  3 3         3.19 
##  4 4         1.69 
##  5 5         6    
##  6 6         2.69 
##  7 7         4.75 
##  8 8         0.75 
##  9 9         0.688
## 10 10        3.69 
## # ... with 90 more rows
Two categorical (>2 level) variables

mtcars %>%
  specify(cyl ~ am) %>% # alt: response = cyl, explanatory = am
  hypothesize(null = "independence") %>%
  generate(reps = 100, type = "permute") %>%
  calculate(stat = "Chisq")
## # A tibble: 100 x 2
##    replicate  stat
##        <int> <dbl>
##  1         1 1.34 
##  2         2 1.63 
##  3         3 1.63 
##  4         4 2.63 
##  5         5 3.90 
##  6         6 1.74 
##  7         7 0.126
##  8         8 1.74 
##  9         9 1.34 
## 10        10 1.34 
## # ... with 90 more rows
One numerical variable one categorical (2 levels) (diff in means)

mtcars %>%
  specify(mpg ~ am) %>% # alt: response = mpg, explanatory = am
  hypothesize(null = "independence") %>%
  generate(reps = 100, type = "permute") %>%
  calculate(stat = "diff in means", order = c("0", "1"))
## # A tibble: 100 x 2
##    replicate   stat
##        <int>  <dbl>
##  1         1 -1.10 
##  2         2  0.217
##  3         3 -1.08 
##  4         4 -3.80 
##  5         5  3.08 
##  6         6  0.489
##  7         7  2.34 
##  8         8  4.10 
##  9         9 -1.86 
## 10        10 -0.210
## # ... with 90 more rows
One numerical variable one categorical (2 levels) (diff in medians)

mtcars %>%
  specify(mpg ~ am) %>% # alt: response = mpg, explanatory = am
  hypothesize(null = "independence") %>%
  generate(reps = 100, type = "permute") %>%
  calculate(stat = "diff in medians", order = c("0", "1"))
## # A tibble: 100 x 2
##    replicate  stat
##        <int> <dbl>
##  1         1  0.5 
##  2         2 -1.10
##  3         3  5.20
##  4         4  1.8 
##  5         5  0.5 
##  6         6  3.3 
##  7         7 -1.60
##  8         8 -2.3 
##  9         9  2.90
## 10        10 -0.5 
## # ... with 90 more rows
One numerical one categorical (>2 levels) - ANOVA

mtcars %>%
  specify(mpg ~ cyl) %>% # alt: response = mpg, explanatory = cyl
  hypothesize(null = "independence") %>%
  generate(reps = 100, type = "permute") %>%
  calculate(stat = "F")
## # A tibble: 100 x 2
##    replicate  stat
##        <int> <dbl>
##  1         1 1.43 
##  2         2 1.65 
##  3         3 0.318
##  4         4 0.393
##  5         5 1.05 
##  6         6 0.826
##  7         7 1.32 
##  8         8 0.833
##  9         9 0.144
## 10        10 0.365
## # ... with 90 more rows
Two numerical vars - SLR

mtcars %>%
  specify(mpg ~ hp) %>% # alt: response = mpg, explanatory = cyl
  hypothesize(null = "independence") %>%
  generate(reps = 100, type = "permute") %>%
  calculate(stat = "slope")
## # A tibble: 100 x 2
##    replicate     stat
##        <int>    <dbl>
##  1         1 -0.0151 
##  2         2  0.00224
##  3         3 -0.0120 
##  4         4  0.00292
##  5         5  0.0203 
##  6         6 -0.00730
##  7         7 -0.0246 
##  8         8  0.00555
##  9         9  0.0109 
## 10        10  0.0176 
## # ... with 90 more rows
One numerical variable (standard deviation)

Not currently implemented

mtcars %>%
  specify(response = mpg) %>% # formula alt: mpg ~ NULL
  hypothesize(null = "point", sigma = 5) %>% 
  generate(reps = 100, type = "bootstrap") %>% 
  calculate(stat = "sd")
Confidence intervals
One numerical (one mean)

mtcars %>%
  specify(response = mpg) %>%
  generate(reps = 100, type = "bootstrap") %>%
  calculate(stat = "mean")
## # A tibble: 100 x 2
##    replicate  stat
##        <int> <dbl>
##  1         1  19.6
##  2         2  21.8
##  3         3  18.7
##  4         4  19.2
##  5         5  21.6
##  6         6  19.9
##  7         7  20.7
##  8         8  19.3
##  9         9  21.2
## 10        10  21.3
## # ... with 90 more rows
One numerical (one median)

mtcars %>%
  specify(response = mpg) %>%
  generate(reps = 100, type = "bootstrap") %>%
  calculate(stat = "median")
## # A tibble: 100 x 2
##    replicate  stat
##        <int> <dbl>
##  1         1  19.2
##  2         2  20.1
##  3         3  21  
##  4         4  17.8
##  5         5  20.1
##  6         6  19.2
##  7         7  18.4
##  8         8  19.2
##  9         9  19.2
## 10        10  18.0
## # ... with 90 more rows
One numerical (standard deviation)

mtcars %>%
  specify(response = mpg) %>%
  generate(reps = 100, type = "bootstrap") %>%
  calculate(stat = "sd")
## # A tibble: 100 x 2
##    replicate  stat
##        <int> <dbl>
##  1         1  5.28
##  2         2  6.74
##  3         3  5.29
##  4         4  5.41
##  5         5  5.56
##  6         6  5.65
##  7         7  6.17
##  8         8  6.40
##  9         9  6.31
## 10        10  6.11
## # ... with 90 more rows
One categorical (one proportion)

mtcars %>%
  specify(response = am, success = "1") %>%
  generate(reps = 100, type = "bootstrap") %>%
  calculate(stat = "prop")
## # A tibble: 100 x 2
##    replicate  stat
##        <int> <dbl>
##  1         1 0.375
##  2         2 0.406
##  3         3 0.406
##  4         4 0.312
##  5         5 0.312
##  6         6 0.469
##  7         7 0.438
##  8         8 0.281
##  9         9 0.438
## 10        10 0.5  
## # ... with 90 more rows
One numerical variable one categorical (2 levels) (diff in means)

mtcars %>%
  specify(mpg ~ am) %>%
  generate(reps = 100, type = "bootstrap") %>%
  calculate(stat = "diff in means", order = c("0", "1"))
## # A tibble: 100 x 2
##    replicate   stat
##        <int>  <dbl>
##  1         1  -9.38
##  2         2  -5.11
##  3         3  -4.88
##  4         4  -5.39
##  5         5  -9.19
##  6         6  -7.20
##  7         7  -5.34
##  8         8  -3.20
##  9         9  -5.95
## 10        10 -11.0 
## # ... with 90 more rows
Two categorical variables (diff in proportions)

mtcars %>%
  specify(am ~ vs, success = "1") %>%
  generate(reps = 100, type = "bootstrap") %>%
  calculate(stat = "diff in props", order = c("0", "1"))
## # A tibble: 100 x 2
##    replicate   stat
##        <int>  <dbl>
##  1         1 -0.352
##  2         2 -0.15 
##  3         3 -0.294
##  4         4 -0.254
##  5         5 -0.438
##  6         6 -0.126
##  7         7 -0.188
##  8         8  0.167
##  9         9 -0.143
## 10        10 -0.5  
## # ... with 90 more rows
Two numerical vars - SLR

mtcars %>%
  specify(mpg ~ hp) %>% 
  generate(reps = 100, type = "bootstrap") %>%
  calculate(stat = "slope")
## # A tibble: 100 x 2
##    replicate    stat
##        <int>   <dbl>
##  1         1 -0.0850
##  2         2 -0.0512
##  3         3 -0.0736
##  4         4 -0.0569
##  5         5 -0.0930
##  6         6 -0.0659
##  7         7 -0.0710
##  8         8 -0.0767
##  9         9 -0.0556
## 10        10 -0.0627
## # ... with 90 more rows
Two numerical vars - correlation

mtcars %>%
  specify(mpg ~ hp) %>% 
  generate(reps = 100, type = "bootstrap") %>%
  calculate(stat = "correlation")
## # A tibble: 100 x 2
##    replicate   stat
##        <int>  <dbl>
##  1         1 -0.821
##  2         2 -0.812
##  3         3 -0.802
##  4         4 -0.723
##  5         5 -0.885
##  6         6 -0.777
##  7         7 -0.752
##  8         8 -0.758
##  9         9 -0.826
## 10        10 -0.779
## # ... with 90 more rows


---

Two sample t test example using nycflights13 flights data

https://cran.r-project.org/web/packages/infer/vignettes/two_sample_t.html


Two sample t test example using nycflights13 flights data
Chester Ismay
2018-11-15
Note: The type argument in generate() is automatically filled based on the entries for specify() and hypothesize(). It can be removed throughout the examples that follow. It is left in to reiterate the type of generation process being performed.

Data preparation
library(nycflights13)
library(dplyr)
library(stringr)
library(infer)
set.seed(2017)
fli_small <- flights %>% 
  sample_n(size = 500) %>% 
  mutate(half_year = case_when(
    between(month, 1, 6) ~ "h1",
    between(month, 7, 12) ~ "h2"
  )) %>% 
  mutate(day_hour = case_when(
    between(hour, 1, 12) ~ "morning",
    between(hour, 13, 24) ~ "not morning"
  )) %>% 
  select(arr_delay, dep_delay, half_year, 
         day_hour, origin, carrier)
Two numeric - arr_delay, dep_delay
Two categories
half_year ("h1", "h2"),
day_hour ("morning", "not morning")
Three categories - origin ("EWR", "JFK", "LGA")
Sixteen categories - carrier
One numerical variable, one categorical (2 levels)
Calculate observed statistic
The recommended approach is to use specify() %>% calculate():

obs_t <- fli_small %>%
  specify(arr_delay ~ half_year) %>%
  calculate(stat = "t", order = c("h1", "h2"))
## Warning: Removed 15 rows containing missing values.
The observed t statistic is
stat
0.8685
.

Or using t_test in infer

obs_t <- fli_small %>% 
  t_test(formula = arr_delay ~ half_year, alternative = "two_sided",
         order = c("h1", "h2")) %>% 
  dplyr::pull(statistic)
The observed t statistic is 0.8685.

Or using another shortcut function in infer:

obs_t <- fli_small %>% 
  t_stat(formula = arr_delay ~ half_year, order = c("h1", "h2"))
The observed t statistic is
statistic
0.8685
.

Randomization approach to t-statistic
t_null_perm <- fli_small %>%
  # alt: response = arr_delay, explanatory = half_year
  specify(arr_delay ~ half_year) %>%
  hypothesize(null = "independence") %>%
  generate(reps = 1000, type = "permute") %>%
  calculate(stat = "t", order = c("h1", "h2"))
## Warning: Removed 15 rows containing missing values.
visualize(t_null_perm) +
  shade_p_value(obs_stat = obs_t, direction = "two_sided")

Calculate the randomization-based p-value
t_null_perm %>% 
  get_p_value(obs_stat = obs_t, direction = "two_sided")
p_value
0.408
Theoretical distribution
t_null_theor <- fli_small %>%
  # alt: response = arr_delay, explanatory = half_year
  specify(arr_delay ~ half_year) %>%
  hypothesize(null = "independence") %>%
  # generate() ## Not used for theoretical
  calculate(stat = "t", order = c("h1", "h2"))
## Warning: Removed 15 rows containing missing values.
visualize(t_null_theor, method = "theoretical") +
  shade_p_value(obs_stat = obs_t, direction = "two_sided")
## Warning: Check to make sure the conditions have been met for the
## theoretical method. {infer} currently does not check these for you.

Overlay appropriate t distribution on top of permuted t-statistics
visualize(t_null_perm, method = "both") +
  shade_p_value(obs_stat = obs_t, direction = "two_sided")
## Warning: Check to make sure the conditions have been met for the
## theoretical method. {infer} currently does not check these for you.

Compute theoretical p-value
fli_small %>% 
  t_test(formula = arr_delay ~ half_year,
         alternative = "two_sided",
         order = c("h1", "h2")) %>% 
  dplyr::pull(p_value)
## [1] 0.3855
```




---





<!--chapter:end:CompareMeans.Rmd-->

---
title: "My R Codes For Data Analysis"
subtitle: "In this repository I am going to collect `R codes` for data analysis. Codes are from various resources and I try to give original link as much as possible."
author: "[Serdar Balcı, MD, Pathologist](https://www.serdarbalci.com/)"
date: '`{r #  format(Sys.Date())`'
---



### Compare Proportions

```{r eval=FALSE, include=FALSE, echo=TRUE}
prop.test(numerator,denominator)
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
table(impetigo = scabies$impetigo_active, scabies = scabies$scabies_infestation)
# dependent ~ independent
```

```{r eval=FALSE, include=FALSE, echo=TRUE}
#See that because 'no' is the 'base' level the table is laid out
#               No Disease      Has Disease

# Not-exposed

# Exposed

#This is dependent on how your data is coded so you need to check this before using epi.2by2
#If the table is laid out correctly then you can input straight into epi.2by2, otherwise you #need to recode or re-order the variables so that the table will be laid out correctly

#epi.2by2 wants the data with the exposed/disease group in top right corner
#So we just tell R to order the variables differently when we draw the table

epiR::epi.2by2(table(relevel(scabies$scabies_infestation,"yes"), relevel(scabies$impetigo_active,"yes")))
```








<!--chapter:end:CompareProportions.Rmd-->

---
title: "contingency tables"
---


```{r eval=FALSE, include=FALSE, echo=TRUE}
?chisq.test()
```



```
chisq.test {stats}	R Documentation
Pearson's Chi-squared Test for Count Data
Description
chisq.test performs chi-squared contingency table tests and goodness-of-fit tests.

Usage
chisq.test(x, y = NULL, correct = TRUE,
           p = rep(1/length(x), length(x)), rescale.p = FALSE,
           simulate.p.value = FALSE, B = 2000)
Arguments
x	
a numeric vector or matrix. x and y can also both be factors.

y	
a numeric vector; ignored if x is a matrix. If x is a factor, y should be a factor of the same length.

correct	
a logical indicating whether to apply continuity correction when computing the test statistic for 2 by 2 tables: one half is subtracted from all |O - E| differences; however, the correction will not be bigger than the differences themselves. No correction is done if simulate.p.value = TRUE.

p	
a vector of probabilities of the same length of x. An error is given if any entry of p is negative.

rescale.p	
a logical scalar; if TRUE then p is rescaled (if necessary) to sum to 1. If rescale.p is FALSE, and p does not sum to 1, an error is given.

simulate.p.value	
a logical indicating whether to compute p-values by Monte Carlo simulation.

B	
an integer specifying the number of replicates used in the Monte Carlo test.

Details
If x is a matrix with one row or column, or if x is a vector and y is not given, then a goodness-of-fit test is performed (x is treated as a one-dimensional contingency table). The entries of x must be non-negative integers. In this case, the hypothesis tested is whether the population probabilities equal those in p, or are all equal if p is not given.

If x is a matrix with at least two rows and columns, it is taken as a two-dimensional contingency table: the entries of x must be non-negative integers. Otherwise, x and y must be vectors or factors of the same length; cases with missing values are removed, the objects are coerced to factors, and the contingency table is computed from these. Then Pearson's chi-squared test is performed of the null hypothesis that the joint distribution of the cell counts in a 2-dimensional contingency table is the product of the row and column marginals.

If simulate.p.value is FALSE, the p-value is computed from the asymptotic chi-squared distribution of the test statistic; continuity correction is only used in the 2-by-2 case (if correct is TRUE, the default). Otherwise the p-value is computed for a Monte Carlo test (Hope, 1968) with B replicates.

In the contingency table case simulation is done by random sampling from the set of all contingency tables with given marginals, and works only if the marginals are strictly positive. Continuity correction is never used, and the statistic is quoted without it. Note that this is not the usual sampling situation assumed for the chi-squared test but rather that for Fisher's exact test.

In the goodness-of-fit case simulation is done by random sampling from the discrete distribution specified by p, each sample being of size n = sum(x). This simulation is done in R and may be slow.

Value
A list with class "htest" containing the following components:

statistic	
the value the chi-squared test statistic.

parameter	
the degrees of freedom of the approximate chi-squared distribution of the test statistic, NA if the p-value is computed by Monte Carlo simulation.

p.value	
the p-value for the test.

method	
a character string indicating the type of test performed, and whether Monte Carlo simulation or continuity correction was used.

data.name	
a character string giving the name(s) of the data.

observed	
the observed counts.

expected	
the expected counts under the null hypothesis.

residuals	
the Pearson residuals, (observed - expected) / sqrt(expected).

stdres	
standardized residuals, (observed - expected) / sqrt(V), where V is the residual cell variance (Agresti, 2007, section 2.4.5 for the case where x is a matrix, n * p * (1 - p) otherwise).

Source
The code for Monte Carlo simulation is a C translation of the Fortran algorithm of Patefield (1981).

References
Hope, A. C. A. (1968). A simplified Monte Carlo significance test procedure. Journal of the Royal Statistical Society Series B, 30, 582–598. http://www.jstor.org/stable/2984263.

Patefield, W. M. (1981). Algorithm AS 159: An efficient method of generating r x c tables with given row and column totals. Applied Statistics, 30, 91–97. doi: 10.2307/2346669.

Agresti, A. (2007). An Introduction to Categorical Data Analysis, 2nd ed. New York: John Wiley & Sons. Page 38.

See Also
For goodness-of-fit testing, notably of continuous distributions, ks.test.

Examples

## From Agresti(2007) p.39
M <- as.table(rbind(c(762, 327, 468), c(484, 239, 477)))
dimnames(M) <- list(gender = c("F", "M"),
                    party = c("Democrat","Independent", "Republican"))
(Xsq <- chisq.test(M))  # Prints test summary
Xsq$observed   # observed counts (same as M)
Xsq$expected   # expected counts under the null
Xsq$residuals  # Pearson residuals
Xsq$stdres     # standardized residuals


## Effect of simulating p-values
x <- matrix(c(12, 5, 7, 7), ncol = 2)
chisq.test(x)$p.value           # 0.4233
chisq.test(x, simulate.p.value = TRUE, B = 10000)$p.value
                                # around 0.29!

## Testing for population probabilities
## Case A. Tabulated data
x <- c(A = 20, B = 15, C = 25)
chisq.test(x)
chisq.test(as.table(x))             # the same
x <- c(89,37,30,28,2)
p <- c(40,20,20,15,5)
try(
chisq.test(x, p = p)                # gives an error
)
chisq.test(x, p = p, rescale.p = TRUE)
                                # works
p <- c(0.40,0.20,0.20,0.19,0.01)
                                # Expected count in category 5
                                # is 1.86 < 5 ==> chi square approx.
chisq.test(x, p = p)            #               maybe doubtful, but is ok!
chisq.test(x, p = p, simulate.p.value = TRUE)

## Case B. Raw data
x <- trunc(5 * runif(100))
chisq.test(table(x))            # NOT 'chisq.test(x)'!
[Package stats version 3.5.1 Index]
```



```{r eval=FALSE, include=FALSE, echo=TRUE}
observed_table <- matrix(c(35, 15, 50, 10, 30, 60), nrow = 2, ncol = 3, byrow = T)
rownames(observed_table) <- c('Female', 'Male')
colnames(observed_table) <- c('Archery', 'Boxing', 'Cycling')
observed_table
```







```{r eval=FALSE, include=FALSE, echo=TRUE}
X <- chisq.test(observed_table)
X
```




```{r eval=FALSE, include=FALSE, echo=TRUE}
X$expected
```



---

# infer

Chi-squared test example using nycflights13 flights data  
https://cran.r-project.org/web/packages/infer/vignettes/chisq_test.html  


```{r eval=FALSE, include=FALSE, echo=TRUE}
library(nycflights13)
library(dplyr)
library(ggplot2)
library(stringr)
library(infer)
set.seed(2017)
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
fli_small <- flights %>% 
  na.omit() %>% 
  sample_n(size = 500) %>% 
  mutate(season = case_when(
    month %in% c(10:12, 1:3) ~ "winter",
    month %in% c(4:9) ~ "summer"
  )) %>% 
  mutate(day_hour = case_when(
    between(hour, 1, 12) ~ "morning",
    between(hour, 13, 24) ~ "not morning"
  )) %>% 
  select(arr_delay, dep_delay, season, 
         day_hour, origin, carrier)

fli_small
```




```{r eval=FALSE, include=FALSE, echo=TRUE}
obs_chisq <- fli_small %>%
  specify(origin ~ season) %>% # alt: response = origin, explanatory = season
  calculate(stat = "Chisq")

obs_chisq
```



```{r eval=FALSE, include=FALSE, echo=TRUE}
obs_chisq <- fli_small %>% 
  chisq_test(formula = origin ~ season) %>% 
  dplyr::select(statistic)
obs_chisq
```



```{r eval=FALSE, include=FALSE, echo=TRUE}
obs_chisq <- fli_small %>% 
  chisq_stat(formula = origin ~ season)
obs_chisq
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
chisq_null_perm <- fli_small %>%
  specify(origin ~ season) %>% # alt: response = origin, explanatory = season
  hypothesize(null = "independence") %>%
  generate(reps = 1000, type = "permute") %>%
  calculate(stat = "Chisq")

visualize(chisq_null_perm) +
  shade_p_value(obs_stat = obs_chisq, direction = "greater")
```



```{r eval=FALSE, include=FALSE, echo=TRUE}
chisq_null_perm %>% 
  get_p_value(obs_stat = obs_chisq, direction = "greater")
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
chisq_null_theor <- fli_small %>%
  specify(origin ~ season) %>% 
  hypothesize(null = "independence") %>%
  # generate() ## Not used for theoretical
  calculate(stat = "Chisq")
chisq_null_theor
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
visualize(chisq_null_theor, method = "theoretical") +
  shade_p_value(obs_stat = obs_chisq, direction = "right")
```



```{r eval=FALSE, include=FALSE, echo=TRUE}
visualize(chisq_null_perm, method = "both") +
  shade_p_value(obs_stat = obs_chisq, direction = "right")
```



```{r eval=FALSE, include=FALSE, echo=TRUE}
fli_small %>% 
  chisq_test(formula = origin ~ season) %>% 
  dplyr::pull(p_value)
```













<!--chapter:end:ContingencyTables.Rmd-->

---
title: "Correlations"
---

# comparisons between correlations

http://comparingcorrelations.org/

# Exploring correlations in R with corrr
 
 
 https://drsimonj.svbtle.com/exploring-correlations-in-r-with-corrr







<!--chapter:end:Correlations.Rmd-->

---
title: "d3rain"
---


```{r eval=FALSE, include=FALSE, echo=TRUE}
library(dplyr)
library(d3rain)

armed_levels <- rev(c('Unarmed', 'Knife', 'Non-lethal firearm', 'Firearm'))
pk <- fivethirtyeight::police_killings %>% 
  mutate(armed = recode(armed, No = "Unarmed")) %>% 
  mutate(armed = factor(armed, levels = armed_levels)) %>% 
  filter(armed %in% armed_levels,
         !is.na(age))
pk %>% 
  arrange(age) %>% 
  d3rain(age, armed, toolTip = age, title = "2015 Police Killings by Age, Armed Status") %>% 
  drip_settings(dripSequence = 'iterate',
                ease = 'linear',
                jitterWidth = 25,
                dripSpeed = 500,
                dripFill = 'firebrick',
                iterationSpeedX = 20) %>% 
  chart_settings(fontFamily = 'times',
                 yAxisTickLocation = 'left')  
```

<!--chapter:end:d3rain.Rmd-->

---
title: "My R Codes For Data Analysis"
subtitle: "Open Data that can be used for projects"
author: "[Serdar Balcı, MD, Pathologist](https://www.serdarbalci.com/)"
date: '`{r #  format(Sys.Date())`'
---

```
output: 
  html_notebook: 
    fig_caption: yes
    highlight: tango
    number_sections: yes
    theme: paper
    toc: yes
    toc_depth: 5
    toc_float: yes
  html_document: 
    code_folding: hide
    df_print: kable
    keep_md: yes
    number_sections: yes
    theme: cerulean
    toc: yes
    toc_float: yes
    highlight: kate
```


# Data List

- Learning Clinical Epidemiology with R

http://datacompass.lshtm.ac.uk/599/

- ISLR

```{r eval=FALSE, include=FALSE, echo=TRUE}
data(package = "ISLR")
```

- acs

Download, Manipulate, and Present American Community Survey and Decennial 

Data from the US Census

https://cran.r-project.org/web/packages/acs/index.html

- eurostat

Tools for Eurostat Open Data

https://cran.r-project.org/web/packages/eurostat/index.html

- Rilostat

https://github.com/ilostat/Rilostat

- OECD

https://cran.r-project.org/web/packages/OECD/vignettes/oecd_vignette_main.pdf

- gapminder

Factfulness: Building Gapminder Income Mountains

http://staff.math.su.se/hoehle/blog/2018/07/02/factfulness.html

- nycflights13

- fivethirtyeight

- projects

https://www.analyticsvidhya.com/blog/2014/11/data-science-projects-learn/

- Miscellaneous Datasets

http://users.stat.ufl.edu/~winner/datasets.html


- datasets

https://www.rdocumentation.org/packages/datasets/versions/3.5.1






<!--chapter:end:DataList.Rmd-->

---
title: "Data Science Live Book"
---


https://livebook.datascienceheroes.com/


```{r eval=FALSE, include=FALSE, echo=TRUE}
# Loading funModeling!
library(funModeling)
library(dplyr)
data(heart_disease)
```

```{r eval=FALSE, include=FALSE, echo=TRUE}
# Profiling the data input
df_status(heart_disease)
```


<!--chapter:end:DataScienceLiveBook.Rmd-->

---
title: "data.table package"
---


# Rdatatable

https://github.com/Rdatatable

```{r eval=FALSE, include=FALSE, echo=TRUE}
library(data.table)
```


## Introduction to data.table


https://cloud.r-project.org/web/packages/data.table/vignettes/datatable-intro.html


```{r eval=FALSE, include=FALSE, echo=TRUE}
input <- if (file.exists("flights14.csv")) {
   "flights14.csv"
} else {
  "https://raw.githubusercontent.com/Rdatatable/data.table/master/vignettes/flights14.csv"
}
flights <- fread(input)
flights
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
?fread
```

```
DT = data.table(
  ID = c("b","b","b","a","a","c"),
  a = 1:6,
  b = 7:12,
  c = 13:18
)
DT


class(DT$ID)


getOption("datatable.print.nrows")



ans <- flights[origin == "JFK" & month == 6L]
head(ans)


ans <- flights[1:2]
ans

ans <- flights[origin == "JFK" & month == 6L][1:2]
head(ans)


ans <- flights[order(origin, -dest)]
head(ans)


ans <- flights[, arr_delay]
head(ans)



ans <- flights[, arr_delay, dest]
head(ans)

ans <- flights[, list(arr_delay)]
head(ans)


ans <- flights[, .(arr_delay)]
head(ans)

ans <- flights[, .(arr_delay, dep_delay)]
head(ans)


ans <- flights[, .(delay_arr = arr_delay, delay_dep = dep_delay)]
head(ans)


ans <- flights[, sum( (arr_delay + dep_delay) < 0 )]
ans

ans <- flights[origin == "JFK" & month == 6L,
               .(m_arr = mean(arr_delay), m_dep = mean(dep_delay))]
ans

ans <- flights[origin == "JFK" & month == 6L, length(dest)]
ans

ans <- flights[origin == "JFK" & month == 6L, .N]
ans

ans <- flights[, c("arr_delay", "dep_delay")]
head(ans)

select_cols = c("arr_delay", "dep_delay")
flights[ , ..select_cols]


flights[ , select_cols, with = FALSE]

ans <- flights[, !c("arr_delay", "dep_delay")]

ans <- flights[, -c("arr_delay", "dep_delay")]

ans <- flights[, year:day]

ans <- flights[, day:year]

ans <- flights[, -(year:day)]
ans <- flights[, !(year:day)]


ans <- flights[, .(.N), by = .(origin)]
ans


ans <- flights[, .(.N), by = "origin"]
ans

ans <- flights[, .N, by = origin]
ans


ans <- flights[carrier == "AA", .N, by = origin]
ans


ans <- flights[carrier == "AA", .N, by = .(origin, dest)]
head(ans)


ans <- flights[carrier == "AA", .N, by = c("origin", "dest")]
ans


ans <- flights[carrier == "AA",
        .(mean(arr_delay), mean(dep_delay)),
        by = .(origin, dest, month)]
ans


ans <- flights[carrier == "AA",
        .(mean(arr_delay), mean(dep_delay)),
        keyby = .(origin, dest, month)]
ans


ans <- flights[carrier == "AA", .N, by = .(origin, dest)]
ans


ans <- flights[carrier == "AA", .N, by = .(origin, dest)][order(origin, -dest)]
head(ans, 10)


ans <- flights[, .N, .(dep_delay>0, arr_delay>0)]
ans

flights[, .N, .(dep_delayed = dep_delay>0, arr_delayed = arr_delay>0)]
```






# cheat sheet



https://www.datacamp.com/community/tutorials/data-table-cheat-sheet

https://s3.amazonaws.com/assets.datacamp.com/blog_assets/datatable_Cheat_Sheet_R.pdf


```{r eval=FALSE, include=FALSE, echo=TRUE}
library(data.table)
```
http://r-datatable.com

https://github.com/Rdatatable/data.table/wiki

<!-- [![](https://raw.githubusercontent.com/wiki/Rdatatable/data.table/pictures/syntax1.jpg)](https://github.com/Rdatatable/data.table/wiki/talks/ArunSrinivasanSatRdaysBudapest2016.pdf) -->


```{r eval=FALSE, include=FALSE, echo=TRUE}
set.seed(45L)
DT <- data.table(V1 = c(1L,2L),
                 V2 = LETTERS[1:3],
                 V3 = round(rnorm(4),4),
                 V4 = 1:12)
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
DT
```



```{r eval=FALSE, include=FALSE, echo=TRUE}
typeof(DT)
```

```{r eval=FALSE, include=FALSE, echo=TRUE}
class(DT)
```





## Subsetting Rows Using i

```{r eval=FALSE, include=FALSE, echo=TRUE}
DT[3:5,]
```



```{r eval=FALSE, include=FALSE, echo=TRUE}
DT[3:5]
```

```{r eval=FALSE, include=FALSE, echo=TRUE}
DT[V2=="A"]
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
DT[V2 %in% c("A","C")]
```


## Manipulating on Columns in j

sonuç vektör olarak alınacaksa sadece sütun ismi yazılıyor

```{r eval=FALSE, include=FALSE, echo=TRUE}
DT[,V2]
```

sonuç data.frame olarak alınacaksa sütun ismi önünde `.` yazılıyor

```{r eval=FALSE, include=FALSE, echo=TRUE}
DT[,.(V2)]
```



```{r eval=FALSE, include=FALSE, echo=TRUE}
DT[,.(V2,V3)]
```

tek sütun üzerinden özet alma

```{r eval=FALSE, include=FALSE, echo=TRUE}
DT[,sum(V1)]
```

birden fazla sütun üzerinden özet alma

```{r eval=FALSE, include=FALSE, echo=TRUE}
DT[,.(sum(V1),sd(V3))]
```

```{r eval=FALSE, include=FALSE, echo=TRUE}
DT[,.(Aggregate = sum(V1),
      Sd.V3 = sd(V3))]
```

```{r eval=FALSE, include=FALSE, echo=TRUE}
DT[,.(V1,Sd.V3=sd(V3))]
```

```{r eval=FALSE, include=FALSE, echo=TRUE}
DT[,.(print(V2),
      plot(V3),
      NULL)]
```


## Doing j by Group

```{r eval=FALSE, include=FALSE, echo=TRUE}
DT[,.(V4.Sum = sum(V4)),by = V1]
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
DT[,.(V4.Sum = sum(V4)),
   by = .(V1,V2)]
```




```{r eval=FALSE, include=FALSE, echo=TRUE}
DT[,.(V4.Sum = sum(V4)),
   by = sign(V1-1)]
```



```{r eval=FALSE, include=FALSE, echo=TRUE}
DT[,.(V4.Sum = sum(V4)),
   by = .(V1.01 = sign(V1 - 1))]
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
DT[1:5,.(V4.Sum = sum(V4)), 
   by = V1]
```

```{r eval=FALSE, include=FALSE, echo=TRUE}
DT[,.N,by = V1]
```

## Adding/Updating Columns By Reference in j Using :=

```{r eval=FALSE, include=FALSE, echo=TRUE}
DT[,V1:=round(exp(V1),2)]
DT
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
DT[,V5:=round(exp(V1),2)]
DT
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
DT[,c("V1","V2"):=list(round(exp(V1),2),
                       LETTERS[4:6])]
DT
```



```{r eval=FALSE, include=FALSE, echo=TRUE}
DT[,':='(V1=round(exp(V1),2),
         V2=LETTERS[4:6])][]
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
DT[,V1:=NULL]
DT
```

```{r eval=FALSE, include=FALSE, echo=TRUE}
DT[,c("V1","V2"):=NULL][]
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
Cols.chosen = c("A", "B")
DT[,Cols.Chosen:=NULL]
```

```{r eval=FALSE, include=FALSE, echo=TRUE}
Cols.chosen = c("A", "B")
DT[,(Cols.Chosen):=NULL]
```


## Indexing And Keys


```{r eval=FALSE, include=FALSE, echo=TRUE}
setkey(DT,V2)
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
DT["A"]
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
DT[c("A","C")]
```



```{r eval=FALSE, include=FALSE, echo=TRUE}
DT["A",mult="first"]
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
DT["A",mult="last"]
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
DT[c("A","D")]
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
DT[c("A","D"),nomatch=0]
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
DT[c("A","C"),sum(V4)]
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
DT[c("A","C"),
   sum(V4),
   by=.EACHI]
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
setkey(DT,V1,V2)
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
DT[.(2,"C")]
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
DT[.(2,c("A","C"))]
```


<!--chapter:end:datatable.Rmd-->

---
title: "Data Tools"
---

- Installations for Data Science. Anaconda, RStudio, Spark, TensorFlow, AWS (Amazon Web Services).

https://medium.com/@GalarnykMichael

https://github.com/mGalarnyk/Installations_Mac_Ubuntu_Windows

- Google Cloud for Data Science: Beginner's Guide
https://www.datacamp.com/community/tutorials/google-cloud-data-science

- Deep Learning With Jupyter Notebooks In The Cloud
https://www.datacamp.com/community/tutorials/deep-learning-jupyter-aws

https://www.datacamp.com/community/tutorials/homebrew-install-use





```
system() function works when I use R from terminal but not from RStudio #2193

https://github.com/rstudio/rstudio/issues/2193

myTerm <- rstudioapi::terminalCreate(show = FALSE)
rstudioapi::terminalSend(myTerm, "esearch -db pubmed -query '(diabetes AND pregnancy) AND (\"2017/01/01\"[PDAT] : \"2017/12/31\"[PDAT])' | efetch -format xml | xtract -pattern Grant -element Agency | sort-uniq-count-rank | head -n 10 > myquery.txt \n")
Sys.sleep(1)
repeat{
    Sys.sleep(0.1)
    if(rstudioapi::terminalBusy(myTerm) == FALSE){
        print("Code Executed")
        break
    }
}
```



```{r eval=FALSE, include=FALSE, echo=TRUE}
library(datasets) # initialize
library(help=datasets) # display the datasets
```



```{r eval=FALSE, include=FALSE, echo=TRUE}
class(airquality)  # get class
sapply(airquality, class)  # get class of all columns
str(airquality)  # structure
summary(airquality)  # summary of airquality
head(airquality)  # view the first 6 obs
fix(airquality)  # view spreadsheet like grid
View(airquality)
rownames(airquality)  # row names
colnames(airquality)  # columns names
nrow(airquality)  # number of rows
ncol(airquality)  # number of columns
```









<!--chapter:end:DataTools.Rmd-->

---
title: "My R Codes For Data Analysis"
subtitle: "Decision Tree / Karar Ağacı"
author: "[Serdar Balcı, MD, Pathologist](https://www.serdarbalci.com/)"
date: '`{r #  format(Sys.Date())`'
---


# Decision Trees



```{r eval=FALSE, include=FALSE, echo=TRUE}
# install.packages("ISLR")
library(ISLR)
data(package = "ISLR")
carseats <- Carseats
carseats
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
# install.packages("tree")
library(tree)
require(tree)
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
names(carseats)

```


```{r eval=FALSE, include=FALSE, echo=TRUE}
hist(carseats$Sales)
```

```{r eval=FALSE, include=FALSE, echo=TRUE}
High <- ifelse(carseats$Sales <= 8, "No", "Yes")
carseats <- data.frame(carseats, High)
carseats
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
tree.carseats <- tree::tree(High~.-Sales, data = carseats)
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
tree.carseats

```



```{r eval=FALSE, include=FALSE, echo=TRUE}
set.seed(101)
train <- sample(1:nrow(carseats), 250)
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
train
```


```{r eval=FALSE, fig.height=6, fig.width=12, include=FALSE}
tree.carseats <- tree(High~.-Sales, carseats, subset=train)
plot(tree.carseats)
text(tree.carseats, pretty=0)
```



```{r eval=FALSE, include=FALSE, echo=TRUE}
tree.pred <- predict(tree.carseats, carseats[-train,], type = "class")
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
tree.pred
```




```{r eval=FALSE, include=FALSE, echo=TRUE}
with(carseats[-train,], table(tree.pred, High))
```



```{r eval=FALSE, include=FALSE, echo=TRUE}
cv.carseats <- cv.tree(tree.carseats, FUN = prune.misclass)
cv.carseats
```



```{r eval=FALSE, include=FALSE, echo=TRUE}
plot(cv.carseats)
```




```
prune.carseats = prune.misclass(tree.carseats, best = 12)
plot(prune.carseats)
text(prune.carseats, pretty=0)
It's a bit shallower than previous trees, and you can actually read the labels. Let's evaluate it on the test dataset again.

tree.pred = predict(prune.carseats, carseats[-train,], type="class")
with(carseats[-train,], table(tree.pred, High))
(74 + 39) / 150
Seems like the correct classifications dropped a little bit. It has done about the same as your original tree, so pruning did not hurt much with respect to misclassification errors, and gave a simpler tree.

Often case, trees don't give very good prediction errors, so let's go ahead take a look at random forests and boosting, which tend to outperform trees as far as prediction and misclassification are concerned.

Random Forests
For this part, you will use the Boston housing data to explore random forests and boosting. The dataset is located in the MASS package. It gives housing values and other statistics in each of 506 suburbs of Boston based on a 1970 census.

library(MASS)
data(package="MASS")
boston<-Boston
dim(boston)
names(boston)
Let's also load the randomForest package.

require(randomForest)
To prepare data for random forest, let's set the seed and create a sample training set of 300 observations.

set.seed(101)
train = sample(1:nrow(boston), 300)
In this dataset, there are 506 surburbs of Boston. For each surburb, you have variables such as crime per capita, types of industry, average # of rooms per dwelling, average proportion of age of the houses etc. Let's use medv - the median value of owner-occupied homes for each of these surburbs, as the response variable.

Let's fit a random forest and see how well it performs. As being said, you use the response medv, the median housing value (in $1K dollars), and the training sample set.

rf.boston = randomForest(medv~., data = boston, subset = train)
rf.boston
Printing out the random forest gives its summary: the # of trees (500 were grown), the mean squared residuals (MSR), and the percentage of variance explained. The MSR and % variance explained are based on the out-of-bag estimates, a very clever device in random forests to get honest error estimates.

The only tuning parameter in a random Forests is the argument called mtry, which is the number of variables that are selected at each split of each tree when you make a split. As seen here, mtry is 4 of the 13 exploratory variables (excluding medv) in the Boston Housing data - meaning that each time the tree comes to split a node, 4 variables would be selected at random, then the split would be confined to 1 of those 4 variables. That's how randomForests de-correlates the trees.

You're going to fit a series of random forests. There are 13 variables, so let's have mtry range from 1 to 13:

In order to record the errors, you set up 2 variables oob.err and test.err.

In a loop of mtry from 1 to 13, you first fit the randomForest with that value of mtry on the train dataset, restricting the number of trees to be 350.

Then you extract the mean-squared-error on the object (the out-of-bag error).

Then you predict on the test dataset (boston[-train]) using fit (the fit of randomForest).

Lastly, you compute the test error: mean-squared error, which is equals to mean( (medv - pred) ^ 2 ).

oob.err = double(13)
test.err = double(13)
for(mtry in 1:13){
  fit = randomForest(medv~., data = boston, subset=train, mtry=mtry, ntree = 350)
  oob.err[mtry] = fit$mse[350]
  pred = predict(fit, boston[-train,])
  test.err[mtry] = with(boston[-train,], mean( (medv-pred)^2 ))
}
Basically you just grew 4550 trees (13 times 350). Now let's make a plot using the matplot command. The test error and the out-of-bag error are binded together to make a 2-column matrix. There are a few other arguments in the matrix, including the plotting character values (pch = 23 means filled diamond), colors (red and blue), type equals both (plotting both points and connecting them with the lines), and name of y-axis (Mean Squared Error). You can also put a legend at the top right corner of the plot.

matplot(1:mtry, cbind(test.err, oob.err), pch = 23, col = c("red", "blue"), type = "b", ylab="Mean Squared Error")
legend("topright", legend = c("OOB", "Test"), pch = 23, col = c("red", "blue"))
Ideally, these 2 curves should line up, but it seems like the test error is a bit lower. However, there's a lot of variability in these test error estimates. Since the out-of-bag error estimate was computed on one dataset and the test error estimate was computed on another dataset, these differences are pretty much well within the standard errors.

Notice that the red curve is smoothly above the blue curve? These error estimates are very correlated, because the randomForest with mtry = 4 is very similar to the one with mtry = 5. That's why each of the curves is quite smooth. What you see is that mtry around 4 seems to be the most optimal choice, at least for the test error. This value of mtry for the out-of-bag error equals 9.

So with very few tiers, you have fitted a very powerful prediction model using random forests. How so? The left-hand side shows the performance of a single tree. The mean squared error on out-of-bag is 26, and you've dropped down to about 15 (just a bit above half). This means you reduced the error by half. Likewise for the test error, you reduced the error from 20 to 12.

Boosting
Compared to random forests, boosting grows smaller and stubbier trees and goes at the bias. You will use the package GBM (Gradient Boosted Modeling), in R.

require(gbm)
GBM asks for the distribution, which is Gaussian, because you'll be doing squared error loss. You're going to ask GBM for 10,000 trees, which sounds like a lot, but these are going to be shallow trees. Interaction depth is the number of splits, so you want 4 splits in each tree. Shrinkage is 0.01, which is how much you're going to shrink the tree step back.

boost.boston = gbm(medv~., data = boston[train,], distribution = "gaussian", n.trees = 10000, shrinkage = 0.01, interaction.depth = 4)
summary(boost.boston)
The summary function gives a variable importance plot. It seems like there are 2 variables that have high relative importance: rm (number of rooms) and lstat (percentage of lower economic status people in the community). Let's plot these 2 variables:

plot(boost.boston,i="lstat")
plot(boost.boston,i="rm")
The 1st plot shows that the higher the proportion of lower status people in the suburb, the lower the value of the housing prices. The 2nd plot shows the reversed relationship with the number of rooms: the average number of rooms in the house increases as the price increases.

It's time to predict a boosted model on the test dataset. Let's look at the test performance as a function of the number of trees:

First, you make a grid of number of trees in steps of 100 from 100 to 10,000.

Then, you run the predict function on the boosted model. It takes n.trees as an argument, and produces a matrix of predictions on the test data.

The dimensions of the matrix are 206 test observations and 100 different predict vectors at the 100 different values of tree.

n.trees = seq(from = 100, to = 10000, by = 100)
predmat = predict(boost.boston, newdata = boston[-train,], n.trees = n.trees)
dim(predmat)
It's time to compute the test error for each of the predict vectors:

predmat is a matrix, medv is a vector, thus (predmat - medv) is a matrix of differences. You can use the apply function to the columns of these square differences (the mean). That would compute the column-wise mean squared error for the predict vectors.

Then you make a plot using similar parameters to that one used for Random Forest. It would show a boosting error plot.

boost.err = with(boston[-train,], apply( (predmat - medv)^2, 2, mean) )
plot(n.trees, boost.err, pch = 23, ylab = "Mean Squared Error", xlab = "# Trees", main = "Boosting Test Error")
abline(h = min(test.err), col = "red")
The boosting error pretty much drops down as the number of trees increases. This is an evidence showing that boosting is reluctant to overfit. Let's also include the best test error from the randomForest into the plot. Boosting actually gets a reasonable amount below the test error for randomForest.

Conclusion
So that's the end of this R tutorial on building decision tree models: classification trees, random forests, and boosted trees. The latter 2 are powerful methods that you can use anytime as needed. In my experience, boosting usually outperforms RandomForest, but RandomForest is easier to implement. In RandomForest, the only tuning parameter is the number of trees; while in boosting, more tuning parameters are required besides the number of trees, including the shrinkage and the interaction depth.

If you would like to learn more, be sure to take a look at our Machine Learning Toolbox course for R.
```





# decision tree
https://analytics4all.org/2016/11/23/r-decision-trees-regression/



# DECISION TREE CLASSIFIER IMPLEMENTATION IN R

---

https://dataaspirant.com/2017/01/30/how-decision-tree-algorithm-works/





---

https://dataaspirant.com/2017/02/03/decision-tree-classifier-implementation-in-r/


# caret

Classification And REgression Training



```{r eval=FALSE, include=FALSE, echo=TRUE}
library(caret)
library(rpart.plot)
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
data_url <- c("https://archive.ics.uci.edu/ml/machine-learning-databases/car/car.data")
download.file(url = data_url, destfile = "data/car.data")

car_df <- read.csv("data/car.data", sep = ',', header = FALSE)
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
set.seed(3033)
intrain <- createDataPartition(y = car_df$V7, p= 0.7, list = FALSE)
training <- car_df[intrain,]
testing <- car_df[-intrain,]
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
#check dimensions of train & test set
dim(training); dim(testing);
```



```{r eval=FALSE, include=FALSE, echo=TRUE}
anyNA(car_df)
```

```{r eval=FALSE, include=FALSE, echo=TRUE}
summary(car_df)
```

```{r eval=FALSE, include=FALSE, echo=TRUE}
trctrl <- trainControl(method = "repeatedcv", number = 10, repeats = 3)

# The “method” parameter holds the details about resampling method. We can set “method” with many values like  “boot”, “boot632”, “cv”, “repeatedcv”, “LOOCV”, “LGOCV” etc. For this tutorial, let’s try to use repeatedcv i.e, repeated cross-validation.
# 
# The “number” parameter holds the number of resampling iterations. The “repeats ” parameter contains the complete sets of folds to compute for our repeated cross-validation. We are using setting number =10 and repeats =3. This trainControl() methods returns a list. We are going to pass this on our train() method.


set.seed(3333)

dtree_fit <- train(V7 ~., data = training, method = "rpart",
                   parms = list(split = "information"),
                   trControl=trctrl,
                   tuneLength = 10)


# train() method should be passed with “method” parameter as “rpart”. There is another package “rpart”, it is specifically available for decision tree implementation. Caret links its train function with others to make our work simple.
# 
# We are passing our target variable V7. The “V7~.” denotes a formula for using all attributes in our classifier and V7 as the target variable. The “trControl” parameter should be passed with results from our trianControl() method.





```



```{r eval=FALSE, include=FALSE, echo=TRUE}
?rpart
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
dtree_fit
```

```{r eval=FALSE, include=FALSE, echo=TRUE}
prp(dtree_fit$finalModel, box.palette = "Reds", tweak = 1.2)
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
testing[1,]
```

```{r eval=FALSE, include=FALSE, echo=TRUE}
predict(dtree_fit, newdata = testing[1,])
```

```{r eval=FALSE, include=FALSE, echo=TRUE}
test_pred <- predict(dtree_fit, newdata = testing)
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
confusionMatrix(test_pred, testing$V7 )  #check accuracy
```




```{r eval=FALSE, include=FALSE, echo=TRUE}
set.seed(3333)
dtree_fit_gini <- train(V7 ~., data = training, method = "rpart",
                   parms = list(split = "gini"),
                   trControl=trctrl,
                   tuneLength = 10)
dtree_fit_gini
```

```{r eval=FALSE, include=FALSE, echo=TRUE}
prp(dtree_fit_gini$finalModel, box.palette = "Blues", tweak = 1.2)
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
test_pred_gini <- predict(dtree_fit_gini, newdata = testing)
confusionMatrix(test_pred_gini, testing$V7 )  #check accuracy
```







<!--chapter:end:DecisionTreeKararAgaci.Rmd-->

---
title: "My R Codes For Data Analysis"
---

## Descriptive Statistics


```{r eval=FALSE, include=FALSE, echo=TRUE}
Epi::stat.table(gender,mean(age), data = scabies)

```

```{r eval=FALSE, include=FALSE, echo=TRUE}
table <- Epi::stat.table(gender,mean(age), data = scabies)

pander::pander(table)


```

```{r eval=FALSE, include=FALSE, echo=TRUE}
#Tabulate, by gender, the mean age from the scabies dataset

Epi::stat.table(gender,list(mean(age),median(age)), data = scabies)
```

```{r eval=FALSE, include=FALSE, echo=TRUE}
summary_data <- arsenal::tableby(gender~age+scabies_infestation,data=scabies)
summary(summary_data)

```


## skimr

https://cran.r-project.org/web/packages/skimr/vignettes/Using_skimr.html

```{r eval=FALSE, include=FALSE, echo=TRUE}
require(skimr)
```

```{r eval=FALSE, include=FALSE, echo=TRUE}
summary(iris)
```

```{r eval=FALSE, include=FALSE, echo=TRUE}
summary(iris$Sepal.Length)
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
fivenum(iris$Sepal.Length)
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
summary(iris$Species)
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
skim(iris)
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
iris_results <- skim(iris)
str(iris_results)
iris_results$variable
iris_results$type
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
skimr::skim(iris) %>%
  dplyr::filter(stat == "mean")
```

```{r eval=FALSE, include=FALSE, echo=TRUE}
head(iris_results, n=15)
```



```{r eval=FALSE, include=FALSE, echo=TRUE}
mtcars %>%
  dplyr::group_by(gear) %>%
  skim()
```



```{r eval=FALSE, include=FALSE, echo=TRUE}
skim(iris, Sepal.Length, Species)
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
skim(iris, starts_with("Sepal"))
```



```{r eval=FALSE, include=FALSE, echo=TRUE}
skim(datasets::lynx)
```


- Exploratory Data Analysis in R (introduction)


https://blog.datascienceheroes.com/exploratory-data-analysis-in-r-intro/


```{r eval=FALSE, include=FALSE, echo=TRUE}
library(tidyverse)
library(summarytools)
# library(funModeling) 
library(tidyverse) 
library(Hmisc)

basic_eda <- function(data)
{
  glimpse(data)
 # df_status(data)
 # freq(data) 
 # profiling_num(data)
 # plot_num(data)
  describe(data)
}

basic_eda(irisdata)
```

---

- **What's so hard about histograms?**

http://tinlizzie.org/~aran/histograms/



---


# DataExplorer


---

# Webinar: Tidyverse Exploratory Analysis (Emily Robinson)


<iframe src="https://www.facebook.com/plugins/video.php?href=https%3A%2F%2Fwww.facebook.com%2F726282547396228%2Fvideos%2F584417861986887%2F&show_text=1&width=560" width="560" height="529" style="border:none;overflow:hidden" scrolling="no" frameborder="0" allowTransparency="true" allow="encrypted-media" allowFullScreen="true"></iframe>


<iframe width="560" height="315" src="https://www.youtube.com/embed/uG3igAGX7UE" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>


https://hookedondata.org/the-lesser-known-stars-of-the-tidyverse/

https://www.rstudio.com/resources/videos/the-lesser-known-stars-of-the-tidyverse/

https://github.com/robinsones/robinsones_blog/blob/master/content/post/multipleChoiceResponses.csv

https://github.com/robinsones/robinsones_blog/blob/master/content/post/2018-11-16-the-lesser-known-stars-of-the-tidyverse.Rmd



# I “only” use R for descriptive stats — and that’s OK

https://rforeval.com/descriptive-stats-r/


# histograms

http://tinlizzie.org/histograms/





<!--chapter:end:DescriptiveStatistics.Rmd-->


<!-- --- -->
<!-- title: "" -->
<!-- author: "" -->
<!-- date: "" -->
<!-- params: -->
<!--   gdrive_folder_url: "https://drive.google.com/drive/u/0/folders/FIXME" -->
<!-- output: -->
<!--   html_document: -->
<!--     keep_md: true -->
<!--     theme: simplex -->
<!--     highlight: monochrome -->
<!-- --- -->


<!-- ```{r init, eval=FALSE, include=FALSE} -->
<!-- knitr::opts_chunk$set( -->
<!--   message = FALSE, -->
<!--   warning = FALSE, dev = c("png", "cairo_pdf"), -->
<!--   echo = TRUE, -->
<!--   fig.retina = 2, -->
<!--   fig.width = 10, -->
<!--   fig.height = 6, -->
<!--   fig.path = "prod/charts/" -->
<!-- ) -->
<!-- ``` -->

<!-- ```{r boilerplate-libraries, eval=FALSE, cache=FALSE, include=FALSE} -->
<!-- library(gt) -->
<!-- library(stringi) -->
<!-- library(hrbrthemes) -->
<!-- library(googledrive) -->
<!-- library(tidyverse) -->

<!-- # ensure fonts are available -->
<!-- extrafont::loadfonts("postscript", quiet = TRUE) -->
<!-- extrafont::loadfonts("pdf", quiet = TRUE) -->
<!-- ``` -->

<!-- # analysis code goes here ------------------------------------------------- -->


<!-- # Upload to production ---------------------------------------------------- -->

<!-- ```{r prod-upload, echo = TRUE, message = TRUE, warning = TRUE} -->
<!-- googledrive::drive_auth() -->

<!-- # locate the folder -->
<!-- gdrive_prod_folder <- googledrive::as_id(params$gdrive_folder_url) -->

<!-- # clean it out -->
<!-- gdrls <- googledrive::drive_ls(gdrive_prod_folder) -->
<!-- if (nrow(gdrls) > 0) { -->
<!--   dplyr::pull(gdrls, id) %>% -->
<!--     purrr::walk(~googledrive::drive_rm(googledrive::as_id(.x))) -->
<!-- } -->

<!-- # upload new -->
<!-- list.files(here::here("prod/charts"), recursive = TRUE, full.names = TRUE) %>% -->
<!--   purrr::walk(googledrive::drive_upload, path = gdrive_prod_folder) -->
<!-- ``` -->


<!--chapter:end:drive.Rmd-->

---
title: "Bibliographic Studies"
---



```{r eval=FALSE, include=FALSE, echo=TRUE}
knitr::opts_chunk$set(fig.width = 12, fig.height = 8, fig.path = 'figure/', echo = TRUE, warning = FALSE, message = FALSE, error = FALSE, eval = TRUE, tidy = TRUE, comment = NA)
```

```{r , include=FALSE}
library(tidyverse)
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
state.name
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
# install.packages("maps")
# library(maps)
# x <- map("world", plot=FALSE)
# glimpse(x)
# x$names

```

```{r eval=FALSE, include=FALSE, echo=TRUE}
install.packages("rworldmap")
library(rworldmap)
vignette('rworldmap')
data(countryExData)
countryExData
```












SEER China vs others

https://www.rdocumentation.org/packages/bayesTFR/versions/6.1-2/topics/country.names


https://stat.ethz.ch/R-manual/R-devel/library/datasets/html/state.html




# Who works on SEER {.tabset .tabset-fade .tabset-pills}

If you want to see the code used in the analysis please click the code button on the right upper corner or throughout the page.  
Select from the tabs below.

---

## Aim 

**Aim:**



---

## Data retriveal from PubMed using EDirect 

Articles are downloaded as `xml`.

```{r Search PubMed write all data as xml, eval=FALSE, include=FALSE, echo=TRUE}
myTerm <- rstudioapi::terminalCreate(show = FALSE)
rstudioapi::terminalSend(
    myTerm,
    "esearch -db pubmed -query \"'SEER Program'[Mesh]
\" -datetype PDAT -mindate 1800 -maxdate 3000 | efetch -format xml > data/pubmed_result_SEER_MeSH.xml \n"
)
Sys.sleep(1)
repeat {
    Sys.sleep(0.1)
    if (rstudioapi::terminalBusy(myTerm) == FALSE) {
        print("Code Executed")
        break
    }
}
```



```{r extract journal names from all data xml, eval=FALSE, message=FALSE, warning=FALSE, include=FALSE}
myTerm <- rstudioapi::terminalCreate(show = FALSE)
rstudioapi::terminalSend(
myTerm,
"xtract -input data/pubmed_result_SEER_MeSH.xml -pattern PubmedArticle -sep ' ' -def 'na' -element MedlineCitation/PMID PubDate/Year Affiliation> data/SEER_countries.csv \n"
)
Sys.sleep(1)
repeat {
Sys.sleep(0.1)
if (rstudioapi::terminalBusy(myTerm) == FALSE) {
print("Code Executed")
break
}
}
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
library(readr)
SEER_countries <- read_delim("data/SEER_countries.csv", 
    "\t", escape_double = FALSE, col_names = c("PMID", "year", "Affiliations"), 
    na = "NA", trim_ws = TRUE)
# View(SEER_countries)
```

```{r eval=FALSE, include=FALSE, echo=TRUE}
countries <- read_delim("data/countries.txt", delim = "|", col_names = c("abb", "country"))

country <- countries$country

country <- c(country, state.name)

country[80] <- "Georgia_"

```



```{r eval=FALSE, include=FALSE, echo=TRUE}
# SEER_countries <- cbind(SEER_countries, setNames(lapply(country, function(x) x=NA), country))

# names(SEER_countries)[254] <- "GeorgiaUSA"


```

```{r eval=FALSE, include=FALSE, echo=TRUE}
# grepl(pattern = "China", x = SEER_countries$Affiliations)
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
# deneme1 <- grepl(pattern = country[44], x = SEER_countries$Affiliations)
    
# deneme2 <- sapply(country, function(x) grepl(x, SEER_countries$Affiliations))

# sum(deneme1 != deneme2[,44])

```

```{r eval=FALSE, include=FALSE, echo=TRUE}
# deneme2 <- as.data.frame(deneme2)

# sum(deneme2$Turkey)
```

```{r eval=FALSE, include=FALSE, echo=TRUE}
SEER_countries <- cbind(SEER_countries, sapply(country, function(x) grepl(x, SEER_countries$Affiliations)))
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
dim(SEER_countries)[1]
```

At the time of the research the number of articles with 'SEER Program'[Mesh] formula is "r dim(SEER_countries)[1]".

```{r eval=FALSE, include=FALSE, echo=TRUE}
# deneme <- colSums(SEER_countries[,-(1:3)])

# deneme <- as.data.frame(deneme)

# deneme <- rownames_to_column(deneme, var = "countries")

# names(deneme) <- c("countries", "number")

# deneme %>% arrange(desc(number))


```

```{r eval=FALSE, include=FALSE, echo=TRUE}
SEER_countries[SEER_countries == FALSE] <- 0

SEER_countries[SEER_countries == TRUE] <- 1

```



```{r eval=FALSE, include=FALSE, echo=TRUE}
countryTotals <- SEER_countries %>% 
  select(-c(1:3)) %>% 
  summarise_all(funs(sum)) 

countryTotals[which(countryTotals>0)]

publisherCountries <- names(countryTotals[which(countryTotals>0)])

SEER_countries <- SEER_countries %>% 
  select(c(1:3, publisherCountries))

```



```{r eval=FALSE, include=FALSE, echo=TRUE}
deneme <- SEER_countries %>% 
  gather(key = "Country", value = "Number", -c(1:3)) %>% 
  group_by(Country, year) %>% 
  summarise(total = sum(Number))
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
deneme %>% 
  filter(year != "na") %>%
  filter(year != "2017") %>% 
  filter(year != "2018") %>% 
ggplot() +
  aes(y = total, x = year, group = Country, color = Country) +
  geom_line() + 
  guides(fill=FALSE, color=FALSE) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
USAnames <- names(SEER_countries) %in% state.name

Others <- setdiff(names(SEER_countries[-c(1:3)]), c(USAnames,"United States", "China"))


deneme2 <- SEER_countries %>% 
  mutate(
    sumUSA = rowSums(
      select(., one_of(USAnames), `United States`)
      )
    ) %>% 
mutate(
    sumOthers = rowSums(
      select(., one_of(Others))
      )
    ) %>% 
  select(PMID, year, China, USA = sumUSA, Others = sumOthers)

```

```{r eval=FALSE, include=FALSE, echo=TRUE}
deneme3 <- deneme2 %>% 
  gather(key = "Country", value = "Number", -c(1:2)) %>% 
  group_by(PMID, Country, year) %>% 
  summarise(total = sum(Number)) %>% 
  filter(year != "na") %>%
  filter(year != "2017") %>% 
  filter(year != "2018") %>% 
  filter(total != "0")
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
# which(duplicated(deneme3$PMID))
# which(duplicated(deneme3$PMID))-1

# deneme3[which(duplicated(deneme3$PMID)),]

together <- bind_cols(
First = deneme3$Country[which(duplicated(deneme3$PMID))],
Second = deneme3$Country[which(duplicated(deneme3$PMID))-1]
)

table(together$First, together$Second) %>% addmargins()
bind_cols(

```



```{r eval=FALSE, include=FALSE, echo=TRUE}
deneme4 <- deneme2 %>% 
  gather(key = "Country", value = "Number", -c(1:2)) %>% 
  group_by(Country, year) %>% 
  summarise(total = sum(Number)) %>% 
  filter(year != "na") %>%
  filter(year != "2017") %>% 
  filter(year != "2018") %>% 
  filter(total != "0")
```



```{r eval=FALSE, include=FALSE, echo=TRUE}
deneme4 %>% 
ggplot() +
  aes(y = total, x = year, group = Country, color = Country) +
  geom_line() + 
  # guides(fill=FALSE, color=FALSE) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```



<iframe src="https://www.facebook.com/plugins/post.php?href=https%3A%2F%2Fwww.facebook.com%2Fserdarbalcimdpathologist%2Fposts%2F1919706581479009&width=500" width="500" height="529" style="border:none;overflow:hidden" scrolling="no" frameborder="0" allowTransparency="true" allow="encrypted-media"></iframe>


<blockquote class="twitter-tweet" data-lang="en"><p lang="en" dir="ltr">While helping the preparation of <a href="https://twitter.com/hashtag/PBPath?src=hash&amp;ref_src=twsrc%5Etfw">#PBPath</a> Journal Watch (<a href="https://t.co/WiBsJixzlc">https://t.co/WiBsJixzlc</a>)  I thought that many SEER <a href="https://twitter.com/NCICancerStats?ref_src=twsrc%5Etfw">@NCICancerStats</a> studies are from China. So using edirect <a href="https://twitter.com/NCBI?ref_src=twsrc%5Etfw">@NCBI</a> and <a href="https://twitter.com/hashtag/RStats?src=hash&amp;ref_src=twsrc%5Etfw">#RStats</a> I draw the attached graph. What do you think? Do Chinese do research on SEER that much? <a href="https://t.co/3Op5r9ofbK">pic.twitter.com/3Op5r9ofbK</a></p>&mdash; Serdar Balcı (@serdarbalci) <a href="https://twitter.com/serdarbalci/status/1048663302916788224?ref_src=twsrc%5Etfw">October 6, 2018</a></blockquote><script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>


```{r eval=FALSE, include=FALSE, echo=TRUE}
p <- deneme4 %>% 
ggplot() +
  aes(y = total, x = year, group = Country, color = Country) +
  geom_line() + 
  # guides(fill=FALSE, color=FALSE) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))


```














<!-- ```{r Search PubMed write 2018 data as xml, eval=FALSE, include=FALSE, echo=TRUE} -->
<!-- myTerm <- rstudioapi::terminalCreate(show = FALSE) -->
<!-- rstudioapi::terminalSend( -->
<!--     myTerm, -->
<!--     "esearch -db pubmed -query \"Turkey[Affiliation]\" -datetype PDAT -mindate 2018 -maxdate 3000 | efetch -format xml > data/Turkey_2018.xml \n" -->
<!-- ) -->
<!-- Sys.sleep(1) -->
<!-- repeat { -->
<!--     Sys.sleep(0.1) -->
<!--     if (rstudioapi::terminalBusy(myTerm) == FALSE) { -->
<!--         print("Code Executed") -->
<!--         break -->
<!--     } -->
<!-- } -->
<!-- ``` -->




<!-- ```{r Search PubMed write all data as xml, eval=FALSE, include=FALSE, echo=TRUE} -->
<!-- myTerm <- rstudioapi::terminalCreate(show = FALSE) -->
<!-- rstudioapi::terminalSend( -->
<!--     myTerm, -->
<!--     "esearch -db pubmed -query \"Turkey[Affiliation]\" -datetype PDAT -mindate 1800 -maxdate 3000 | efetch -format xml > data/Turkey_all.xml \n" -->
<!-- ) -->
<!-- Sys.sleep(1) -->
<!-- repeat { -->
<!--     Sys.sleep(0.1) -->
<!--     if (rstudioapi::terminalBusy(myTerm) == FALSE) { -->
<!--         print("Code Executed") -->
<!--         break -->
<!--     } -->
<!-- } -->
<!-- ``` -->





<!-- ```{r Search PubMed get 2018 data on the fly, eval=FALSE, include=FALSE, echo=TRUE} -->
<!-- myTerm <- rstudioapi::terminalCreate(show = FALSE) -->
<!-- rstudioapi::terminalSend( -->
<!--     myTerm, -->
<!--     "esearch -db pubmed -query \"Turkey[Affiliation]\" -datetype PDAT -mindate 2018 -maxdate 3000 | efetch -format xml | xtract  -pattern PubmedArticle -element MedlineCitation/PMID PubDate/Year Journal/ISSN ISOAbbreviation> data/onthefly_Turkey_2018.csv \n" -->
<!-- ) -->
<!-- Sys.sleep(1) -->
<!-- repeat { -->
<!--     Sys.sleep(0.1) -->
<!--     if (rstudioapi::terminalBusy(myTerm) == FALSE) { -->
<!--         print("Code Executed") -->
<!--         break -->
<!--     } -->
<!-- } -->
<!-- ``` -->



<!-- ```{r Search PubMed get all data on the fly, eval=FALSE, include=FALSE, echo=TRUE} -->
<!-- myTerm <- rstudioapi::terminalCreate(show = FALSE) -->
<!-- rstudioapi::terminalSend( -->
<!--     myTerm, -->
<!--     "esearch -db pubmed -query \"Turkey[Affiliation]\" -datetype PDAT -mindate 1800 -maxdate 3000 | efetch -format xml | xtract  -pattern PubmedArticle -element MedlineCitation/PMID PubDate/Year Journal/ISSN ISOAbbreviation> data/onthefly_Turkey_all.csv \n" -->
<!-- ) -->
<!-- Sys.sleep(1) -->
<!-- repeat { -->
<!--     Sys.sleep(0.1) -->
<!--     if (rstudioapi::terminalBusy(myTerm) == FALSE) { -->
<!--         print("Code Executed") -->
<!--         break -->
<!--     } -->
<!-- } -->
<!-- ``` -->



<!-- Journal Names are extracted from xml. -->



<!-- ```{r extract journal names from 2018 xml, eval=FALSE, message=FALSE, warning=FALSE, include=FALSE} -->
<!-- myTerm <- rstudioapi::terminalCreate(show = FALSE) -->
<!-- rstudioapi::terminalSend( -->
<!-- myTerm, -->
<!-- "xtract -input data/Turkey_2018.xml -pattern PubmedArticle -element MedlineCitation/PMID PubDate/Year Journal/ISSN ISOAbbreviation > data/Turkey2018.csv \n" -->
<!-- ) -->
<!-- Sys.sleep(1) -->
<!-- repeat { -->
<!-- Sys.sleep(0.1) -->
<!-- if (rstudioapi::terminalBusy(myTerm) == FALSE) { -->
<!-- print("Code Executed") -->
<!-- break -->
<!-- } -->
<!-- } -->
<!-- ``` -->


<!-- ```{r extract journal names from all data xml, message=FALSE, warning=FALSE} -->
<!-- myTerm <- rstudioapi::terminalCreate(show = FALSE) -->
<!-- rstudioapi::terminalSend( -->
<!-- myTerm, -->
<!-- "xtract -input data/Turkey_all.xml -pattern PubmedArticle -sep ' ' -def 'na' -element MedlineCitation/PMID Journal/ISSN ISOAbbreviation PubDate/Year > data/TurkeyAll.csv \n" -->
<!-- ) -->
<!-- Sys.sleep(1) -->
<!-- repeat { -->
<!-- Sys.sleep(0.1) -->
<!-- if (rstudioapi::terminalBusy(myTerm) == FALSE) { -->
<!-- print("Code Executed") -->
<!-- break -->
<!-- } -->
<!-- } -->
<!-- ``` -->



<!-- ---- -->


<!-- ## Retrieval of Data of Properties of Journals -->



<!-- [nlmcatalog_result_journals_pmc.xml](https://www.ncbi.nlm.nih.gov/portal/utils/file_backend.cgi?Db=nlmcatalog&HistoryId=NCID_1_69755278_130.14.18.97_5555_1534585934_3590606783_0MetA0_S_HStore&QueryKey=2&Sort=PubDate&Filter=all&CompleteResultCount=2559&Mode=file&View=xml&p$l=Email&portalSnapshot=%2Fprojects%2Fentrez%2Fpubmed%2FPubMedGroup@1.136&BaseUrl=&PortName=live&RootTag=NLMCatalogRecordSet&DocType=NLMCatalogRecordSet%20PUBLIC%20%22-%2F%2FNLM%2F%2FDTD%20NLMCatalogRecordSet,%201st%20June%202017%2F%2FEN%22%20%22https://www.nlm.nih.gov/databases/dtd/nlmcatalogrecordset_170601.dtd%22&FileName=&ContentType=xml) -->


<!-- [nlmcatalog_result_currentlyindexed.xml](https://www.ncbi.nlm.nih.gov/portal/utils/file_backend.cgi?Db=nlmcatalog&HistoryId=NCID_1_69755278_130.14.18.97_5555_1534585934_3590606783_0MetA0_S_HStore&QueryKey=1&Sort=PubDate&Filter=all&CompleteResultCount=5242&Mode=file&View=xml&p$l=Email&portalSnapshot=%2Fprojects%2Fentrez%2Fpubmed%2FPubMedGroup@1.136&BaseUrl=&PortName=live&RootTag=NLMCatalogRecordSet&DocType=NLMCatalogRecordSet%20PUBLIC%20%22-%2F%2FNLM%2F%2FDTD%20NLMCatalogRecordSet,%201st%20June%202017%2F%2FEN%22%20%22https://www.nlm.nih.gov/databases/dtd/nlmcatalogrecordset_170601.dtd%22&FileName=&ContentType=xml) -->


<!-- [scimagojr2017.csv](https://www.scimagojr.com/journalrank.php?out=xls) -->

<!-- [scimagojr2017-wos.csv](https://www.scimagojr.com/journalrank.php?wos=true&out=xls) -->


<!-- ![](images/scidata.png) -->





<!-- --- -->






<!-- ## Analysis -->

<!-- ```{r Organize Journal Data 1, message=FALSE, warning=FALSE} -->
<!-- library(tidyverse) -->
<!-- library(readr) -->

<!-- TurkeyAll <- read_delim("data/TurkeyAll.csv",  -->
<!--     "\t", escape_double = FALSE, col_names = FALSE, -->
<!--     na = "na", trim_ws = TRUE) -->

<!-- names(TurkeyAll) <- c("PMID", "ISSN", "JournalName", "Year") -->

<!-- # dim(TurkeyAll)[1] -->

<!-- # min(TurkeyAll[,4], na.rm = TRUE) -->

<!-- # max(TurkeyAll[,4], na.rm = TRUE) -->

<!-- # glimpse(TurkeyAll) -->

<!-- ``` -->



<!-- ```{r Organize Journal Data 2} -->
<!-- uniqueJournals <- TurkeyAll %>%  -->
<!--     select(JournalName, ISSN) %>%  -->
<!--     unique() -->

<!-- # dim(uniqueJournals)[1] -->

<!-- ``` -->


<!-- ```{r Organize Journal Data 3} -->

<!-- TurkeyAll2 <- TurkeyAll %>%  -->
<!--     mutate(Journal = paste(JournalName, ISSN, sep = " ISSN ")) -->

<!-- ArticlesByYear <- TurkeyAll2 %>%  -->
<!--     group_by(Journal, Year) %>%  -->
<!--     summarise(n = n()) -->

<!-- ArticlesByYear <- ArticlesByYear %>%  -->
<!--     spread(key = Year, value = n) -->

<!-- TurkeyAll2 <- TurkeyAll2 %>%  -->
<!--     select(Journal, JournalName, ISSN) %>%  -->
<!--     unique() -->

<!-- ArticlesByYear <- left_join(ArticlesByYear, TurkeyAll2, by = "Journal")  -->

<!-- ArticlesByYear <- ArticlesByYear %>% -->
<!--     select( -->
<!--     Journal, JournalName, ISSN, everything() -->
<!--     ) -->

<!-- ``` -->


<!-- ```{r scimagojr2017} -->

<!-- ``` -->



<!-- -element MedlineTA  NLMCatalogRecord/NlmUniqueID -def 'na' -sep '\t' -block TitleAlternate/Title   -element TitleAlternate/Title    -->

<!-- "xtract -input data/nlmcatalog_result_currentlyindexed.xml -pattern NCBICatalogRecord -element ISSNLinking -def 'na' -sep ' ' -block TitleAlternate/Title -if TitleAlternate/Title@Sort -equals N -element TitleAlternate/Title  > data/nlmcatalog.csv \n" -->

<!-- -sep '\t'  -->


<!-- NLMCatalogRecord/NlmUniqueID ISSNLinking -->

<!-- ```{r nlmcatalog, message=FALSE, warning=FALSE} -->
<!-- myTerm <- rstudioapi::terminalCreate(show = FALSE) -->
<!-- rstudioapi::terminalSend( -->
<!-- myTerm, -->
<!-- "xtract -input data/nlmcatalog_result_currentlyindexed.xml -pattern NCBICatalogRecord -tab '|' -element NLMCatalogRecord/NlmUniqueID -block ISSNLinking -tab '|' -element ISSNLinking -block Title -if Title@Sort -equals N -def 'na' -tab '|' -element TitleAlternate/Title > data/nlmcatalog.csv \n" -->
<!-- ) -->
<!-- Sys.sleep(1) -->
<!-- repeat { -->
<!-- Sys.sleep(0.1) -->
<!-- if (rstudioapi::terminalBusy(myTerm) == FALSE) { -->
<!-- print("Code Executed") -->
<!-- break -->
<!-- } -->
<!-- } -->

<!-- ```  -->


<!-- ```{r eval=FALSE, include=FALSE, echo=TRUE} -->
<!-- library(readr) -->
<!-- nlmcatalog <- read_delim("data/nlmcatalog.csv",  -->
<!--                          delim = "|", -->
<!--                          escape_double = FALSE, -->
<!--                          col_names = FALSE,  -->
<!--                          trim_ws = TRUE) -->

<!-- ``` -->

<!-- ```{r eval=FALSE, include=FALSE, echo=TRUE} -->

<!-- library(xml2) -->
<!-- data <- read_xml("data/nlmcatalog_result_currentlyindexed.xml") -->

<!-- # Point locations -->
<!-- point <- data %>% xml_find_all("//pointer") -->
<!-- point %>% xml_attr("latitude") %>% as.numeric() -->
<!-- point %>% xml_attr("longitude") %>% as.numeric() -->

<!-- # Start time -->
<!-- data %>%  -->
<!--   xml_find_all("//start-valid-time") %>%  -->
<!--   xml_text() -->

<!-- # Temperature -->
<!-- data %>%  -->
<!--   xml_find_all("//temperature[@type='hourly']/value") %>%  -->
<!--   xml_text() %>%  -->
<!--   as.integer() -->



<!-- ``` -->


<!-- --- -->

<!-- ## Results -->

<!-- - PubMed'de **"r min(TurkeyAll[,4], na.rm = TRUE)"-"r max(TurkeyAll[,4], na.rm = TRUE)"** tarihleri arasında, *Türkiye* adresli **"r dim(TurkeyAll)[1]"** adet yayın mevcuttur. -->

<!-- - PubMed'de **`{r #  min(TurkeyAll[,4], na.rm = TRUE)`-`{r #  max(TurkeyAll[,4], na.rm = TRUE)`** tarihleri arasında, *Türkiye* adresli yayınlar **`{r #  dim(uniqueJournals)[1]`** farklı dergide yayımlanmıştır. -->





<!-- --- -->

<!-- ## Discussion -->

<!-- türkiye adresli olup da pubmedde yer alan makaleler hangi dergilerde kaçar adet yayınlanmış -->


<!-- The retrieved information was compiled in a table. -->





<!-- **Methods:** -->









<!-- **Result:** -->


<!-- ```{r plot 1} -->
<!-- ggplot(data = articles_per_journal, aes(x = Journal, y = n, group = Country, -->
<!--                                      colour = Country, shape = Country, -->
<!--                                      levels = Country -->
<!-- )) + -->
<!--     geom_point() + -->
<!--     labs(x = "Journals with decreasing impact factor", y = "Number of Articles") + -->
<!--     ggtitle("Pathology Articles Per Journal") +  -->
<!--     theme(plot.title = element_text(hjust = 0.5), -->
<!--           axis.text.x=element_blank()) -->

<!-- ``` -->


<!-- **Comment:** -->




<!-- --- -->


<!-- ## Feedback -->

<!-- [Serdar Balcı, MD, Pathologist](https://github.com/sbalci) would like to hear your feedback: https://goo.gl/forms/YjGZ5DHgtPlR1RnB3 -->

<!-- This document will be continiously updated and the last update was on `{r #  Sys.Date()`. -->

<!-- --- -->

<!-- ## Back to Main Menu -->

<!-- [Main Page for Bibliographic Analysis](https://sbalci.github.io/pubmed/BibliographicStudies.html) -->

<!-- --- -->

<!--chapter:end:edirect-addin.Rmd-->

---
title: "Eurostat"
---

- eurostat

http://ec.europa.eu/eurostat

http://ec.europa.eu/eurostat/data/database



- eurostat R package

http://ropengov.github.io/eurostat/


- Retrieval and Analysis of Eurostat Open Data with the eurostat Package

https://journal.r-project.org/archive/2017/RJ-2017-019/index.html

- CheatSheet

https://github.com/rOpenGov/eurostat/blob/master/vignettes/cheatsheet/eurostat_cheatsheet.pdf

https://github.com/rstudio/cheatsheets/raw/master/eurostat.pdf

- Searching, downloading and manipulating Eurostat data with R

http://ropengov.github.io/r/2015/05/01/eurostat-package-examples/

- Mapping Eurostat information

https://www.mytinyshinys.com/2017/07/11/eurostat/

- eurostat-package published

https://rpubs.com/muuankarski/27120

- Tutorial (vignette) for the eurostat R package

http://ropengov.github.io/eurostat/articles/eurostat_tutorial.html





```{r eval=FALSE, include=FALSE, echo=TRUE}
# install.packages("eurostat")
library(eurostat)
```



```{r eval=FALSE, include=FALSE, echo=TRUE}
TOC <- get_eurostat_toc()
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
TOC
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
query <- search_eurostat("road accidents", type = "table")

query
```



```{r eval=FALSE, include=FALSE, echo=TRUE}
query$code[[1]]
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
  query$title[[1]]
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
dat <- get_eurostat(id = "sdg_11_40", time_format = "num")
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
dat
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
countries <- c("UK", "SK", "FR", "PL", "ES", "PT", "TR")
t1 <- get_eurostat("sdg_11_40", filters = list(geo = countries))
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
t1
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
t2 <- get_eurostat(id = "sdg_11_40", time_format = "num")
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
table(t2$geo)
```





<!--chapter:end:eurostat.Rmd-->

---
title: "Evidence Synthesis Projects"
---

# revtools

revtools: Tools to Support Evidence Synthesis

https://cran.r-project.org/package=revtools

https://revtools.net/

https://revtools.net/user_manual/1_introduction.html



```{r eval=FALSE, include=FALSE, echo=TRUE}
# install.packages("revtools")
# devtools:: install_github("mjwestgate/revtools")
library(revtools)
```



```
data1 <- read_bibliography("my_data.ris")
data2 <- read_bibliography("my_data.bib")
```



```{r eval=FALSE, include=FALSE, echo=TRUE}
# data1 <- read_bibliography(file.choose())

data1 <- read_bibliography("data/citations.nbib")


```


```
# If the files are in the working directory:
file_names <- list.files()

# Or if they are in a subdirectory:
file_names <- paste0(
  "./raw_data/",
  list.files(path = "./raw_data/")
)

# Then import to a list
data_list <- lapply(
  file_names,
  function(x){read_bibliography(x)}
)
```

```{r eval=FALSE, include=FALSE, echo=TRUE}
data2 <- read_bibliography(
  "data/citations.nbib",
  return_df = FALSE
)

class(data2)

class(data2[[1]])

names(data2[[1]])
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
write_bibliography(data2, "data/denemeRIS", format = "ris")
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
# revtools::format_citation()
```


```
data <- read_bibliography("my_data.ris")

matches <- find_duplicates(
  data = data,
  match_variable = "title",
  group_variable = NULL,
  match_function = "fuzzdist",
  method = "fuzz_partial_ratio",
  threshold = 0
)
```

```
data_unique <- extract_unique_references(data, matches)
```

# screen_duplicates

https://revtools.net/user_manual/4_removing_duplicates.html

```{r eval=FALSE, include=FALSE, echo=TRUE}
screen_duplicates(data1)
```



```
# 1. standalone; load in data in the app
screen_titles()

# 2. the same, but save back to workspace on exit
result <- screen_titles() # ditto,

data <- read_bibliography("my_data.ris") # load in data

# 3. launch the app using data from the workspace
screen_titles(data)  

# 4. specify an object to return data to
result <- screen_titles(data)
```

```{r eval=FALSE, include=FALSE, echo=TRUE}
screen_titles(data1)
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
screen_abstracts(data1)
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
screen_topics(data1)
```




```{r eval=FALSE, include=FALSE, echo=TRUE}
library(revtools)
data <- read_bibliography("data/deneme2.ris")
dtm <- make_DTM(data)
model <- topicmodels::LDA(
  dtm,
  k = 15,
  LDA.control = list(
    burnin = 1000,
    iter = 6000,
    keep = 100
  )
)
```



```{r eval=FALSE, include=FALSE, echo=TRUE}
articles <- as.data.frame(data)
articles$topic <- topics(model)

# cross-tabulate to show number of articles per topic per year
popularity <- as.data.frame(
  xtabs(
    ~ year + topic,
    data = articles,
    drop.unused.levels = FALSE
  ),
stringsAsFactors = FALSE
)
popularity$year <- scale(
  as.numeric(popularity$year)
)
popularity$topic <- as.factor(popularity$topic)

# create a mixed model
library(lme4)
popularity_model <- glmer(Freq ~ 1 + (1 | topic) + (year -1 | topic),
	family = poisson(link = "log"),
	data = popularity
)

# export the results of this model
popularity_results <- ranef(popularity_model)$topic
colnames(popularity_results) <- c("intercept", "slope")

```



```{r eval=FALSE, include=FALSE, echo=TRUE}
library(ggplot2)
p <- ggplot(popularity_results,
  aes(x = intercept, y = slope)
) +
geom_point()
p
```


# RefManageR

RefManageR: Straightforward 'BibTeX' and 'BibLaTeX' Bibliography Management


https://cran.r-project.org/web/packages/RefManageR/index.html


# bibtex

bibtex: Bibtex Parser

https://cran.r-project.org/web/packages/bibtex/index.html


<!--chapter:end:EvidenceSynthesisProjects.Rmd-->

---
title: "Explatory Data Analysis & Summary Statistics"
---


# DataExplorer

https://cran.r-project.org/web/packages/DataExplorer/vignettes/dataexplorer-intro.html


https://boxuancui.github.io/DataExplorer/



<!--chapter:end:ExplatoryDataAnalysisSummaryStatistics.Rmd-->

---
title: "My R Codes For Data Analysis"
---


## File organization best practices

This page summarises how to organize files and analysis before everything gets jumbled up:
[Setting up a reproducible data analysis workflow in R](https://andrewbtran.github.io/NICAR/2018/workflow/docs/01-workflow_intro.html)

Basically they suggest:
- using a project and project folder in RStudio for each analysis
- using `packrat` as much as possible

`setwd()` and `getwd()` is not necesary when you use projects.



---

- **Why should I use the here package when I'm already using projects?**

https://malco.io/2018/11/05/why-should-i-use-the-here-package/


```{r eval=FALSE, include=FALSE, echo=TRUE}
library(here)
here()
```

```{r eval=FALSE, include=FALSE, echo=TRUE}
dr_here()
```

```{r eval=FALSE, include=FALSE, echo=TRUE}
here("figure", "figure.png")
```

```{r eval=FALSE, include=FALSE, echo=TRUE}
file.path("figure", "figure.png")
```

```{r eval=FALSE, include=FALSE, echo=TRUE}
read_csv(here("data", "mtcars.csv"))
```


<!--chapter:end:FileOrganization.Rmd-->

---
title: "All tables examples"
author: "Ewen Harrison"
---

```
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{All tables examples}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
```


## 1 Cross tables

Two-way tables are used extensively in healthcare research, e.g. a 2x2 table comparing two factors with two levels each, or table 1 from a typical clinical study or trial

The main functions all take a `dependent` variable - the outcome (maximum of 5 levels) - and `explanatory` variables - predictors or exposures (any number categorical or continuous variables). 

### 1.01 Default

```{r eval=FALSE, include=FALSE, echo=TRUE}
library(finalfit)
explanatory = c("age", "age.factor", "sex.factor", "obstruct.factor")
dependent = "perfor.factor"
colon_s %>%
  summary_factorlist(dependent, explanatory) -> t
```

```{r eval=FALSE, include=FALSE, echo=TRUE}
library(knitr)
kable(t, row.names=FALSE, align = c("l", "l", "r", "r"))
```

Note, chi-squared warnings will be generated when the expected count in any cell is less than 5. Fisher's exact test can be used as below, or go straight to a univariable logistic regression, e.g. `colon_s %>% finalfit(dependent, explanatory)`

### 1.02 Add or edit variable labels

```{r eval=FALSE, include=FALSE, echo=TRUE}
library(finalfit)
library(dplyr)
explanatory = c("age", "age.factor", "sex.factor", "obstruct.factor")
dependent = "perfor.factor"
colon_s %>%
	mutate(
		sex.factor = ff_label(sex.factor, "Gender")
	) %>% 
  summary_factorlist(dependent, explanatory) -> t
```

```{r eval=FALSE, include=FALSE, echo=TRUE}
library(knitr)
kable(t, row.names=FALSE, align = c("l", "l", "r", "r"))
```

### 1.03 P-value for hypothesis test

Chi-squared for categorical, Kruskal-Wallis/Mann-Whitney for continuous

```{r eval=FALSE, include=FALSE, echo=TRUE, warning=FALSE}
library(finalfit)
explanatory = c("age", "age.factor", "sex.factor", "obstruct.factor")
dependent = "perfor.factor"
colon_s %>%
	summary_factorlist(dependent, explanatory, p = TRUE) -> t
```

```{r eval=FALSE, include=FALSE, echo=TRUE}
library(knitr)
kable(t, row.names=FALSE, align = c("l", "l", "r", "r", "r"))
```

### 1.04 With Fisher's exact test

```{r eval=FALSE, include=FALSE, echo=TRUE}
library(finalfit)
explanatory = c("age", "age.factor", "sex.factor", "obstruct.factor")
dependent = "perfor.factor"
colon_s %>%
  summary_factorlist(dependent, explanatory, p = TRUE, catTest = catTestfisher) -> t
```

```{r eval=FALSE, include=FALSE, echo=TRUE}
library(knitr)
kable(t, row.names=FALSE, align = c("l", "l", "r", "r"))
```

### 1.05 Median (interquartile range) instead of mean (standard deviation)

... for continuous variables.

```{r eval=FALSE, include=FALSE, echo=TRUE, warning=FALSE}
library(finalfit)
explanatory = c("age", "age.factor", "sex.factor", "obstruct.factor")
dependent = "perfor.factor"
colon_s %>%
	summary_factorlist(dependent, explanatory, p = TRUE, cont = "median") -> t
```

```{r eval=FALSE, include=FALSE, echo=TRUE}
library(knitr)
kable(t, row.names=FALSE, align = c("l", "l", "r", "r", "r"))
```

### 1.06 Missing values for the explanatory variables

Always do this when describing your data.

```{r eval=FALSE, include=FALSE, echo=TRUE, warning=FALSE}
library(finalfit)
explanatory = c("age", "age.factor", "sex.factor", "obstruct.factor")
dependent = "perfor.factor"
colon_s %>%
	summary_factorlist(dependent, explanatory, p = TRUE, cont = "median", na_include = TRUE) -> t
```

```{r eval=FALSE, include=FALSE, echo=TRUE}
library(knitr)
kable(t, row.names=FALSE, align = c("l", "l", "r", "r", "r"))
```

### 1.07 Column proportions (rather than row)

```{r eval=FALSE, include=FALSE, echo=TRUE, warning=FALSE}
library(finalfit)
explanatory = c("age", "age.factor", "sex.factor", "obstruct.factor")
dependent = "perfor.factor"
colon_s %>%
	summary_factorlist(dependent, explanatory, p = TRUE, cont = "median", na_include = TRUE,
										 column = TRUE) -> t
```

```{r eval=FALSE, include=FALSE, echo=TRUE}
library(knitr)
kable(t, row.names=FALSE, align = c("l", "l", "r", "r", "r"))
```

### 1.08 Total column

```{r eval=FALSE, include=FALSE, echo=TRUE, warning=FALSE}
library(finalfit)
explanatory = c("age", "age.factor", "sex.factor", "obstruct.factor")
dependent = "perfor.factor"
colon_s %>%
	summary_factorlist(dependent, explanatory, p = TRUE, cont = "median", na_include = TRUE,
										 column = TRUE, total_col = TRUE) -> t
```

```{r eval=FALSE, include=FALSE, echo=TRUE}
library(knitr)
kable(t, row.names=FALSE, align = c("l", "l", "r", "r", "r", "r"))
```

### 1.09 Order a variable by total

This is intended for when there is only one `explanatory` variable.  

```{r eval=FALSE, include=FALSE, echo=TRUE, warning=FALSE}
library(finalfit)
explanatory = c("extent.factor")
dependent = "perfor.factor"
colon_s %>%
	summary_factorlist(dependent, explanatory, p = TRUE, cont = "median", na_include = TRUE,
										 column = TRUE, total_col = TRUE, orderbytotal = TRUE) -> t
```

```{r eval=FALSE, include=FALSE, echo=TRUE}
library(knitr)
kable(t, row.names=FALSE, align = c("l", "l", "r", "r", "r", "r"))
```

### 1.10 Label with `dependent` name

```{r eval=FALSE, include=FALSE, echo=TRUE, warning=FALSE}
library(finalfit)
explanatory = c("age", "age.factor", "sex.factor", "obstruct.factor")
dependent = "perfor.factor"
colon_s %>%
	summary_factorlist(dependent, explanatory, p = TRUE, cont = "median", na_include = TRUE,
										 column = TRUE, total_col = TRUE, add_dependent_label = TRUE) -> t
```

```{r eval=FALSE, include=FALSE, echo=TRUE}
library(knitr)
kable(t, row.names=FALSE, align = c("l", "l", "r", "r", "r", "r"))
```

The dependent name cannot be passed directly to the table intentionally. This is to avoid errors when code is copied and the name is not updated. Change the dependent label using the following. The prefix ("Dependent: ") and any suffix can be altered. 

```{r eval=FALSE, include=FALSE, echo=TRUE, warning=FALSE}
library(finalfit)
explanatory = c("age", "age.factor", "sex.factor", "obstruct.factor")
dependent = "perfor.factor"
colon_s %>%
  dplyr::mutate(
    perfor.factor = ff_label(perfor.factor, "Perforated cancer")
	) %>% 
  summary_factorlist(dependent, explanatory, p = TRUE, cont = "median", na_include = TRUE,
    column = TRUE, total_col = TRUE, add_dependent_label = TRUE, dependent_label_prefix = "") -> t
```

```{r eval=FALSE, include=FALSE, echo=TRUE}
library(knitr)
kable(t, row.names=FALSE, align = c("l", "l", "r", "r", "r", "r"))
```

### 1.11 Dependent variable with any number of factor levels supported

```{r eval=FALSE, include=FALSE, echo=TRUE, warning=FALSE}
library(finalfit)
explanatory = c("age", "age.factor", "sex.factor", "obstruct.factor")
dependent = "extent.factor"
colon_s %>%
  dplyr::mutate(
    perfor.factor = ff_label(perfor.factor, "Perforated cancer")
  ) %>% 
  summary_factorlist(dependent, explanatory, p = TRUE, cont = "median", na_include = TRUE,
    column = TRUE, total_col = TRUE, add_dependent_label = TRUE, dependent_label_prefix = "") -> t
```

```{r eval=FALSE, include=FALSE, echo=TRUE}
library(knitr)
kable(t, row.names=FALSE, align = c("l", "l", "r", "r", "r", "r", "r", "r"))
```

### 1.12 Explanatory variable defaults to factor when ≤5 distinct values

```{r eval=FALSE, include=FALSE, echo=TRUE, warning=FALSE}
library(finalfit)

# Here, `extent` is a continuous variable with 4 distinct values. 
# Any continuous variable with 5 or fewer unique values is converted silently to factor 
# e.g.
explanatory = c("extent")
dependent = "mort_5yr"
colon_s %>%
  summary_factorlist(dependent, explanatory) -> t
```

```{r eval=FALSE, include=FALSE, echo=TRUE}
library(knitr)
kable(t, row.names=FALSE, align = c("l", "l", "r", "r", "r", "r", "r", "r"))
```

### 1.13 Keep as continous variable when ≤5 distinct values

```{r eval=FALSE, include=FALSE, echo=TRUE, warning=FALSE}
library(finalfit)
explanatory = c("extent")
dependent = "mort_5yr"
colon_s %>%
  summary_factorlist(dependent, explanatory, cont_cut = 3) -> t
```

```{r eval=FALSE, include=FALSE, echo=TRUE}
library(knitr)
kable(t, row.names=FALSE, align = c("l", "l", "r", "r", "r", "r", "r", "r"))
```

### 1.14 Stratified crosstables

I've been meaning to include support for table stratification for a while. I have delayed for a good reason. Perhaps the most straightforward way to implement stratificiation is with `dplyr::group_by()`. However, the non-standard evaluation required for multiple strata may confuse as it is not implemented else where in the package (doesn't work with `group_by_`). This translates to whether variable names are passed in quotes or not. Finally,. `dplyr::do()` is planned for deprecation, but there is no good alternative at the moment. Anyway, here is a solution, which while not that pretty, is very effective. 

```{r eval=FALSE, include=FALSE, echo=TRUE, warning=FALSE}
library(dplyr)
# Piped function to generate stratified crosstabs table
explanatory = c("age.factor", "sex.factor")
dependent = "rx.factor"

# Pick option below
split = "rx.factor"
split = c("perfor.factor", "node4.factor")

colon_s %>%
  group_by(!!! syms(split)) %>% #Looks awkward, but this keeps quoted var names (rather than unquoted)
  do(
    summary_factorlist(., dependent, explanatory, p = TRUE)
  ) %>%
  data.frame() %>%
  dependent_label(colon_s, dependent, prefix = "") %>%
  colname2label(split) -> t
```




```{r eval=FALSE, include=FALSE, echo=TRUE}
library(knitr)
kable(t, row.names=FALSE, align = c("l", "l", "l", "l", "r", "r", "r"))
```

## 2 Model tables with `finalfit()`

### 2.01 Default

Logistic regression first.

```{r eval=FALSE, include=FALSE, echo=TRUE, warning=FALSE, message=FALSE}
library(finalfit)
explanatory = c("age.factor", "sex.factor", "obstruct.factor", "perfor.factor")
dependent = "mort_5yr"
colon_s %>%
	finalfit(dependent, explanatory) -> t
```

```{r eval=FALSE, include=FALSE, echo=TRUE}
library(knitr)
kable(t, row.names=FALSE, align = c("l", "l", "r", "r", "r", "r", "r", "r"))
```

### 2.02 Hide reference levels

Most appropriate when all explanatory variables are continuous or well-known binary variables, such as sex. 

```{r eval=FALSE, include=FALSE, echo=TRUE, warning=FALSE, message=FALSE}
library(finalfit)
explanatory = c("age", "sex.factor")
dependent = "mort_5yr"
colon_s %>%
	finalfit(dependent, explanatory, add_dependent_label = FALSE) %>% 
	ff_remove_ref() %>% 
	dependent_label(colon_s, dependent)-> t
```

```{r eval=FALSE, include=FALSE, echo=TRUE}
library(knitr)
kable(t, row.names=FALSE, align = c("l", "l", "r", "r", "r", "r", "r", "r"))
```

### 2.03 Model metrics

```{r eval=FALSE, include=FALSE, echo=TRUE, warning=FALSE, message=FALSE}
library(finalfit)
explanatory = c("age.factor", "sex.factor", "obstruct.factor", "perfor.factor")
dependent = "mort_5yr"
colon_s %>%
	finalfit(dependent, explanatory, metrics = TRUE) -> t
```

```{r eval=FALSE, include=FALSE, echo=TRUE}
library(knitr)
kable(t[[1]], row.names=FALSE, align = c("l", "l", "r", "r", "r", "r", "r", "r"))
kable(t[[2]], row.names=FALSE, align = c("l", "l", "r", "r", "r", "r", "r", "r"), col.names = "")
```

### 2.04 Model metrics can be applied to all supported base models

```{r eval=FALSE, include=FALSE, echo=TRUE, warning=FALSE, message=FALSE}
library(finalfit)
glm(mort_5yr ~ age.factor + sex.factor + obstruct.factor + perfor.factor, data = colon_s, family = "binomial") %>% 
	ff_metrics() -> t
```

```{r eval=FALSE, include=FALSE, echo=TRUE}
library(knitr)
kable(t, row.names=FALSE, align = c("l", "l", "r", "r", "r", "r", "r", "r"), col.names = "")
```

### 2.05 Reduced model

```{r eval=FALSE, include=FALSE, echo=TRUE, warning=FALSE, message=FALSE}
library(finalfit)
explanatory = c("age.factor", "sex.factor", "obstruct.factor", "perfor.factor")
explanatory_multi = c("age.factor", "obstruct.factor")
dependent = "mort_5yr"
colon_s %>%
	finalfit(dependent, explanatory, explanatory_multi) -> t
```

```{r eval=FALSE, include=FALSE, echo=TRUE}
library(knitr)
kable(t, row.names=FALSE, align = c("l", "l", "r", "r", "r", "r", "r", "r"))
```


### 2.06 Include all models

```{r eval=FALSE, include=FALSE, echo=TRUE, warning=FALSE, message=FALSE}
library(finalfit)
explanatory = c("age.factor", "sex.factor", "obstruct.factor", "perfor.factor")
explanatory_multi = c("age.factor", "obstruct.factor")
dependent = "mort_5yr"
colon_s %>%
	finalfit(dependent, explanatory, explanatory_multi, metrics = TRUE, keep_models = TRUE) -> t
```

```{r eval=FALSE, include=FALSE, echo=TRUE}
library(knitr)
kable(t[[1]], row.names=FALSE, align = c("l", "l", "r", "r", "r", "r", "r", "r"))
kable(t[[2]], row.names=FALSE, align = c("l", "l", "r", "r", "r", "r", "r", "r"), col.names = "")
```

### 2.06 Interactions

Interactions can be specified in the normal way. Formatting the output is trickier. At the moment, we have left the default model output. This can be adjusted as necessary.

```{r eval=FALSE, include=FALSE, echo=TRUE, warning=FALSE, message=FALSE}
library(finalfit)
explanatory = c("age.factor*sex.factor", "obstruct.factor", "perfor.factor")
dependent = "mort_5yr"
colon_s %>%
	finalfit(dependent, explanatory) -> t
```

```{r eval=FALSE, include=FALSE, echo=TRUE}
library(knitr)
kable(t, row.names=FALSE, align = c("l", "l", "r", "r", "r", "r", "r", "r"))
```

### 2.07 Interactions: create interaction variable with two factors

```{r eval=FALSE, include=FALSE, echo=TRUE, warning=FALSE, message=FALSE}
library(finalfit)
#explanatory = c("age.factor*sex.factor", "obstruct.factor", "perfor.factor")
explanatory = c("obstruct.factor", "perfor.factor")
dependent = "mort_5yr"
colon_s %>%
	ff_interaction(age.factor, sex.factor) %>% 
	finalfit(dependent, c(explanatory, "age.factor__sex.factor")) -> t
```

```{r eval=FALSE, include=FALSE, echo=TRUE}
library(knitr)
kable(t, row.names=FALSE, align = c("l", "l", "r", "r", "r", "r", "r", "r"))
```

### 2.08 Dependent name

The dependent name cannot be specified directly intentionally. This is to prevent errors when copying code. Re-label using `ff_label()`. The dependent prefix and suffix can also be altered. 

```{r eval=FALSE, include=FALSE, echo=TRUE, warning=FALSE, message=FALSE}
library(finalfit)
explanatory = c("age.factor", "sex.factor", "obstruct.factor", "perfor.factor")
dependent = "mort_5yr"
colon_s %>%
	dplyr::mutate(
		mort_5yr = ff_label(mort_5yr, "5-year mortality")
	) %>% 
	finalfit(dependent, explanatory, dependent_label_prefix = "",
					 dependent_label_suffix = " (full model)") -> t
```

```{r eval=FALSE, include=FALSE, echo=TRUE}
library(knitr)
kable(t, row.names=FALSE, align = c("l", "l", "r", "r", "r", "r", "r", "r"))
```

### 2.09 Estimate name

```{r eval=FALSE, include=FALSE, echo=TRUE, warning=FALSE, message=FALSE}
library(finalfit)
explanatory = c("age.factor", "sex.factor", "obstruct.factor", "perfor.factor")
dependent = "mort_5yr"
colon_s %>%
	finalfit(dependent, explanatory, estimate_name = "Odds ratio") -> t
```

```{r eval=FALSE, include=FALSE, echo=TRUE}
library(knitr)
kable(t, row.names=FALSE, align = c("l", "l", "r", "r", "r", "r", "r", "r"))
```


### 2.10 Digits / decimal places

Number of digits to round to regression results. (1) estimate, (2) confidence interval limits, (3) p-value. Default is c(2,2,3). Trailing zeros are preserved. Number of decimal places for counts and mean (sd) / median (IQR) not currently supported. Defaults are senisble :)

```{r eval=FALSE, include=FALSE, echo=TRUE, warning=FALSE, message=FALSE}
library(finalfit)
explanatory = c("age.factor", "sex.factor", "obstruct.factor", "perfor.factor")
dependent = "mort_5yr"
colon_s %>%
	finalfit(dependent, explanatory, digits = c(3,3,4)) -> t
```

```{r eval=FALSE, include=FALSE, echo=TRUE}
library(knitr)
kable(t, row.names=FALSE, align = c("l", "l", "r", "r", "r", "r", "r", "r"))
```

### 2.11 Confidence interval type

One of `c("profile", "default")` for GLM models (`confint.glm()`). Note, a little awkwardly, the 'default' setting is `profile`, rather than `default`. Profile levels are probably a little more accurate. Only go to default if taking a significant length of time for profile, i.e. data is greater than hundreds of thousands of lines.

For glmer/lmer models (`confint.merMod()`), `c("profile", "Wald", "boot")`. Not implemented for `lm()`, `coxph()` or `coxphlist`, which use default.

```{r eval=FALSE, include=FALSE, echo=TRUE, warning=FALSE, message=FALSE}
library(finalfit)
explanatory = c("age.factor", "sex.factor", "obstruct.factor", "perfor.factor")
dependent = "mort_5yr"
colon_s %>%
	finalfit(dependent, explanatory, confint_type = "default") -> t
```

```{r eval=FALSE, include=FALSE, echo=TRUE}
library(knitr)
kable(t, row.names=FALSE, align = c("l", "l", "r", "r", "r", "r", "r", "r"))
```

### 2.12 Confidence interval level

Probably never change this :) Note, the p-value is intentionally not included for confidence levels other than 95% to avoid confusion.

```{r eval=FALSE, include=FALSE, echo=TRUE, warning=FALSE, message=FALSE}
library(finalfit)
explanatory = c("age.factor", "sex.factor", "obstruct.factor", "perfor.factor")
dependent = "mort_5yr"
colon_s %>%
	finalfit(dependent, explanatory, confint_level = 0.90) -> t
```

```{r eval=FALSE, include=FALSE, echo=TRUE}
library(knitr)
kable(t, row.names=FALSE, align = c("l", "l", "r", "r", "r", "r", "r", "r"))
```

### 2.13 Confidence interval separation

Some like to avoid the hyphen so as not to confuse with minus sign. Obviously not an issue in logistic regression.   

```{r eval=FALSE, include=FALSE, echo=TRUE, warning=FALSE, message=FALSE}
library(finalfit)
explanatory = c("age.factor", "sex.factor", "obstruct.factor", "perfor.factor")
dependent = "mort_5yr"
colon_s %>%
	finalfit(dependent, explanatory, confint_sep = " to ") -> t
```

```{r eval=FALSE, include=FALSE, echo=TRUE}
library(knitr)
kable(t, row.names=FALSE, align = c("l", "l", "r", "r", "r", "r", "r", "r"))
```

### 2.14 Mixed effects random-intercept model

At its simplest, a random-intercept model can be specified using a single quoted variable. In this example, it is the equivalent of quoting `{r # andom_effect = "(1 | hospital)"`. 

```{r eval=FALSE, include=FALSE, echo=TRUE, warning=FALSE, message=FALSE}
library(finalfit)
explanatory = c("age.factor", "sex.factor", "obstruct.factor", "perfor.factor")
dependent = "mort_5yr"
random_effect = "hospital"
colon_s %>%
	finalfit(dependent, explanatory, random_effect = random_effect,
					 dependent_label_suffix = " (random intercept)") -> t
```

```{r eval=FALSE, include=FALSE, echo=TRUE}
library(knitr)
kable(t, row.names=FALSE, align = c("l", "l", "r", "r", "r", "r", "r", "r"))
```

### 2.15 Mixed effects random-slope model

In the example below, allow the effect of age on outcome to vary by hospital. Note, this specification must have parentheses included.

```{r eval=FALSE, include=FALSE, echo=TRUE, warning=FALSE, message=FALSE}
library(finalfit)
explanatory = c("age.factor", "sex.factor", "obstruct.factor", "perfor.factor")
dependent = "mort_5yr"
random_effect = "(age.factor | hospital)"
colon_s %>%
	finalfit(dependent, explanatory, random_effect = random_effect,
					 dependent_label_suffix = " (random slope: age)") -> t
```

```{r eval=FALSE, include=FALSE, echo=TRUE}
library(knitr)
kable(t, row.names=FALSE, align = c("l", "l", "r", "r", "r", "r", "r", "r"))
```

### 2.16 Mixed effects random-slope model directly from `lme4`

Clearly, as models get more complex, parameters such as random effect group variances may require to be extracted directly from model outputs. 

```{r eval=FALSE, include=FALSE, echo=TRUE, warning=FALSE, message=FALSE}
library(finalfit)
explanatory = c("age.factor", "sex.factor", "obstruct.factor", "perfor.factor")
dependent = "mort_5yr"
random_effect = "(age.factor | hospital)"
colon_s %>% 
	lme4::glmer(mort_5yr ~ age.factor + (age.factor | hospital), family = "binomial", data = .) %>% 
	broom::tidy() -> t
```

```{r eval=FALSE, include=FALSE, echo=TRUE}
library(knitr)
kable(t, row.names=FALSE, align = c("l", "l", "r", "r", "r", "r", "r", "r"))
```

### 2.17 Exclude all missing data in final model from univariable analyses 

This can be useful if you want the numbers in the final table to match the final multivariable model. However, be careful to include a full explanation of this in the methods and the reason for exluding the missing data. 

```{r eval=FALSE, include=FALSE, echo=TRUE, warning=FALSE, message=FALSE}
library(finalfit)
explanatory = c("age.factor", "sex.factor", "obstruct.factor", "perfor.factor")
dependent = 'mort_5yr'
colon_s %>%
	dplyr::select(explanatory, dependent) %>%
	na.omit() %>%
	finalfit(dependent, explanatory) -> t
```

```{r eval=FALSE, include=FALSE, echo=TRUE}
library(knitr)
kable(t, row.names=FALSE, align = c("l", "l", "r", "r", "r", "r", "r", "r"))
```

### 2.18 Linear regression

```{r eval=FALSE, include=FALSE, echo=TRUE, warning=FALSE, message=FALSE}
library(finalfit)
explanatory = c("age.factor", "sex.factor", "obstruct.factor", "perfor.factor")
dependent = 'nodes'
colon_s %>%
	finalfit(dependent, explanatory) -> t
```

```{r eval=FALSE, include=FALSE, echo=TRUE}
library(knitr)
kable(t, row.names=FALSE, align = c("l", "l", "r", "r", "r", "r", "r", "r"))
```

### 2.19 Mixed effects random-intercept linear regression

```{r eval=FALSE, include=FALSE, echo=TRUE, warning=FALSE, message=FALSE}
library(finalfit)
explanatory = c("age.factor", "sex.factor", "obstruct.factor", "perfor.factor")
dependent = "nodes"
random_effect = "hospital"
colon_s %>%
	finalfit(dependent, explanatory, random_effect = random_effect,
					 dependent_label_suffix = " (random intercept)") -> t
```

```{r eval=FALSE, include=FALSE, echo=TRUE}
library(knitr)
kable(t, row.names=FALSE, align = c("l", "l", "r", "r", "r", "r", "r", "r"))
```

### 2.20 Mixed effects random-slope linear regression

```{r eval=FALSE, include=FALSE, echo=TRUE, warning=FALSE, message=FALSE}
library(finalfit)
explanatory = c("age.factor", "sex.factor", "obstruct.factor", "perfor.factor")
dependent = "nodes"
random_effect = "(age.factor | hospital)"
colon_s %>%
	finalfit(dependent, explanatory, random_effect = random_effect,
					 dependent_label_suffix = " (random slope: age)") -> t
```

```{r eval=FALSE, include=FALSE, echo=TRUE}
library(knitr)
kable(t, row.names=FALSE, align = c("l", "l", "r", "r", "r", "r", "r", "r"))
```

### 2.21 Cox proportional hazards model (survival / time to event)

```{r eval=FALSE, include=FALSE, echo=TRUE, warning=FALSE, message=FALSE}
library(finalfit)
explanatory = c("age.factor", "sex.factor", "obstruct.factor", "perfor.factor")
dependent = "Surv(time, status)"
colon_s %>%
	finalfit(dependent, explanatory) -> t
```

```{r eval=FALSE, include=FALSE, echo=TRUE}
library(knitr)
kable(t, row.names=FALSE, align = c("l", "l", "r", "r", "r", "r", "r", "r"))
```

### 2.22 Cox proportional hazards model: change dependent label

As above, the dependent label cannot be specfied directly in the model to avoid errors. However, in survival modelling the surivial object specification can be long or awkward. Therefore, here is the work around. 

```{r eval=FALSE, include=FALSE, echo=TRUE, warning=FALSE, message=FALSE}
library(finalfit)
explanatory = c("age.factor", "sex.factor", "obstruct.factor", "perfor.factor")
dependent = "Surv(time, status)"
colon_s %>%
	finalfit(dependent, explanatory, add_dependent_label = FALSE) %>% 
	dplyr::rename("Overall survival" = label) %>% 
	dplyr::rename(" " = levels)	-> t
```

```{r eval=FALSE, include=FALSE, echo=TRUE}
library(knitr)
kable(t, row.names=FALSE, align = c("l", "l", "r", "r", "r", "r", "r", "r"))
```

## 3 Model tables manually using `ff_merge()`

### 3.1 Basic table

Note `summary_factorlist()` needs argument, `fit_id = TRUE`. 

```{r eval=FALSE, include=FALSE, echo=TRUE, warning=FALSE, message=FALSE}
library(finalfit)
library(dplyr)
explanatory = c("age.factor", "sex.factor", "obstruct.factor", "perfor.factor")
dependent = "mort_5yr"

## Crosstable
colon_s %>%
	summary_factorlist(dependent, explanatory, fit_id=TRUE) -> table_1

## Univariable
colon_s %>%
	glmuni(dependent, explanatory) %>%
	fit2df(estimate_suffix=" (univariable)") -> table_2

## Merge

table_1 %>% 
	ff_merge(table_2) %>% 
	select(-c(fit_id, index)) %>% 
	dependent_label(colon_s, dependent)-> t
```

```{r eval=FALSE, include=FALSE, echo=TRUE}
library(knitr)
kable(t, row.names=FALSE, align = c("l", "l", "r", "r", "r", "r", "r", "r"))
```

### 3.2 Complex table (all in single pipe)

```{r eval=FALSE, include=FALSE, echo=TRUE, warning=FALSE, message=FALSE}
library(finalfit)
library(dplyr)
explanatory = c("age.factor", "sex.factor", "obstruct.factor", "perfor.factor")
random_effect = "hospital"
dependent = "mort_5yr"

# All in one pipe

colon_s %>%
	## Crosstable
	summary_factorlist(dependent, explanatory, fit_id=TRUE)  %>% 
	
	## Add univariable
	ff_merge(
		glmuni(colon_s, dependent, explanatory) %>%
			fit2df(estimate_suffix=" (univariable)")
	) %>% 
	
	## Add multivariable
	ff_merge(
		glmmulti(colon_s, dependent, explanatory) %>%
			fit2df(estimate_suffix=" (multivariable)")
	) %>% 
	
	## Add mixed effects
	ff_merge(
		glmmixed(colon_s, dependent, explanatory, random_effect) %>%
			fit2df(estimate_suffix=" (multilevel)") 
	) %>% 
	select(-c(fit_id, index)) %>% 
	dependent_label(colon_s, dependent) -> t
```

```{r eval=FALSE, include=FALSE, echo=TRUE}
library(knitr)
kable(t, row.names=FALSE, align = c("l", "l", "r", "r", "r", "r", "r", "r"))
```

### 3.3 Other GLM models

#### Poisson

```{r eval=FALSE, include=FALSE, echo=TRUE, warning=FALSE, message=FALSE}
library(finalfit)
library(dplyr)

## Dobson (1990) Page 93: Randomized Controlled Trial :
counts = c(18,17,15,20,10,20,25,13,12)
outcome = gl(3,1,9)
treatment = gl(3,3)
d.AD <- data.frame(treatment, outcome, counts)

dependent = "counts"
explanatory = c("outcome", "treatment")

fit_uni = d.AD %>% 
	glmuni(dependent, explanatory, family = poisson) %>% 
	fit2df(estimate_name = "Rate ratio (univariable)")

fit_multi = d.AD %>% 
	glmmulti(dependent, explanatory, family = poisson) %>% 
	fit2df(estimate_name = "Rate ratio (multivariable)")

# All in one pipe
d.AD %>%
	## Crosstable
	summary_factorlist(dependent, explanatory, cont = "median", fit_id=TRUE)  %>% 
	
	## Add univariable
	ff_merge(fit_uni, estimate_name = "Rate ratio") %>% 
	
	## Add multivariable
	ff_merge(fit_multi, estimate_name = "Rate ratio") %>% 
	
	select(-c(fit_id, index)) %>% 
	dependent_label(d.AD, dependent) -> t
```

```{r eval=FALSE, include=FALSE, echo=TRUE}
library(knitr)
kable(t, row.names=FALSE, align = c("l", "l", "r", "r", "r", "r", "r", "r"))
```

#### Gamma

```{r eval=FALSE, include=FALSE, echo=TRUE, warning=FALSE, message=FALSE}
library(finalfit)
library(dplyr)

# A Gamma example, from McCullagh & Nelder (1989, pp. 300-2)
clotting <- data.frame(
    u = c(5,10,15,20,30,40,60,80,100),
    lot1 = c(118,58,42,35,27,25,21,19,18),
    lot2 = c(69,35,26,21,18,16,13,12,12))

dependent = "lot1"
explanatory = "log(u)"

fit_uni = clotting %>% 
	glmuni(dependent, explanatory, family = Gamma) %>% 
	fit2df(estimate_name = "Coefficient", exp = FALSE, digits = c(3,3,4))

# All in one pipe
clotting %>%
	## Crosstable
	summary_factorlist(dependent, explanatory, cont = "median", fit_id=TRUE)  %>% 
	
	## Add fit
	ff_merge(fit_uni) %>% 
	
	select(-c(fit_id, index)) %>% 
	dependent_label(colon_s, dependent) -> t
```

```{r eval=FALSE, include=FALSE, echo=TRUE}
library(knitr)
kable(t, row.names=FALSE, align = c("l", "l", "r", "r", "r", "r", "r", "r"))
```

### 3.4 Weighted regression

```{r eval=FALSE, include=FALSE, echo=TRUE, warning=FALSE, message=FALSE}
library(finalfit)
library(dplyr)
explanatory = c("age.factor", "sex.factor", "obstruct.factor", "perfor.factor")
dependent = "mort_5yr"
weights = runif(dim(colon_s)[1]) # random just for example

# All in one pipe
colon_s %>%
	## Crosstable
	summary_factorlist(dependent, explanatory, fit_id=TRUE)  %>% 
	
	## Add univariable
	ff_merge(
		glmuni(colon_s, dependent, explanatory, weights = weights, family = quasibinomial) %>%
			fit2df(estimate_suffix=" (univariable)")
	) %>% 
	
	## Add multivariable
	ff_merge(
		glmmulti(colon_s, dependent, explanatory, weights = weights, family = quasibinomial) %>%
			fit2df(estimate_suffix=" (multivariable)")
	) %>% 
	select(-c(fit_id, index)) %>% 
	dependent_label(colon_s, dependent) -> t
```

```{r eval=FALSE, include=FALSE, echo=TRUE}
library(knitr)
kable(t, row.names=FALSE, align = c("l", "l", "r", "r", "r", "r", "r", "r"))
```

### 3.5 Using base R functions

Note `ff_formula()` convenience function to make multivariable formula (`y ~ x1 + x2 + x3` etc.) from a `dependent` and `explanatory` vector of names. 

```{r eval=FALSE, include=FALSE, echo=TRUE, warning=FALSE, message=FALSE}
library(finalfit)
library(dplyr)
explanatory = c("age.factor", "sex.factor", "obstruct.factor", "perfor.factor")
dependent = "mort_5yr"

# All in one pipe

colon_s %>%
	## Crosstable
	summary_factorlist(dependent, explanatory, fit_id=TRUE)  %>% 
	
	## Add univariable
	ff_merge(
		glmuni(colon_s, dependent, explanatory) %>%
			fit2df(estimate_suffix=" (univariable)")
	) %>% 
	
	## Add multivariable
	ff_merge(
		glm(
			ff_formula(dependent, explanatory), data = colon_s, family = "binomial", weights = NULL
		) %>%
			fit2df(estimate_suffix=" (multivariable)")
	) %>% 
	
	select(-c(fit_id, index)) %>% 
	dependent_label(colon_s, dependent) -> t
```

```{r eval=FALSE, include=FALSE, echo=TRUE}
library(knitr)
kable(t, row.names=FALSE, align = c("l", "l", "r", "r", "r", "r", "r", "r"))
```

### 3.6 Edit table rows

This can be done as any dataframe would be edited. 

```{r eval=FALSE, include=FALSE, echo=TRUE, warning=FALSE, message=FALSE}
library(finalfit)
library(dplyr)
explanatory = c("age.factor*sex.factor", "obstruct.factor", "perfor.factor")
dependent = "mort_5yr"

# Run model for term test
fit <- glm(
	ff_formula(dependent, explanatory), 
	data=colon_s, family = binomial
)

# Not run
#term_test <- survey::regTermTest(fit, "age.factor:sex.factor")

# Run final table with results of term test
colon_s %>%
	finalfit(dependent, explanatory) %>%
	rbind(c(
		"age.factor:sex.factor (overall)",
		"Interaction",
		"-",
		"-",
		"-",
		paste0("p = 0.775")
	))-> t
```

```{r eval=FALSE, include=FALSE, echo=TRUE}
library(knitr)
kable(t, row.names=FALSE, align = c("l", "l", "r", "r", "r", "r", "r", "r"))
```

### 3.7 Base model + individual explanatory variables

This was an email enquiry about how to build on a base model. The example request was in a survival context.

```{r eval=FALSE, include=FALSE, echo=TRUE, warning=FALSE, message=FALSE}
library(finalfit)
library(dplyr)

mydata = colon_s
base_explanatory = c("age.factor", "sex.factor")
explanatory = c("obstruct.factor", "perfor.factor", "node4.factor")
dependent = "Surv(time, status)"

mydata %>%
	# Counts
	summary_factorlist(dependent, c(base_explanatory,
																	explanatory),
										 column = TRUE,
										 fit_id = TRUE) %>% 
	
	# Univariable
	ff_merge(
		coxphuni(mydata, dependent, c(base_explanatory, explanatory)) %>% 
			fit2df(estimate_suffix = " (Univariable)")
	) %>% 
	
	# Base
	ff_merge(
		coxphmulti(mydata, dependent, base_explanatory) %>% 
			fit2df(estimate_suffix = " (Base model)")
	) %>% 
	
	# Model 1
	ff_merge(
		coxphmulti(mydata, dependent, c(base_explanatory, explanatory[1])) %>% 
			fit2df(estimate_suffix = " (Model 1)")
	) %>% 
	
	# Model 2
	ff_merge(
		coxphmulti(mydata, dependent, c(base_explanatory, explanatory[2])) %>% 
			fit2df(estimate_suffix = " (Model 2)")
	) %>% 
	
	# Model 3
	ff_merge(
		coxphmulti(mydata, dependent, c(base_explanatory, explanatory[3])) %>% 
			fit2df(estimate_suffix = " (Model 3)")
	) %>% 
	
	# Full
	ff_merge(
		coxphmulti(mydata, dependent, c(base_explanatory, explanatory)) %>% 
			fit2df(estimate_suffix = " (Full)")
	) %>% 
	
	# Tidy-up
	select(-c(fit_id, index)) %>% 
	rename("Overall survival" = label) %>% 
	rename(" " = levels) %>% 
	rename(`n (%)` = all) -> t
```

```{r eval=FALSE, include=FALSE, echo=TRUE}
library(knitr)
kable(t, row.names=FALSE, align = c("l", "l", "r", "r", "r", "r", "r", "r", "r", "r"))
```

## 4 Support for complex survey structures via `library(survey)`

### 4.1 Linear regression

Examples taken from `survey::svyglm()` help page.

```{r eval=FALSE, include=FALSE, echo=TRUE, warning=FALSE, message=FALSE}
library(survey)
library(dplyr)

data(api)
dependent = "api00"
explanatory = c("ell", "meals", "mobility")

# Label data frame
apistrat = apistrat %>%
  mutate(
  api00 = ff_label(api00, "API in 2000 (api00)"),
  ell = ff_label(ell, "English language learners (percent)(ell)"),
  meals = ff_label(meals, "Meals eligible (percent)(meals)"),
  mobility = ff_label(mobility, "First year at the school (percent)(mobility)"),
  sch.wide = ff_label(sch.wide, "School-wide target met (sch.wide)")
  )

# Linear example
dependent = "api00"
explanatory = c("ell", "meals", "mobility")

# Stratified design
dstrat = svydesign(id=~1,strata=~stype, weights=~pw, data=apistrat, fpc=~fpc)

# Univariable fit
fit_uni = dstrat %>%
  svyglmuni(dependent, explanatory) %>%
  fit2df(estimate_suffix = " (univariable)")

# Multivariable fit
fit_multi = dstrat %>%
  svyglmmulti(dependent, explanatory) %>%
  fit2df(estimate_suffix = " (multivariable)")

# Pipe together
apistrat %>%
  summary_factorlist(dependent, explanatory, fit_id = TRUE) %>%
  ff_merge(fit_uni) %>%
  ff_merge(fit_multi) %>%
  select(-fit_id, -index) %>%
  dependent_label(apistrat, dependent) -> t
```

```{r eval=FALSE, include=FALSE, echo=TRUE}
library(knitr)
kable(t, row.names=FALSE, align = c("l", "l", "r", "r", "r", "r", "r", "r", "r", "r"))
```

### 4.2 Binomial example

Note model family needs specified and exponentiation set to `TRUE` if desired.

```{r eval=FALSE, include=FALSE, echo=TRUE, warning=FALSE, message=FALSE}
library(survey)
library(dplyr)

data(api)
dependent = "sch.wide"
explanatory = c("ell", "meals", "mobility")

# Label data frame
apistrat = apistrat %>%
  mutate(
  api00 = ff_label(api00, "API in 2000 (api00)"),
  ell = ff_label(ell, "English language learners (percent)(ell)"),
  meals = ff_label(meals, "Meals eligible (percent)(meals)"),
  mobility = ff_label(mobility, "First year at the school (percent)(mobility)"),
  sch.wide = ff_label(sch.wide, "School-wide target met (sch.wide)")
  )
  
# Univariable fit
fit_uni = dstrat %>%
  svyglmuni(dependent, explanatory, family = "quasibinomial") %>%
  fit2df(exp = TRUE, estimate_name = "OR", estimate_suffix = " (univariable)")

# Multivariable fit
fit_multi = dstrat %>%
  svyglmmulti(dependent, explanatory, family = "quasibinomial") %>%
  fit2df(exp = TRUE, estimate_name = "OR", estimate_suffix = " (multivariable)")

# Pipe together
apistrat %>%
  summary_factorlist(dependent, explanatory, fit_id = TRUE) %>%
  ff_merge(fit_uni) %>%
  ff_merge(fit_multi) %>%
  select(-fit_id, -index) %>%
  dependent_label(apistrat, dependent) -> t
```

```{r eval=FALSE, include=FALSE, echo=TRUE}
library(knitr)
kable(t, row.names=FALSE, align = c("l", "l", "r", "r", "r", "r", "r", "r", "r", "r"))
```



<!--chapter:end:finalfit_all_tables_examples.Rmd-->

---
title: "finalfit"
---

```
devtools::install_github("ewenharrison/finalfit")
```

```{r eval=FALSE, include=FALSE, echo=TRUE}
library(finalfit)
library(dplyr)
```

```{r eval=FALSE, include=FALSE, echo=TRUE}
colon_s
```




```{r eval=FALSE, include=FALSE, echo=TRUE}
dependent <- "differ.factor"

# Specify explanatory variables of interest
explanatory <- c("age", "sex.factor", 
  "extent.factor", "obstruct.factor", 
  "nodes")
```

```{r eval=FALSE, include=FALSE, echo=TRUE}
# colon_s %>%
#   select(age, sex.factor, 
#   extent.factor, obstruct.factor, nodes) %>% 
#   names() -> explanatory
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
colon_s %>% 
  summary_factorlist(dependent, explanatory, 
  p=TRUE, na_include=FALSE)
```

```{r eval=FALSE, include=FALSE, echo=TRUE}
Hmisc::label(colon_s$nodes) <- "Lymph nodes involved"
explanatory = c("age", "sex.factor", 
  "extent.factor", "nodes")

colon_s %>% 
  summary_factorlist(dependent, explanatory, 
  p=TRUE, na_include=FALSE, 
  add_dependent_label=TRUE) -> table1

table1
```

```{r eval=FALSE, include=FALSE, echo=TRUE}
explanatory <- c("age", "sex.factor", 
  "extent.factor", "nodes", 
  "differ.factor")
dependent <- "mort_5yr"

colon_s %>% 
  finalfit(dependent = dependent, explanatory = explanatory, fit_id=TRUE, 
  dependent_label_prefix = "") -> table2

kableExtra::kable(table2)

```



```{r eval=FALSE, include=FALSE, echo=TRUE}
colon_s %>% 
  or_plot(dependent, explanatory, 
  breaks = c(0.5, 1, 5, 10, 20, 30))
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
# Save objects for knitr/markdown
save(table1, table2, dependent, explanatory, file = "out.rda")
```


---


```{r eval=FALSE, include=FALSE, echo=TRUE}
# Load data into global environment. 
library(finalfit)
library(dplyr)
library(knitr)
load("out.rda")
```

## Table 1 - Demographics
```{r table1x, echo = TRUE, results='asis'}
kable(table1, row.names=FALSE, align=c("l", "l", "r", "r", "r", "r"))
```

## Table 2 - Association between tumour factors and 5 year mortality
```{r table2x, echo = TRUE, results='asis'}
kable(table2, row.names=FALSE, align=c("l", "l", "r", "r", "r", "r"))
```

## Figure 1 - Association between tumour factors and 5 year mortality
```{r figure1x, eval=FALSE, include=FALSE, echo=TRUE, warning=FALSE, message=FALSE}
colon_s %>% 
  or_plot(dependent, explanatory)
```






















<!--chapter:end:finalfit.Rmd-->

---
title: "finalfit"
---

```
devtools::install_github("ewenharrison/finalfit")
```

```{r eval=FALSE, include=FALSE, echo=TRUE}
library(finalfit)
library(dplyr)
```

```{r eval=FALSE, include=FALSE, echo=TRUE}
colon_s
```




```{r eval=FALSE, include=FALSE, echo=TRUE}
dependent <- "differ.factor"

# Specify explanatory variables of interest
explanatory <- c("age", "sex.factor", 
  "extent.factor", "obstruct.factor", 
  "nodes")
```

```{r eval=FALSE, include=FALSE, echo=TRUE}
# colon_s %>%
#   select(age, sex.factor, 
#   extent.factor, obstruct.factor, nodes) %>% 
#   names() -> explanatory
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
colon_s %>% 
  summary_factorlist(dependent, explanatory, 
  p=TRUE, na_include=FALSE)
```

```{r eval=FALSE, include=FALSE, echo=TRUE}
Hmisc::label(colon_s$nodes) <- "Lymph nodes involved"
explanatory = c("age", "sex.factor", 
  "extent.factor", "nodes")

colon_s %>% 
  summary_factorlist(dependent, explanatory, 
  p=TRUE, na_include=FALSE, 
  add_dependent_label=TRUE) -> table1

table1
```

```{r eval=FALSE, include=FALSE, echo=TRUE}
explanatory <- c("age", "sex.factor", 
  "extent.factor", "nodes", 
  "differ.factor")
dependent <- "mort_5yr"

colon_s %>% 
  finalfit(dependent = dependent, explanatory = explanatory, fit_id=TRUE, 
  dependent_label_prefix = "") -> table2

kableExtra::kable(table2)

```



```{r eval=FALSE, include=FALSE, echo=TRUE}
colon_s %>% 
  or_plot(dependent, explanatory, 
  breaks = c(0.5, 1, 5, 10, 20, 30))
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
# Save objects for knitr/markdown
save(table1, table2, dependent, explanatory, file = "out.rda")
```


---


```{r eval=FALSE, include=FALSE, echo=TRUE}
# Load data into global environment. 
library(finalfit)
library(dplyr)
library(knitr)
load("out.rda")
```

## Table 1 - Demographics
```{r table1 4, echo = TRUE, results='asis'}
kable(table1, row.names=FALSE, align=c("l", "l", "r", "r", "r", "r"))
```

## Table 2 - Association between tumour factors and 5 year mortality
```{r table2 4, echo = TRUE, results='asis'}
kable(table2, row.names=FALSE, align=c("l", "l", "r", "r", "r", "r"))
```

## Figure 1 - Association between tumour factors and 5 year mortality
```{r figure1 4, eval=FALSE, include=FALSE, echo=TRUE, warning=FALSE, message=FALSE}
colon_s %>% 
  or_plot(dependent, explanatory)
```






















<!--chapter:end:finalfit2.Rmd-->

---
title: "Example knitr/R Markdown document"
author: "Ewen Harrison"
date: "21/5/2018"
output:
  pdf_document: default
geometry: margin=0.75in
---

```{r eval=FALSE, include=FALSE, echo=TRUE}
# Load data into global environment. 
library(finalfit)
library(dplyr)
library(knitr)
library(kableExtra)
load("out.rda")
```

## Table 1 - Demographics
```{r table1 3, echo = TRUE, results='asis'}
kable(table1, row.names=FALSE, align=c("l", "l", "r", "r", "r", "r"),
						booktabs=TRUE)
```

## Table 2 - Association between tumour factors and 5 year mortality
```{r table2 3, eval=FALSE, include=FALSE, results='asis'}
kable(table2, row.names=FALSE, align=c("l", "l", "r", "r", "r", "r"),
			booktabs=TRUE) %>% 
	kable_styling(font_size=8)
```

## Figure 1 - Association between tumour factors and 5 year mortality
```{r figure1-, warning=FALSE, message=FALSE, fig.width=10, eval=FALSE, include=FALSE, echo=TRUE, warning=FALSE, message=FALSE}
colon_s %>% 
  or_plot(dependent, explanatory)
```

<!--chapter:end:finalfit2x.Rmd-->

---
title: "finalfit"
---

```
devtools::install_github("ewenharrison/finalfit")
```

```{r eval=FALSE, include=FALSE, echo=TRUE}
library(finalfit)
library(dplyr)
```

```{r eval=FALSE, include=FALSE, echo=TRUE}
colon_s
```




```{r eval=FALSE, include=FALSE, echo=TRUE}
dependent <- "differ.factor"

# Specify explanatory variables of interest
explanatory <- c("age", "sex.factor", 
  "extent.factor", "obstruct.factor", 
  "nodes")
```

```{r eval=FALSE, include=FALSE, echo=TRUE}
# colon_s %>%
#   select(age, sex.factor, 
#   extent.factor, obstruct.factor, nodes) %>% 
#   names() -> explanatory
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
colon_s %>% 
  summary_factorlist(dependent, explanatory, 
  p=TRUE, na_include=FALSE)
```

```{r eval=FALSE, include=FALSE, echo=TRUE}
Hmisc::label(colon_s$nodes) <- "Lymph nodes involved"
explanatory = c("age", "sex.factor", 
  "extent.factor", "nodes")

colon_s %>% 
  summary_factorlist(dependent, explanatory, 
  p=TRUE, na_include=FALSE, 
  add_dependent_label=TRUE) -> table1

table1
```

```{r eval=FALSE, include=FALSE, echo=TRUE}
explanatory <- c("age", "sex.factor", 
  "extent.factor", "nodes", 
  "differ.factor")
dependent <- "mort_5yr"

colon_s %>% 
  finalfit(dependent = dependent, explanatory = explanatory, fit_id=TRUE, 
  dependent_label_prefix = "") -> table2

kableExtra::kable(table2)

```



```{r eval=FALSE, include=FALSE, echo=TRUE}
colon_s %>% 
  or_plot(dependent, explanatory, 
  breaks = c(0.5, 1, 5, 10, 20, 30))
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
# Save objects for knitr/markdown
save(table1, table2, dependent, explanatory, file = "out.rda")
```


---


```{r eval=FALSE, include=FALSE, echo=TRUE}
# Load data into global environment. 
library(finalfit)
library(dplyr)
library(knitr)
load("out.rda")
```

## Table 1 - Demographics
```{r table1y 2, echo = TRUE, results='asis'}
kable(table1, row.names=FALSE, align=c("l", "l", "r", "r", "r", "r"))
```

## Table 2 - Association between tumour factors and 5 year mortality
```{r table2 2, echo = TRUE, results='asis'}
kable(table2, row.names=FALSE, align=c("l", "l", "r", "r", "r", "r"))
```

## Figure 1 - Association between tumour factors and 5 year mortality
```{r figure1, eval=FALSE, include=FALSE, echo=TRUE, warning=FALSE, message=FALSE}
colon_s %>% 
  or_plot(dependent, explanatory)
```






















<!--chapter:end:finalfitx.Rmd-->

---
title: "R Notebook"
---

## Flipping Coin


```{r eval=FALSE, include=FALSE, echo=TRUE}
rbinom(n = 1, size = 1, prob = 0.5)
```

```{r eval=FALSE, include=FALSE, echo=TRUE}
rbinom(n = 10, size = 1, prob = 0.5)
```

```{r eval=FALSE, include=FALSE, echo=TRUE}
rbinom(n = 1, size = 10, prob = 0.5)
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
rbinom(n = 100, size = 100, prob = 0.5)
```

```{r eval=FALSE, include=FALSE, echo=TRUE}
rbinom(n = 10, size = 10, prob = 0.3)
```


<!--chapter:end:FlippingCoin.Rmd-->

---
title: "formattable"
---

https://www.littlemissdata.com/blog/prettytables

```{r eval=FALSE, include=FALSE, echo=TRUE}
library(data.table)
library(dplyr)
library(formattable)
library(tidyr)
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
#Set a few color variables to make our table more visually appealing
customGreen0 = "#DeF7E9"
customGreen = "#71CA97"
customRed = "#ff7f7f"
```















<!--chapter:end:formattable.Rmd-->

---
title: "General Linear Models"
---



# 5 Alternatives to the Default R Outputs for GLMs and Linear Models


https://www.displayr.com/5-alternatives-to-the-default-r-outputs-for-glms-and-linear-models/?utm_medium=Feed&utm_source=Syndication

## Classic Output


```{r eval=FALSE, include=FALSE, echo=TRUE}
churn <- read.csv("https://community.watsonanalytics.com/wp-content/uploads/2015/03/WA_Fn-UseC_-Telco-Customer-Churn.csv")
my.glm <- glm(Churn ~ SeniorCitizen + tenure + InternetService + MonthlyCharges,
             data = churn, 
             family = binomial(logit))
summary(my.glm)
```

## stargazer


```{r eval=FALSE, include=FALSE, echo=TRUE}
write(stargazer::stargazer(my.glm, type = "html"), "stargazer.html")
```


## formattable


```{r eval=FALSE, include=FALSE, echo=TRUE}
library(formattable)
my.glm
```


## flipRegression


```{r eval=FALSE, include=FALSE, echo=TRUE}
# devtools::install_github("Displayr/flipPlots")
# devtools::install_github("Displayr/flipRegression")
library(flipPlots)
library(flipRegression)
my.regression <- Regression(Churn ~ SeniorCitizen + tenure + InternetService + MonthlyCharges,
                           data = churn,
                           show.labels = TRUE,
                           type ="Binary Logit")

my.regression
```



```{r eval=FALSE, include=FALSE, echo=TRUE}
library(flipRegression)
Regression(Churn ~ SeniorCitizen + tenure + InternetService + MonthlyCharges,
           data = churn,
           show.labels = TRUE,
           output = "Relative Importance Analysis",
           type ="Binary Logit")
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
library(effects)
my.glm = glm(Churn ~ SeniorCitizen + tenure + InternetService + MonthlyCharges,
             data = churn, 
             family = binomial(logit))
effects = allEffects(my.glm)
plot(effects,
     col = 2,
     ylab = "Probability(Churn)", 
     ylim = c(0, .6),  
     type = "response")
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
library(httr)
# GET("https://docs.displayr.com/images/f/f0/Churn.xlsx", 
    # write_disk(tf <- tempfile(fileext = ".xlsx")))
# df <- readxl::read_excel(tf, 1L)
library(mgcv)
my.gam <- gam(Churn ~ SeniorCitizen + s(tenure) + InternetService + s(MonthlyCharges), 
                data = churn, 
                family = binomial(logit))


my.gam
```

### Building Online Interactive Simulators for Predictive Models in R

https://www.displayr.com/building-online-interactive-simulators-for-predictive-models-in-r/






<!--chapter:end:GeneralLinearModels.Rmd-->

---
title: "General Resources"
---

# Data Science Live Book

https://livebook.datascienceheroes.com/


https://toolbox.google.com/datasetsearch

http://archive.ics.uci.edu/ml/index.php


http://asdfree.com/

https://rstudio-education.github.io/hopr/


- **What I Wish I Knew When I Started R**  

https://www.williamrchase.com/slides/intro_r_anthropology_2018




https://sbalci.gitbooks.io/pathology-notes/content/pathology-residents/computational-pathology.html

http://web.stanford.edu/class/bios221/book/

https://kbroman.org/minimal_make/

https://www.gnu.org/software/make/

https://kbroman.org/minimal_make/


https://www.datacamp.com/community/tutorials/shell-commands-data-scientist

https://moderndive.com/3-viz.html

https://www.causeweb.org/cause/ecots/ecots18/breakouts/7

https://plotly-book.cpsievert.me/

http://r-bio.github.io/01-intro-R/

https://www.rdatagen.net/post/by-vs-within/?platform=hootsuite

http://www.biomart.org/download.html

https://ropensci.org/blog/2018/07/24/educollab-challenges/



https://www.datacamp.com/community/tutorials/data-science-pitfalls


https://serialmentor.com/dataviz/preface.html


- https://news.codecademy.com/errors-in-code-think-differently/?utm_source=customer.io&utm_medium=email&utm_campaign=fortnightly_8-1-18&utm_content=ErrorFortnightly




- Data Science Live Book

https://livebook.datascienceheroes.com/

- School of Psychology at the University of New South Wales http://www.compcogscisydney.org/teaching/

   - Of Minds and Machines
http://www.compcogscisydney.org/mm/

   - psyr: Using R in Psychological Science
http://www.compcogscisydney.org/psyr/


   - Perception and Cognition
http://www.compcogscisydney.org/psyc2071/

   - Learning Statistics with R
http://www.compcogscisydney.org/learning-statistics-with-r/

   - Computational Cognitive Science
http://www.compcogscisydney.org/ccs/


- Advanced R

https://adv-r.hadley.nz/

- One Page R

https://togaware.com/onepager/

- htmlwidgets for R

http://www.htmlwidgets.org/

http://gallery.htmlwidgets.org/

- Learning R for Clinical Epidemiologists

http://rpubs.com/michaelmarks/R-Clin-Epi

- r-tutor

http://www.r-tutor.com/

- Statistics Meets Big Data

http://www.statsoft.org/

- ModernDive

https://moderndive.com/

<!-- ![](https://moderndive.com/images/flowcharts/flowchart/flowchart.002.png) -->

- Laerd Statistics

https://statistics.laerd.com/

- statpages

http://statpages.info/index.html

- The R class R programming for biologists

http://r-bio.github.io/

- Sosyal Bilimler Araştırmaları İçin R

https://bookdown.org/connect/#/apps/1531/access

- R for Psychological Science An introductory resource

http://compcogscisydney.org/psyr/


- Jamovi tutorial

https://datalab.cc/tools/jamovi

https://www.youtube.com/playlist?list=PLkk92zzyru5OAtc_ItUubaSSq6S_TGfRn


---

## master course links








---

# Do More with R

https://www.infoworld.com/video/series/8563/do-more-with-r


<!--chapter:end:GeneralResources.Rmd-->

---
title: "My R Codes For Data Analysis"
---

# Getting Data into R / Veriyi R'a yükleme

## Import Data

### Import using RStudio

### Import CSV File

```{r eval=FALSE, include=FALSE, echo=TRUE}
scabies <- read.csv(file = "http://datacompass.lshtm.ac.uk/607/2/S1-Dataset_CSV.csv", header = TRUE, sep = ",")
scabies
```


#### How to import multiple .csv files at once?

https://stackoverflow.com/questions/11433432/how-to-import-multiple-csv-files-at-once


```
temp = list.files(pattern="*.csv")
myfiles = lapply(temp, read.delim)

temp = list.files(pattern="*.csv")
for (i in 1:length(temp)) assign(temp[i], read.csv(temp[i]))

temp = list.files(pattern="*.csv")
list2env(
  lapply(setNames(temp, make.names(gsub("*.csv$", "", temp))), 
         read.csv), envir = .GlobalEnv)

```


```
# Get the files names
files = list.files(pattern="*.csv")
# First apply read.csv, then rbind
myfiles = do.call(rbind, lapply(files, function(x) read.csv(x, stringsAsFactors = FALSE)))
```

```
library(data.table)
DT = do.call(rbind, lapply(files, fread))
# The same using `{r # bindlist`
DT = rbindlist(lapply(files, fread))
```

```
library(readr)
library(dplyr)
tbl = lapply(files, read_csv) %>% bind_rows()
```

```
data <- read.csv(
  switch(animal, 
         "dog" = "dogdata.csv", 
         "cat" = "catdata.csv",
         "rabbit" = "rabbitdata.csv")
)
```


### Import TXT File

```{r eval=FALSE, include=FALSE, echo=TRUE}
ebola <- read.csv(file = "http://datacompass.lshtm.ac.uk/608/1/mmc1.txt", header = TRUE, sep = ",")
ebola
```


### Import Excel File

```
my_data <- read_excel(file.choose())


files <- list.files(pattern = ".xlsx")

data_xlsx_df <- map_df(set_names(files), function(file) {
  file %>% 
    excel_sheets() %>% 
    set_names() %>% 
    map_df(
      ~ read_xlsx(path = file, sheet = .x, range = "H3"),
      .id = "sheet")
}, .id = "file")
```

#### Import Sheets

<!-- # Sheets -->

<!-- library(tidyverse) -->
<!-- library(readxl) -->


<!-- path <- "ICPN_RAW.xls" -->
<!-- sheetlist <- path %>%  -->
<!--     excel_sheets() %>%  -->
<!--     set_names() %>%  -->
<!--     map(read_excel, skip = 1, path = path) -->

<!-- sheetlist <- sheetlist[-29] -->


<!-- sheetlist <- sheetlist %>% -->
<!--     reduce(left_join, by = "Respondant", .id = "id", suffix = c("1","2")) -->

<!-- names(sheetlist) -->

<!-- columnNames <- c( -->
<!--     "ID", -->
<!--     "DX", -->
<!--     "y/n", -->
<!--     "~plastic", -->
<!--     "WHO- 2010 classification", -->
<!--     "GRADE", -->
<!--     "INVASIVE CA", -->
<!--     "CELL LINEAGE", -->
<!--     "IPMN Lineage (PRE/MIN)", -->
<!--     "experience with this invasion", -->
<!--     "COMMENTS" -->
<!-- ) -->

<!-- columnNumbers <- rep(c(1:28), each = 11) -->

<!-- columnNames2 <- paste(columnNames, columnNumbers, sep = "") -->

<!-- columnNames2 <- c("Respondant", columnNames2) -->

<!-- names(sheetlist) <- columnNames2 -->



<!-- # "Respondant"	"ID"	"y/n"	"WHO- 2010 classification"	"GRADE"	"INVASIVE CA"	"CELL LINEAGE"	"IPMN Lineage (PRE/MIN)" -->

<!-- deneme <- sheetlist %>%  -->
<!--     select(starts_with("Respondant"), starts_with("GRADE")) -->







<!-- # deneme[deneme=="HIGH"] <- 3 -->
<!-- # deneme[deneme=="HIGH (focal)"] <- 3 -->
<!-- # deneme[deneme=="MODERATE"] <- 2 -->
<!-- # deneme[deneme=="LOW"] <- 1 -->
<!-- #  -->
<!-- # deneme <- as.data.frame(deneme) -->
<!-- #  -->
<!-- # row.names(deneme) <- deneme$Respondant -->
<!-- #  -->
<!-- # deneme <- deneme[-1] -->
<!-- #  -->
<!-- # deneme[is.na(deneme)] <- 1 -->
<!-- #  -->
<!-- # deneme <- map_df(deneme, as.matrix) -->
<!-- #  -->
<!-- # glimpse(deneme) -->
<!-- #  -->
<!-- # heatmap(deneme) -->

<!-- write_csv(deneme, "deneme.csv") -->






### Import SPSS File

### Keep SPSS labels

```{r eval=FALSE, include=FALSE, echo=TRUE}
library(foreign) # foreign paketi yükleniyor
```

read.spss komutu ile değer etiketlerini almasını ve bunu liste olarak değil de data.frame olarak kaydetmesini istiyoruz

```{r eval=FALSE, include=FALSE, echo=TRUE}
mydata <- read.spss("mydata.sav", use.value.labels = TRUE, to.data.frame = TRUE)
```

aktardığımız data.frame'in özellikleri (attr) içinde değişkenlerin etiketleri var, bunları dışarı çıkartıyoruz

```{r eval=FALSE, include=FALSE, echo=TRUE}
VariableLabels <- as.data.frame(attr(mydata, "variable.labels"))
```


elde ettiğimiz data.frame'deki satır isimleri değişkenlerin isimleri oluyor, karşılarında da değişken etiketleri var
satır isimlerini de dışarı çıkartıyoruz

```{r eval=FALSE, include=FALSE, echo=TRUE}
VariableLabels$original <- rownames(VariableLabels)
```

Değişken etiketi olanları etiketleri ile diğerlerini olduğu gibi saklıyoruz  

```{r eval=FALSE, include=FALSE, echo=TRUE}
VariableLabels$label[VariableLabels$label ==""] <- NA 
VariableLabels$colname <- VariableLabels$original
VariableLabels$colname[!is.na(VariableLabels$label)] <- as.vector(VariableLabels$label[!is.na(VariableLabels$label)])
```

son olarak da data.frame'deki sütun isimlerini değiştiriyoruz

```{r eval=FALSE, include=FALSE, echo=TRUE}
names(mydata) <- VariableLabels$colname
```





# Export Data

### Export to SPSS, while keeping labels

R'da `factor` olan label verdiğiniz değişkenleri `SPSS` ya da diğer istatistik programlarına aktardığınızda bu tanımlamaları korumak işimize yarar. Bunun için `foreign` paketi ile bir `txt` dosyası ve bir `sps` dosyası oluşturuyoruz. SPSS'te `sps` dosyasını açıp kodu çalıştırarak tekrar atanan değerler geri yükleniyor.


```{r eval=FALSE, include=FALSE, echo=TRUE}
library(foreign)
write.foreign(mydata, "mydata.txt", "mydata.sps",   package = "SPSS")
```


---

https://twitter.com/WeAreRLadies/status/1034817323922804737

```
f <- list.files( "my_folder", pattern = "*.csv", full.names = TRUE)
d <- purrr::map_df(f, readr::read_csv, .id = "id")
```

---

```
m <- lm(mpg ~ qsec + wt, data = mtcars)
broom::tidy(m)
```

---

Import a Directory of CSV Files at Once Using {purrr} and {readr}

https://www.gerkelab.com/blog/2018/09/import-directory-csv-purrr-readr/

```
data_dir %>% 
  dir_ls(regexp = "\\.csv$") %>% 
  map_dfr(read_csv, .id = "source") %>% 
  mutate(Month_Year = myd(Month_Year, truncated = 1))
```  


---

https://suatatan.wordpress.com/2017/10/07/bulk-replacing-turkish-characters-in-r/

Turkish character sometimes became the menace for the data scientist. To avoid the risks you may want to change it with safe characters. To do that you can use this code:

```
#turkce karakter donusumu
to.plain <- function(s) {

# 1 character substitutions
old1 <- “çğşıüöÇĞŞİÖÜ”
new1 <- “cgsiuocgsiou”
s1 <- chartr(old1, new1, s)

# 2 character substitutions
old2 <- c(“œ”, “ß”, “æ”, “ø”)
new2 <- c(“oe”, “ss”, “ae”, “oe”)
s2 <- s1
for(i in seq_along(old2)) s2 <- gsub(old2[i], new2[i], s2, fixed = TRUE)

s2
}
df$source=as.vector(sapply(df$source,to.plain))


to.plain(make.names(tolower(names(df))))
```


- Remove all special characters from a string in R?

https://stackoverflow.com/questions/10294284/remove-all-special-characters-from-a-string-in-r

```
x <- "a1~!@#$%^&*(){}_+:\"<>?,./;'[]-="
stringr::str_replace_all(x, "[[:punct:]]", " ")
stringr::str_replace_all(x, "[^[:alnum:]]", " ")
```

```
astr <- "Ábcdêãçoàúü"
iconv(astr, from = 'UTF-8', to = 'ASCII//TRANSLIT')
```

```
Data  <- gsub("[^0-9A-Za-z///' ]","'" , Data ,ignore.case = TRUE)

Data <- gsub("''","" , Data ,ignore.case = TRUE)
```





# pdftables

https://cran.r-project.org/web/packages/pdftables/vignettes/convert_pdf_tables.html


# tabulizer

Extract Tables from PDFs


https://github.com/ropensci/tabulizer



# rio

Import, Export, and Convert Data Files

https://thomasleeper.com/rio/index.html

https://cran.r-project.org/web/packages/rio/vignettes/rio.html


# read with purrr


R tip: Iterate with purrr's map_df function


https://www.infoworld.com/video/89075/r-tip-iterate-with-purrrs-map-df-function



# The janitor package

https://garthtarr.github.io/meatR/janitor.html

```{r eval=FALSE, include=FALSE, echo=TRUE}
# install.packages("janitor")
```

```{r eval=FALSE, include=FALSE, echo=TRUE}
library(tidyverse)
library(janitor)
library(xlsx)
```

```{r eval=FALSE, include=FALSE, echo=TRUE}
# mymsa <-  data.table::fread("https://garthtarr.com/data/mymsa.xlsx", fill = TRUE)
mymsa <-  read_excel("data/mymsa.xlsx")
mymsa$çğşüö <- 2
x <-  janitor::clean_names(mymsa)
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
data.frame(mymsa = colnames(mymsa), x = colnames(x))
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
tabyl(x, meat_colour) %>%
  knitr::kable()
```



```{r eval=FALSE, include=FALSE, echo=TRUE}
table(x$meat_colour)
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
# Load dplyr for the %>% pipe 
library(dplyr)
x %>% tabyl(meat_colour) %>%
  knitr::kable()
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
x %>% 
  tabyl(meat_colour) %>% 
  adorn_pct_formatting(digits = 0, affix_sign = TRUE) %>%
  knitr::kable()
```



```{r eval=FALSE, include=FALSE, echo=TRUE}
x %>% tabyl(spare)
```



```{r eval=FALSE, include=FALSE, echo=TRUE}
x = remove_empty(x, which = c("rows","cols"))
```



```{r eval=FALSE, include=FALSE, echo=TRUE}
x = read_excel("data/mymsa.xlsx") %>% 
  clean_names() %>% remove_empty()
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
x %>% tabyl(meat_colour, plant) %>%
  knitr::kable()

# can also make 3 way tables
```




```{r eval=FALSE, include=FALSE, echo=TRUE}
# row totals
x %>% 
  tabyl(meat_colour, plant) %>% 
  adorn_totals(where = "row") %>%
  knitr::kable()
```

  

```{r eval=FALSE, include=FALSE, echo=TRUE}
# column totals
x %>% 
  tabyl(meat_colour, plant) %>% 
  adorn_totals(where = "col") %>%
  knitr::kable()
```




```{r eval=FALSE, include=FALSE, echo=TRUE}
# row and column totals
x %>% 
  tabyl(meat_colour, plant) %>% 
  adorn_totals(where = c("row","col"))
```
  
  
  
```{r eval=FALSE, include=FALSE, echo=TRUE}
x %>% 
  tabyl(meat_colour, plant) %>% 
  adorn_totals(where = c("row","col")) %>% 
  adorn_percentages(denominator = "col") %>% 
  adorn_pct_formatting(digits = 0) 
```

  
```{r eval=FALSE, include=FALSE, echo=TRUE}
x %>% 
  tabyl(meat_colour, plant) %>% 
  adorn_totals(where = c("row","col")) %>% 
  adorn_percentages(denominator = "col") %>% 
  adorn_pct_formatting(digits = 0) %>% 
  adorn_ns(position = "front")
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
adorn_cumulative <- function(dat, colname, dir = "down"){

  if(!missing(colname)){
    colname <- rlang::enquo(colname)
  } else if("valid_percent" %in% names(dat)) {
  colname <- rlang::sym("valid_percent")
  } else if("percent" %in% names(dat)){
    colname <- rlang::sym("percent")
  } else {
    stop("\"colname\" not specified and default columns valid_percent and percent are not present in data.frame dat")
  }

  target <- dplyr::pull(dat, !! colname)

  if(dir == "up"){
    target <- rev(target)
  }
  dat$cumulative <- cumsum(ifelse(is.na(target), 0, target)) + target*0 # an na.rm version of cumsum, from https://stackoverflow.com/a/25576972
  if(dir == "up"){
    dat$cumulative <- rev(dat$cumulative)
    names(dat)[names(dat) %in% "cumulative"] <- "cumulative_up"
  }
  dat
}
```





  

```{r eval=FALSE, include=FALSE, echo=TRUE}
x %>% get_dupes(rfid)
```
  

```{r eval=FALSE, include=FALSE, echo=TRUE}
x1 = x %>% slice(1:3)
x2 = bind_rows(x1,x)
x2 %>% get_dupes(rfid)
```

  

## convert excel number into date


```{r eval=FALSE, include=FALSE, echo=TRUE}
janitor::excel_numeric_to_date(41103)
```










<!--chapter:end:GettingDataVeriYukleme.Rmd-->



```
output:
  pdf_document: default
  html_document: default
header-includes:
- \usepackage{pdflscape}
- \usepackage{xcolor}
- \newcommand{\blandscape}{\begin{landscape}}
- \newcommand{\elandscape}{\end{landscape}}
```


# ggplot2 ----

mpg



```{r, background='#fff5e6'}
library("tidyverse")
ggplot(mpg) + 
    geom_point(aes(x = displ, y = hwy))
```



ggplot(mpg, aes(model, manufacturer)) + geom_point()



ggplot(mpg, aes(displ, cty, colour = year)) + 
    geom_point()



ggplot(mpg, aes(displ, hwy)) + 
    geom_point(aes(shape = year))


ggplot(mpg, aes(displ, hwy)) + 
    geom_point() + 
    geom_smooth(span = 0.2)



ggplot(mpg, aes(hwy)) +
    geom_histogram() +
    geom_freqpoly()




ggplot(mpg, aes(cty, hwy)) + 
    geom_point() +
    geom_smooth()


ggplot(mpg, aes(class, hwy)) + geom_boxplot()
ggplot(mpg, aes(reorder(class, hwy), hwy)) + geom_boxplot()


# gganimate ----



library(gganimate)

p <- ggplot(iris, aes(x = Petal.Width, y = Petal.Length)) + 
    geom_point()

plot(p)


anim <- p + 
    transition_states(Species,
                      transition_length = 2,
                      state_length = 1)

anim


p + 
    enter_appear()




```{r eval=FALSE, include=FALSE, echo=TRUE}
sometext <-strsplit(
    paste0("You can even try to make some crazy things like this paragraph. ","It may seem like a useless feature right now but it's so cool ","and nobody can resist. ;)"), " ")[[1]]

text_formatted <-paste(
    kableExtra::text_spec(sometext,
              "latex",
              color = kableExtra::spec_color(1:length(sometext), end = 0.9),
              font_size =kableExtra::spec_font_size(1:length(sometext), begin = 5, end = 20)),collapse = " ")

mytext <-  kableExtra::text_spec("Serdar", color = "blue", background = "black")

```


`{r #  mytext`


To display the text, type `{r #  text_formatted` outside of the chunk




```{r eval=FALSE, include=FALSE, echo=TRUE}

library(kableExtra)

my_text <- paste0("İstatistik Metod:",
"Sürekli verilerin ortalama, standart sapma, median, minimum ve maksimum değerleri verildi.",

"R Core Team (2019). R: A language and environment for statistical computing. R Foundation for Statistical Computing, Vienna, Austria. URL https://www.R-project.org/.",
  
"Therneau T (2015). A Package for Survival Analysis in S. version 2.38, URL:https://CRAN.R-project.org/package=survival",

"Terry M. Therneau, Patricia M. Grambsch (2000). Modeling Survival Data: Extending the Cox Model. Springer, New York. ISBN 0-387-98784-3.",

"Ewen Harrison, Tom Drake and Riinu Ots (2019). finalfit: Quickly Create Elegant Regression Results Tables and Plots when Modelling. R package version 0.9.6. https://github.com/ewenharrison/finalfit",

sep = "\n"
)

my_text <- paste0(
  "You can even try to make some crazy things like this paragraph. ",
  "It may seem like a useless feature right now but it's so cool ",
  "and nobody can resist. ;)")


my_text_html <- paste(
  text_spec(
    my_text,
    "html",
    color = "red",
    background = "yellow"
    ),
  collapse = " ")



sometext <-strsplit(my_text, " ")[[1]]

my_text_latex <- paste(
  text_spec(
    sometext,
    "latex",
    color = "red",
    background = "yellow"
    ),
  collapse = " ")


```



`r #  my_text_html`


`r #  my_text_latex`





<!--chapter:end:ggplot2.Rmd-->

---
title: "ggpubr"
---

# ggpubr

```
https://rpkgs.datanovia.com/ggpubr



if(!require(devtools)) install.packages("devtools")
devtools::install_github("kassambara/ggpubr")
Distribution
library(ggpubr)


set.seed(1234)
wdata = data.frame(
   sex = factor(rep(c("F", "M"), each=200)),
   weight = c(rnorm(200, 55), rnorm(200, 58)))
head(wdata, 4)




ggdensity(wdata, x = "weight",
   add = "mean", rug = TRUE,
   color = "sex", fill = "sex",
   palette = c("#00AFBB", "#E7B800"))



gghistogram(wdata, x = "weight",
   add = "mean", rug = TRUE,
   color = "sex", fill = "sex",
   palette = c("#00AFBB", "#E7B800"))




data("ToothGrowth")
df <- ToothGrowth
head(df, 4)


 p <- ggboxplot(df, x = "dose", y = "len",
                color = "dose", palette =c("#00AFBB", "#E7B800", "#FC4E07"),
                add = "jitter", shape = "dose")
 p


 
 # Add p-values comparing groups
 # Specify the comparisons you want
my_comparisons <- list( c("0.5", "1"), c("1", "2"), c("0.5", "2") )
p + stat_compare_means(comparisons = my_comparisons)+ # Add pairwise comparisons p-value
  stat_compare_means(label.y = 50)                   # Add global p-value



ggviolin(df, x = "dose", y = "len", fill = "dose",
         palette = c("#00AFBB", "#E7B800", "#FC4E07"),
         add = "boxplot", add.params = list(fill = "white"))+
  stat_compare_means(comparisons = my_comparisons, label = "p.signif")+ # Add significance levels
  stat_compare_means(label.y = 50)                                      # Add global the p-value 




data("mtcars")
dfm <- mtcars

dfm$cyl <- as.factor(dfm$cyl)

dfm$name <- rownames(dfm)

head(dfm[, c("name", "wt", "mpg", "cyl")])


ggbarplot(dfm, x = "name", y = "mpg",
          fill = "cyl",               # change fill color by cyl
          color = "white",            # Set bar border colors to white
          palette = "jco",            # jco journal color palett. see ?ggpar
          sort.val = "desc",          # Sort the value in dscending order
          sort.by.groups = FALSE,     # Don't sort inside each group
          x.text.angle = 90           # Rotate vertically x axis texts
          )




ggbarplot(dfm, x = "name", y = "mpg",
          fill = "cyl",               # change fill color by cyl
          color = "white",            # Set bar border colors to white
          palette = "jco",            # jco journal color palett. see ?ggpar
          sort.val = "asc",           # Sort the value in dscending order
          sort.by.groups = TRUE,      # Sort inside each group
          x.text.angle = 90           # Rotate vertically x axis texts
          )




dfm$mpg_z <- (dfm$mpg -mean(dfm$mpg))/sd(dfm$mpg)
dfm$mpg_grp <- factor(ifelse(dfm$mpg_z < 0, "low", "high"), 
                     levels = c("low", "high"))

head(dfm[, c("name", "wt", "mpg", "mpg_z", "mpg_grp", "cyl")])


ggbarplot(dfm, x = "name", y = "mpg_z",
          fill = "mpg_grp",           # change fill color by mpg_level
          color = "white",            # Set bar border colors to white
          palette = "jco",            # jco journal color palett. see ?ggpar
          sort.val = "asc",           # Sort the value in ascending order
          sort.by.groups = FALSE,     # Don't sort inside each group
          x.text.angle = 90,          # Rotate vertically x axis texts
          ylab = "MPG z-score",
          xlab = FALSE,
          legend.title = "MPG Group"
          )



ggbarplot(dfm, x = "name", y = "mpg_z",
          fill = "mpg_grp",           # change fill color by mpg_level
          color = "white",            # Set bar border colors to white
          palette = "jco",            # jco journal color palett. see ?ggpar
          sort.val = "desc",          # Sort the value in descending order
          sort.by.groups = FALSE,     # Don't sort inside each group
          x.text.angle = 90,          # Rotate vertically x axis texts
          ylab = "MPG z-score",
          legend.title = "MPG Group",
          rotate = TRUE,
          ggtheme = theme_minimal()
          )



ggdotchart(dfm, x = "name", y = "mpg",
           color = "cyl",                                # Color by groups
           palette = c("#00AFBB", "#E7B800", "#FC4E07"), # Custom color palette
           sorting = "ascending",                        # Sort value in descending order
           add = "segments",                             # Add segments from y = 0 to dots
           ggtheme = theme_pubr()                        # ggplot2 theme
           )




ggdotchart(dfm, x = "name", y = "mpg",
           color = "cyl",                                # Color by groups
           palette = c("#00AFBB", "#E7B800", "#FC4E07"), # Custom color palette
           sorting = "descending",                       # Sort value in descending order
           add = "segments",                             # Add segments from y = 0 to dots
           rotate = TRUE,                                # Rotate vertically
           group = "cyl",                                # Order by groups
           dot.size = 6,                                 # Large dot size
           label = round(dfm$mpg),                        # Add mpg values as dot labels
           font.label = list(color = "white", size = 9, 
                             vjust = 0.5),               # Adjust label parameters
           ggtheme = theme_pubr()                        # ggplot2 theme
           )




ggdotchart(dfm, x = "name", y = "mpg_z",
           color = "cyl",                                # Color by groups
           palette = c("#00AFBB", "#E7B800", "#FC4E07"), # Custom color palette
           sorting = "descending",                       # Sort value in descending order
           add = "segments",                             # Add segments from y = 0 to dots
           add.params = list(color = "lightgray", size = 2), # Change segment color and size
           group = "cyl",                                # Order by groups
           dot.size = 6,                                 # Large dot size
           label = round(dfm$mpg_z,1),                        # Add mpg values as dot labels
           font.label = list(color = "white", size = 9, 
                             vjust = 0.5),               # Adjust label parameters
           ggtheme = theme_pubr()                        # ggplot2 theme
           )+
  geom_hline(yintercept = 0, linetype = 2, color = "lightgray")




ggdotchart(dfm, x = "name", y = "mpg",
           color = "cyl",                                # Color by groups
           palette = c("#00AFBB", "#E7B800", "#FC4E07"), # Custom color palette
           sorting = "descending",                       # Sort value in descending order
           rotate = TRUE,                                # Rotate vertically
           dot.size = 2,                                 # Large dot size
           y.text.col = TRUE,                            # Color y text by groups
           ggtheme = theme_pubr()                        # ggplot2 theme
           )+
  theme_cleveland()                                      # Add dashed grids

```















<!--chapter:end:ggpubr.Rmd-->

---
title: "R Notebook"
---

---

```
print(paste0("Git Update Started at: ", Sys.time()))
CommitMessage <- paste("updated on: ", Sys.time(), sep = "")
wd <- "~/serdarbalci"
setorigin <- "git remote set-url origin git@github.com:sbalci/MyJournalWatch.git \n"
gitCommand <- paste("cd ", wd, " \n git add . \n git commit --message '", CommitMessage, "' \n", setorigin, "git push origin master \n",  sep = "")
system(command = paste(gitCommand, "\n") , intern = TRUE, wait = TRUE)
Sys.sleep(5)
print(paste0("Git Update Ended at: ", Sys.time()))
```






---

# Happy Git and GitHub for the useR

https://happygitwithr.com

---


- An introduction to Git and how to use it with RStudio

http://r-bio.github.io/intro-git-rstudio/

https://andrewbtran.github.io/NICAR/2018/workflow/docs/03-integrating_github.html

https://aberdeenstudygroup.github.io/studyGroup/lessons/SG-T1-GitHubVersionControl/VersionControl/

http://r-bio.github.io/intro-git-rstudio/

https://stackoverflow.com/questions/41688164/using-rstudio-to-make-pull-requests-in-git

https://bookdown.org/rdpeng/RProgDA/version-control-and-github.html

https://www.r-bloggers.com/rstudio-and-github/

http://happygitwithr.com/fork.html

https://kbroman.org/github_tutorial/

https://kbroman.org/simple_site/

- Helping you make your first pull request!

https://github.com/thisisnic/first-contributions





```{r eval=FALSE, include=FALSE, echo=TRUE}
require(rstudioapi)
CommitMessage <- paste("updated on ", Sys.time(), sep = "")
wd <- getwd()
gitCommand <- paste("cd", wd, " \n git add . \n git commit --message '", CommitMessage, "' \n git push origin master \n", sep = "")
Sys.sleep(time = 1)
gitTerm <- rstudioapi::terminalCreate(show = FALSE)
Sys.sleep(time = 1)
rstudioapi::terminalSend(gitTerm, gitCommand)
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
CommitMessage <- paste("updated on ", Sys.time(), sep = "")
wd <- getwd()
gitCommand <- paste("cd ", wd, " \n git add . \n git commit --message '", CommitMessage, "' \n git push origin master \n", sep = "")
system(command = gitCommand, intern = TRUE)
```


<!--chapter:end:GitHub.Rmd-->


<!-- --- -->
<!-- title: "" -->
<!-- author: "" -->
<!-- date: "" -->
<!-- params: -->
<!--   gdrive_folder_url: "https://drive.google.com/drive/u/0/folders/FIXME" -->
<!-- output: -->
<!--   html_document: -->
<!--     keep_md: true -->
<!--     theme: simplex -->
<!--     highlight: monochrome -->
<!-- --- -->
<!-- ```{r init, include=FALSE} -->
<!-- knitr::opts_chunk$set( -->
<!--   message = FALSE, -->
<!--   warning = FALSE, dev = c("png", "cairo_pdf"), -->
<!--   echo = TRUE, -->
<!--   fig.retina = 2, -->
<!--   fig.width = 10, -->
<!--   fig.height = 6, -->
<!--   fig.path = "prod/charts/" -->
<!-- ) -->
<!-- ``` -->

<!-- ```{r boilerplate-libraries, cache=FALSE} -->
<!-- library(gt) -->
<!-- library(stringi) -->
<!-- library(hrbrthemes) -->
<!-- library(googledrive) -->
<!-- library(tidyverse) -->

<!-- # ensure fonts are available -->
<!-- extrafont::loadfonts("postscript", quiet = TRUE) -->
<!-- extrafont::loadfonts("pdf", quiet = TRUE) -->
<!-- ``` -->

<!-- # analysis code goes here ------------------------------------------------- -->


<!-- # Upload to production ---------------------------------------------------- -->

<!-- ```{r prod-upload, echo = TRUE, message = TRUE, warning = TRUE} -->
<!-- googledrive::drive_auth() -->

<!-- # locate the folder -->
<!-- gdrive_prod_folder <- googledrive::as_id(params$gdrive_folder_url) -->

<!-- # clean it out -->
<!-- gdrls <- googledrive::drive_ls(gdrive_prod_folder) -->
<!-- if (nrow(gdrls) > 0) { -->
<!--   dplyr::pull(gdrls, id) %>% -->
<!--     purrr::walk(~googledrive::drive_rm(googledrive::as_id(.x))) -->
<!-- } -->

<!-- # upload new -->
<!-- list.files(here::here("prod/charts"), recursive = TRUE, full.names = TRUE) %>% -->
<!--   purrr::walk(googledrive::drive_upload, path = gdrive_prod_folder) -->
<!-- ``` -->

<!--chapter:end:googledrive-trial.Rmd-->

---
title: "R Notebook"
---

### scholar
Analyse citation data from Google Scholar: https://github.com/jkeirstead/scholar/


### coauthornetwork
Exploring Google Scholar coauthorship: https://cimentadaj.github.io/blog/2018-06-19-exploring-google-scholar-coauthorship/exploring-google-scholar-coauthorship/

```{r eval=FALSE, include=FALSE, echo=TRUE}
# devtools::install_github("cimentadaj/coauthornetwork")
library(coauthornetwork)
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
network <- grab_network("citations?user=q40DcqYAAAAJ&hl=en")
network
```



```{r eval=FALSE, include=FALSE, echo=TRUE}
plot_coauthors(grab_network("citations?user=q40DcqYAAAAJ&hl=en", n_coauthors = 15), size_labels = 2)

```


```{r eval=FALSE, include=FALSE, echo=TRUE}
plot_coauthors(grab_network("citations?user=RJNKLHgAAAAJ&hl=en", n_coauthors = 15), size_labels = 2)

```


```{r eval=FALSE, fig.height=5, fig.width=6, include=FALSE}
plot_coauthors(grab_network("citations?user=VYE2H0wAAAAJ&hl=en", n_coauthors = 15), size_labels = 3)

```


```{r eval=FALSE, include=FALSE, echo=TRUE}
plot_coauthors(grab_network("citations?user=joN_UxsAAAAJ&hl=en", n_coauthors = 15), size_labels = 1)

```

## scholar.shiny

A shiny application that interacts with Google Scholar

https://github.com/agbarnett/scholar.shiny


<!--chapter:end:GoogleScholar.Rmd-->

---
title: "Graphs"
---


---


# flatly

Texas Housing Prices: flatly theme


https://elastic-lovelace-155848.netlify.com/gallery/themes/flatly.html





---

# easyalluvial


https://github.com/erblast/easyalluvial  

https://www.datisticsblog.com/2018/10/intro_easyalluvial/#features  

https://cran.r-project.org/web/packages/easyalluvial/index.html



```{r eval=FALSE, include=FALSE, echo=TRUE}
# install.packages('easyalluvial')
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
suppressPackageStartupMessages(require(tidyverse))
suppressPackageStartupMessages(require(easyalluvial))
```



```{r eval=FALSE, include=FALSE, echo=TRUE}
## mtcars2 is included in the current development version

# mtcars2 <- within(mtcars, {
#   vs <- factor(vs, labels = c("V", "S"))
#   am <- factor(am, labels = c("automatic", "manual"))
#   cyl  <- ordered(cyl)
#   gear <- ordered(gear)
#   carb <- ordered(carb)
# })
# 
# mtcars2$id = row.names(mtcars)
# 
# mtcars2 = dplyr::as_tibble(mtcars2)

knitr::kable(head(mtcars2))
```





```{r eval=FALSE, include=FALSE, echo=TRUE}
library(easyalluvial)
alluvial_wide(data = mtcars2
                , max_variables = 5
                , fill_by = 'first_variable' )
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
knitr::kable( head(quarterly_flights) )
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
alluvial_long( quarterly_flights
               , key = qu
               , value = mean_arr_delay
               , id = tailnum
               , fill = carrier )
```






---

# RColorBrewer

How to expand color palette with ggplot and RColorBrewer

https://www.r-bloggers.com/how-to-expand-color-palette-with-ggplot-and-rcolorbrewer/

---

# highcharter

http://jkunst.com/highcharter/

https://github.com/jbkunst/highcharter

http://www.htmlwidgets.org/index.html

https://cran.r-project.org/web/packages/highcharter/index.html

https://www.datacamp.com/community/tutorials/data-visualization-highcharter-r


```{r eval=FALSE, include=FALSE, echo=TRUE}
library(tidyverse)
library(highcharter)
```

```{r eval=FALSE, include=FALSE, echo=TRUE}
data("pokemon")
# glimpse(pokemon)
```






```
hchart works like ggplot2's qplot.
hc_add_series works like ggplot2's geom_S.
hcaes works like ggplot2's aes.
```





```{r eval=FALSE, include=FALSE, echo=TRUE}
pokemon %>%
  count(type_1) %>%
  arrange(n) %>%
  hchart(type = "bar", hcaes(x = "type_1", y = "n"))
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
pokemon %>%
  count(type_1) %>%
  arrange(n) %>%
  hchart(type = "column", hcaes(x = "type_1", y = "n"))
```



```{r eval=FALSE, include=FALSE, echo=TRUE}
pokemon %>%
  count(type_1) %>%
  arrange(n) %>%
  hchart(type = "treemap", hcaes(x = "type_1", value = "n", color = "n"))
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
highchart() %>%
  hc_add_series(pokemon, "scatter", hcaes(x = "height", y = "weight"))
```




```{r eval=FALSE, include=FALSE, echo=TRUE}
data(diamonds, package = "ggplot2")

set.seed(123)
data <- sample_n(diamonds, 300)

modlss <- loess(price ~ carat, data = data)
fit <- arrange(broom::augment(modlss), carat)

highchart() %>%
  hc_add_series(data, type = "scatter",
                hcaes(x = "carat", y = "price", size = "depth", group = "cut")) %>%
  hc_add_series(fit, type = "line", hcaes(x = "carat", y = ".fitted"),
                name = "Fit", id = "fit") %>%
  hc_add_series(fit, type = "arearange",
                hcaes(x = "carat", low = ".fitted - 2*.se.fit",
                      high = ".fitted + 2*.se.fit"),
                linkedTo = "fit")


```




```{r eval=FALSE, include=FALSE, echo=TRUE}
highchart() %>%
  hc_chart(type = "area") %>%
  hc_title(text = "Historic and Estimated Worldwide Population Distribution by Region") %>%
  hc_subtitle(text = "Source: Wikipedia.org") %>%
  hc_xAxis(categories = c("1750", "1800", "1850", "1900", "1950", "1999", "2050"),
           tickmarkPlacement = "on",
           title = list(enabled = FALSE)) %>%
  hc_yAxis(title = list(text = "Percent")) %>%
  hc_tooltip(pointFormat = "<span style=\"color:{series.color}\">{series.name}</span>:
             <b>{point.percentage:.1f}%</b> ({point.y:,.0f} millions)<br/>",
             shared = TRUE) %>%
  hc_plotOptions(area = list(
     stacking = "percent",
     lineColor = "#ffffff",
     lineWidth = 1,
     marker = list(
       lineWidth = 1,
       lineColor = "#ffffff"
       ))
     ) %>%
  hc_add_series(name = "Asia", data = c(502, 635, 809, 947, 1402, 3634, 5268)) %>%
  hc_add_series(name = "Africa", data = c(106, 107, 111, 133, 221, 767, 1766)) %>%
  hc_add_series(name = "Europe", data = c(163, 203, 276, 408, 547, 729, 628)) %>%
  hc_add_series(name = "America", data = c(18, 31, 54, 156, 339, 818, 1201)) %>%
  hc_add_series(name = "Oceania", data = c(2, 2, 2, 6, 13, 30, 46))
```




```{r eval=FALSE, include=FALSE, echo=TRUE}
x <- quantmod::getSymbols("GOOG", auto.assign = FALSE)

hchart(x)
```




```{r eval=FALSE, include=FALSE, echo=TRUE}
y <- quantmod::getSymbols("AMZN", auto.assign = FALSE)

highchart(type = "stock") %>%
  hc_add_series(x) %>%
  hc_add_series(y, type = "ohlc")
```

Highmaps - Map Collection  
https://code.highcharts.com/mapdata/  



```{r eval=FALSE, include=FALSE, echo=TRUE}
hcmap("https://code.highcharts.com/mapdata/countries/in/in-all.js")%>%
  hc_title(text = "India")
```



```{r eval=FALSE, include=FALSE, echo=TRUE}
hcmap("https://code.highcharts.com/mapdata/countries/tr/tr-all.js")%>%
  hc_title(text = "Turkey")
```

```
download_map_data: Download the geojson data from the highcharts collection.
get_data_from_map: Get the properties for each region in the map, as the keys from the map data.
```



```{r eval=FALSE, include=FALSE, echo=TRUE}
mapdata <- get_data_from_map(download_map_data("https://code.highcharts.com/mapdata/countries/in/in-all.js"))
# glimpse(mapdata)
```




```{r eval=FALSE, include=FALSE, echo=TRUE}

#population state wise
pop <-  as.data.frame(c(84673556, 1382611, 31169272, 103804637, 1055450, 25540196, 342853, 242911, 18980000, 1457723, 60383628, 25353081, 6864602,
12548926, 32966238, 61130704, 33387677, 64429, 72597565, 112372972, 2721756, 2964007, 1091014, 1980602, 41947358, 1244464,
27704236, 68621012, 607688, 72138958, 3671032, 207281477, 10116752,91347736))

state <-  mapdata%>%
  select(`hc-a2`)%>%
  arrange(`hc-a2`)

State_pop <-  as.data.frame(c(state, pop))
names(State_pop)= c("State", "Population")

hcmap("https://code.highcharts.com/mapdata/countries/in/in-all.js", data = State_pop, value = "Population",
      joinBy = c("hc-a2", "State"), name = "Fake data",
      dataLabels = list(enabled = TRUE, format = '{point.name}'),
      borderColor = "#FAFAFA", borderWidth = 0.1,
      tooltip = list(valueDecimals = 0))

```




```{r eval=FALSE, include=FALSE, echo=TRUE}
data(mpg, package = "ggplot2")

mpgg <- mpg %>%
  filter(class %in% c("suv", "compact", "midsize")) %>%
  group_by(class, manufacturer) %>%
  summarize(count = n())

categories_grouped <- mpgg %>%
  group_by(name = class) %>%
  do(categories = .$manufacturer) %>%
  list_parse()

highchart() %>%
  hc_xAxis(categories = categories_grouped) %>%
  hc_add_series(data = mpgg, type = "bar", hcaes(y = "count", color = "manufacturer"),
                showInLegend = FALSE)
```








```{r eval=FALSE, include=FALSE, echo=TRUE}
df <- data_frame(
  name = c("Animals", "Fruits", "Cars"),
  y = c(5, 2, 4),
  drilldown = tolower(name)
)


ds <- list_parse(df)
names(ds) <- NULL


hc <- highchart() %>%
  hc_chart(type = "column") %>%
  hc_title(text = "Basic drilldown") %>%
  hc_xAxis(type = "category") %>%
  hc_legend(enabled = FALSE) %>%
  hc_plotOptions(
    series = list(
      boderWidth = 0,
      dataLabels = list(enabled = TRUE)
    )
  ) %>%
  hc_add_series(
    name = "Things",
    colorByPoint = TRUE,
    data = ds
  )

dfan <- data_frame(
  name = c("Cats", "Dogs", "Cows", "Sheep", "Pigs"),
  value = c(4, 3, 1, 2, 1)
)

dffru <- data_frame(
  name = c("Apple", "Organes"),
  value = c(4, 2)
)

dfcar <- data_frame(
  name = c("Toyota", "Opel", "Volkswage"),
  value = c(4, 2, 2)
)

second_el_to_numeric <- function(ls){

  map(ls, function(x){
    x[[2]] <- as.numeric(x[[2]])
    x
  })

}

dsan <- second_el_to_numeric(list_parse2(dfan))

dsfru <- second_el_to_numeric(list_parse2(dffru))

dscar <- second_el_to_numeric(list_parse2(dfcar))

hc %>%
  hc_drilldown(
    allowPointDrilldown = TRUE,
    series = list(
      list(
        id = "animals",
        data = dsan
      ),
      list(
        id = "fruits",
        data = dsfru
      ),
      list(
        id = "cars",
        data = dscar
      )
    )
  )

```






```{r eval=FALSE, include=FALSE, echo=TRUE}
tm <- pokemon %>%
  mutate(type_2 = ifelse(is.na(type_2), paste("only", type_1), type_2),
         type_1 = type_1) %>%
  group_by(type_1, type_2) %>%
  summarise(n = n()) %>%
  ungroup() %>%
  treemap::treemap(index = c("type_1", "type_2"),
                   vSize = "n", vColor = "type_1")

tm$tm <- tm$tm %>%
  tbl_df() %>%
  left_join(pokemon %>% select(type_1, type_2, color_f) %>% distinct(), by = c("type_1", "type_2")) %>%
  left_join(pokemon %>% select(type_1, color_1) %>% distinct(), by = c("type_1")) %>%
  mutate(type_1 = paste0("Main ", type_1),
         color = ifelse(is.na(color_f), color_1, color_f))

highchart() %>%
  hc_add_series_treemap(tm, allowDrillToNode = TRUE,
                        layoutAlgorithm = "squarified")
```



```{r eval=FALSE, include=FALSE, echo=TRUE}
pokemon%>%
  count(type_1)%>%
  arrange(n)%>%
  hchart(type = "bar", hcaes(x = "type_1", y = "n", color = "type_1"))%>%
  hc_exporting(enabled = TRUE)
```




```{r eval=FALSE, include=FALSE, echo=TRUE}
pokemon%>%
  count(type_1)%>%
  arrange(n)%>%
  hchart(type = "bar", hcaes(x = "type_1", y = "n", color = "type_1"))%>%
  hc_exporting(enabled = TRUE)%>%
hc_add_theme(hc_theme_chalk())
```






```{r eval=FALSE, include=FALSE, echo=TRUE}
data("weather")

x <- c("Min", "Mean", "Max")
y <- sprintf("{point.%s}", c("min_temperaturec", "mean_temperaturec", "max_temperaturec"))
tltip <- tooltip_table(x, y)

hchart(weather, type = "columnrange",
       hcaes(x = "date", low = "min_temperaturec", high = "max_temperaturec",
             color = "mean_temperaturec")) %>%
  hc_chart(polar = TRUE) %>%
  hc_yAxis( max = 30, min = -10, labels = list(format = "{value} C"),
            showFirstLabel = FALSE) %>%
  hc_xAxis(
    title = list(text = ""), gridLineWidth = 0.5,
    labels = list(format = "{value: %b}")) %>%
  hc_tooltip(useHTML = TRUE, pointFormat = tltip,
             headerFormat = as.character(tags$small("{point.x:%d %B, %Y}")))

```



---


# taucharts


https://www.infoworld.com/video/87337/r-tip-how-to-create-easy-interactive-scatter-plots-with-taucharts


---
  
<iframe src="//content.jwplatform.com/players/GkbRp8n7-BIrn9usY.html" width="640" height="360" frameborder="0" scrolling="auto"></iframe>  

---




```{r eval=FALSE, include=FALSE, echo=TRUE}
devtools::install_github("hrbrmstr/taucharts")
# githubinstall::githubinstall("taucharts")
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
library(tidyverse)
library(taucharts)
data("mtcars")
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
mtcars2 <- mtcars %>% 
  select(wt, mpg) %>% 
  mutate(model = row.names(mtcars))
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
taucharts::tauchart(mtcars2) %>% 
  tau_point(x = "wt", y = "mpg") %>% 
  tau_tooltip() %>% 
  tau_trendline()
```


# gganimate

https://www.infoworld.com/video/89987/r-tip-animations-in-r


# ggplot2


http://r-statistics.co/ggplot2-Tutorial-With-R.html


```{r eval=FALSE, include=FALSE, echo=TRUE}
library(ggplot2)
diamonds
ggplot(diamonds)  # if only the dataset is known.
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
ggplot(diamonds, aes(x=carat))  # if only X-axis is known. The Y-axis can be specified in respective geoms.
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
ggplot(diamonds, aes(x=carat, y=price))  # if both X and Y axes are fixed for all layers.
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
ggplot(diamonds, aes(x=carat, color=cut))  # Each category of the 'cut' variable will now have a distinct  color, once a geom is added.
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
ggplot(diamonds, aes(x=carat), color="steelblue")
```



- https://ggplot2.tidyverse.org/reference/


```{r eval=FALSE, include=FALSE, echo=TRUE}
ggplot(diamonds, aes(x=carat, y=price, color=cut)) + 
  geom_point() + 
  geom_smooth()
# Adding scatterplot geom (layer1) and smoothing geom (layer2).
```



```{r eval=FALSE, include=FALSE, echo=TRUE}
ggplot(diamonds) + 
  geom_point(aes(x=carat, y=price, color=cut)) + 
  geom_smooth(aes(x=carat, y=price, color=cut))
# Same as above but specifying the aesthetics inside the geoms.
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
library(ggplot2)
ggplot(diamonds) + 
  geom_point(aes(x=carat, y=price, color=cut)) + 
  geom_smooth(aes(x=carat, y=price)) # Remove color from geom_smooth
ggplot(diamonds, aes(x=carat, y=price)) + 
  geom_point(aes(color=cut)) + 
  geom_smooth()  # same but simpler
```


continue from here
http://r-statistics.co/ggplot2-Tutorial-With-R.html


# gganimate

https://cran.r-project.org/web/packages/gganimate/vignettes/gganimate.html

```{r eval=FALSE, include=FALSE, echo=TRUE}
library(gganimate)
#> Loading required package: ggplot2

# We'll start with a static plot
p <- ggplot(iris, aes(x = Petal.Width, y = Petal.Length)) + 
  geom_point()

plot(p)
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
anim <- p + 
  transition_states(Species,
                    transition_length = 2,
                    state_length = 1)

anim
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
anim + 
  ease_aes('cubic-in-out') # Slow start and end for a smoother look
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
anim + 
  ease_aes('cubic-in-out',
           y = 'bounce-out') # Sets special ease for y aesthetic
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
anim + 
  ggtitle('Now showing {closest_state}',
          subtitle = 'Frame {frame} of {nframes}')
```

```{r eval=FALSE, include=FALSE, echo=TRUE}
ggplot(iris, aes(x = Petal.Width, y = Petal.Length)) + 
  geom_line(aes(group = rep(1:50, 3)), colour = 'grey') + 
  geom_point()
```



```{r eval=FALSE, include=FALSE, echo=TRUE}
ggplot(iris, aes(x = Petal.Width, y = Petal.Length)) + 
  geom_point(aes(colour = Species)) + 
  transition_states(Species,
                    transition_length = 2,
                    state_length = 1)
```



---


# ggforce


```{r eval=FALSE, include=FALSE, echo=TRUE}
install.packages("ggforce")
library(ggforce)
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
Titanic
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
titanic <- reshape2::melt(Titanic)

head(titanic)
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
titanic <- gather_set_data(titanic, 1:4)
head(titanic)
# View(titanic)
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
ggplot(titanic, aes(x, id = id, split = y, value = value)) +
  geom_parallel_sets(aes(fill = Sex), alpha = 0.3, axis.width = 0.1) +
  geom_parallel_sets_axes(axis.width = 0.1) +
  geom_parallel_sets_labels(colour = 'white')


```



---


# g2r

```
remotes::install_github("JohnCoene/g2r")
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
library(g2r)

g2(iris, asp(Petal.Length, Petal.Width, color = Species)) %>% 
  fig_point() %>%
  plane_wrap(planes(Species))
```













<!--chapter:end:Graphs.Rmd-->

---
title: "h2o"
---


http://h2o-release.s3.amazonaws.com/h2o/rel-wright/10/docs-website/h2o-r/docs/articles/getting_started.html


```{r eval=FALSE, include=FALSE, echo=TRUE}
if ("package:h2o" %in% search()) { detach("package:h2o", unload=TRUE) }
if ("h2o" %in% rownames(installed.packages())) { remove.packages("h2o") }

# Next, download packages that H2O depends on.

pkgs <- c("RCurl","jsonlite")
for (pkg in pkgs) {
  if (! (pkg %in% rownames(installed.packages()))) { install.packages(pkg) }
}

# Download and install the latest H2O package for R.

install.packages("h2o", type="source", repos=(c("http://h2o-release.s3.amazonaws.com/h2o/latest_stable_R")))

# Initialize H2O and run a demo to see H2O at work.

library(h2o)
h2o.init()
demo(h2o.kmeans)
```


<!--chapter:end:h2o.Rmd-->

---
title: "Hierarchical Clustering"
---


https://datascienceplus.com/hierarchical-clustering-in-r/







<!--chapter:end:HierarchicalClustering.Rmd-->

---
title: "How to Prepare Data for Histopathology Research?"
---

```
author: '[Serdar Balcı, MD, Pathologist](https://sbalci.github.io/)'
date: "`{r #  format(Sys.Date())`"
output:
  revealjs::revealjs_presentation:
    incremental: yes
    theme: sky
    highlight: pygments
    center: no
    smart: yes
    transition: fade
    self_contained: yes
    ig_width: 7
    fig_height: 6
    fig_caption: yes
    reveal_options:
      slideNumber: yes
      previewLinks: yes
  prettydoc::html_pretty:
    theme: leonids
    highlight: github
  rmdshower::shower_presentation: null
  beamer_presentation:
    incremental: yes
    highlight: tango
  html_notebook:
    fig_caption: yes
    highlight: kate
    number_sections: yes
    theme: flatly
    toc: yes
    toc_depth: 5
    toc_float: yes
  slidy_presentation: null
  pdf_document:
    toc: yes
    toc_depth: '5'
  html_document:
    fig_caption: yes
    keep_md: yes
    toc: yes
    toc_depth: 5
    toc_float: yes
  xaringan::moon_reader:
    lib_dir: libs
    nature:
      beforeInit:
      - macros.js
      - https://platform.twitter.com/widgets.js
      highlightStyle: github
      highlightLines: yes
      countIncrementalSlides: no
    self_contained: yes
  ioslides_presentation:
    incremental: yes
    highlight: github
institute: '[serdarbalci.com](https://www.serdarbalci.com)'
editor_options:
  chunk_output_type: inline
```

<!-- Open all links in new tab-->  
<base target="_blank"/>  


<!-- Go to www.addthis.com/dashboard to customize your tools --> <script type="text/javascript" src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5bc36900a405090b">  
</script>



```{r eval=FALSE, include=FALSE, echo=TRUE}
knitr::opts_chunk$set(fig.width = 12, fig.height = 8, fig.path = 'Figs/', echo = TRUE, warning = FALSE, message = FALSE, error = FALSE, eval = TRUE, tidy = TRUE, comment = NA, cache = TRUE)
```


```{r strings , include=FALSE}
PubMedString <- "PubMed: https://www.ncbi.nlm.nih.gov/pubmed/?term="

doiString <- "doi: https://doi.org/"

dimensionString1 <- "<script async='' charset='utf-8' src='https://badge.dimensions.ai/badge.js'></script> <span class='__dimensions_badge_embed__' data-doi='"

dimensionString2 <- "' data-style='small_circle' data-hide-zero-citations='true' data-legend='always'></span>"

altmetricString1 <-"<script type='text/javascript' src='https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js'></script> <span class='altmetric-embed' data-link-target='_blank' data-badge-details='right' data-badge-type='donut' data-doi='"

altmetricString2 <- "' data-hide-no-mentions='true'></span>"

addthis_String1 <- "<div class='addthis_inline_share_toolbox' data-url='pbpath.org/current-journal-watch/' data-title='See this abstract on #PBPath #JournalWatch : "

addthis_String2 <- "'></div>"

```


```{r run xaringan, eval=FALSE, message=FALSE, warning=FALSE, include=FALSE}
# xaringan::inf_mr()
# servr::daemon_stop(1)
```


# How to Prepare Data for Histopathology Research?

**Outline**

- Why is Data Preparation Important?
- Do I need a specific Software?
- What are the Golden Rules?
- What do I do with Data after analysis?
- I got all the tables from the biostatistician, is it enough?
- What is a Good (Clean/Ideal/Tidy) Data?
- What is a Bad (Dirty/Common/Untidy) Data?
- Do I need to know statistics before collecting Data?
- Do I need to have a hypothesis before collecting Data?
- Do I need a research question before collecting Data?


---


# How to Prepare Data for Histopathology Research?

**We Should Collect the Data Related to What We will Report**

- Recommendations for reporting histopathology studies: a proposal

```{r 25846513, include=FALSE}

PMID_25846513 <- RefManageR::ReadPubMed('25846513', database = 'PubMed')

citation_25846513 <- paste0(PMID_25846513$journal,' ', PMID_25846513$year, ' ', PMID_25846513$month,';', PMID_25846513$volume,'(', PMID_25846513$number,'):', PMID_25846513$pages)

PubMed_25846513 <- paste0(PubMedString, PMID_25846513$eprint)

doi_25846513 <- paste0(doiString, PMID_25846513$doi)

dimensionBadge_25846513 <- paste0(dimensionString1, PMID_25846513$doi,dimensionString2)

altmetricBadge_25846513 <- paste0(altmetricString1, PMID_25846513$doi, altmetricString2 )

addthis_inline_25846513 <- paste0(addthis_String1, PMID_25846513$title , " PMID: 25846513" , addthis_String2)
```


<!-- <details open> <summary> -->
- **`{r #  PMID_25846513$title`**
<!-- </summary>  -->

*`{r #  citation_25846513`*

`{r #  PubMed_25846513`

`{r #  addthis_inline_25846513`

`{r #  PMID_25846513$abstract`

`{r #  doi_25846513`

`{r #  dimensionBadge_25846513`

`{r #  altmetricBadge_25846513`

<!-- </details> -->


---


# Tables and Graphs to be Formed  

- Table One: Clinical Features Related to this disease and Histopathological Features (like a CAP synoptic)

- Cross Tables

- IHC Tables

- Survival Tables and Graphs



---

# Age


---


# Gender

- Male
- Female
- Non-binary (based on research)

For missing values:

{gender} 📦

https://lincolnmullen.com/software/gender/

https://github.com/ropensci/gender



---

# Surgery Type

---





# Histopatoloji çalışmalarında istatistik analizi için nasıl veri hazırlanır?

İstatistik analizlerinde en çok vakit alan kısım verilerin düzenlenmesi ve analize hazır hale getirilmesidir. Bu durum o kadar belirgindir ki veri analizi ile ilgili eğitimlerin de özel bir kısmını "veri temizleme" dersleri oluşturmaktadır \(Coursera\). Analize hazır haldeki veri "temiz veri" olarak adlandırılır \(Tidy Data, H.Wickham\). İstatistikçilerin kısıtlı vakti olduğunu düşünüldüğünde verinin temiz olarak teslim edilmesi onların veriyi rahat anlamalarına ve veri temizleme için ayıracakları vakit yerine sizin araştırmanızdaki ilginç noktalara odaklanmalarına yardımcı olacaktır. Ayrıca temiz veri ile çalışmanın istatistikçileri daha mutlu ettiğini ve özen gösterilmiş bir veride onların da daha özenli çalıştığını gözlemlediğimi belirtmek isterim.

Bu yazıda hipotetik bir histopatoloji çalışması için basamak basamak veri hazırlanma süreci anlatılacaktır.

Histopatolojik makalelerde bulunması gereken minimum bilgiler

Bu yazıda kendi karşılaştığım problemleri ve literatürdeki önerileri \(Virchows Arch \(2015\) 466:611–615\) derlemeye çalıştım.

Statistical Problems to Document and to Avoid Manuscript Checklist for Authors

[http://biostat.mc.vanderbilt.edu/wiki/Main/ManuscriptChecklist](http://biostat.mc.vanderbilt.edu/wiki/Main/ManuscriptChecklist)

Aslında bir "eski tümöre yeni boya" olarak adlandırılan ve sık yapılan bir çalışma türünü inceleyeceğiz. Bunun için yapılacak ilk iş çalışılacak tümörle ilgili CAP protokolünü dikkatlice okumaktır. CAP protokollerinin özellikle not ve açıklama kısımlarındaki detaylar çok faydalı olacaktır. Bundan sonra bir boş kağıt alıp CAP protokolünde raporda belirtilmesi gereken konular maddeler halinde sıralanmalıdır. Bu maddeler çalışmanın tasarlamasından, analizine, yorumuna ve tartışmasına çok yardımcı olacaktır.

* # Temiz veri için dikkat edilmesi gereken kurallar:
* Her satır tek hasta

* Her sütun tek bilgi

* Her bilgi tek bir şekilde ifade edilecek

* # Verinin girileceği bilgisayar programı

Aynı değerin farklı şekilde yazılması

Veri hazırlamak için excel ya da filemaker kullanılmasını öneririm.

* # Vaka numarası

Çalışmaya kaç vaka alınacak?

Her değişken için 10 vaka?

Vakaların seçilme şekli: Gelişigüzel? Randomize? Birbirini takip eden \(consequative\)

* # Yıl

Hangi yıl aralığı tercih edilmeli?

Yıl aralığınının belirtilmesi nadir vakalarda vaka sayısı ile ilgili bilgi verebilir. Bu nedenle klinikteki toplam vaka sayısı ile karşılaştırma yapılması

Bir klinikten çıkan vaka sayısı da o klinikte bu işin ne kadar ciddi yapıldığının ve tecrübenin göstergesi. Kabul şansını arttıran faktör.

İmmünohistokimya için eski vakalar mı tercih edilecek yeni vakalar mı?

* # Biyopsi No
* # TC Kimlik, Hasta No, Ad Soyad

  * hasta bazlı çalışma vs örnek bazlı çalışma
  * HIPAA kuralları
  * 
* # Yaş

Yıl, ay

Eğer tümör belli bir yaş aralığında görülüyor, ya da bimodal dağılım gösteriyorsa \(osteosarkom gibi\) bu durumu

* # Doğum Tarihi
* # Cinsiyet
* 
* # Tümör çapı
* # T evresi
* # N evresi

  * **Lenf nodu**
    Direk invazyon
* # M evresi
* # TNM/AJCC evresi
* # Histopatolojik tip
* 
* # **Lenfovasküler İnvazyon \(LVI\)**

Lenfovasküler invazyon çoğu tümör raporlarında belirtilmesi gereken bir özelliktir.

Önerilen kodlama şekli var ise 1, yok ise 0 şeklindedir.

Lenfatik ve vasküler invazyon ayrı ayrı da kodlanabilir. Mesela kolon tümörlerinde ekstramural venöz invazyonun belirtilmesi gibi.

CAP protokollerinde "equivocal" olarak belirtilen şüpheli durumlardan mümkün oldukça kaçınmak analizlerin daha rahat yapılabilmesi için gereklidir.

"Extensive retraction artefact" gibi özellikli durumlar çalışmıyorsa immünohistokimyasal çalışmalara gerek olmadan rutin H&E değerlendirme yeterlidir.

Raporlardan elde edilen bulgular da analiz için kullanılabilir. Özellikle rutin rapora göre tedavi planlanan durumlarda, doğal seyri seyretmek istediğiniz çalışmalarda bunu yapabilirsiniz.

Patologların ise yaptıkları çalışmalarda mutlaka tüm vakalara yeniden bakmaları önerilir. Bazen araştırmacılar sadece "negatif" olarak raporlanan vakalara bakıp, bunlarda "atlanan" lenfovasküler invazyonu yakalamaya çalışırlar. Bu durumda lenfovasküler invazyon yüzdeniz literatürden yüksek çıkacaktır \(Yanlış negatifler azalacaktır\). Pozitif olan olgulara da bakılmalı, "pozitif" olarak raporlanan ve aslında lenfovasküler invazyonu olmayan vakalar \(yanlış pozitif\) ise negatif olarak analize alınmalıdır.

Lenf nodunda metastaz olan olgularda lenfovasküler invazyonu pozitif olarak kabul etmek uygun değildir. Lenf noduna metastaz yapmanın ayrı peritümöral lenfovasküler invazyon tespit edilmesinin ayrı tümör gelişim basamakları olduğu düşünülmelidir.

* # **Perinöral \(perinöryal\) invazyon**
* # Cerrahi sınır
* # Ek hastalık
* # İkinci primer

Birden fazla tümörü olan olgularda klinik gidişi ve sağkalımı diğer tümör etkiliyor olabilir. Sistoprostatektomilerde ürotelyal karsinom sağkalıma, prostat tümörlerinden daha fazla etki edecektir.

Bu vakaların çıkartılması da insidansı etkileyebilir. Bu nedenle çalışmanın tasarımına göre bu vakaları eklemek ya da çıakrtmak gerekecektir.

* # **İmmünohistokimya**

  * Pozitif, negatif

  * Kayıp, korunmuş

  * Şiddet

  * Yaygınlık

  * H-skor, Allred score, Quick score

  * Hangi hücre pozitif

  * Hangi komponent pozitif \(nükleer, sitoplazmik, membranöz\)

  * Yüzde

  * Sonuçları gruplama

  * Uygun antikor klonunu seçmek ayrı bir yazı konusu olmalı.
* # **Ameliyat şekli**
* # Sağkalım

Sağkalım verisi de hassas bilgilerdendir.

Ölüm bildirim sistemi

Tarihler

Tanı tarihi

Son tarih

Tarih girerken neye dikkat edelim \(İngilizce ve Türkçe farklı tarih formatları\)

* # Overall survival
* # Disease Free Survival
* # Vertikal tarama
* # Bilinmeyen veriler, Eksik veriler, Missing values

Her eksik hücre, çok değişkenli analizden o vakanın düşmesine neden olacaktır.

Eksik camlar

Eksik verileri excelde kontrol etme

* # Tek merkez, çok merkezli çalışma
* # İstatistikçiye sorulması gereken sorular

Çalışmaya başlamadan önce, hangi soruları soracağınızı zaten planlamış olmanız ve buna göre verilerinizi düzenlemiş olmanız gerekir. Yine de çalışma sürerken ve çalışmanın sonunda yeni sorular ve düşünceler ortaya çıkabilir. Sorulacak sorular ve yapılacak analizler için bir ön hazırlık yapmak ve bunları düzgün cümleler halinde kaydetmek önemlidir. Mesela "tümör tipleri ile X protein ekspresyonunu karşılaştırmak istiyorum" bir soru olabilir. Ama daha iyisi "X proteininin ekspresyonunun A tümöründe B tümörüne göre daha fazla olduğunu düşünüyorum, bunun öyle olup olmadığını analiz etmenizi istiyorum" daha da anlaşılır bir soru olacaktır.

* # Bana p değeri ver
* # Hangi istatistik yöntemlerini bilmem lazım

Tıp fakültesinin ilk yıllarında öğrenilen istatistikle ilgili kavramlar yıllar içinde unutuluyor. Elbette herkesin detaylı olarak istatistik metodlarını bilmesine gerek yok. Ancak yine de bir istatistik okuryazarlığının \(statistical literacy\) olmasında fayda var.



* * ANOVA testi

30 vaka sayılı, tercihen verilerin normal dağıldığı durumlarda ve veriler ölçülebilir ve sürekli nitelikte ise kullanılır. Mesela yaş, özefagus lümeninin, özefagus duvarına oranı gibi durumlarda kullanılabilir.

Ancak histopatolojik dercelendirme ya da evreleme gibi kesikli değişkenlerin olduğu durumlarda parametrik test olan ANOVA önerilmez. Grade 1 ila grade 2 arasındaki fark ile grade 2 ila grade 3 arasındaki fark matematiksel olarak eşit değildir. Grade 2, grade 1 den 2 kat kötü, grade 3 ise grade 1'den 3 kat kötüdür gibi bir yorum yapılmaz.

Hastaların kanser evresinin ortalama 2,5 , ya da tümör grade'inin ortalama 1,2 olarak verilmesi önerilmez. Bunun yerine ortanca ve çeyrekler arası fark \(median, interquartile range\) kullanılması daha uygun olur. Bu nedenle yapılacak test de ANOVA'nın nonparametrik karşılığı olan Kruskal Wallis testidir.

Ölçüm şeklinde olan, sürekli değişkenlerde bile vaka sayısının 30'dan az ise ya da veriler normal dağılmıyorsa birden fazla grubun karşılaştırmasında da Kruskal Wallis testi kullanılır.

İstatistik dışı bakış açısı ile; Kanser evreleme çalışmalarında \(lenf nodu sayısında\) logaritmik dönüşüm çok kullanılıyor. Ve hemen tüm çalışmalarda işe yarıyor. Örnek [https://www.ncbi.nlm.nih.gov/pubmed/28094085](https://www.ncbi.nlm.nih.gov/pubmed/28094085) Ama klinikte bilgisayar destekli bir karar sistemi kullanılmadığı zaman bu logaritmik değerler çok afaki kalabiliyor. Model anlamlı olsa da pratikte anlaması zor oluyor. Normallik yoksa nonparametrik testleri bir kademe daha rahat anlayabiliyorum.

# Top ten errors of statistical analysis in observational studies for cancer research

[https://rd.springer.com/article/10.1007%2Fs12094-017-1817-9](https://rd.springer.com/article/10.1007%2Fs12094-017-1817-9)



---

# Tweets

```{r eval=FALSE, include=FALSE, echo=TRUE}
library(rtweet)

how_to_prepare_data_tweet_1 <- paste0(
  "1/n
  
  
  "
  )

post_tweet(how_to_prepare_data_tweet_1)

```





<!--chapter:end:How-to-Prepare-Data-for-Histopathology-Research.Rmd-->



<!-- --- -->
<!-- title: "How To Use R With Excel" #Moving from Excel to R -->
<!-- subtitle: <h5>A Comprehensive Guide to Transitioning from Excel to R\h -->
<!-- author: "Alyssa Columbus" -->
<!-- date: '`{r #  format(Sys.Date(), "%B %d, %Y")`' -->
<!-- output: -->
<!--   html_document: -->
<!--     df_print: paged -->
<!--     toc: TRUE -->
<!--     toc_float: TRUE -->
<!--     toc_depth: '4' -->
<!-- --- -->


<!-- https://github.com/acolum/r-and-excel/blob/master/How-To-Use-R-With-Excel.Rmd -->




<!-- ```{r eval=FALSE, include=FALSE, echo=TRUE} -->
<!-- knitr::opts_chunk$set(echo = TRUE) -->
<!-- ``` -->

<!-- <style type="text/css"> -->

<!-- h1.title { -->
<!--   font-size: 38px; -->
<!--   color: Black; -->
<!--   text-align: center; -->
<!-- } -->
<!-- h3.subtitle { -->
<!--   font-size: 38px; -->
<!--   color: Black; -->
<!--   text-align: center; -->
<!-- } -->
<!-- h4.author {  -->
<!--     font-size: 18px; -->
<!--   /*font-family: "Times New Roman", Times, serif;*/ -->
<!--   color: DarkBlue; -->
<!--   text-align: center; -->
<!-- } -->
<!-- h4.date {  -->
<!--   font-size: 18px; -->
<!--   /*font-family: "Times New Roman", Times, serif;*/ -->
<!--   color: CornflowerBlue; -->
<!--   text-align: center; -->
<!-- } -->
<!-- h1 { -->
<!--   font-size: 38px; -->
<!--   color: Black; -->
<!-- } -->
<!-- h2 { -->
<!--   color: DarkBlue; -->
<!-- } -->
<!-- h3 { -->
<!--   color: CornflowerBlue; -->
<!-- } -->
<!-- h4 { -->
<!--   color: LightSlateGray; -->
<!-- } -->
<!-- </style> -->

<!-- # 0. Prerequisites -->
<!-- Before you get started, make sure you have [R](https://www.r-project.org/) and [RStudio](https://www.rstudio.com/) installed on your machine. If you don't have these installed on your machine, the following are instructions for you to install this software. -->

<!-- ## 0.1 Installing R -->
<!-- R is a powerful, open-source statistical programming language that anyone can download for free! You can simply go to [CRAN's website](https://www.r-project.org/) and install R by following these steps: -->

<!-- ### 0.1.1 Under the 'Download' heading, select 'CRAN' -->
<!-- ![](CRAN.png) -->

<!-- ### 0.1.2 Install a CRAN Mirror -->

<!-- Install the CRAN mirror that's nearest to your geographic location. For example, if you live in Orange County, California, you should install the [UCLA CRAN Mirror.](http://cran.stat.ucla.edu/) -->

<!-- ### 0.1.3 Install your Machine's Version of R -->
<!-- ![](DownloadR.png) -->

<!-- #### 0.1.3.1 R for Windows -->
<!-- ![Install R for the first time.](WindowsR1.png) -->

<!-- ![After you click this link, follow the instructions given in the installation.](WindowsR2.png) -->

<!-- #### 0.1.3.2 R for (Mac) OS X -->
<!-- ![After you click this link, follow the instructions given in the installation.](MacR1.png) -->

<!-- #### 0.1.3.3 R for Linux -->
<!-- ![Install the R version relevant to your Linux server, and follow the instructions given in the installation.](LinuxR1.png) -->

<!-- ## 0.2 Installing RStudio -->
<!-- RStudio is an open-source professional software that makes R much easier to use. Download the free, open-source license version from [RStudio's website](https://www.rstudio.com/products/rstudio/download/#download). The installation steps are very similar to those of R's for all operating systems. -->

<!-- # 1. Importing and Exporting Excel Files in R -->
<!-- The open-source community has developed a few smooth and convenient ways for you to upload and download Excel files in R. -->

<!-- ## 1.1 Importing Excel Files to R -->
<!-- Install the `{r # eadxl` package into RStudio by typing  -->

<!-- ```{r, eval=FALSE} -->
<!-- install.packages("readxl") -->
<!-- ```  -->

<!-- into the Console. -->

<!-- ![](InstallPackage_readxl.png) -->



<!-- After the package is finished installing, call the package by typing  -->

<!-- ```{r, eval=FALSE}  -->
<!-- library(readxl) -->
<!-- ```  -->

<!-- into the Console. You can now upload your Excel file into R (and assign it to the variable `data`) by typing -->

<!-- ```{r, eval=FALSE}  -->
<!-- data <- read_excel("D:/path/file.xlsx") -->
<!-- ```  -->

<!-- into the Console. -->

<!-- Another way you can import an Excel file into R after calling the `{r # eadxl` library is to, in RStudio, go to `File > Import Dataset > From Excel`, and browse for your Excel file. -->

<!-- ![](ExcelImport1.png) -->


<!-- After you're satisfied with how your data is being imported, select `Import` on the bottom right of the window, and your Excel data will be imported into R. -->

<!-- ![](ExcelImport2.png) -->

<!-- ## 1.2 Exporting Excel Files from R -->
<!-- Install the `openxlsx` package into RStudio by typing  -->

<!-- ```{r, eval=FALSE} -->
<!-- install.packages("openxlsx") -->
<!-- ```   -->

<!-- into the Console. -->

<!-- After the package is finished installing, call the package by typing  -->

<!-- ```{r, eval=FALSE}  -->
<!-- library(openxlsx) -->
<!-- ```  -->

<!-- into the Console. You can now download your data (assigned to the variable `data` here) to a custom file name by typing -->

<!-- ```{r, eval=FALSE}  -->
<!-- write.xlsx(data, file = "D:/path/your_file_name.xlsx") -->
<!-- ```  -->

<!-- into the Console. -->

<!-- # 2. How to Transition From Excel to R -->

<!-- ## 2.1 Navigating RStudio -->

<!-- ![We will be working out of RStudio, a powerful IDE (Integrated Development Environment) for R.](RStudioGuide.png) -->

<!-- ## 2.2 Data Structures and Variables in R -->

<!-- ![](RDataStructures.png) -->


<!-- Try to get comfortable with the different types of data structures in R. -->

<!-- Excel spreadsheets are typically recognized as **dataframes** in R, which are composed of multiple lists. -->

<!-- ### 2.2.1 Creating Your First Dataframe -->

<!-- Start by creating some lists and assigning them to variables using `<-` (not `=`, as this may confuse R in some function calls): -->

<!-- ```{r eval=FALSE, include=FALSE, echo=TRUE} -->
<!-- teams <- c("Angels","Dodgers","Cubs") # c() command stands for combine; use whenever creating a list of objects in R -->
<!-- wins <- c(70,56,64) -->
<!-- ca_team <- c(TRUE, TRUE, FALSE) -->

<!-- baseball <- data.frame(teams,wins,ca_team) # combine lists into dataframe -->

<!-- baseball # view the dataframe -->
<!-- ``` -->

<!-- Now you have a dataframe with 3 columns, or lists (string, numeric, and boolean). -->

<!-- ## 2.3 Working with Dataframes -->

<!-- ### 2.3.1 Looking Up Number of Observations -->

<!-- Using the `iris` dataset that comes with R, and the `nrow()` function, you can find the number of observations or rows: -->

<!-- ```{r eval=FALSE, include=FALSE, echo=TRUE} -->
<!-- # View(iris) # use this command to view the iris dataset -->
<!-- nrow(iris) # displays the number of rows in the iris dataset -->
<!-- ``` -->

<!-- Similarly, you can also find the number of columns: -->

<!-- ```{r eval=FALSE, include=FALSE, echo=TRUE} -->
<!-- ncol(iris) # displays the number of columns in the iris dataset -->
<!-- ``` -->

<!-- To find the number of rows and columns at the same time, use `dim()`: -->

<!-- ```{r eval=FALSE, include=FALSE, echo=TRUE} -->
<!-- dim(iris) -->
<!-- ``` -->

<!-- ### 2.3.2 Focusing on the First/Last X Rows -->

<!-- ```{r eval=FALSE, include=FALSE, echo=TRUE} -->
<!-- head(iris, 10) # displays the first 10 rows of the iris dataset -->
<!-- tail(iris, 10) # displays the last 10 rows of the iris dataset -->
<!-- ``` -->

<!-- ### 2.3.3 Looking at the Structure of a Dataframe -->

<!-- ```{r eval=FALSE, include=FALSE, echo=TRUE} -->
<!-- str(iris) # investigate the structure of the iris dataframe -->
<!-- ``` -->

<!-- ### 2.3.4 Focusing on Certain Columns, Rows, or Cells -->

<!-- Sometimes you may want to look at a single cell, column, or row. To do so, add brackets to the dataframe in the console, like:  -->

<!-- ```{r, eval=F} -->
<!-- iris[1,3] # look at the value in the 1st row, 3rd column -->

<!-- iris[1,] # look at all of the values in the 1st row; don't forget the comma! -->

<!-- iris[,3] # look at all of the values in the 3rd column; iris[3] also works -->

<!-- iris[1:3,] # look at all of the values in rows 1 through 3; ':' can also be applied to selecting columns -->
<!-- ``` -->

<!-- A helpful phrase to remember is "Data RoCks!" As in, `Dataframe[Rownumber,Columnnumber]`. -->

<!-- Most of the time, it's difficult to reference columns by number - especially when there's so many columns. Here are alternative ways to select one or multiple columns: -->

<!-- ```{r, eval=F} -->
<!-- iris$Sepal.Length # selecting the one Sepal.Length column from the iris dataset -->

<!-- iris$Sepal.Length[3] # selecting the 3rd value in the Sepal.Length column, or the 3rd row value in the Sepal.Length column -->

<!-- iris[c("Sepal.Length","Sepal.Width")] # selecting multiple columns (Sepal.Length, Sepal.Width) from the iris dataset -->
<!-- ``` -->

<!-- ### 2.3.5 Changing the Format of a Column -->

<!-- Say you imported a dataset from Excel with `$` or `%` signs appended to some numeric values. If you didn't specify these values to be numeric when you imported your dataset, R will likely interpret those values to be strings instead of currency or percentages. This means that R will not be able to perform calculations with these values.  -->

<!-- However, in the R console, you can perform the following short commands to remove `$` or `%` signs and convert the string values to numeric values that can then be used in calculations: -->

<!-- ```{r,eval=F} -->
<!-- # delete the dollar signs in a currency column in the dataset -->
<!-- dataset$currency <- gsub("\\$", "", dataset$currency) -->
<!-- # change the format of the currency column to numeric -->
<!-- dataset$currency <- as.numeric(dataset$currency) -->

<!-- # similarly, with percentages, delete the percent signs in a percentages column in the dataset -->
<!-- dataset$percentages <- gsub("\\%", "", dataset$percentages) -->
<!-- # change the format of the percentages column to numeric -->
<!-- dataset$percentages <- as.numeric(dataset$percentages) -->
<!-- ``` -->

<!-- ### 2.3.6 Sorting a Dataframe Based on One Column -->

<!-- Back to the `iris` dataset: -->

<!-- ```{r,eval=T} -->
<!-- iris_sort_lg <- iris[order(iris$Sepal.Length),] # order the iris dataset from least to greatest value of Sepal.Length -->
<!-- iris_sort_lg -->
<!-- iris_sort_gl <- iris[order(-iris$Sepal.Length),] # order the iris dataset from greatest to least value of Sepal.Length -->
<!-- iris_sort_gl -->
<!-- ``` -->

<!-- ### 2.3.7 Filtering Data by Column Content -->

<!-- ```{r eval=FALSE, include=FALSE, echo=TRUE} -->
<!-- # Create a new dataset called 'iris2' that only contains Sepal.Length > 6.7 -->
<!-- iris2 <- subset(iris, Sepal.Length > 6.7) -->
<!-- iris2 -->
<!-- ``` -->

<!-- ### 2.3.8 Calculating Quantities with Columns -->

<!-- ```{r eval=FALSE, include=FALSE, echo=TRUE} -->
<!-- # Adding 2 columns and assigning values to new column, Sepal_Sum -->
<!-- iris$Sepal_Sum <- iris$Sepal.Length + iris$Sepal.Width -->

<!-- # Subtracting 2 columns and assigning values to new column, Sepal_Diff -->
<!-- iris$Sepal_Diff <- iris$Sepal.Length - iris$Sepal.Width -->

<!-- head(iris) -->
<!-- ``` -->

<!-- ### 2.3.9 Calculations on Columns -->

<!-- ```{r eval=FALSE, include=FALSE, echo=TRUE} -->
<!-- # Adding the values of the Sepal.Length column and assigning the value to sepal_sum -->
<!-- sepal_sum <- sum(iris$Sepal.Length) -->
<!-- sepal_sum -->

<!-- # Finding the average of the values of the Sepal.Length column and assigning the value to sepal_avg -->
<!-- sepal_avg <- mean(iris$Sepal.Length) -->
<!-- sepal_avg -->

<!-- # Finding the median of the values of the Sepal.Length column and assigning the value to sepal_median -->
<!-- sepal_median <- median(iris$Sepal.Length) -->
<!-- sepal_median -->
<!-- ``` -->

<!-- ## 2.4 File Names and Version Control -->

<!-- ### 2.4.1 File Names -->

<!-- As in Excel spreadsheets, the file names of R scripts are important, as they need to be discernable by both users and computers. -->

<!-- Here are some bad examples of file names: -->

<!-- ``` -->
<!-- myscript.R -->
<!-- Joe's Filenames Use Spaces and Punctuation;"'.R -->
<!-- figure 1.R -->
<!-- fig 2.R -->
<!-- JW7d^(2sl@pleasedontdeletethisWx2*.R -->
<!-- ``` -->

<!-- and here are some good examples of file names: -->

<!-- ``` -->
<!-- 2010-07-17_disneyland-55th-bday.R -->
<!-- joes-filenames-are-getting-better.R -->
<!-- fig01_scatterplot-length-vs-interest.R -->
<!-- fig02_histogram-attendance.R -->
<!-- 2018-06-18_predictive-modeling-contest-week1.R -->
<!-- ``` -->

<!-- The following sections detail why the above examples are categorized as good and bad file names. -->

<!-- #### 2.4.1.1 Three Principles of Good File Names -->

<!-- **1. Machine readable** -->

<!-- Avoid spaces, punctuation, accented characters, and case sensitivity to achieve optimal machine readability. Make your file names easy to compute on with a deliberate use of delimiters. -->

<!-- The deliberate use of `_` and `-` allows you to recover metadata from the file names. -->

<!-- For example, for the following filenames, -->

<!-- ``` -->
<!-- 2018-07-05_Q2ANALYSIS_GMWB-Mix_T01.R -->
<!-- 2018-07-05_Q2ANALYSIS_GMWB-Mix_T02.R -->
<!-- 2018-07-05_Q2ANALYSIS_GMWB-Mix_T03.R -->
<!-- 2018-07-05_Q2ANALYSIS_GMWB-Mix_all.R -->
<!-- ... -->
<!-- 2018-07-18_Q2ANALYSIS_Non-GMWB-Mix_T01.R -->
<!-- 2018-07-18_Q2ANALYSIS_Non-GMWB-Mix_T02.R -->
<!-- 2018-07-18_Q2ANALYSIS_Non-GMWB-Mix_P01.R -->
<!-- 2018-07-18_Q2ANALYSIS_Non-GMWB-Mix_P02.R -->
<!-- 2018-07-18_Q2ANALYSIS_Non-GMWB-Mix_all.R -->
<!-- ``` -->

<!-- you can create a list of file names only containing "Non-GMWB-Mix" in R by typing the following into the Console. -->

<!-- ```{r,eval=F} -->
<!-- # Make sure you have the `dplyr` package installed first! -->
<!-- flist <- list.files(pattern = "Non-GMWB-Mix") %>% head -->
<!-- ``` -->

<!-- If you also type -->

<!-- ```{r,eval=F} -->
<!-- # Make sure you have the `stringr` package installed first! -->
<!-- stringr::str_split_fixed(flist,"[_\\.]",4) -->
<!-- ``` -->

<!-- into the Console, the date, quarter analysis, analysis type, and output type will be displayed separately, creating a nicely organized metadata table.  -->

<!-- The underscore `_` is used to delimit units of metadata you want to retrieve later, and the hyphen `-` is used to delimit separate words within these units of metadata. -->

<!-- By making your file names machine readable, you make it easy to: -->

<!-- 1. search for files later, -->

<!-- 2. narrow file lists based on names, and -->

<!-- 3. extract information from file names (e.g. by splitting). -->

<!-- **2. Human readable** -->

<!-- Include information on content in your file names to make them human readable. This connects with the concept of slugs from [semantic URLs](http://en.wikipedia.org/wiki/Semantic_URL). -->

<!-- For example,  -->

<!-- ``` -->
<!-- 2018-07-05_Q2ANALYSIS_GMWB-Mix_T01.R -->
<!-- 2018-07-05_Q2ANALYSIS_GMWB-Mix_T02.R -->
<!-- 2018-07-05_Q2ANALYSIS_GMWB-Mix_T03.R -->
<!-- 2018-07-05_Q2ANALYSIS_GMWB-Mix_all.R -->
<!-- ... -->
<!-- 2018-07-18_Q2ANALYSIS_Non-GMWB-Mix_T01.R -->
<!-- 2018-07-18_Q2ANALYSIS_Non-GMWB-Mix_T02.R -->
<!-- 2018-07-18_Q2ANALYSIS_Non-GMWB-Mix_P01.R -->
<!-- 2018-07-18_Q2ANALYSIS_Non-GMWB-Mix_P02.R -->
<!-- 2018-07-18_Q2ANALYSIS_Non-GMWB-Mix_all.R -->
<!-- ``` -->

<!-- these file names "embrace the slug" by having each of the subanalyses and subtypes in the Q2 Analysis labeled at the end of each file name.  -->

<!-- "Embracing the slug" allows you to more easily figure out which R files you're looking at or trying to find. -->

<!-- **3. Plays well with default ordering** -->

<!-- To play well with default ordering, you need to: -->

<!-- 1. put something numeric first, -->

<!-- 2. use the ISO 8601 standard for dates (YYYY-MM-DD), and -->

<!-- 3. left-pad other numbers with zeros. -->

<!-- Our example file names -->

<!-- ``` -->
<!-- 2018-07-05_Q2ANALYSIS_GMWB-Mix_T01.R -->
<!-- 2018-07-05_Q2ANALYSIS_GMWB-Mix_T02.R -->
<!-- 2018-07-05_Q2ANALYSIS_GMWB-Mix_T03.R -->
<!-- 2018-07-05_Q2ANALYSIS_GMWB-Mix_all.R -->
<!-- ... -->
<!-- 2018-07-18_Q2ANALYSIS_Non-GMWB-Mix_T01.R -->
<!-- 2018-07-18_Q2ANALYSIS_Non-GMWB-Mix_T02.R -->
<!-- 2018-07-18_Q2ANALYSIS_Non-GMWB-Mix_P01.R -->
<!-- 2018-07-18_Q2ANALYSIS_Non-GMWB-Mix_P02.R -->
<!-- 2018-07-18_Q2ANALYSIS_Non-GMWB-Mix_all.R -->
<!-- ``` -->

<!-- put something numeric first (the date, which also fits the ISO 8601 standard), so they're organized in chronological order. Also, the version numbers at the end of each file name are left-padded with zeros so when double-digit versions are created, the later versions won't be ordered before the single-digit versions. -->

<!-- ### 2.4.2 Version Control -->

<!-- An example of a series of good file names for version control is: -->

<!-- ``` -->
<!-- 2018-07-05_Q2ANALYSIS_GMWB-Mix_T01.R -->
<!-- 2018-07-05_Q2ANALYSIS_GMWB-Mix_T02.R -->
<!-- 2018-07-05_Q2ANALYSIS_GMWB-Mix_T03.R -->
<!-- 2018-07-05_Q2ANALYSIS_GMWB-Mix_all.R -->
<!-- ... -->
<!-- 2018-07-18_Q2ANALYSIS_Non-GMWB-Mix_T01.R -->
<!-- 2018-07-18_Q2ANALYSIS_Non-GMWB-Mix_T02.R -->
<!-- 2018-07-18_Q2ANALYSIS_Non-GMWB-Mix_P01.R -->
<!-- 2018-07-18_Q2ANALYSIS_Non-GMWB-Mix_P02.R -->
<!-- 2018-07-18_Q2ANALYSIS_Non-GMWB-Mix_all.R -->
<!-- ``` -->

<!-- Note how these file names not only follow all of the conventions for good file names, but they also indicate the type of script (with `T` for table(s) and `P` for plot(s)) and version number at the end. -->

<!-- # 3. Common Excel Functions in R -->

<!-- To find more information about how to use each of these R functions, type `?function` (e.g. `?sum`) in the RStudio Console, and you'll be provided with comprehensive and clear R documentation. If you need even more information about an R function, type "how to use ___ function in R" or something similar into Google. -->

<!-- | Excel Formula | R Function | Type | -->
<!-- |---------------|------------|------| -->
<!-- | ABS | `abs` | Arithmetic | -->
<!-- | ADDRESS | `assign` | Essentials | -->
<!-- | AND | `&`,`&&`,`all` | Boolean | -->
<!-- | AVERAGE, AVG, AVERAGEIF | `mean` | Arithmetic |  -->
<!-- | BETADIST | `pbeta` | Statistics | -->
<!-- | BETAINV | `qbeta` | Statistics | -->
<!-- | BINOMDIST | `pbinom` when cumulative,`dbinom` when not | Statistics | -->
<!-- | CEILING | `ceiling` | Arithmetic | -->
<!-- | CELL | `str` has the same idea | Essentials | -->
<!-- | CHIDIST, CHISQDIST | `pchisq` | Statistics | -->
<!-- | CHIINV, CHISQINV | `qchisq` | Statistics | -->
<!-- | CHITEST | `chisq.test` | Statistics | -->
<!-- | CHOOSE | `switch` | Essentials | -->
<!-- | CLEAN | `gsub` | Text | -->
<!-- | COLS, COLUMNS | `ncol` | Essentials | -->
<!-- | COLUMN | `col`,`:`,`seq` | Essentials | -->
<!-- | COMBIN | `choose` | Essentals | -->
<!-- | CONCATENATE | `paste` | Text | -->
<!-- | CONFIDENCE | `-qnorm(alpha/2)*std/sqrt(n)` | Statistics | -->
<!-- | CORREL | `cor` | Statistics | -->
<!-- | COUNT, COUNTIF | `length` | Arithmetic | -->
<!-- | COVAR | `cov` | Statistics | -->
<!-- | CRITBINOM | `qbinom` | Statistics | -->
<!-- | DELTA | `identical` | Boolean | -->
<!-- | EXACT | `==` | Boolean | -->
<!-- | EXP | `exp` | Arithmetic | -->
<!-- | EXPONDIST | `pexp` when cumulative,`dexp` when not | Statistics | -->
<!-- | FACT | `factorial` | Arithmetic | -->
<!-- | FACTDOUBLE | `dfactorial` in the `phangorn` package | Arithmetic | -->
<!-- | FDIST | `pf` | Statistics | -->
<!-- | FIND | `{r # egexpr`,`grepl`,`grep` | Text | -->
<!-- | FINV | `qf` | Statistics | -->
<!-- | FISHER | `atanh` | Arithmetic | -->
<!-- | FISHERINV | `tanh` | Arithmetic | -->
<!-- | FIXED | `format`,`sprintf`,`formatC` | Essentials | -->
<!-- | FLOOR | `floor` | Arithmetic | -->
<!-- | FORECAST | `predict` on an `lm` object | Statistics | -->
<!-- | FREQUENCY | `cut`,`table` | Arithmetic | -->
<!-- | FTEST | `var.test` | Statistics | -->
<!-- | GAMMADIST | `pgamma` if last argument T,`dgamma` if last arg. F | Statistics | -->
<!-- | GAMMAINV | `qgamma` | Statistics | -->
<!-- | GAMMALN | `lgamma` | Statistics | -->
<!-- | GAUSS | `pnorm(x) - 0.5` | Statistics | -->
<!-- | GCD | `gcd` | Arithmetic | -->
<!-- | GEOMEAN | `exp(mean(log(x)))` | Arithmetic | -->
<!-- | GESTEP | `>=` | Boolean | -->
<!-- | HARMEAN | `harmonic.mean` in the `psych` package | Arithmetic | -->
<!-- | HLOOKUP | `match`,`merge` | Essentials | -->
<!-- | HYPGEOMDIST | `dhyper` | Statistics | -->
<!-- | IF | `if`,`ifelse` | Essentials | -->
<!-- | IFERROR | `try`,`tryCatch` | Essentials | -->
<!-- | INDEX | `x[y,z]` | Essentials | -->
<!-- | INDIRECT | `get` | Essentials | -->
<!-- | INT | `as.integer`(not for negative numbers),`floor` | Arithmetic | -->
<!-- | INTERCEPT | first element of `coef` of an `lm` object | Statistics | -->
<!-- | ISLOGICAL | `is.logical` | Boolean | -->
<!-- | ISNA | `is.na` | Boolean | -->
<!-- | ISNUMBER | `is.numeric` | Boolean | -->
<!-- | ISTEXT | `is.character` | Boolean | -->
<!-- | KURT | `kurtosis` in the `moments` package | Statistics | -->
<!-- | LARGE | `sort` | Statistics | -->
<!-- | LCM | `scm` in the `schoolmath` package | Arithmetic | -->
<!-- | LEFT | `substr` | Text | -->
<!-- | LEN, LENGTH | `nchar` | Text | -->
<!-- | LINEST | `lm` | Statistics | -->
<!-- | LN, LOG | `log` | Arithmetic | -->
<!-- | LOG10 | `log10` | Arithmetic | -->
<!-- | LOGINV | `qlnorm` | Statistics | -->
<!-- | LOGNORMDIST | `plnorm` | Statistics | -->
<!-- | LOWER | `tolower` | Text | -->
<!-- | MATCH | `match`,`which` | Essentials | -->
<!-- | MAX | `max` (sometimes `pmax`) | Arithmetic | -->
<!-- | MDETERM | `det` | Arithmetic | -->
<!-- | MEDIAN | `median` | Arithmetic | -->
<!-- | MID | `substr` | Text | -->
<!-- | MIN | `min` (sometimes `pmin`) | Arithmetic | -->
<!-- | MINVERSE | `solve` | Arithmetic | -->
<!-- | MMULT | `%*%` | Arithmetic | -->
<!-- | MOD | `%%` | Arithmetic | -->
<!-- | MODE | `as.numeric(names(which.max(table(x))))` | Arithmetic | -->
<!-- | MUNIT | `diag` | Arithmetic | -->
<!-- | N | `as.numeric` | Arithmetic | -->
<!-- | NEGBINOMDIST | `dnbinom` | Statistics | -->
<!-- | NORMDIST, NORMSDIST | `pnorm` when cumulative,`dnorm` when not | Statistics | -->
<!-- | NORMINV, NORMSINV | `qnorm` | Statistics | -->
<!-- | NOT | `!` | Boolean | -->
<!-- | NOW | `date`,`Sys.time` | Essentials | -->
<!-- | OR | `|`,`||`,`any` | Boolean | -->
<!-- | PEARSON | `cor` | Statistics | -->
<!-- | PERCENTILE | `quantile` | Statistics | -->
<!-- | PERCENTRANK | `ecdf` | Statistics | -->
<!-- | PERMUT | `function(n,k) {choose(n,k)*factorial(k)}` | Arithmetic | -->
<!-- | PERMUTATIONA | `n^k` | Arithmetic | -->
<!-- | PHI | `dnorm` | Statistics | -->
<!-- | POISSON | `ppois` when cumulatic,`dpois` when not | Statistics | -->
<!-- | POWER | `^` | Arithmetic | -->
<!-- | PROB | `ecdf` | Statistics | -->
<!-- | PRODUCT | `prod` | Arithmetic | -->
<!-- | PROPER | `toupper` | Text | -->
<!-- | QUARTILE | `quantile` | Arithmetic | -->
<!-- | QUOTIENT | `%/%` | Arithmetic | -->
<!-- | RAND | `{r # unif` | Arithmetic | -->
<!-- | RANDBETWEEN | `sample` | Arithmetic | -->
<!-- | RANK | `{r # ank` | Essentials | -->
<!-- | REPLACE | `sub`,`gsub` | Text | -->
<!-- | REPT | `{r # ep` and `paste` or `paste0` | Text | -->
<!-- | RIGHT | `substring` | Text | -->
<!-- | ROUND | `{r # ound` | Arithmetic | -->
<!-- | ROUNDDOWN | `floor` | Arithmetic | -->
<!-- | ROUNDUP | `ceiling` | Arithmetic | -->
<!-- | ROW | `{r # ow`,`:`,`seq` | Essentials | -->
<!-- | ROWS | `nrow` | Essentials | -->
<!-- | RSQ | `summary` of `lm` object | Statistics | -->
<!-- | SEARCH | `{r # egexpr`,`grep` | Text | -->
<!-- | SIGN | `sign` | Arithmetic | -->
<!-- | SKEW | `skewness` in the `moments` package | Statistics | -->
<!-- | SLOPE | in `coef` of `lm` object | Statistics | -->
<!-- | SMALL | `sort` | Arithmetic | -->
<!-- | SQRT | `sqrt` | Arithmetic | -->
<!-- | STANDARDIZE | `scale` | Statitics | -->
<!-- | STD, STDEV | `sd` | Arithmetic | -->
<!-- | STEYX | `predict` on an `lm` object | Statistics | -->
<!-- | STRING | `format`,`sprintf`,`formatC` | Text | -->
<!-- | SUBSTITUTE | `sub`,`gsub`,`paste` | Essentials | -->
<!-- | SUM, SUMIF | `sum` | Arithmetic | -->
<!-- | SUMPRODUCT | `crossprod` | Arithmetic | -->
<!-- | TDIST | `pt` | Statistics | -->
<!-- | TEXT | `format`,`sprintf`,`formatC` | Text | -->
<!-- | TINV | `abs(qt(x/2,data))` | Statistics | -->
<!-- | TODAY | `Sys.Date` | Essentials | -->
<!-- | TRANSPOSE | `t` | Arithmetic | -->
<!-- | TREND | `fitted` of an `lm` object | Statistics | -->
<!-- | TRIM | `sub` | Essentials | -->
<!-- | TRIMMEAN | `mean(x,trim=tr/2)` | Arithmetic | -->
<!-- | TRUNC | `trunc` | Essentials | -->
<!-- | TTEST | `t.test` | Statistics | -->
<!-- | TYPE | `typeof`,`mode`,`class` | Essentials | -->
<!-- | UPPER | `toupper` | Text | -->
<!-- | VALUE | `as.numeric` | Arithmetic | -->
<!-- | VAR | `var` | Essentials | -->
<!-- | VLOOKUP | `match`,`merge` | Essentials | -->
<!-- | WEEKDAY | `weekdays` | Essentials | -->
<!-- | WEIBULL | `pweibull` when cumulative,`dweibull` when not | Statistics | -->
<!-- | ZTEST | `pnorm` | Statistics |  -->

<!-- # 4. Common Excel Plots in R -->

<!-- ## 4.1 Distribution Plots -->

<!-- ### 4.1.1 Density Plot -->

<!-- Using the `iris` dataset that's pre-installed in R, -->

<!-- ```{r,eval=T} -->
<!-- # If you don't have the ggplot2 library, be sure to -->
<!-- # install it by typing `install.packages("ggplot2")` -->
<!-- # into the Console. -->
<!-- library(ggplot2)   -->

<!-- ggplot(iris, aes(Petal.Width, colour=Species, fill=Species)) + -->
<!--   geom_density(alpha=0.55) -->
<!-- ``` -->

<!-- ### 4.1.2 Histogram -->

<!-- Using the `iris` dataset that's pre-installed in R, -->

<!-- ```{r,eval=T} -->
<!-- # If you don't have the ggplot2 library, be sure to -->
<!-- # install it by typing `install.packages("ggplot2")` -->
<!-- # into the Console. -->
<!-- library(ggplot2) -->

<!-- # Basic histogram -->
<!-- ggplot(iris, aes(x=Petal.Width)) + geom_histogram() -->

<!-- # Custom Bin Width -->
<!-- ggplot(iris, aes(x=Petal.Width)) + geom_histogram(binwidth = 0.05) -->

<!-- # Custom color -->
<!-- ggplot(iris, aes(x=Petal.Width)) +  -->
<!--     geom_histogram(binwidth = 0.2, color="white", -->
<!--                    fill=rgb(0.2,0.7,0.1,0.4))  -->
<!-- ``` -->

<!-- ### 4.1.3 Boxplot (Box and Whisker Plot) -->

<!-- Using the `iris` dataset that's pre-installed in R, -->

<!-- ```{r,eval=T} -->
<!-- # If you don't have the ggplot2 library, be sure to -->
<!-- # install it by typing `install.packages("ggplot2")` -->
<!-- # into the Console. -->
<!-- library(ggplot2) -->

<!-- ggplot(iris, aes(x=Species, y=Petal.Width)) +  -->
<!--     geom_boxplot(fill="slateblue", alpha=0.2) -->
<!-- ``` -->

<!-- ## 4.2 Correlation Plots -->

<!-- ### 4.2.1 Scatterplot -->

<!-- Using the `iris` dataset that's pre-installed in R, -->

<!-- ```{r,eval=T} -->
<!-- # If you don't have the ggplot2 library, be sure to -->
<!-- # install it by typing `install.packages("ggplot2")` -->
<!-- # into the Console. -->
<!-- library(ggplot2) -->

<!-- # Basic scatterplot -->
<!-- ggplot(iris, aes(x=Sepal.Length, y=Sepal.Width)) +  -->
<!--     geom_point() -->

<!-- # Basic scatterplot with regression line -->
<!-- ggplot(iris, aes(x=Sepal.Length, y=Sepal.Width)) + -->
<!--     geom_smooth(method = 'lm', formula = y~x) + -->
<!--     geom_point() -->

<!-- # Basic connected scatterplot -->
<!-- ggplot(iris, aes(x=Sepal.Length, y=Sepal.Width)) +  -->
<!--     geom_line() + -->
<!--     geom_point() -->
<!-- ``` -->

<!-- ### 4.2.2 Heatmap -->

<!-- Using the `mtcars` dataset that's pre-installed in R, -->

<!-- ```{r,eval=T} -->
<!-- # converting `mtcars` from a dataframe to a matrix -->
<!-- data <- as.matrix(mtcars)  -->
<!-- # making the heatmap -->
<!-- heatmap(data, Colv = NA, Rowv = NA, scale="column") -->
<!-- ``` -->

<!-- ### 4.2.3 Bubble -->

<!-- Using the `iris` dataset that's pre-installed in R, -->

<!-- ```{r,eval=T} -->
<!-- # If you don't have the ggplot2 library, be sure to -->
<!-- # install it by typing `install.packages("ggplot2")` -->
<!-- # into the Console. -->
<!-- library(ggplot2) -->

<!-- # Bubble Chart -->
<!-- ggplot(iris, aes(x=Sepal.Length, y=Sepal.Width,  -->
<!--                  size = Petal.Width, color = Species)) +  -->
<!--     geom_point() -->
<!-- ``` -->

<!-- ## 4.3 Ranking Plots -->

<!-- ### 4.3.1 Barplot -->

<!-- Using the `mtcars` dataset that's pre-installed in R, -->

<!-- ```{r,eval=T} -->
<!-- # If you don't have the ggplot2 library, be sure to -->
<!-- # install it by typing `install.packages("ggplot2")` -->
<!-- # into the Console. -->
<!-- library(ggplot2) -->

<!-- # Vertical Barplot -->
<!-- ggplot(mtcars, aes(x=as.factor(cyl), fill=as.factor(cyl))) + -->
<!--   geom_bar()  -->

<!-- # Horizontal Barplot -->
<!-- ggplot(mtcars, aes(x=as.factor(cyl), fill=as.factor(cyl))) + -->
<!--   geom_bar() +  -->
<!--   coord_flip() -->

<!-- # Horizontal Barplot with Custom Bar Width -->
<!-- ggplot(mtcars, aes(x=as.factor(cyl), fill=as.factor(cyl))) + -->
<!--   geom_bar(width=0.4) + -->
<!--   coord_flip() -->
<!-- ``` -->

<!-- For stacked and grouped barplots: -->

<!-- ```{r,eval=T} -->
<!-- # Create the data -->
<!-- set.seed(1234) -->
<!-- data=matrix(sample(1:30,15), nrow=3) -->
<!-- colnames(data)=c("A","B","C","D","E") -->
<!-- rownames(data)=c("var1","var2","var3") -->
<!-- head(data) -->

<!-- # Stacked barplot -->
<!-- barplot(data, col=colors()[c(23,89,12)], border="white", -->
<!--         space=0.04, font.axis=2, xlab="group") -->

<!-- # Grouped barplot -->
<!-- barplot(data, col=colors()[c(23,89,12)], border="white", -->
<!--         font.axis=2, beside=T, legend=rownames(data), -->
<!--         xlab="group", font.lab=2) -->
<!-- ``` -->

<!-- ## 4.4 "Part of a Whole" Plots -->

<!-- ### 4.4.1 Treemap -->

<!-- Using the `mtcars` dataset that's pre-installed in R, -->

<!-- ```{r,eval=T} -->
<!-- # If you don't have the ggplot2 or treemapify libraries,  -->
<!-- # be sure to install them by typing  -->
<!-- # `install.packages("ggplot2")` and -->
<!-- # `install.packages("treemapify") -->
<!-- # into the Console. -->
<!-- library(ggplot2) -->
<!-- library(treemapify) -->

<!-- # creating a smaller copy of `mtcars` -->
<!-- cars <- head(mtcars) -->
<!-- # creating a new column (`carname`) in `cars` -->
<!-- cars$carname <- rownames(cars) -->
<!-- # making `cyl` in `cars` a factor -->
<!-- cars$cyl <- as.factor(cars$cyl) -->
<!-- # making the treemap -->
<!-- ggplot(cars, aes(area = disp, fill = cyl, label = carname)) + -->
<!--   geom_treemap() + -->
<!--   geom_treemap_text() -->
<!-- ``` -->

<!-- ### 4.4.2 Pie Chart -->

<!-- ```{r,eval=T} -->
<!-- # Create Data -->
<!-- Prop=c(3,7,9,1,2) -->

<!-- # Default Pie Chart -->
<!-- pie(Prop) -->

<!-- # Pie Chart with Custom Labels -->
<!-- pie(Prop, labels = c("Gr-A","Gr-B","Gr-C","Gr-D","Gr-E")) -->
<!-- ``` -->

<!-- ## 4.5 Evolution/Time Series Plots -->

<!-- ### 4.5.1 Area Chart -->

<!-- ```{r,eval=T} -->
<!-- # If you don't have the ggplot2 library, be sure to -->
<!-- # install it by typing `install.packages("ggplot2")` -->
<!-- # into the Console. -->
<!-- library(ggplot2) -->

<!-- # Create data -->
<!-- data=data.frame(my_x=seq(1,10), my_size=sample(seq(1,20),10)) -->

<!-- # Area Chart -->
<!-- ggplot(data, aes(x=my_x , y=my_size)) +  -->
<!--   geom_area( fill="blue", alpha=.2) +  -->
<!--   geom_line() -->
<!-- ``` -->

<!-- ### 4.5.2 Stacked Area Chart -->

<!-- Using the `iris` dataset that's pre-installed in R, -->

<!-- ```{r,eval=T} -->
<!-- # If you don't have the ggplot2 or RColorBrewer libraries, -->
<!-- # be sure to install them by typing  -->
<!-- # `install.packages("ggplot2")` and -->
<!-- # `install.packages("RColorBrewer") into the Console. -->
<!-- library(ggplot2) -->
<!-- library(RColorBrewer) -->

<!-- # Creating data -->
<!-- set.seed(1234) -->
<!-- Sector <- rep(c("S01","S02","S03","S04", -->
<!--                 "S05","S06","S07"),times=7) -->
<!-- Year <- as.numeric(rep(c("1950","1960","1970", -->
<!--                          "1980","1990","2000","2010"),each=7)) -->
<!-- Value <- runif(49, 10, 100) -->
<!-- data <- data.frame(Sector,Year,Value) -->

<!-- # Stacked Area Chart -->
<!-- ggplot(data, aes(x=Year, y=Value, fill=Sector)) +  -->
<!--     geom_area() -->

<!-- # Stacked Area Chart with Custom Color Palette -->
<!-- ggplot(data, aes(x=Year, y=Value, fill=Sector)) + -->
<!--     geom_area(colour="black", size=.2, alpha=.4) + -->
<!--     scale_fill_brewer(palette="Greens", -->
<!--                       breaks=rev(levels(data$Sector))) -->
<!-- ``` -->

<!-- ## 4.6 Interactive Plots -->

<!-- To make any of the above plots interactive, use the `plotly` package. Here is an example of how to use `plotly`: -->

<!-- ```{r,eval=T,message=F,warning=F} -->
<!-- # If you don't have the ggplot2 or plotly libraries, be sure to -->
<!-- # install them by typing `install.packages("ggplot2")` and  -->
<!-- # `install.packages("plotly")` into the Console. -->
<!-- library(ggplot2) -->
<!-- library(plotly) -->

<!-- # Basic scatterplot with interactivity -->
<!-- graph <- ggplot(iris, aes(x=Sepal.Length, y=Sepal.Width)) + -->
<!--     geom_smooth(method = 'lm', formula = y~x) + -->
<!--     geom_point() -->
<!-- ggplotly(graph) -->
<!-- ``` -->

<!-- ## 4.7 Other Plots -->

<!-- If you can't find your desired plot in this manual, please refer to the [R Graph Gallery](https://www.r-graph-gallery.com/). -->

<!-- # 5. Integrating R and Excel with BERT (Basic Excel R Toolkit) -->

<!-- [BERT](https://bert-toolkit.com/) is a tool for connecting Excel and R on Windows machines. Specifically, BERT is designed to: -->

<!-- 1. Support running R functions from Excel spreadsheet cells. In Excel terms, this is known as writing User-Defined Functions (UDFs) in R. -->

<!-- > All you have to do is write the function. Everything else - loading the function into Excel, managing parameters, and handling type conversion - is done automatically for you. -->

<!-- 2. Control Excel in real time, right from your R code and a convenient BERT console.  -->

<!-- 3. Call R functions from VBA. -->

<!-- ## 5.1 Installing BERT -->

<!-- Since BERT is free and open-source (just like R), you can download it without any assistance from IT. [Go to this download page](https://bert-toolkit.com/download-bert) and follow the instructions given for downloading the software. -->

<!-- ## 5.2 Getting Started with BERT -->

<!-- Refer to [BERT's well-written documentation](https://bert-toolkit.com/bert-quick-start) for help getting started with BERT. -->

<!-- ### 5.2.1 BERT Example Functions -->

<!-- BERT has conveniently written [some example functions](https://bert-toolkit.com/bert-example-functions) for basic data management and analysis tasks. -->

<!-- ### 5.2.2 Excel Scripting Interface in R -->

<!-- You can talk to Excel from R using the Excel scripting interface. Refer to [BERT's well-written documentation](https://bert-toolkit.com/excel-scripting-interface-in-r) for help getting started.  -->

<!-- # Appendix -->

<!-- ## Updating R and RStudio -->
<!-- If the aforementioned packages and functions start to not work after an extended period of time, you may need to update your versions of R, R packages, and RStudio software to the latest versions. -->

<!-- ### Updating R -->
<!-- To update your version of R, first close any R or RStudio windows you have open. -->

<!-- #### Updating R on Windows -->
<!-- Open the R GUI (x64, not i386). This is **not** the same as RStudio. The R GUI program icon should look very similar to this: -->

<!-- ![](RGui.png) -->


<!-- Install the `installr` package into R by typing -->

<!-- ```{r, eval=FALSE} -->
<!-- install.packages("installr") -->
<!-- ```   -->

<!-- into the Console. -->

<!-- After the package is finished installing, call the package by typing  -->

<!-- ```{r, eval=FALSE}  -->
<!-- library(installr) -->
<!-- ```  -->

<!-- into the Console. You can now update your R software and packages to the latest versions by typing  -->

<!-- ```{r, eval=FALSE} -->
<!-- updateR()  -->
<!-- ``` -->

<!-- into the Console. Then R will walk you through a detailed and intuitive process of updating your R software and packages to the latest versions. -->

<!-- #### Updating R on (Mac) OS X -->

<!-- Open RStudio again, and type the following lines of code into the Console: -->

<!-- ```{r, eval=FALSE} -->
<!-- install.packages('devtools') #assuming it isn't already installed -->
<!-- library(devtools) -->
<!-- install_github('andreacirilloac/updateR') -->
<!-- library(updateR) -->
<!-- updateR(admin_password = "os_admin_user_password") -->
<!-- ``` -->

<!-- R will then walk you through a detailed and intuitive process of updating your R software and packages to the latest versions. -->

<!-- #### Updating R on Linux -->

<!-- [This resource](https://stackoverflow.com/questions/10476713/how-to-upgrade-r-in-ubuntu) will walk you through how to update your R software and packages to the latest versions on Linux. -->

<!-- ### Updating R Packages Without Updating R -->

<!-- Updating out-of-date packages that were installed from CRAN (with `install.packages()`) is easy with the `update.packages()` function. Type this function into the RStudio Console. -->

<!-- ```{r, eval=FALSE} -->
<!-- update.packages() -->
<!-- ``` -->

<!-- After entering this function, it will ask you what packages you want to update. To update all packages at once, use `ask = FALSE`. -->

<!-- ```{r, eval=FALSE} -->
<!-- update.packages(ask = FALSE) -->
<!-- ``` -->

<!-- To update packages installed from `devtools::install_github()`, type the following function into your RStudio Console (I would also recommend saving this function in an R Script for later use): -->

<!-- ```{r, eval=FALSE} -->
<!-- update_github_pkgs <- function() { -->
<!--   # check/load necessary packages -->
<!--   # devtools package -->
<!--   if (!("package:devtools" %in% search())) { -->
<!--     tryCatch(require(devtools), error = function(x) {warning(x); cat("Cannot load devtools package \n")}) -->
<!--     on.exit(detach("package:devtools", unload=TRUE)) -->
<!--   } -->

<!--   pkgs <- installed.packages(fields = "RemoteType") -->
<!--   github_pkgs <- pkgs[pkgs[, "RemoteType"] %in% "github", "Package"] -->

<!--   print(github_pkgs) -->
<!--   lapply(github_pkgs, function(pac) { -->
<!--     message("Updating ", pac, " from GitHub...") -->

<!--     repo = packageDescription(pac, fields = "GithubRepo") -->
<!--     username = packageDescription(pac, fields = "GithubUsername") -->

<!--     install_github(repo = paste0(username, "/", repo)) -->
<!--   }) -->
<!-- } -->
<!-- ``` -->

<!-- Then call the function. -->

<!-- ```{r, eval=FALSE} -->
<!-- update_github_pkgs() -->
<!-- ``` -->

<!-- ### Updating RStudio -->
<!-- To update RStudio, open RStudio and go to `Help > Check for Updates` to install the newest version. -->

<!-- ## References -->

<!-- 1. [Excel Functions (Alphabetical)](https://support.office.com/en-us/article/Excel-functions-alphabetical-b3944572-255d-4efb-bb96-c6d90033e188) -->

<!-- 2. [R for Excel Users](https://www.rforexcelusers.com/) -->

<!-- 3. [Burns Statistics](http://www.burns-stat.com/) -->

<!-- 4. [The R Graph Gallery](https://www.r-graph-gallery.com/) -->

<!-- 5. [BERT Documentation](https://bert-toolkit.com/) -->

<!-- ## Contact -->

<!-- If you have any comments or suggestions about this manual, please contact the author, [Alyssa Columbus](mailto:acolumbu@uci.edu). -->


<!-- ### Please consider a donation! -->

<!-- I am a huge advocate of open source software (e.g. R) and open science, so a version of this manual will always be freely available! That said, updating this manual takes a bit of time, so if you like this book and want to see it get even better, consider donating at [paypal.me/AlyssaColumbus](https://paypal.me/AlyssaColumbus). It really means a lot! -->

<!--chapter:end:How-To-Use-R-With-Excel.Rmd-->

---
title: R ile analize başlarken^[Bu bir derlemedir, mümkün mertebe alıntılara referans
  vermeye çalıştım.]
---

```
author: "Derleyen [Serdar Balcı, MD, Pathologist](https://sbalci.github.io/)"
date: "`{r #  format(Sys.Date())`"
output:
  rmdformats::html_clean:
    highlight: kate
  html_notebook:
    fig_caption: yes
    highlight: kate
    number_sections: yes
    theme: flatly
    toc: yes
    toc_depth: 5
    toc_float: yes
  pdf_document:
    toc: yes
    toc_depth: '5'
  html_document: 
    fig_caption: yes
    keep_md: yes
    toc: yes
    toc_depth: 5
    toc_float: yes
```



```{r , echo=TRUE, cache=FALSE}
library(knitr)
library(rmdformats)

## Global options
options(max.print="75")
opts_chunk$set(echo=TRUE,
	             cache=TRUE,
               prompt=FALSE,
               tidy=TRUE,
               comment=NA,
               message=FALSE,
               warning=FALSE)
opts_knit$set(width=75)
```



<!-- Open all links in new tab-->  
<base target="_blank"/>   


<!-- Go to www.addthis.com/dashboard to customize your tools --> <script type="text/javascript" src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5bc36900a405090b">  
</script> 




<!-- [![](http://res.cloudinary.com/dyd911kmh/image/upload/f_auto,q_auto:best/v1530113077/Image_2_vfy48b.png)](https://www.datacamp.com/community/tutorials/data-science-pitfalls) -->


- R generation

https://rss.onlinelibrary.wiley.com/doi/10.1111/j.1740-9713.2018.01169.x


# R yükleme

http://www.youtube.com/watch?v=XcBLEVknqvY

[![What is R?](http://img.youtube.com/vi/XcBLEVknqvY/0.jpg)](http://www.youtube.com/watch?v=XcBLEVknqvY)


## R-project

https://cran.r-project.org/

---

[![](https://ismayc.github.io/talks/ness-infer/img/engine.png)](https://ismayc.github.io/talks/ness-infer/slide_deck.html#6)

---

## RStudio

https://www.rstudio.com/

https://www.rstudio.com/products/rstudio/download/

https://moderndive.com/2-getting-started.html

---

<!-- [![](http://www-users.york.ac.uk/~er13/RStudio%20Anatomy.svg)](https://buzzrbeeline.blog/2018/07/04/rstudio-anatomy/) -->



---

### RStudio eklentileri

- Discover and install useful RStudio addins

https://cran.r-project.org/web/packages/addinslist/README.html

https://rstudio.github.io/rstudioaddins/


```{r eval=FALSE, include=FALSE, echo=TRUE}
# devtools::install_github("rstudio/addinexamples", type = "source")
```


---

## X11

https://www.xquartz.org/

---

## Java OS

https://support.apple.com/kb/dl1572

---


# R zor şeyler için kolay, kolay şeyler için zor


- [R makes easy things hard, and hard things easy](http://r4stats.com/articles/why-r-is-hard-to-learn/)


- Aynı şeyi çok fazla şekilde yapmak mümkün

R Syntax Comparison::CHEAT SHEET

https://www.amelia.mn/Syntax-cheatsheet.pdf



---


# R paketleri


## Neden paketler var

[![](https://ismayc.github.io/talks/ness-infer/img/appstore.png)](https://ismayc.github.io/talks/ness-infer/slide_deck.html#7)

---

<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script><blockquote class="twitter-tweet" data-lang="en"><p lang="en" dir="ltr">I love the <a href="https://twitter.com/hashtag/rstats?src=hash&amp;ref_src=twsrc%5Etfw">#rstats</a> community.<br>Someone is like, &quot;oh hey peeps, I saw a big need for this mundane but difficult task that I infrequently do, so I created a package that will literally scrape the last bits of peanut butter out of the jar for you. It&#39;s called pbplyr.&quot;<br>What a tribe.</p>&mdash; Frank Elavsky ᴰᵃᵗᵃ ᵂᶦᶻᵃʳᵈ (@Frankly_Data) <a href="https://twitter.com/Frankly_Data/status/1014189095294291968?ref_src=twsrc%5Etfw">July 3, 2018</a></blockquote>

---



https://blog.mitchelloharawild.com/blog/user-2018-feature-wall/

---

![](https://blog.mitchelloharawild.com/blog/2018-07-11-user-2018-feature-wall_files/final.jpg)

---

## Paketleri nereden bulabiliriz

- Available CRAN Packages By Name  
https://cran.r-project.org/web/packages/available_packages_by_name.html

- Bioconductor  
https://www.bioconductor.org

- RecommendR  
http://recommendr.info/

- pkgsearch  
CRAN package search  
https://github.com/metacran/pkgsearch

- Awesome R  
https://awesome-r.com/  


## Kendi paket evrenini oluştur

- pkgverse: Build a Meta-Package Universe  
https://cran.r-project.org/web/packages/pkgverse/index.html



---

## R için yardım bulma


```
# ?mean
# ??efetch
# help(merge)
# example(merge)
```



- Vignette

![](figures/vignette.png)

---

- RDocumentation
https://www.rdocumentation.org

- R Package Documentation
https://rdrr.io/

- GitHub

- Stackoverflow

https://stackoverflow.com/

- Google uygun anahtar kelime



<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script><blockquote class="twitter-tweet" data-lang="en"><p lang="en" dir="ltr">How I use <a href="https://twitter.com/hashtag/rstats?src=hash&amp;ref_src=twsrc%5Etfw">#rstats</a> <br>h/t <a href="https://twitter.com/ThePracticalDev?ref_src=twsrc%5Etfw">@ThePracticalDev</a> <a href="https://t.co/erRnTG0Ujr">pic.twitter.com/erRnTG0Ujr</a></p>&mdash; Emily Bovee (@ebovee09) <a href="https://twitter.com/ebovee09/status/1028037594947485696?ref_src=twsrc%5Etfw">August 10, 2018</a></blockquote>


---


![](figures/Google-package-name.png)

---



![](figures/Google-start-with-R.png)

---

- Awesome Cheatsheet
https://github.com/detailyang/awesome-cheatsheet

http://cran.r-project.org/doc/contrib/Baggott-refcard-v2.pdf

https://www.rstudio.com/resources/cheatsheets/


- Awesome R

https://github.com/qinwf/awesome-R#readme

https://awesome-r.com/




- Twitter

https://twitter.com/hashtag/rstats?src=hash


- Reproducible Examples

<blockquote class="twitter-tweet" data-lang="en"><p lang="en" dir="ltr">Got a question to ask on <a href="https://twitter.com/SlackHQ?ref_src=twsrc%5Etfw">@SlackHQ</a> or post on <a href="https://twitter.com/github?ref_src=twsrc%5Etfw">@github</a>? No time to read the long post on how to use reprex? Here is a 20-second gif for you to format your R codes nicely and for others to reproduce your problem. (An example from a talk given by <a href="https://twitter.com/JennyBryan?ref_src=twsrc%5Etfw">@JennyBryan</a>) <a href="https://twitter.com/hashtag/rstat?src=hash&amp;ref_src=twsrc%5Etfw">#rstat</a> <a href="https://t.co/gpuGXpFIsX">pic.twitter.com/gpuGXpFIsX</a></p>&mdash; ZhiYang (@zhiiiyang) <a href="https://twitter.com/zhiiiyang/status/1053006003711569920?ref_src=twsrc%5Etfw">October 18, 2018</a></blockquote><script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>





---

## R paket yükleme

```
install.packages("tidyverse", dependencies = TRUE)
install.packages("jmv", dependencies = TRUE)
install.packages("questionr", dependencies = TRUE)
install.packages("Rcmdr", dependencies = TRUE)
install.packages("summarytools")
```

```{r}
# install.packages("tidyverse", dependencies = TRUE)
# install.packages("jmv", dependencies = TRUE)
# install.packages("questionr", dependencies = TRUE)
# install.packages("Rcmdr", dependencies = TRUE)
# install.packages("summarytools")
```


```{r, error=FALSE, message = FALSE, warning = FALSE, eval = TRUE, include = TRUE}
# require(tidyverse)
# require(jmv)
# require(questionr)
# library(summarytools)
# library(gganimate)
```

---

# R studio ile proje oluşturma

https://support.rstudio.com/hc/en-us/articles/200526207-Using-Projects

![](http://www.rstudio.com/images/docs/projects_new.png)

---

# RStudio ile veri yükleme

https://support.rstudio.com/hc/en-us/articles/218611977-Importing-Data-with-RStudio

![](https://support.rstudio.com/hc/en-us/article_attachments/206277618/data-import-overview.gif)

---

## Excel

## SPSS

## csv


---

# Veriyi görüntüleme

<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script><blockquote class="twitter-tweet" data-lang="en"><p lang="en" dir="ltr">Spreadsheet users using <a href="https://twitter.com/hashtag/rstats?src=hash&amp;ref_src=twsrc%5Etfw">#rstats</a>:  where&#39;s the data?<a href="https://twitter.com/hashtag/rstats?src=hash&amp;ref_src=twsrc%5Etfw">#rstats</a> users using spreadsheets:  where&#39;s the code?</p>&mdash; Leonard Kiefer (@lenkiefer) <a href="https://twitter.com/lenkiefer/status/1015587475580956672?ref_src=twsrc%5Etfw">July 7, 2018</a></blockquote>



```{r, results="markup"}
# library(nycflights13)
# summary(flights)
```



```
View(data)
```


```
data
```


```
head
```


```
tail
```


```
glimpse
```


```
str
```


```
skimr::skim()
```

---


# Veriyi değiştirme

## Veriyi kod ile değiştirelim

## Veriyi eklentilerle değiştirme

![](figures/change_data.png)


---


## RStudio aracılığıyla recode

*questionr* paketi kullanılacak

![](figures/level_recode.png)


---



https://juba.github.io/questionr/articles/recoding_addins.html


![](https://raw.githubusercontent.com/juba/questionr/master/resources/screenshots/irec_1.png)


---

![](https://raw.githubusercontent.com/juba/questionr/master/resources/screenshots/irec_2.png)


---

![](https://raw.githubusercontent.com/juba/questionr/master/resources/screenshots/irec_3.png)


---

# Basit tanımlayıcı istatistikler

```
summary()
```

```
mean
```

```
median
```

```
min
```

```
max
```

```
sd
```

```
table()
```


```{r, echo=TRUE, include = TRUE}
library(readr)
irisdata <- read_csv("data/iris.csv")

jmv::descriptives(
    data = irisdata,
    vars = "Sepal.Length",
    splitBy = "Species",
    freq = TRUE,
    hist = TRUE,
    dens = TRUE,
    bar = TRUE,
    box = TRUE,
    violin = TRUE,
    dot = TRUE,
    mode = TRUE,
    sum = TRUE,
    sd = TRUE,
    variance = TRUE,
    range = TRUE,
    se = TRUE,
    skew = TRUE,
    kurt = TRUE,
    quart = TRUE,
    pcEqGr = TRUE)
```

---

```{r, echo=TRUE, include=FALSE}
# install.packages("scatr")

scatr::scat(
    data = irisdata,
    x = "Sepal.Length",
    y = "Sepal.Width",
    group = "Species",
    marg = "dens",
    line = "linear",
    se = TRUE)

```

## summarytools

https://cran.r-project.org/web/packages/summarytools/vignettes/Introduction.html

```{r, include=FALSE, comment=NA, prompt=FALSE, cache=FALSE, echo=TRUE, results='asis'}
library(summarytools)
summarytools::freq(iris$Species, style = "rmarkdown")
```

```{r, include=FALSE, comment=NA, prompt=FALSE, cache=FALSE, echo=TRUE, results='asis'}
summarytools::freq(iris$Species, report.nas = FALSE, style = "rmarkdown", headings = TRUE)
```

```{r, include=FALSE, comment=NA, prompt=FALSE, cache=FALSE, echo=TRUE, results='asis'}
with(tobacco, print(ctable(smoker, diseased), method = 'render'))
```


```{r, include=FALSE, comment=NA, prompt=FALSE, cache=FALSE, echo=TRUE, results='asis'}
with(tobacco, 
     print(ctable(smoker, diseased, prop = 'n', totals = FALSE), 
           headings = TRUE, method = "render"))
```



```{r, include=FALSE, comment=NA, prompt=FALSE, cache=FALSE, echo=TRUE, results='asis'}
summarytools::descr(iris, style = "rmarkdown")
```



```{r, include=FALSE, comment=NA, prompt=FALSE, cache=FALSE, echo=TRUE, results='asis'}
descr(iris, stats = c("mean", "sd", "min", "med", "max"), transpose = TRUE, 
      headings = TRUE, style = "rmarkdown")
```



```{r, include=FALSE, comment=NA, prompt=FALSE, cache=FALSE, echo=TRUE, results='asis'}
# view(dfSummary(iris))

```


![](figures/dfsummary.png)



```{r, include=FALSE, comment=NA, prompt=FALSE, cache=FALSE, echo=TRUE, results='asis'}
dfSummary(tobacco, plain.ascii = FALSE, style = "grid")
```


```{r, include=FALSE, comment=NA, prompt=FALSE, cache=FALSE, echo=TRUE, results='asis'}

# First save the results

iris_stats_by_species <- by(data = iris, 
                            INDICES = iris$Species, 
                            FUN = descr, stats = c("mean", "sd", "min", "med", "max"), 
                            transpose = TRUE)

# Then use view(), like so:

view(iris_stats_by_species, method = "pander", style = "rmarkdown")
```

```{r, include=FALSE, comment=NA, prompt=FALSE, cache=FALSE, echo=TRUE, results='asis'}
# view(iris_stats_by_species)
```

![](figures/DescriptiveStatistics.png)

---

```{r, include=FALSE, comment=NA, prompt=FALSE, cache=FALSE, echo=TRUE, results='asis'}
data(tobacco) # tobacco is an example dataframe included in the package
BMI_by_age <- with(tobacco, 
                   by(BMI, age.gr, descr, 
                      stats = c("mean", "sd", "min", "med", "max")))
view(BMI_by_age, "pander", style = "rmarkdown")
```

---

```{r, include=FALSE, comment=NA, prompt=FALSE, cache=FALSE, echo=TRUE, results='asis'}
BMI_by_age <- with(tobacco, 
                   by(BMI, age.gr, descr,  transpose = TRUE,
                      stats = c("mean", "sd", "min", "med", "max")))

view(BMI_by_age, "pander", style = "rmarkdown", headings = TRUE)
```

---

```{r, include=FALSE, comment=NA, prompt=FALSE, cache=FALSE, echo=TRUE, results='asis'}
tobacco_subset <- tobacco[ ,c("gender", "age.gr", "smoker")]
freq_tables <- lapply(tobacco_subset, freq)

# view(freq_tables, footnote = NA, file = 'freq-tables.html')
```

---

```{r, include=FALSE, comment=NA, prompt=FALSE, cache=FALSE, echo=TRUE, results='asis'}
what.is(iris)
```

---

```{r eval=FALSE, include=FALSE, echo=TRUE}
freq(tobacco$gender, style = 'rmarkdown')
```

---

```{r eval=FALSE, include=FALSE, echo=TRUE}
print(freq(tobacco$gender), method = 'render')
```

---

## skimr

```
library(skimr)
skim(df)
```

---

## DataExplorer

```
library(DataExplorer)
DataExplorer::create_report(df)
```


[![](https://static1.squarespace.com/static/58eef8846a4963e429687a4d/t/5bdfc2fb4d7a9c04ee50b7aa/1541391160702/dataExplorerGifLg.gif?format=1500w)](https://www.littlemissdata.com/blog/simple-eda)



---

## Grafikler

```{r eval=FALSE, include=FALSE, echo=TRUE}
# library(ggplot2)
# library(mosaic)
# mPlot(irisdata)
```

---

```{r eval=FALSE, include=FALSE, echo=TRUE}
ctable(tobacco$gender, tobacco$smoker, style = 'rmarkdown')
```

---

```{r eval=FALSE, include=FALSE, echo=TRUE}
print(ctable(tobacco$gender, tobacco$smoker), method = 'render')
```

```
descr(tobacco, style = 'rmarkdown')

print(descr(tobacco), method = 'render', table.classes = 'st-small')

dfSummary(tobacco, style = 'grid', plain.ascii = FALSE)

print(dfSummary(tobacco, graph.magnif = 0.75), method = 'render')
```


---



<blockquote class="twitter-tweet" data-lang="en"><p lang="en" dir="ltr">Here, building up a <a href="https://twitter.com/hashtag/ggplot2?src=hash&amp;ref_src=twsrc%5Etfw">#ggplot2</a> as slowly as possible, <a href="https://twitter.com/hashtag/rstats?src=hash&amp;ref_src=twsrc%5Etfw">#rstats</a>.  Incremental adjustments.  <a href="https://twitter.com/hashtag/rstatsteachingideas?src=hash&amp;ref_src=twsrc%5Etfw">#rstatsteachingideas</a> <a href="https://t.co/nUulQl8bPh">pic.twitter.com/nUulQl8bPh</a></p>&mdash; Gina Reynolds (@EvaMaeRey) <a href="https://twitter.com/EvaMaeRey/status/1029104656763572226?ref_src=twsrc%5Etfw">August 13, 2018</a></blockquote><script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>


---


[![](https://raw.githubusercontent.com/dreamRs/esquisse/master/man/figures/esquisse.gif)](https://github.com/dreamRs/esquisse)


<blockquote class="twitter-tweet" data-lang="en"><p lang="en" dir="ltr">Dreaming of a fancy <a href="https://twitter.com/hashtag/Rstats?src=hash&amp;ref_src=twsrc%5Etfw">#Rstats</a> <a href="https://twitter.com/hashtag/ggplot?src=hash&amp;ref_src=twsrc%5Etfw">#ggplot</a> <a href="https://twitter.com/hashtag/dataviz?src=hash&amp;ref_src=twsrc%5Etfw">#dataviz</a> but still scared of typing <a href="https://twitter.com/hashtag/code?src=hash&amp;ref_src=twsrc%5Etfw">#code</a>? <a href="https://twitter.com/_pvictorr?ref_src=twsrc%5Etfw">@_pvictorr</a> esquisse package has you covered <a href="https://t.co/1vIDXcVAAF">https://t.co/1vIDXcVAAF</a> <a href="https://t.co/RlTkptnrNv">pic.twitter.com/RlTkptnrNv</a></p>&mdash; Radoslaw Panczak (@RPanczak) <a href="https://twitter.com/RPanczak/status/1047019588658040832?ref_src=twsrc%5Etfw">October 2, 2018</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>










---

# Rcmdr

```
library(Rcmdr)

Rcmdr::Commander()

```


- A Comparative Review of the R Commander GUI for R

http://r4stats.com/articles/software-reviews/r-commander/


---

# jamovi

https://www.jamovi.org/

![![](https://www.jamovi.org/assets/main-screenshot.png)](https://www.jamovi.org/)


https://blog.jamovi.org/2018/07/30/rj.html

![![](https://blog.jamovi.org/assets/images/rj.png)](https://blog.jamovi.org/2018/07/30/rj.html)

---

# Sonraki Konular

- RStudio ile GitHub
- Hipotez testleri
- R Markdown ve R Notebook ile tekrarlanabilir rapor


---

# Diğer kodlar

- Diğer kodlar için bakınız: [https://sbalci.github.io/](https://sbalci.github.io/)


---

# Geri Bildirim

- Geri bildirim için tıklayınız: _[Geri bildirim formu](https://goo.gl/forms/YjGZ5DHgtPlR1RnB3)_


---

<script id="dsq-count-scr" src="//https-sbalci-github-io.disqus.com/count.js" async></script>

<div id="disqus_thread"></div>
<script>

/**
*  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
*  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/
/*
var disqus_config = function () {
this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};
*/
(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');
s.src = 'https://https-sbalci-github-io.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>

---






<!--chapter:end:htmlclean.Rmd-->

---
title: R ile analize başlarken^[Bu bir derlemedir, mümkün mertebe alıntılara referans
  vermeye çalıştım.]
---

```
author: "Derleyen [Serdar Balcı, MD, Pathologist](https://sbalci.github.io/)"
date: "`{r #  format(Sys.Date())`"
output:
  rmdformats::html_clean:
    highlight: kate
  html_notebook:
    fig_caption: yes
    highlight: kate
    number_sections: yes
    theme: flatly
    toc: yes
    toc_depth: 5
    toc_float: yes
  pdf_document:
    toc: yes
    toc_depth: '5'
  html_document: 
    fig_caption: yes
    keep_md: yes
    toc: yes
    toc_depth: 5
    toc_float: yes
```



```{r , eval=FALSE, cache=FALSE, include=FALSE}
library(knitr)
library(rmdformats)

## Global options
options(max.print="75")
opts_chunk$set(echo=TRUE,
	             cache=TRUE,
               prompt=FALSE,
               tidy=TRUE,
               comment=NA,
               message=FALSE,
               warning=FALSE)
opts_knit$set(width=75)
```



<!-- Open all links in new tab-->  
<base target="_blank"/>   


<!-- Go to www.addthis.com/dashboard to customize your tools --> <script type="text/javascript" src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5bc36900a405090b">  
</script> 




<!-- [![](http://res.cloudinary.com/dyd911kmh/image/upload/f_auto,q_auto:best/v1530113077/Image_2_vfy48b.png)](https://www.datacamp.com/community/tutorials/data-science-pitfalls) -->


- R generation

https://rss.onlinelibrary.wiley.com/doi/10.1111/j.1740-9713.2018.01169.x


# R yükleme

http://www.youtube.com/watch?v=XcBLEVknqvY

[![What is R?](http://img.youtube.com/vi/XcBLEVknqvY/0.jpg)](http://www.youtube.com/watch?v=XcBLEVknqvY)


## R-project

https://cran.r-project.org/

---

[![](https://ismayc.github.io/talks/ness-infer/img/engine.png)](https://ismayc.github.io/talks/ness-infer/slide_deck.html#6)

---

## RStudio

https://www.rstudio.com/

https://www.rstudio.com/products/rstudio/download/

https://moderndive.com/2-getting-started.html

---

<!-- [![](http://www-users.york.ac.uk/~er13/RStudio%20Anatomy.svg)](https://buzzrbeeline.blog/2018/07/04/rstudio-anatomy/) -->



---

### RStudio eklentileri

- Discover and install useful RStudio addins

https://cran.r-project.org/web/packages/addinslist/README.html

https://rstudio.github.io/rstudioaddins/


```{r eval=FALSE, include=FALSE, echo=TRUE}
# devtools::install_github("rstudio/addinexamples", type = "source")
```


---

## X11

https://www.xquartz.org/

---

## Java OS

https://support.apple.com/kb/dl1572

---


# R zor şeyler için kolay, kolay şeyler için zor


- [R makes easy things hard, and hard things easy](http://r4stats.com/articles/why-r-is-hard-to-learn/)


- Aynı şeyi çok fazla şekilde yapmak mümkün

R Syntax Comparison::CHEAT SHEET

https://www.amelia.mn/Syntax-cheatsheet.pdf



---


# R paketleri


## Neden paketler var

[![](https://ismayc.github.io/talks/ness-infer/img/appstore.png)](https://ismayc.github.io/talks/ness-infer/slide_deck.html#7)

---

<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script><blockquote class="twitter-tweet" data-lang="en"><p lang="en" dir="ltr">I love the <a href="https://twitter.com/hashtag/rstats?src=hash&amp;ref_src=twsrc%5Etfw">#rstats</a> community.<br>Someone is like, &quot;oh hey peeps, I saw a big need for this mundane but difficult task that I infrequently do, so I created a package that will literally scrape the last bits of peanut butter out of the jar for you. It&#39;s called pbplyr.&quot;<br>What a tribe.</p>&mdash; Frank Elavsky ᴰᵃᵗᵃ ᵂᶦᶻᵃʳᵈ (@Frankly_Data) <a href="https://twitter.com/Frankly_Data/status/1014189095294291968?ref_src=twsrc%5Etfw">July 3, 2018</a></blockquote>

---



https://blog.mitchelloharawild.com/blog/user-2018-feature-wall/

---

![](https://blog.mitchelloharawild.com/blog/2018-07-11-user-2018-feature-wall_files/final.jpg)

---

## Paketleri nereden bulabiliriz

- Available CRAN Packages By Name  
https://cran.r-project.org/web/packages/available_packages_by_name.html

- Bioconductor  
https://www.bioconductor.org

- RecommendR  
http://recommendr.info/

- pkgsearch  
CRAN package search  
https://github.com/metacran/pkgsearch

- Awesome R  
https://awesome-r.com/  


## Kendi paket evrenini oluştur

- pkgverse: Build a Meta-Package Universe  
https://cran.r-project.org/web/packages/pkgverse/index.html



---

## R için yardım bulma


```
# ?mean
# ??efetch
# help(merge)
# example(merge)
```



- Vignette

![](figures/vignette.png)

---

- RDocumentation
https://www.rdocumentation.org

- R Package Documentation
https://rdrr.io/

- GitHub

- Stackoverflow

https://stackoverflow.com/

- Google uygun anahtar kelime



<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script><blockquote class="twitter-tweet" data-lang="en"><p lang="en" dir="ltr">How I use <a href="https://twitter.com/hashtag/rstats?src=hash&amp;ref_src=twsrc%5Etfw">#rstats</a> <br>h/t <a href="https://twitter.com/ThePracticalDev?ref_src=twsrc%5Etfw">@ThePracticalDev</a> <a href="https://t.co/erRnTG0Ujr">pic.twitter.com/erRnTG0Ujr</a></p>&mdash; Emily Bovee (@ebovee09) <a href="https://twitter.com/ebovee09/status/1028037594947485696?ref_src=twsrc%5Etfw">August 10, 2018</a></blockquote>


---


![](figures/Google-package-name.png)

---



![](figures/Google-start-with-R.png)

---

- Awesome Cheatsheet
https://github.com/detailyang/awesome-cheatsheet

http://cran.r-project.org/doc/contrib/Baggott-refcard-v2.pdf

https://www.rstudio.com/resources/cheatsheets/


- Awesome R

https://github.com/qinwf/awesome-R#readme

https://awesome-r.com/




- Twitter

https://twitter.com/hashtag/rstats?src=hash


- Reproducible Examples

<blockquote class="twitter-tweet" data-lang="en"><p lang="en" dir="ltr">Got a question to ask on <a href="https://twitter.com/SlackHQ?ref_src=twsrc%5Etfw">@SlackHQ</a> or post on <a href="https://twitter.com/github?ref_src=twsrc%5Etfw">@github</a>? No time to read the long post on how to use reprex? Here is a 20-second gif for you to format your R codes nicely and for others to reproduce your problem. (An example from a talk given by <a href="https://twitter.com/JennyBryan?ref_src=twsrc%5Etfw">@JennyBryan</a>) <a href="https://twitter.com/hashtag/rstat?src=hash&amp;ref_src=twsrc%5Etfw">#rstat</a> <a href="https://t.co/gpuGXpFIsX">pic.twitter.com/gpuGXpFIsX</a></p>&mdash; ZhiYang (@zhiiiyang) <a href="https://twitter.com/zhiiiyang/status/1053006003711569920?ref_src=twsrc%5Etfw">October 18, 2018</a></blockquote><script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>





---

## R paket yükleme

```
install.packages("tidyverse", dependencies = TRUE)
install.packages("jmv", dependencies = TRUE)
install.packages("questionr", dependencies = TRUE)
install.packages("Rcmdr", dependencies = TRUE)
install.packages("summarytools")
```

```{r}
# install.packages("tidyverse", dependencies = TRUE)
# install.packages("jmv", dependencies = TRUE)
# install.packages("questionr", dependencies = TRUE)
# install.packages("Rcmdr", dependencies = TRUE)
# install.packages("summarytools")
```


```{r, error=FALSE, message = FALSE, warning = FALSE, eval = TRUE, include = TRUE}
# require(tidyverse)
# require(jmv)
# require(questionr)
# library(summarytools)
# library(gganimate)
```

---

# R studio ile proje oluşturma

https://support.rstudio.com/hc/en-us/articles/200526207-Using-Projects

![](http://www.rstudio.com/images/docs/projects_new.png)

---

# RStudio ile veri yükleme

https://support.rstudio.com/hc/en-us/articles/218611977-Importing-Data-with-RStudio

![](https://support.rstudio.com/hc/en-us/article_attachments/206277618/data-import-overview.gif)

---

## Excel

## SPSS

## csv


---

# Veriyi görüntüleme

<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script><blockquote class="twitter-tweet" data-lang="en"><p lang="en" dir="ltr">Spreadsheet users using <a href="https://twitter.com/hashtag/rstats?src=hash&amp;ref_src=twsrc%5Etfw">#rstats</a>:  where&#39;s the data?<a href="https://twitter.com/hashtag/rstats?src=hash&amp;ref_src=twsrc%5Etfw">#rstats</a> users using spreadsheets:  where&#39;s the code?</p>&mdash; Leonard Kiefer (@lenkiefer) <a href="https://twitter.com/lenkiefer/status/1015587475580956672?ref_src=twsrc%5Etfw">July 7, 2018</a></blockquote>



```{r, results="markup"}
# library(nycflights13)
# summary(flights)
```



```
View(data)
```


```
data
```


```
head
```


```
tail
```


```
glimpse
```


```
str
```


```
skimr::skim()
```

---


# Veriyi değiştirme

## Veriyi kod ile değiştirelim

## Veriyi eklentilerle değiştirme

![](figures/change_data.png)


---


## RStudio aracılığıyla recode

*questionr* paketi kullanılacak

![](figures/level_recode.png)


---



https://juba.github.io/questionr/articles/recoding_addins.html


![](https://raw.githubusercontent.com/juba/questionr/master/resources/screenshots/irec_1.png)


---

![](https://raw.githubusercontent.com/juba/questionr/master/resources/screenshots/irec_2.png)


---

![](https://raw.githubusercontent.com/juba/questionr/master/resources/screenshots/irec_3.png)


---

# Basit tanımlayıcı istatistikler

```
summary()
```

```
mean
```

```
median
```

```
min
```

```
max
```

```
sd
```

```
table()
```


```{r, eval=FALSE, include=FALSE}
library(readr)
irisdata <- read_csv("data/iris.csv")

jmv::descriptives(
    data = irisdata,
    vars = "Sepal.Length",
    splitBy = "Species",
    freq = TRUE,
    hist = TRUE,
    dens = TRUE,
    bar = TRUE,
    box = TRUE,
    violin = TRUE,
    dot = TRUE,
    mode = TRUE,
    sum = TRUE,
    sd = TRUE,
    variance = TRUE,
    range = TRUE,
    se = TRUE,
    skew = TRUE,
    kurt = TRUE,
    quart = TRUE,
    pcEqGr = TRUE)
```

---

```{r, echo=TRUE, include=FALSE}
# install.packages("scatr")

scatr::scat(
    data = irisdata,
    x = "Sepal.Length",
    y = "Sepal.Width",
    group = "Species",
    marg = "dens",
    line = "linear",
    se = TRUE)

```

## summarytools

https://cran.r-project.org/web/packages/summarytools/vignettes/Introduction.html

```{r eval=FALSE, include=FALSE, comment=NA, prompt=FALSE, cache=FALSE, echo=TRUE, results='asis'}
library(summarytools)
summarytools::freq(iris$Species, style = "rmarkdown")
```

```{r, include=FALSE, comment=NA, prompt=FALSE, cache=FALSE, echo=TRUE, results='asis'}
summarytools::freq(iris$Species, report.nas = FALSE, style = "rmarkdown", headings = TRUE)
```

```{r, include=FALSE, comment=NA, prompt=FALSE, cache=FALSE, echo=TRUE, results='asis'}
with(tobacco, print(ctable(smoker, diseased), method = 'render'))
```


```{r, include=FALSE, comment=NA, prompt=FALSE, cache=FALSE, echo=TRUE, results='asis'}
with(tobacco, 
     print(ctable(smoker, diseased, prop = 'n', totals = FALSE), 
           headings = TRUE, method = "render"))
```



```{r, include=FALSE, comment=NA, prompt=FALSE, cache=FALSE, echo=TRUE, results='asis'}
summarytools::descr(iris, style = "rmarkdown")
```



```{r, include=FALSE, comment=NA, prompt=FALSE, cache=FALSE, echo=TRUE, results='asis'}
descr(iris, stats = c("mean", "sd", "min", "med", "max"), transpose = TRUE, 
      headings = TRUE, style = "rmarkdown")
```



```{r, include=FALSE, comment=NA, prompt=FALSE, cache=FALSE, echo=TRUE, results='asis'}
# view(dfSummary(iris))

```


![](figures/dfsummary.png)



```{r, include=FALSE, comment=NA, prompt=FALSE, cache=FALSE, echo=TRUE, results='asis'}
dfSummary(tobacco, plain.ascii = FALSE, style = "grid")
```


```{r, include=FALSE, comment=NA, prompt=FALSE, cache=FALSE, echo=TRUE, results='asis'}

# First save the results

iris_stats_by_species <- by(data = iris, 
                            INDICES = iris$Species, 
                            FUN = descr, stats = c("mean", "sd", "min", "med", "max"), 
                            transpose = TRUE)

# Then use view(), like so:

view(iris_stats_by_species, method = "pander", style = "rmarkdown")
```

```{r, include=FALSE, comment=NA, prompt=FALSE, cache=FALSE, echo=TRUE, results='asis'}
# view(iris_stats_by_species)
```

![](figures/DescriptiveStatistics.png)

---

```{r, include=FALSE, comment=NA, prompt=FALSE, cache=FALSE, echo=TRUE, results='asis'}
data(tobacco) # tobacco is an example dataframe included in the package
BMI_by_age <- with(tobacco, 
                   by(BMI, age.gr, descr, 
                      stats = c("mean", "sd", "min", "med", "max")))
view(BMI_by_age, "pander", style = "rmarkdown")
```

---

```{r, include=FALSE, comment=NA, prompt=FALSE, cache=FALSE, echo=TRUE, results='asis'}
BMI_by_age <- with(tobacco, 
                   by(BMI, age.gr, descr,  transpose = TRUE,
                      stats = c("mean", "sd", "min", "med", "max")))

view(BMI_by_age, "pander", style = "rmarkdown", headings = TRUE)
```

---

```{r, include=FALSE, comment=NA, prompt=FALSE, cache=FALSE, echo=TRUE, results='asis'}
tobacco_subset <- tobacco[ ,c("gender", "age.gr", "smoker")]
freq_tables <- lapply(tobacco_subset, freq)

# view(freq_tables, footnote = NA, file = 'freq-tables.html')
```

---

```{r, include=FALSE, comment=NA, prompt=FALSE, cache=FALSE, echo=TRUE, results='asis'}
what.is(iris)
```

---

```{r eval=FALSE, include=FALSE, echo=TRUE}
freq(tobacco$gender, style = 'rmarkdown')
```

---

```{r eval=FALSE, include=FALSE, echo=TRUE}
print(freq(tobacco$gender), method = 'render')
```

---

## skimr

```
library(skimr)
skim(df)
```

---

## DataExplorer

```
library(DataExplorer)
DataExplorer::create_report(df)
```


[![](https://static1.squarespace.com/static/58eef8846a4963e429687a4d/t/5bdfc2fb4d7a9c04ee50b7aa/1541391160702/dataExplorerGifLg.gif?format=1500w)](https://www.littlemissdata.com/blog/simple-eda)



---

## Grafikler

```{r eval=FALSE, include=FALSE, echo=TRUE}
# library(ggplot2)
# library(mosaic)
# mPlot(irisdata)
```

---

```{r eval=FALSE, include=FALSE, echo=TRUE}
ctable(tobacco$gender, tobacco$smoker, style = 'rmarkdown')
```

---

```{r eval=FALSE, include=FALSE, echo=TRUE}
print(ctable(tobacco$gender, tobacco$smoker), method = 'render')
```

```
descr(tobacco, style = 'rmarkdown')

print(descr(tobacco), method = 'render', table.classes = 'st-small')

dfSummary(tobacco, style = 'grid', plain.ascii = FALSE)

print(dfSummary(tobacco, graph.magnif = 0.75), method = 'render')
```


---



<blockquote class="twitter-tweet" data-lang="en"><p lang="en" dir="ltr">Here, building up a <a href="https://twitter.com/hashtag/ggplot2?src=hash&amp;ref_src=twsrc%5Etfw">#ggplot2</a> as slowly as possible, <a href="https://twitter.com/hashtag/rstats?src=hash&amp;ref_src=twsrc%5Etfw">#rstats</a>.  Incremental adjustments.  <a href="https://twitter.com/hashtag/rstatsteachingideas?src=hash&amp;ref_src=twsrc%5Etfw">#rstatsteachingideas</a> <a href="https://t.co/nUulQl8bPh">pic.twitter.com/nUulQl8bPh</a></p>&mdash; Gina Reynolds (@EvaMaeRey) <a href="https://twitter.com/EvaMaeRey/status/1029104656763572226?ref_src=twsrc%5Etfw">August 13, 2018</a></blockquote><script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>


---


[![](https://raw.githubusercontent.com/dreamRs/esquisse/master/man/figures/esquisse.gif)](https://github.com/dreamRs/esquisse)


<blockquote class="twitter-tweet" data-lang="en"><p lang="en" dir="ltr">Dreaming of a fancy <a href="https://twitter.com/hashtag/Rstats?src=hash&amp;ref_src=twsrc%5Etfw">#Rstats</a> <a href="https://twitter.com/hashtag/ggplot?src=hash&amp;ref_src=twsrc%5Etfw">#ggplot</a> <a href="https://twitter.com/hashtag/dataviz?src=hash&amp;ref_src=twsrc%5Etfw">#dataviz</a> but still scared of typing <a href="https://twitter.com/hashtag/code?src=hash&amp;ref_src=twsrc%5Etfw">#code</a>? <a href="https://twitter.com/_pvictorr?ref_src=twsrc%5Etfw">@_pvictorr</a> esquisse package has you covered <a href="https://t.co/1vIDXcVAAF">https://t.co/1vIDXcVAAF</a> <a href="https://t.co/RlTkptnrNv">pic.twitter.com/RlTkptnrNv</a></p>&mdash; Radoslaw Panczak (@RPanczak) <a href="https://twitter.com/RPanczak/status/1047019588658040832?ref_src=twsrc%5Etfw">October 2, 2018</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>










---

# Rcmdr

```
library(Rcmdr)

Rcmdr::Commander()

```


- A Comparative Review of the R Commander GUI for R

http://r4stats.com/articles/software-reviews/r-commander/


---

# jamovi

https://www.jamovi.org/

![![](https://www.jamovi.org/assets/main-screenshot.png)](https://www.jamovi.org/)


https://blog.jamovi.org/2018/07/30/rj.html

![![](https://blog.jamovi.org/assets/images/rj.png)](https://blog.jamovi.org/2018/07/30/rj.html)

---

# Sonraki Konular

- RStudio ile GitHub
- Hipotez testleri
- R Markdown ve R Notebook ile tekrarlanabilir rapor


---

# Diğer kodlar

- Diğer kodlar için bakınız: [https://sbalci.github.io/](https://sbalci.github.io/)


---

# Geri Bildirim

- Geri bildirim için tıklayınız: _[Geri bildirim formu](https://goo.gl/forms/YjGZ5DHgtPlR1RnB3)_


---

<script id="dsq-count-scr" src="//https-sbalci-github-io.disqus.com/count.js" async></script>

<div id="disqus_thread"></div>
<script>

/**
*  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
*  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/
/*
var disqus_config = function () {
this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};
*/
(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');
s.src = 'https://https-sbalci-github-io.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>

---






<!--chapter:end:htmlclean2.Rmd-->

---
title: R ile analize başlarken^[Bu bir derlemedir, mümkün mertebe alıntılara referans
  vermeye çalıştım.]
author: "Derleyen [Serdar Balcı, MD, Pathologist](https://sbalci.github.io/)"
date: "`{r #  format(Sys.Date())`"
output:
  rmdformats::html_docco:
    highlight: kate
  html_notebook:
    fig_caption: yes
    highlight: kate
    number_sections: yes
    theme: flatly
    toc: yes
    toc_depth: 5
    toc_float: yes
  pdf_document:
    toc: yes
    toc_depth: '5'
  html_document: 
    fig_caption: yes
    keep_md: yes
    toc: yes
    toc_depth: 5
    toc_float: yes
---


```{r , echo=TRUE, cache=FALSE}
library(knitr)
library(rmdformats)

## Global options
options(max.print="75")
opts_chunk$set(echo=TRUE,
               cache=TRUE,
               prompt=FALSE,
               tidy=TRUE,
               comment=NA,
               message=FALSE,
               warning=FALSE)
opts_knit$set(width=75)
```


<!-- Open all links in new tab-->  
<base target="_blank"/>   


<!-- Go to www.addthis.com/dashboard to customize your tools --> <script type="text/javascript" src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5bc36900a405090b">  
</script> 




[![](http://res.cloudinary.com/dyd911kmh/image/upload/f_auto,q_auto:best/v1530113077/Image_2_vfy48b.png)](https://www.datacamp.com/community/tutorials/data-science-pitfalls)


- R generation

https://rss.onlinelibrary.wiley.com/doi/10.1111/j.1740-9713.2018.01169.x


# R yükleme

http://www.youtube.com/watch?v=XcBLEVknqvY

[![What is R?](http://img.youtube.com/vi/XcBLEVknqvY/0.jpg)](http://www.youtube.com/watch?v=XcBLEVknqvY)


## R-project

https://cran.r-project.org/

---

[![](https://ismayc.github.io/talks/ness-infer/img/engine.png)](https://ismayc.github.io/talks/ness-infer/slide_deck.html#6)

---

## RStudio

https://www.rstudio.com/

https://www.rstudio.com/products/rstudio/download/

https://moderndive.com/2-getting-started.html

---

[![](http://www-users.york.ac.uk/~er13/RStudio%20Anatomy.svg)](https://buzzrbeeline.blog/2018/07/04/rstudio-anatomy/)



---

### RStudio eklentileri

- Discover and install useful RStudio addins

https://cran.r-project.org/web/packages/addinslist/README.html

https://rstudio.github.io/rstudioaddins/


```{r eval=FALSE, include=FALSE, echo=TRUE}
# devtools::install_github("rstudio/addinexamples", type = "source")
```


---

## X11

https://www.xquartz.org/

---

## Java OS

https://support.apple.com/kb/dl1572

---


# R zor şeyler için kolay, kolay şeyler için zor


- [R makes easy things hard, and hard things easy](http://r4stats.com/articles/why-r-is-hard-to-learn/)


- Aynı şeyi çok fazla şekilde yapmak mümkün

R Syntax Comparison::CHEAT SHEET

https://www.amelia.mn/Syntax-cheatsheet.pdf



---


# R paketleri


## Neden paketler var

[![](https://ismayc.github.io/talks/ness-infer/img/appstore.png)](https://ismayc.github.io/talks/ness-infer/slide_deck.html#7)

---

<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script><blockquote class="twitter-tweet" data-lang="en"><p lang="en" dir="ltr">I love the <a href="https://twitter.com/hashtag/rstats?src=hash&amp;ref_src=twsrc%5Etfw">#rstats</a> community.<br>Someone is like, &quot;oh hey peeps, I saw a big need for this mundane but difficult task that I infrequently do, so I created a package that will literally scrape the last bits of peanut butter out of the jar for you. It&#39;s called pbplyr.&quot;<br>What a tribe.</p>&mdash; Frank Elavsky ᴰᵃᵗᵃ ᵂᶦᶻᵃʳᵈ (@Frankly_Data) <a href="https://twitter.com/Frankly_Data/status/1014189095294291968?ref_src=twsrc%5Etfw">July 3, 2018</a></blockquote>

---



https://blog.mitchelloharawild.com/blog/user-2018-feature-wall/

---

![](https://blog.mitchelloharawild.com/blog/2018-07-11-user-2018-feature-wall_files/final.jpg)

---

## Paketleri nereden bulabiliriz

- Available CRAN Packages By Name  
https://cran.r-project.org/web/packages/available_packages_by_name.html

- Bioconductor  
https://www.bioconductor.org

- RecommendR  
http://recommendr.info/

- pkgsearch  
CRAN package search  
https://github.com/metacran/pkgsearch

- Awesome R  
https://awesome-r.com/  


## Kendi paket evrenini oluştur

- pkgverse: Build a Meta-Package Universe  
https://cran.r-project.org/web/packages/pkgverse/index.html



---

## R için yardım bulma


```
# ?mean
# ??efetch
# help(merge)
# example(merge)
```



- Vignette

![](figures/vignette.png)

---

- RDocumentation
https://www.rdocumentation.org

- R Package Documentation
https://rdrr.io/

- GitHub

- Stackoverflow

https://stackoverflow.com/

- Google uygun anahtar kelime



<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script><blockquote class="twitter-tweet" data-lang="en"><p lang="en" dir="ltr">How I use <a href="https://twitter.com/hashtag/rstats?src=hash&amp;ref_src=twsrc%5Etfw">#rstats</a> <br>h/t <a href="https://twitter.com/ThePracticalDev?ref_src=twsrc%5Etfw">@ThePracticalDev</a> <a href="https://t.co/erRnTG0Ujr">pic.twitter.com/erRnTG0Ujr</a></p>&mdash; Emily Bovee (@ebovee09) <a href="https://twitter.com/ebovee09/status/1028037594947485696?ref_src=twsrc%5Etfw">August 10, 2018</a></blockquote>


---


![](figures/Google-package-name.png)

---



![](figures/Google-start-with-R.png)

---

- Awesome Cheatsheet
https://github.com/detailyang/awesome-cheatsheet

http://cran.r-project.org/doc/contrib/Baggott-refcard-v2.pdf

https://www.rstudio.com/resources/cheatsheets/


- Awesome R

https://github.com/qinwf/awesome-R#readme

https://awesome-r.com/




- Twitter

https://twitter.com/hashtag/rstats?src=hash


- Reproducible Examples

<blockquote class="twitter-tweet" data-lang="en"><p lang="en" dir="ltr">Got a question to ask on <a href="https://twitter.com/SlackHQ?ref_src=twsrc%5Etfw">@SlackHQ</a> or post on <a href="https://twitter.com/github?ref_src=twsrc%5Etfw">@github</a>? No time to read the long post on how to use reprex? Here is a 20-second gif for you to format your R codes nicely and for others to reproduce your problem. (An example from a talk given by <a href="https://twitter.com/JennyBryan?ref_src=twsrc%5Etfw">@JennyBryan</a>) <a href="https://twitter.com/hashtag/rstat?src=hash&amp;ref_src=twsrc%5Etfw">#rstat</a> <a href="https://t.co/gpuGXpFIsX">pic.twitter.com/gpuGXpFIsX</a></p>&mdash; ZhiYang (@zhiiiyang) <a href="https://twitter.com/zhiiiyang/status/1053006003711569920?ref_src=twsrc%5Etfw">October 18, 2018</a></blockquote><script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>





---

## R paket yükleme

```
install.packages("tidyverse", dependencies = TRUE)
install.packages("jmv", dependencies = TRUE)
install.packages("questionr", dependencies = TRUE)
install.packages("Rcmdr", dependencies = TRUE)
install.packages("summarytools")
```

```{r}
# install.packages("tidyverse", dependencies = TRUE)
# install.packages("jmv", dependencies = TRUE)
# install.packages("questionr", dependencies = TRUE)
# install.packages("Rcmdr", dependencies = TRUE)
# install.packages("summarytools")
```


```{r, error=FALSE, message = FALSE, warning = FALSE, eval = TRUE, include = TRUE}
# require(tidyverse)
# require(jmv)
# require(questionr)
# library(summarytools)
# library(gganimate)
```

---

# R studio ile proje oluşturma

https://support.rstudio.com/hc/en-us/articles/200526207-Using-Projects

![](http://www.rstudio.com/images/docs/projects_new.png)

---

# RStudio ile veri yükleme

https://support.rstudio.com/hc/en-us/articles/218611977-Importing-Data-with-RStudio

![](https://support.rstudio.com/hc/en-us/article_attachments/206277618/data-import-overview.gif)

---

## Excel

## SPSS

## csv


---

# Veriyi görüntüleme

<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script><blockquote class="twitter-tweet" data-lang="en"><p lang="en" dir="ltr">Spreadsheet users using <a href="https://twitter.com/hashtag/rstats?src=hash&amp;ref_src=twsrc%5Etfw">#rstats</a>:  where&#39;s the data?<a href="https://twitter.com/hashtag/rstats?src=hash&amp;ref_src=twsrc%5Etfw">#rstats</a> users using spreadsheets:  where&#39;s the code?</p>&mdash; Leonard Kiefer (@lenkiefer) <a href="https://twitter.com/lenkiefer/status/1015587475580956672?ref_src=twsrc%5Etfw">July 7, 2018</a></blockquote>



```{r, results="markup"}
# library(nycflights13)
# summary(flights)
```



```
View(data)
```


```
data
```


```
head
```


```
tail
```


```
glimpse
```


```
str
```


```
skimr::skim()
```

---


# Veriyi değiştirme

## Veriyi kod ile değiştirelim

## Veriyi eklentilerle değiştirme

![](figures/change_data.png)


---


## RStudio aracılığıyla recode

*questionr* paketi kullanılacak

![](figures/level_recode.png)


---



https://juba.github.io/questionr/articles/recoding_addins.html


![](https://raw.githubusercontent.com/juba/questionr/master/resources/screenshots/irec_1.png)


---

![](https://raw.githubusercontent.com/juba/questionr/master/resources/screenshots/irec_2.png)


---

![](https://raw.githubusercontent.com/juba/questionr/master/resources/screenshots/irec_3.png)


---

# Basit tanımlayıcı istatistikler

```
summary()
```

```
mean
```

```
median
```

```
min
```

```
max
```

```
sd
```

```
table()
```


```{r, echo=TRUE, include = TRUE}
library(readr)
irisdata <- read_csv("data/iris.csv")

jmv::descriptives(
    data = irisdata,
    vars = "Sepal.Length",
    splitBy = "Species",
    freq = TRUE,
    hist = TRUE,
    dens = TRUE,
    bar = TRUE,
    box = TRUE,
    violin = TRUE,
    dot = TRUE,
    mode = TRUE,
    sum = TRUE,
    sd = TRUE,
    variance = TRUE,
    range = TRUE,
    se = TRUE,
    skew = TRUE,
    kurt = TRUE,
    quart = TRUE,
    pcEqGr = TRUE)
```

---

```{r, echo=TRUE, include=FALSE}
# install.packages("scatr")

scatr::scat(
    data = irisdata,
    x = "Sepal.Length",
    y = "Sepal.Width",
    group = "Species",
    marg = "dens",
    line = "linear",
    se = TRUE)

```

## summarytools

https://cran.r-project.org/web/packages/summarytools/vignettes/Introduction.html

```{r, include=FALSE, comment=NA, prompt=FALSE, cache=FALSE, echo=TRUE, results='asis'}
library(summarytools)
summarytools::freq(iris$Species, style = "rmarkdown")
```

```{r, include=FALSE, comment=NA, prompt=FALSE, cache=FALSE, echo=TRUE, results='asis'}
summarytools::freq(iris$Species, report.nas = FALSE, style = "rmarkdown", headings = TRUE)
```

```{r, include=FALSE, comment=NA, prompt=FALSE, cache=FALSE, echo=TRUE, results='asis'}
with(tobacco, print(ctable(smoker, diseased), method = 'render'))
```


```{r, include=FALSE, comment=NA, prompt=FALSE, cache=FALSE, echo=TRUE, results='asis'}
with(tobacco, 
     print(ctable(smoker, diseased, prop = 'n', totals = FALSE), 
           headings = TRUE, method = "render"))
```



```{r, include=FALSE, comment=NA, prompt=FALSE, cache=FALSE, echo=TRUE, results='asis'}
summarytools::descr(iris, style = "rmarkdown")
```



```{r, include=FALSE, comment=NA, prompt=FALSE, cache=FALSE, echo=TRUE, results='asis'}
descr(iris, stats = c("mean", "sd", "min", "med", "max"), transpose = TRUE, 
      headings = TRUE, style = "rmarkdown")
```



```{r, include=FALSE, comment=NA, prompt=FALSE, cache=FALSE, echo=TRUE, results='asis'}
# view(dfSummary(iris))

```


<!-- ![](figures/dfsummary.png) -->



```{r, include=FALSE, comment=NA, prompt=FALSE, cache=FALSE, echo=TRUE, results='asis'}
dfSummary(tobacco, plain.ascii = FALSE, style = "grid")
```


```{r, include=FALSE, comment=NA, prompt=FALSE, cache=FALSE, echo=TRUE, results='asis'}

# First save the results

iris_stats_by_species <- by(data = iris, 
                            INDICES = iris$Species, 
                            FUN = descr, stats = c("mean", "sd", "min", "med", "max"), 
                            transpose = TRUE)

# Then use view(), like so:

view(iris_stats_by_species, method = "pander", style = "rmarkdown")
```

```{r, include=FALSE, comment=NA, prompt=FALSE, cache=FALSE, echo=TRUE, results='asis'}
# view(iris_stats_by_species)
```

![](figures/DescriptiveStatistics.png)

---

```{r, include=FALSE, comment=NA, prompt=FALSE, cache=FALSE, echo=TRUE, results='asis'}
data(tobacco) # tobacco is an example dataframe included in the package
BMI_by_age <- with(tobacco, 
                   by(BMI, age.gr, descr, 
                      stats = c("mean", "sd", "min", "med", "max")))
view(BMI_by_age, "pander", style = "rmarkdown")
```

---

```{r, include=FALSE, comment=NA, prompt=FALSE, cache=FALSE, echo=TRUE, results='asis'}
BMI_by_age <- with(tobacco, 
                   by(BMI, age.gr, descr,  transpose = TRUE,
                      stats = c("mean", "sd", "min", "med", "max")))

view(BMI_by_age, "pander", style = "rmarkdown", headings = TRUE)
```

---

```{r, include=FALSE, comment=NA, prompt=FALSE, cache=FALSE, echo=TRUE, results='asis'}
tobacco_subset <- tobacco[ ,c("gender", "age.gr", "smoker")]
freq_tables <- lapply(tobacco_subset, freq)

# view(freq_tables, footnote = NA, file = 'freq-tables.html')
```

---

```{r, include=FALSE, comment=NA, prompt=FALSE, cache=FALSE, echo=TRUE, results='asis'}
what.is(iris)
```

---

```{r eval=FALSE, include=FALSE, echo=TRUE}
freq(tobacco$gender, style = 'rmarkdown')
```

---

```{r eval=FALSE, include=FALSE, echo=TRUE}
print(freq(tobacco$gender), method = 'render')
```

---

## skimr

```
library(skimr)
skim(df)
```

---

## DataExplorer

```
library(DataExplorer)
DataExplorer::create_report(df)
```


[![](https://static1.squarespace.com/static/58eef8846a4963e429687a4d/t/5bdfc2fb4d7a9c04ee50b7aa/1541391160702/dataExplorerGifLg.gif?format=1500w)](https://www.littlemissdata.com/blog/simple-eda)



---

## Grafikler

```{r eval=FALSE, include=FALSE, echo=TRUE}
# library(ggplot2)
# library(mosaic)
# mPlot(irisdata)
```

---

```{r eval=FALSE, include=FALSE, echo=TRUE}
ctable(tobacco$gender, tobacco$smoker, style = 'rmarkdown')
```

---

```{r eval=FALSE, include=FALSE, echo=TRUE}
print(ctable(tobacco$gender, tobacco$smoker), method = 'render')
```

```
descr(tobacco, style = 'rmarkdown')

print(descr(tobacco), method = 'render', table.classes = 'st-small')

dfSummary(tobacco, style = 'grid', plain.ascii = FALSE)

print(dfSummary(tobacco, graph.magnif = 0.75), method = 'render')
```


---



<blockquote class="twitter-tweet" data-lang="en"><p lang="en" dir="ltr">Here, building up a <a href="https://twitter.com/hashtag/ggplot2?src=hash&amp;ref_src=twsrc%5Etfw">#ggplot2</a> as slowly as possible, <a href="https://twitter.com/hashtag/rstats?src=hash&amp;ref_src=twsrc%5Etfw">#rstats</a>.  Incremental adjustments.  <a href="https://twitter.com/hashtag/rstatsteachingideas?src=hash&amp;ref_src=twsrc%5Etfw">#rstatsteachingideas</a> <a href="https://t.co/nUulQl8bPh">pic.twitter.com/nUulQl8bPh</a></p>&mdash; Gina Reynolds (@EvaMaeRey) <a href="https://twitter.com/EvaMaeRey/status/1029104656763572226?ref_src=twsrc%5Etfw">August 13, 2018</a></blockquote><script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>


---


[![](https://raw.githubusercontent.com/dreamRs/esquisse/master/man/figures/esquisse.gif)](https://github.com/dreamRs/esquisse)


<blockquote class="twitter-tweet" data-lang="en"><p lang="en" dir="ltr">Dreaming of a fancy <a href="https://twitter.com/hashtag/Rstats?src=hash&amp;ref_src=twsrc%5Etfw">#Rstats</a> <a href="https://twitter.com/hashtag/ggplot?src=hash&amp;ref_src=twsrc%5Etfw">#ggplot</a> <a href="https://twitter.com/hashtag/dataviz?src=hash&amp;ref_src=twsrc%5Etfw">#dataviz</a> but still scared of typing <a href="https://twitter.com/hashtag/code?src=hash&amp;ref_src=twsrc%5Etfw">#code</a>? <a href="https://twitter.com/_pvictorr?ref_src=twsrc%5Etfw">@_pvictorr</a> esquisse package has you covered <a href="https://t.co/1vIDXcVAAF">https://t.co/1vIDXcVAAF</a> <a href="https://t.co/RlTkptnrNv">pic.twitter.com/RlTkptnrNv</a></p>&mdash; Radoslaw Panczak (@RPanczak) <a href="https://twitter.com/RPanczak/status/1047019588658040832?ref_src=twsrc%5Etfw">October 2, 2018</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>










---

# Rcmdr

```
library(Rcmdr)

Rcmdr::Commander()

```


- A Comparative Review of the R Commander GUI for R

http://r4stats.com/articles/software-reviews/r-commander/


---

# jamovi

https://www.jamovi.org/

![![](https://www.jamovi.org/assets/main-screenshot.png)](https://www.jamovi.org/)


https://blog.jamovi.org/2018/07/30/rj.html

![![](https://blog.jamovi.org/assets/images/rj.png)](https://blog.jamovi.org/2018/07/30/rj.html)

---

# Sonraki Konular

- RStudio ile GitHub
- Hipotez testleri
- R Markdown ve R Notebook ile tekrarlanabilir rapor


---

# Diğer kodlar

- Diğer kodlar için bakınız: [https://sbalci.github.io/](https://sbalci.github.io/)


---

# Geri Bildirim

- Geri bildirim için tıklayınız: _[Geri bildirim formu](https://goo.gl/forms/YjGZ5DHgtPlR1RnB3)_


---

<script id="dsq-count-scr" src="//https-sbalci-github-io.disqus.com/count.js" async></script>

<div id="disqus_thread"></div>
<script>

/**
*  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
*  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/
/*
var disqus_config = function () {
this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};
*/
(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');
s.src = 'https://https-sbalci-github-io.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>

---



<!--chapter:end:htmldocco.Rmd-->

```{r , echo = TRUE, warning = FALSE, message = FALSE}

library(knitr)
library(dplyr)
library(huxtable)
options(huxtable.knit_print_df = FALSE)

is_latex <- guess_knitr_output_format() == 'latex'
# is_latex <- TRUE
knitr::knit_hooks$set(
  barrier = function(before, options, envir) {
    if (! before && is_latex) knitr::asis_output('\\FloatBarrier')
  }
)

if (is_latex) knitr::opts_chunk$set(barrier = TRUE)

```


```{r, echo = TRUE}
huxtable::hux_logo(latex = is_latex)
```


```{r, eval = FALSE, echo = TRUE}
# PLAN
# Make single document work in notebook format (maybe with minimal changes)
# Installation
# Dplyr examples
# Examples with where and friends
# Different kinds of output
# Cookbook?
# Limitations

```


# Introduction

## About this document

This is the introductory vignette for the R package 'huxtable', version `{r #  packageVersion('huxtable')`. A current
version is available on the web in [HTML](https://hughjonesd.github.io/huxtable/huxtable.html) or 
[PDF](https://hughjonesd.github.io/huxtable/huxtable.pdf) format.

## Huxtable

Huxtable is a package for creating *text tables*. It is powerful, but easy to use. It is meant to be a
replacement for packages like xtable, which is useful but not always very user-friendly. Huxtable's features
include:

* Export to LaTeX, HTML, Word and Markdown
* Easy integration with knitr and rmarkdown documents
* Multirow and multicolumn cells
* Fine-grained control over cell background, spacing, alignment, size and borders
* Control over text font, style, size, colour, alignment, number format and rotation
* Table manipulation using standard R subsetting, or dplyr functions like `filter` and `select`
* Easy conditional formatting based on table contents
* Quick table themes
* Automatic creation of regression output tables with the `huxreg` function

We will cover all of these features below.

## Installation

If you haven't already installed huxtable, you can do so from the R command line:

```{r, eval = FALSE}
install.packages('huxtable')
```


## Getting started

A huxtable is a way of representing a table of text data in R. You already know that R can represent a
table of data in a data frame. For example, if `mydata` is a data frame, then `mydata[1, 2]` represents the
the data in row 1, column 2, and `mydata$start_time` is all the data in the column called `start_time`.

A huxtable is just a data frame with some extra properties. So, if `myhux` is a huxtable, then `myhux[1, 2]`
represents the data in row 1 column 2, as before. But this cell will also have some other properties - for
example, the font size of the text, or the colour of the cell border.

To create a table with huxtable, use the function `huxtable`, or `hux` for short. This works very much like
`data.frame`.

```{r eval=FALSE, include=FALSE, echo=TRUE}
library(huxtable)
ht <- hux(
        Employee     = c('John Smith', 'Jane Doe', 'David Hugh-Jones'), 
        Salary       = c(50000, 50000, 40000),
        add_colnames = TRUE
      )
```


If you already have your data in a data frame, you can convert it to a huxtable with `as_hux`.

```{r eval=FALSE, include=FALSE, echo=TRUE}
data(mtcars)
car_ht <- as_hux(mtcars)
```


If you look at a huxtable in R, it will print out a simple representation of the data. Notice that we've added the
column names to the data frame itself, using the `add_colnames` argument to `hux`. We're going to print them out, so they need to be part of the actual table. **NB:** This means that row 1 of your data will be row 2 of the huxtable, and the
column names of your data will be the new row 1.

```{r eval=FALSE, include=FALSE, results='markup'}
print_screen(ht)     # on the R command line, you can just type "ht"
```

To print a huxtable out using LaTeX or HTML, just call `print_latex` or `print_html`. In knitr
documents, like this one, you can simply evaluate the hux. It will know what format to print itself
in.

```{r eval=FALSE, include=FALSE, echo=TRUE}
ht
```


# Changing the look and feel

## Huxtable properties

The default output is a very plain table. Let's make it a bit smarter. We'll make the table headings
bold, draw a line under the header row, and add some horizontal space to the cells. We also need to
change that default number formatting to look less scientific.

To do this, we need to set cell level properties. You set properties by assigning to the property name, just as you assign `names(x) <- new_names` in base R. The following commands assign the value 
10 to the `{r # ight_padding` and `left_padding` properties, for all cells in `ht`:

```{r eval=FALSE, include=FALSE, echo=TRUE}
right_padding(ht) <- 10
left_padding(ht)  <- 10
```

Similarly, we can set the `number_format` property to change how numbers are displayed in 
cells:

```{r eval=FALSE, include=FALSE, echo=TRUE}
number_format(ht) <- 2    # 2 decimal places
```

To assign properties to just some cells, you use subsetting, just as in base R. So, to make the
first row of the table **bold** and give it a bottom border, we do:

```{r eval=FALSE, include=FALSE, echo=TRUE}
bold(ht)[1, ]          <- TRUE
bottom_border(ht)[1, ] <- 1
```

After these changes, our table looks smarter:

```{r eval=FALSE, include=FALSE, echo=TRUE}
ht
```


So far, all these properties have been set at cell level. Different cells can have different
alignment, text formatting and so on. By contrast, `caption` is a table-level property. It only
takes one value, which sets a table caption.

```{r eval=FALSE, include=FALSE, echo=TRUE}

caption(ht) <- 'Employee table'
ht

```


As well as cell properties and table properties, there is also one row property, row heights, and
one column property, column widths.

The table below shows a complete list of properties. Most properties work the same for LaTeX and
HTML, though there are some exceptions.

```{r, echo = TRUE}
sides <- c('left_', 'right_', 'top_', 'bottom_')
props <- list()
props[['Cell_Text']] <- sort(c('font', 'text_color', 'wrap', 'bold', 'italic', 'font', 'font_size', 'na_string', 'escape_contents', 'number_format', 'rotation'))

props[['Cell']] <- sort(c('align', 'valign', 'rowspan', 'colspan', 'background_color', paste0(sides, 'border'),
      paste0(sides, 'border_color'), paste0(sides, 'padding')))
props[['Row']]    <- 'row_height'
props[['Column']] <- 'col_width'
props[['Table']]  <- sort(c('width', 'height', 'position', 'caption', 'caption_pos', 'tabular_environment', 'label', 'latex_float'))

maxl <- max(sapply(props, length))
props <- lapply(props, function(x) c(x, rep('', maxl - length(x))))

ss_font <- if (guess_knitr_output_format() == 'latex') 'cmtt' else 'courier'

prop_hux <- hux(as.data.frame(props))                     %>% 
      add_colnames                                        %>% 
      {foo <- .; foo[1,] <- gsub('_', ' ', foo[1,]); foo} %>% 
      set_font(-1, everywhere, ss_font)                   %>% 
      set_bold(1, everywhere, TRUE)                       %>% 
      set_width(0.9)                                      %>% 
      set_background_color(everywhere, evens, grey(.9))   %>% 
      set_left_border(everywhere, 1, 1)                   %>% 
      set_right_border(everywhere, final(), 1)            %>% 
      set_top_border(1, everywhere, 1)                    %>% 
      set_bottom_border(1, everywhere, 1)                 %>% 
      set_bottom_border(final(), everywhere, 1)           %>% 
      set_top_padding(2)                                  %>% 
      set_bottom_padding(4)                               %>% 
      set_caption('Huxtable properties')                  %>% 
      set_position('left') %>% 
      set_col_width(c(.2, .25, .15, .15, .25))

prop_hux
```


## Tidyverse syntax

If you prefer a tidyverse style of code, using the pipe operator `%>%`, then you can use `set_*`
functions. These have the same name as the property, with `set_` prepended. For example, to set
the `bold` property, you use the `set_bold` function.

`set_*` functions return the modified huxtable, so you can chain them together like this:

```{r eval=FALSE, include=FALSE, echo=TRUE}

library(dplyr)
hux(
        Employee     = c('John Smith', 'Jane Doe', 'David Hugh-Jones'), 
        Salary       = c(50000, 50000, 40000),
        add_colnames = TRUE
      )                               %>% 
      set_bold(1, 1:2, TRUE)          %>% 
      set_bottom_border(1, 1:2, 1)    %>%
      set_align(1:4, 2, 'right')      %>%
      set_right_padding(10)           %>%
      set_left_padding(10)            %>% 
      set_caption('Employee table')
  

```


`set_*` functions for cell properties are called like this: `set_xxx(ht, row, col, value)` or like
this: `set_xxx(ht, value)`. If you use the second form, then the value is set for all cells. `set_*`
functions for table properties are always called like `set_xxx(ht, value)`. We'll learn more about
this interface in a moment.

There are also four useful convenience functions:

* `set_all_borders` sets left, right, top and bottom borders for selected cells;
* `set_all_border_colors` sets left, right, top and bottom border colors;
* `set_all_padding` sets left, right, top and bottom padding (the amount of space between the content and the border);
* `set_outer_borders` sets an outer border around a rectangle of cells.

## Getting properties

To get the current properties of a huxtable, just use the properties function without the left arrow:

```{r, results = 'markup', eval=FALSE}
italic(ht)
position(ht)
```


As before, you can use subsetting to get particular rows or columns:

```{r, results = 'markup', eval=FALSE}
bottom_border(ht)[1:2,]
bold(ht)[,'Salary']
```



# Editing content

## Standard subsetting

You can subset, sort and generally data-wrangle a huxtable just like a normal data frame. Cell and 
table properties will be carried over into subsets.

```{r eval=FALSE, include=FALSE, echo=TRUE}
# Select columns by name:
cars_mpg <- car_ht[, c('mpg', 'cyl', 'am')] 
# Order by number of cylinders:
cars_mpg <- cars_mpg[order(cars_mpg$cyl),]

cars_mpg <- cars_mpg                          %>% 
      huxtable::add_rownames(colname = 'Car') %>% 
      huxtable::add_colnames()

cars_mpg[1:5,]
```


## Using dplyr with huxtable

You can also use `dplyr` functions to edit a huxtable:


```{r eval=FALSE, include=FALSE, echo=TRUE}
car_ht <- car_ht                                          %>%
      huxtable::add_rownames(colname = 'Car')             %>%
      slice(1:10)                                         %>% 
      select(Car, mpg, cyl, hp)                           %>% 
      arrange(hp)                                         %>% 
      filter(cyl > 4)                                     %>% 
      rename(MPG = mpg, Cylinders = cyl, Horsepower = hp) %>% 
      mutate(kml = MPG/2.82)                               


car_ht <- car_ht                               %>% 
      set_number_format(1:7, 'kml', 2)         %>% 
      set_col_width(c(.35, .15, .15, .15, .2)) %>% 
      set_width(.6)                            %>% 
      huxtable::add_colnames() 

car_ht
```


In general it is a good idea to prepare your data first, before styling it. For example, it was easier to sort the `cars_mpg` data by cylinder, before adding column names to the data frame itself.

## Functions to insert rows, columns and footnotes

Huxtable has three convenience functions for adding a row or column to your table: `insert_row`,
`insert_column` and `add_footnote`.
`insert_row` and `insert_column` let you add a single row or column. The `after` parameter specifies
where in the table to do the insertion, i.e. after what row or column number.
`add_footnote` adds a single cell in a new row at the bottom. The cell spans the whole table row,
and has a border above.

```{r eval=FALSE, include=FALSE, echo=TRUE}
ht <- insert_row(ht, 'Hadley Wickham', '100000', after = 3)
ht <- add_footnote(ht, 'DHJ deserves a pay rise')
ht
```

# More formatting 


## Number format

You can change how huxtable formats numbers using `number_format`. Set `number_format` to a number of decimal places 
(for more advanced options, see the help files). This affects all numbers, or number-like substrings within your cells.

```{r eval=FALSE, include=FALSE, echo=TRUE}
pointy_ht <- hux(c('Do not pad this.', 11.003, 300, 12.02, '12.1 **')) %>% set_all_borders(1)

number_format(pointy_ht) <- 3
pointy_ht
```

You can also align columns by decimal places. If you want to do this for a cell, just set the `align` property
to '.' (or whatever you use for a decimal point). 

```{r eval=FALSE, include=FALSE, echo=TRUE}
align(pointy_ht)[2:5, ] <- '.' # not the first row
pointy_ht
```

There is currently no true way to align cells by the decimal point in HTML, and only limited possibilities in TeX, so this works by right-padding cells with spaces. The output may look better 
if you use a fixed width font.

## Automatic formatting

By default, when you create a huxtable using `huxtable` or `as_huxtable`, the package will guess
defaults for number formatting and alignment, based on the type of data in your columns. Numeric
data will be right-aligned or aligned on the decimal point; character data will be left aligned;
and the package will try to set sensible defaults for number formatting. If you want to, you can
turn this off with `autoformat = FALSE`:

```{r eval=FALSE, include=FALSE, echo=TRUE}

my_data <- data.frame(
        Employee           = c('John Smith', 'Jane Doe', 'David Hugh-Jones'), 
        Salary             = c(50000L, 50000L, 40000L),
        Performance_rating = c(8.9, 9.2, 7.8)  
      )
as_huxtable(my_data, add_colnames = TRUE) # with automatic formatting

as_huxtable(my_data, add_colnames = TRUE, autoformat = FALSE) # no automatic formatting
```

## Escaping HTML or LaTeX

By default, HTML or LaTeX code will be escaped:

```{r eval=FALSE, include=FALSE, echo=TRUE}
code_ht <- if (is_latex) hux(c('Some maths', '$a^b$')) else 
      hux(c('Copyright symbol', '&copy;'))
code_ht
```

To avoid this, set the `escape_contents` property to `FALSE`.

```{r eval=FALSE, include=FALSE, echo=TRUE}
escape_contents(code_ht)[2, 1] <- FALSE
code_ht
```


## Width and cell wrapping

You can set table widths using the `width` property, and column widths using the `col_width` property. If you use
numbers for these, they will be interpreted as proportions of the table width (or for `width`, a proportion of the
width of the surrounding text). If you use character vectors, they must be valid CSS or LaTeX widths. The only
unit both systems have in common is `pt` for points. 

```{r eval=FALSE, include=FALSE, echo=TRUE}
width(ht) <- 0.35
col_width(ht) <- c(.7, .3)
ht
```


It is best to set table width explicitly, then set column widths as proportions.

By default, if a cell contains long contents, it will be stretched. Use the `wrap` property to allow cell contents
to wrap over multiple lines:

```{r eval=FALSE, include=FALSE, echo=TRUE}
ht_wrapped <- ht
ht_wrapped[5, 1] <- 'David Arthur Shrimpton Hugh-Jones'
wrap(ht_wrapped) <- TRUE
ht_wrapped
```


## Adding row and column names

Just like data frames, huxtables can have row and column names. Often, we want to add these to the final table.
You can do this using either the `add_colnames`/`add_rownames` arguments to `as_huxtable`, or the
`add_colnames()`/`add_rownames()` functions. (Note that earlier versions of `dplyr` used to have functions with the
same name.)

```{r eval=FALSE, include=FALSE, echo=TRUE}
as_hux(mtcars[1:4, 1:4])                           %>% 
      huxtable::add_rownames(colname = 'Car name') %>% 
      huxtable::add_colnames()
```


## Column and row spans

Huxtable cells can span multiple rows or columns, using the `colspan` and `{r # owspan` properties.

```{r eval=FALSE, include=FALSE, echo=TRUE}
cars_mpg <- cbind(car_type = rep("", nrow(cars_mpg)), cars_mpg)
cars_mpg$car_type[1] <- 'Four cylinders'
cars_mpg$car_type[13] <- 'Six cylinders'
cars_mpg$car_type[20] <- 'Eight cylinders'
rowspan(cars_mpg)[1, 1] <- 12
rowspan(cars_mpg)[13, 1] <- 7
rowspan(cars_mpg)[20, 1] <- 14

cars_mpg <- rbind(c('', 'List of cars', '', '', ''), cars_mpg)
colspan(cars_mpg)[1, 2] <- 4
align(cars_mpg)[1, 2] <- 'center'

# a little more formatting:

cars_mpg <- set_all_padding(cars_mpg, 2)
cars_mpg <- set_all_borders(cars_mpg, 1)
valign(cars_mpg)[1,] <- 'top'
col_width(cars_mpg) <- c(.4 , .3 , .1, .1, .1)
number_format(cars_mpg)[, 4:5] <- 0
bold(cars_mpg)[1:2, ] <- TRUE
bold(cars_mpg)[, 1] <- TRUE
if (is_latex) font_size(cars_mpg) <- 10
cars_mpg
```


## Quick themes

Huxtable comes with some predefined themes for formatting.

```{r eval=FALSE, include=FALSE, echo=TRUE}
theme_striped(cars_mpg[14:20,], stripe = 'bisque1', header_col = FALSE, header_row = FALSE)
```


# Selecting rows, columns and cells


## Row and column functions

If you use the `set_*` style functions, huxtable has some convenience functions for selecting rows and columns.

To select all rows, or all columns, use `everywhere` in the row or column specification. To select just even or odd-numbered rows or columns, use `evens` or `odds`. To select the last `n` rows or columns, use `final(n)`.
To select every *n*th row, use `every(n)` and to do this starting from row *m* use `every(n, from = m)`.

With these functions it is easy to add striped backgrounds to tables:

```{r eval=FALSE, include=FALSE, echo=TRUE}
car_ht                                                 %>% 
      set_background_color(evens, everywhere, 'wheat') %>% 
      set_background_color(odds, everywhere, grey(.9)) %>% 
      set_bold(1, everywhere, TRUE)
```


Of course you could also just do `1:nrow(car_ht)`, but, in the middle of a dplyr pipe, you may not know exactly
how many rows or columns you have. Also, these functions make your code easy to read.

You can also use `dplyr` functions like `starts_with()`, `contains()`, and `matches()` to specify columns by column 
name. For a full list of these functions, see `?select_helpers`.

```{r eval=FALSE, include=FALSE, echo=TRUE}
car_ht %>% set_background_color(everywhere, starts_with('C'), 'orange')
car_ht %>% set_italic(everywhere, matches('[aeiou]'), TRUE)
```

Note that unlike in `dplyr`'s `select` function, you have to specify rows as well as columns.

Lastly, remember that you can set a property for every cell by simply omitting the `{r # ow` and `col` arguments, like this:
`set_background_color(ht, 'orange')`.


## Conditional formatting

You may want to apply conditional formatting to cells, based on their contents. Suppose we want to display a table of correlations, and to highlight ones which are significant. We can use the `where()` function to select those cells.

```{r eval=FALSE, include=FALSE, echo=TRUE}
library(psych)
data(attitude)
att_corr <- corr.test(as.matrix(attitude))

att_hux <- as_hux(att_corr$r)                                           %>% 
      # selects cells with p < 0.05:
      set_background_color(where(att_corr$p < 0.05), 'yellow')          %>% 
      # selects cells with p < 0.01:
      set_background_color(where(att_corr$p < 0.01), 'orange')          %>% 
      set_text_color(where(row(att_corr$r) == col(att_corr$r)), 'grey') 


att_hux <- att_hux                                                      %>% 
      huxtable::add_rownames()                                          %>% 
      huxtable::add_colnames()                                          %>%
      set_caption('Correlations in attitudes among 30 departments')     %>% 
      set_bold(1, everywhere, TRUE)                                     %>% 
      set_bold(everywhere, 1, TRUE)                                     %>% 
      set_all_borders(1)                                                %>%
      set_number_format(2)                                              %>% 
      set_position('left')

att_hux

```

We have now seen three ways to call `set_*` functions in huxtable:

* With four arguments, like `set_property(hux_object, rows, cols, value)`;
* With two arguments, like `set_property(hux_object, value)` to set a property everywhere;
* With three arguments, like `set_property(hux_object, where(condition), value)` to set a property for specific cells.

The second argument of the three-argument version must return a 2-column matrix. Each row of the matrix gives one cell.
`where()` does this for you: it takes a logical matrix argument and returns the rows and columns where a condition is
`TRUE`. It's easiest to show this with an example:

```{r eval=FALSE, include=FALSE, echo=TRUE}
m <- matrix(c('dog', 'cat', 'dog', 'dog', 'cat', 'cat', 'cat', 'dog'), 4, 2)
m
where(m == 'dog') # m is equal to 'dog' in cells (1, 1), (3, 1), (4, 1) and (4, 2):
```

`set_*` functions have one more optional argument, the `byrow` argument, which is `FALSE` by default. If you set a
single pattern for many cells, you may want the pattern to fill the matrix by column or by row. The default fills the
pattern in going down columns. If you set `byrow = TRUE`, the pattern goes across rows instead. (This is a bit
confusing: typically, `byrow = TRUE` means that the *columns* will all look the same. But it works the same way as the
`byrow` argument to `matrix()`.)

```{r eval=FALSE, include=FALSE, echo=TRUE}

color_demo <- matrix('text', 7, 7)
rainbow <- c('red', 'orange', 'yellow', 'green', 'blue', 'turquoise', 'violet')
color_demo <- as_hux(color_demo)                  %>% 
      set_text_color(rainbow)                     %>% # text rainbow down columns
      set_background_color(rainbow, byrow = TRUE) %>% # background color rainbow along rows
      set_all_borders(1)                          %>% 
      set_all_border_colors('white')
color_demo

```



# Creating a regression table

A common task for scientists is to create a table of regressions. The function `huxreg` does this for you. Here's a quick example:

```{r eval=FALSE, include=FALSE, echo=TRUE}
data(diamonds, package = 'ggplot2')

lm1 <- lm(price ~ carat, diamonds)
lm2 <- lm(price ~ depth, diamonds)
lm3 <- lm(price ~ carat + depth, diamonds)

huxreg(lm1, lm2, lm3)
```

For more information see the `huxreg` vignette, available online in
[HTML](https://hughjonesd.github.io/huxtable/huxreg.html) or [PDF](https://hughjonesd.github.io/huxtable/huxreg.pdf) or
in R via `vignette('huxreg')`.

# Output to different formats

## Automatic pretty-printing of data frames

If you load huxtable within a knitr document, it will automatically format data frames for you by
installing a `knit_print.data_frame` command.


```{r, echo = TRUE, eval=FALSE}
options(huxtable.knit_print_df = TRUE)
```

```{r eval=FALSE, include=FALSE, echo=TRUE}
head(mtcars)
```

If you don't want this (e.g. if you want to use `knitr::kable` or the [printr package](https://cran.r-project.org/web/packages/printr/vignettes/printr.html), then you can
turn it off like this:

```{r eval=FALSE, include=FALSE, echo=TRUE}
options(huxtable.knit_print_df = FALSE)

head(mtcars) # back to normal
```

## Using huxtables in knitr and rmarkdown

If you use knitr and rmarkdown in RStudio, huxtable objects should automatically display in the
appropriate format (HTML or LaTeX). You need to have some LaTeX packages installed for huxtable to
work. To find out what these are, you can call `{r # eport_latex_dependencies()`. This will print out
and/or return a set of `usepackage{...}` statements. If you use Sweave or knitr without rmarkdown,
you can use this function in your LaTeX preamble to load the packages you need.

Rmarkdown exports to Word via Markdown. You can use huxtable to do this, but since Markdown tables
are rather basic, a lot of formatting will be lost. If you want to create Word or Powerpoint
documents directly, install the [flextable package](https://cran.r-project.org/package=flextable)
from CRAN. You can then convert your huxtable objects to `flextable` objects and include them in
Word or Powerpoint documents. Almost all formatting should work. See the `flextable` and `officer`
documentation and `?as_flextable` for more details.

Similarly, to create formatted reports in Excel, install the [openxlsx
package](https://cran.r-project.org/package=openxlsx). You can then use `as_Workbook` to convert
your huxtables to Workbook objects, and save them using `openxlsx::saveWorkbook`.

Sometimes you may want to select how huxtable objects are printed by default. For example, in an
RStudio notebook (a .Rmd document with `output_format = html_notebook`), huxtable can't
automatically work out what format to use, as of the time of writing. You can set it manually using
`options(huxtable.print = print_notebook)` which prints out HTML in an appropriate format.

You can print a huxtable on screen using `print_screen` (or just by typing its name at the command
line.) Borders, column and row spans and cell alignment are shown. If the
[crayon](https://cran.r-project.org/package=crayon) package is installed, and your terminal or R IDE
supports it, border, text and background colours are also displayed.

```{r, results = 'markup', eval=FALSE}
print_screen(ht)
```


If you need to output to another format, file an 
[issue request](https://github.com/hughjonesd/huxtable) on Github. 

## Quick output commands

Sometimes you quickly want to get your data into a Word, HTML or PDF document. To do this you can
use the `quick_docx`, `quick_html`, `quick_pdf` and `quick_xlsx` functions. These are called with
one or more huxtable objects, or objects which can be turned into a huxtable such as data frames. A
new document of the appropriate type will be created. By default the file will be in the current
directory under the name e.g. `huxtable-output.pdf`. If the file already exists, you'll be asked to
confirm the overwrite. For non-interactive use, you must specify a filename yourself explicitly --
this keeps you from accidentally trashing your files.

```{r, eval = FALSE}
quick_pdf(mtcars) 
quick_pdf(mtcars, file = 'motorcars data.pdf')
```

# End matter

For more information, see the [website](https://hughjonesd.github.io/huxtable) or 
[github](https://github.com/hughjonesd/huxtable).

<!--chapter:end:huxtable3.Rmd-->

---
title: "Hypothesis Testing"
---

# Glue
Glue strings to data in R. Small, fast, dependency free interpreted string literals.

https://glue.tidyverse.org/

# infer package
Statistical Inference: A Tidy Approach
https://ismayc.github.io/talks/ness-infer/slide_deck.html

https://infer.netlify.com/

https://moderndive.netlify.com/

https://cran.r-project.org/web/packages/infer/index.html


# Hypothesis Testing

## Test Selection

### Statkat

https://statkat.com/

https://statkat.com/stattest_overview.php


#### Jamovi Statkat module

https://blog.jamovi.org/2018/06/25/statkat.html

```{r eval=FALSE, include=FALSE, echo=TRUE}
library(jmv)
library(Statkat)
```

```{r eval=FALSE, include=FALSE, echo=TRUE}

Statkat::correlational(
    data = data,
    dep = "len",
    independents = "supp")
```













# infer

Full infer pipeline examples using nycflights13 flights data


https://cran.r-project.org/web/packages/infer/vignettes/observed_stat_examples.html


Full infer pipeline examples using nycflights13 flights data
Chester Ismay
Updated on 2018-06-14
Data preparation
library(nycflights13)
library(dplyr)
library(ggplot2)
library(stringr)
library(infer)
set.seed(2017)
fli_small <- flights %>% 
  na.omit() %>%
  sample_n(size = 500) %>% 
  mutate(season = case_when(
    month %in% c(10:12, 1:3) ~ "winter",
    month %in% c(4:9) ~ "summer"
  )) %>% 
  mutate(day_hour = case_when(
    between(hour, 1, 12) ~ "morning",
    between(hour, 13, 24) ~ "not morning"
  )) %>% 
  select(arr_delay, dep_delay, season, 
         day_hour, origin, carrier)
Two numeric - arr_delay, dep_delay
Two categories
season ("winter", "summer"),
day_hour ("morning", "not morning")
Three categories - origin ("EWR", "JFK", "LGA")
Sixteen categories - carrier
Hypothesis tests
One numerical variable (mean)
Observed stat

( x_bar <- fli_small %>%
  specify(response = dep_delay) %>%
  calculate(stat = "mean") )
stat
10.4
null_distn <- fli_small %>%
  specify(response = dep_delay) %>%
  hypothesize(null = "point", mu = 10) %>%
  generate(reps = 1000) %>%
  calculate(stat = "mean")
## Setting `type = "bootstrap"` in `generate()`.
visualize(null_distn) +
  shade_p_value(obs_stat = x_bar, direction = "two_sided")

null_distn %>%
  get_p_value(obs_stat = x_bar, direction = "two_sided")
p_value
0.794
One numerical variable (standardized mean t)
Observed stat

( t_bar <- fli_small %>%
  specify(response = dep_delay) %>%
  calculate(stat = "t") )
stat
6.93
null_distn <- fli_small %>%
  specify(response = dep_delay) %>%
  hypothesize(null = "point", mu = 8) %>%
  generate(reps = 1000) %>%
  calculate(stat = "t")
## Setting `type = "bootstrap"` in `generate()`.
visualize(null_distn) +
  shade_p_value(obs_stat = t_bar, direction = "two_sided")

null_distn %>%
  get_p_value(obs_stat = t_bar, direction = "two_sided")
p_value
0
One numerical variable (median)
Observed stat

( x_tilde <- fli_small %>%
  specify(response = dep_delay) %>%
  calculate(stat = "median") )
stat
-2
null_distn <- fli_small %>%
  specify(response = dep_delay) %>%
  hypothesize(null = "point", med = -1) %>% 
  generate(reps = 1000) %>% 
  calculate(stat = "median")
## Setting `type = "bootstrap"` in `generate()`.
visualize(null_distn) +
  shade_p_value(obs_stat = x_tilde, direction = "two_sided")

null_distn %>%
  get_p_value(obs_stat = x_tilde, direction = "two_sided")
p_value
0.15
One categorical (one proportion)
Observed stat

( p_hat <- fli_small %>%
  specify(response = day_hour, success = "morning") %>%
  calculate(stat = "prop") )
stat
0.466
null_distn <- fli_small %>%
  specify(response = day_hour, success = "morning") %>%
  hypothesize(null = "point", p = .5) %>%
  generate(reps = 1000) %>%
  calculate(stat = "prop")
## Setting `type = "simulate"` in `generate()`.
visualize(null_distn) +
  shade_p_value(obs_stat = p_hat, direction = "two_sided")

null_distn %>%
  get_p_value(obs_stat = p_hat, direction = "two_sided")
p_value
0.11
Logical variables will be coerced to factors:

null_distn <- fli_small %>%
  mutate(day_hour_logical = (day_hour == "morning")) %>%
  specify(response = day_hour_logical, success = "TRUE") %>%
  hypothesize(null = "point", p = .5) %>%
  generate(reps = 1000) %>%
  calculate(stat = "prop")
## Setting `type = "simulate"` in `generate()`.
One categorical variable (standardized proportion z)
Not yet implemented.

Two categorical (2 level) variables
Observed stat

( d_hat <- fli_small %>% 
  specify(day_hour ~ season, success = "morning") %>%
  calculate(stat = "diff in props", order = c("winter", "summer")) )
stat
-0.0205
null_distn <- fli_small %>%
  specify(day_hour ~ season, success = "morning") %>%
  hypothesize(null = "independence") %>% 
  generate(reps = 1000) %>% 
  calculate(stat = "diff in props", order = c("winter", "summer"))
## Setting `type = "permute"` in `generate()`.
visualize(null_distn) +
  shade_p_value(obs_stat = d_hat, direction = "two_sided")

null_distn %>%
  get_p_value(obs_stat = d_hat, direction = "two_sided")
p_value
0.708
Two categorical (2 level) variables (z)
Standardized observed stat

( z_hat <- fli_small %>% 
  specify(day_hour ~ season, success = "morning") %>%
  calculate(stat = "z", order = c("winter", "summer")) )
stat
-0.4605
null_distn <- fli_small %>%
  specify(day_hour ~ season, success = "morning") %>%
  hypothesize(null = "independence") %>% 
  generate(reps = 1000) %>% 
  calculate(stat = "z", order = c("winter", "summer"))
## Setting `type = "permute"` in `generate()`.
visualize(null_distn) +
  shade_p_value(obs_stat = z_hat, direction = "two_sided")

null_distn %>%
  get_p_value(obs_stat = z_hat, direction = "two_sided")
p_value
0.684
Note the similarities in this plot and the previous one.

One categorical (>2 level) - GoF
Observed stat

Note the need to add in the hypothesized values here to compute the observed statistic.

( Chisq_hat <- fli_small %>%
  specify(response = origin) %>%
  hypothesize(null = "point", 
              p = c("EWR" = .33, "JFK" = .33, "LGA" = .34)) %>% 
  calculate(stat = "Chisq") )
stat
10.4
null_distn <- fli_small %>%
  specify(response = origin) %>%
  hypothesize(null = "point", 
              p = c("EWR" = .33, "JFK" = .33, "LGA" = .34)) %>% 
  generate(reps = 1000, type = "simulate") %>% 
  calculate(stat = "Chisq")

visualize(null_distn) +
  shade_p_value(obs_stat = Chisq_hat, direction = "greater")

null_distn %>%
  get_p_value(obs_stat = Chisq_hat, direction = "greater")
p_value
0.005
Two categorical (>2 level) variables
Observed stat

( Chisq_hat <- fli_small %>%
  specify(formula = day_hour ~ origin) %>% 
  calculate(stat = "Chisq") )
stat
9.027
null_distn <- fli_small %>%
  specify(day_hour ~ origin) %>%
  hypothesize(null = "independence") %>% 
  generate(reps = 1000, type = "permute") %>% 
  calculate(stat = "Chisq")

visualize(null_distn) +
  shade_p_value(obs_stat = Chisq_hat, direction = "greater")

null_distn %>%
  get_p_value(obs_stat = Chisq_hat, direction = "greater")
p_value
0.007
One numerical variable, one categorical (2 levels) (diff in means)
Observed stat

( d_hat <- fli_small %>% 
  specify(dep_delay ~ season) %>% 
  calculate(stat = "diff in means", order = c("summer", "winter")) )
stat
2.266
null_distn <- fli_small %>%
  specify(dep_delay ~ season) %>%
  hypothesize(null = "independence") %>%
  generate(reps = 1000, type = "permute") %>%
  calculate(stat = "diff in means", order = c("summer", "winter"))

visualize(null_distn) +
  shade_p_value(obs_stat = d_hat, direction = "two_sided")

null_distn %>%
  get_p_value(obs_stat = d_hat, direction = "two_sided")
p_value
0.488
One numerical variable, one categorical (2 levels) (t)
Standardized observed stat

( t_hat <- fli_small %>% 
  specify(dep_delay ~ season) %>% 
  calculate(stat = "t", order = c("summer", "winter")) )
stat
0.7542
null_distn <- fli_small %>%
  specify(dep_delay ~ season) %>%
  hypothesize(null = "independence") %>%
  generate(reps = 1000, type = "permute") %>%
  calculate(stat = "t", order = c("summer", "winter"))

visualize(null_distn) +
  shade_p_value(obs_stat = t_hat, direction = "two_sided")

null_distn %>%
  get_p_value(obs_stat = t_hat, direction = "two_sided")
p_value
0.49
Note the similarities in this plot and the previous one.

One numerical variable, one categorical (2 levels) (diff in medians)
Observed stat

( d_hat <- fli_small %>% 
  specify(dep_delay ~ season) %>% 
  calculate(stat = "diff in medians", order = c("summer", "winter")) )
stat
2
null_distn <- fli_small %>%
  specify(dep_delay ~ season) %>% # alt: response = dep_delay, 
  # explanatory = season
  hypothesize(null = "independence") %>%
  generate(reps = 1000, type = "permute") %>%
  calculate(stat = "diff in medians", order = c("summer", "winter"))

visualize(null_distn) +
  shade_p_value(obs_stat = d_hat, direction = "two_sided")

null_distn %>%
  get_p_value(obs_stat = d_hat, direction = "two_sided")
p_value
0.084
One numerical, one categorical (>2 levels) - ANOVA
Observed stat

( F_hat <- fli_small %>% 
  specify(arr_delay ~ origin) %>%
  calculate(stat = "F") )
stat
1.084
null_distn <- fli_small %>%
   specify(arr_delay ~ origin) %>%
   hypothesize(null = "independence") %>%
   generate(reps = 1000, type = "permute") %>%
   calculate(stat = "F")

visualize(null_distn) +
  shade_p_value(obs_stat = F_hat, direction = "greater")

null_distn %>%
  get_p_value(obs_stat = F_hat, direction = "greater")
p_value
0.353
Two numerical vars - SLR
Observed stat

( slope_hat <- fli_small %>% 
  specify(arr_delay ~ dep_delay) %>% 
  calculate(stat = "slope") )
stat
1.017
null_distn <- fli_small %>%
   specify(arr_delay ~ dep_delay) %>% 
   hypothesize(null = "independence") %>%
   generate(reps = 1000, type = "permute") %>%
   calculate(stat = "slope")

visualize(null_distn) +
  shade_p_value(obs_stat = slope_hat, direction = "two_sided")

null_distn %>%
  get_p_value(obs_stat = slope_hat, direction = "two_sided")
p_value
0
Two numerical vars - correlation
Observed stat

( correlation_hat <- fli_small %>% 
  specify(arr_delay ~ dep_delay) %>% 
  calculate(stat = "correlation") )
stat
0.8943
null_distn <- fli_small %>%
   specify(arr_delay ~ dep_delay) %>% 
   hypothesize(null = "independence") %>%
   generate(reps = 1000, type = "permute") %>%
   calculate(stat = "correlation")

visualize(null_distn) +
  shade_p_value(obs_stat = correlation_hat, direction = "two_sided")

null_distn %>%
  get_p_value(obs_stat = correlation_hat, direction = "two_sided")
p_value
0
Two numerical vars - SLR (t)
Not currently implemented since t could refer to standardized slope or standardized correlation.

Confidence intervals
One numerical (one mean)
Point estimate

( x_bar <- fli_small %>% 
  specify(response = arr_delay) %>%
  calculate(stat = "mean") )
stat
4.572
boot <- fli_small %>%
   specify(response = arr_delay) %>%
   generate(reps = 1000, type = "bootstrap") %>%
   calculate(stat = "mean")
( percentile_ci <- get_ci(boot) )
2.5%	97.5%
1.436	7.819
visualize(boot) +
  shade_confidence_interval(endpoints = percentile_ci)

( standard_error_ci <- get_ci(boot, type = "se", point_estimate = x_bar) )
lower	upper
1.267	7.877
visualize(boot) +
  shade_confidence_interval(endpoints = standard_error_ci)

One numerical (one mean - standardized)
Point estimate

( t_hat <- fli_small %>% 
  specify(response = arr_delay) %>%
  calculate(stat = "t") )
stat
2.679
boot <- fli_small %>%
   specify(response = arr_delay) %>%
   generate(reps = 1000, type = "bootstrap") %>%
   calculate(stat = "t")
( percentile_ci <- get_ci(boot) )
2.5%	97.5%
0.9338	4.362
visualize(boot) +
  shade_confidence_interval(endpoints = percentile_ci)

( standard_error_ci <- get_ci(boot, type = "se", point_estimate = t_hat) )
lower	upper
0.9141	4.444
visualize(boot) +
  shade_confidence_interval(endpoints = standard_error_ci)

One categorical (one proportion)
Point estimate

( p_hat <- fli_small %>% 
   specify(response = day_hour, success = "morning") %>%
   calculate(stat = "prop") )
stat
0.466
boot <- fli_small %>%
 specify(response = day_hour, success = "morning") %>%
 generate(reps = 1000, type = "bootstrap") %>%
 calculate(stat = "prop")
( percentile_ci <- get_ci(boot) )
2.5%	97.5%
0.42	0.508
visualize(boot) +
  shade_confidence_interval(endpoints = percentile_ci)

( standard_error_ci <- get_ci(boot, type = "se", point_estimate = p_hat) )
lower	upper
0.4218	0.5102
visualize(boot) +
  shade_confidence_interval(endpoints = standard_error_ci)

One categorical variable (standardized proportion z)
Not yet implemented.

One numerical variable, one categorical (2 levels) (diff in means)
Point estimate

( d_hat <- fli_small %>%
  specify(arr_delay ~ season) %>%
  calculate(stat = "diff in means", order = c("summer", "winter")) )
stat
-0.7452
boot <- fli_small %>%
   specify(arr_delay ~ season) %>%
   generate(reps = 1000, type = "bootstrap") %>%
   calculate(stat = "diff in means", order = c("summer", "winter"))
( percentile_ci <- get_ci(boot) )
2.5%	97.5%
-7.167	6.079
visualize(boot) +
  shade_confidence_interval(endpoints = percentile_ci)

( standard_error_ci <- get_ci(boot, type = "se", point_estimate = d_hat) )
lower	upper
-7.296	5.806
visualize(boot) +
  shade_confidence_interval(endpoints = standard_error_ci)

One numerical variable, one categorical (2 levels) (t)
Standardized point estimate

( t_hat <- fli_small %>%
  specify(arr_delay ~ season) %>%
  calculate(stat = "t", order = c("summer", "winter")) )
stat
-0.2182
boot <- fli_small %>%
   specify(arr_delay ~ season) %>%
   generate(reps = 1000, type = "bootstrap") %>%
   calculate(stat = "t", order = c("summer", "winter"))
( percentile_ci <- get_ci(boot) )
2.5%	97.5%
-2.236	1.718
visualize(boot) +
  shade_confidence_interval(endpoints = percentile_ci)

( standard_error_ci <- get_ci(boot, type = "se", point_estimate = t_hat) )
lower	upper
-2.183	1.746
visualize(boot) +
  shade_confidence_interval(endpoints = standard_error_ci)

Two categorical variables (diff in proportions)
Point estimate

( d_hat <- fli_small %>% 
  specify(day_hour ~ season, success = "morning") %>%
  calculate(stat = "diff in props", order = c("summer", "winter")) )
stat
0.0205
boot <- fli_small %>%
  specify(day_hour ~ season, success = "morning") %>%
  generate(reps = 1000, type = "bootstrap") %>% 
  calculate(stat = "diff in props", order = c("summer", "winter"))
( percentile_ci <- get_ci(boot) )
2.5%	97.5%
-0.0648	0.1083
visualize(boot) +
  shade_confidence_interval(endpoints = percentile_ci)

( standard_error_ci <- get_ci(boot, type = "se", point_estimate = d_hat) )
lower	upper
-0.0676	0.1087
visualize(boot) +
  shade_confidence_interval(endpoints = standard_error_ci)

Two categorical variables (z)
Standardized point estimate

( z_hat <- fli_small %>% 
  specify(day_hour ~ season, success = "morning") %>%
  calculate(stat = "z", order = c("summer", "winter")) )
stat
0.4605
boot <- fli_small %>%
  specify(day_hour ~ season, success = "morning") %>%
  generate(reps = 1000, type = "bootstrap") %>% 
  calculate(stat = "z", order = c("summer", "winter"))
( percentile_ci <- get_ci(boot) )
2.5%	97.5%
-1.479	2.501
visualize(boot) +
  shade_confidence_interval(endpoints = percentile_ci)

( standard_error_ci <- get_ci(boot, type = "se", point_estimate = z_hat) )
lower	upper
-1.522	2.443
visualize(boot) +
  shade_confidence_interval(endpoints = standard_error_ci)

Two numerical vars - SLR
Point estimate

( slope_hat <- fli_small %>% 
  specify(arr_delay ~ dep_delay) %>%
  calculate(stat = "slope") )
stat
1.017
boot <- fli_small %>%
   specify(arr_delay ~ dep_delay) %>% 
   generate(reps = 1000, type = "bootstrap") %>%
   calculate(stat = "slope")
( percentile_ci <- get_ci(boot) )
2.5%	97.5%
0.9728	1.074
visualize(boot) +
  shade_confidence_interval(endpoints = percentile_ci)

( standard_error_ci <- get_ci(boot, type = "se", point_estimate = slope_hat) )
lower	upper
0.9653	1.069
visualize(boot) +
  shade_confidence_interval(endpoints = standard_error_ci)

Two numerical vars - correlation
Point estimate

( correlation_hat <- fli_small %>% 
  specify(arr_delay ~ dep_delay) %>%
  calculate(stat = "correlation") )
stat
0.8943
boot <- fli_small %>%
   specify(arr_delay ~ dep_delay) %>% 
   generate(reps = 1000, type = "bootstrap") %>%
   calculate(stat = "correlation")
( percentile_ci <- get_ci(boot) )
2.5%	97.5%
0.8502	0.9218
visualize(boot) +
  shade_confidence_interval(endpoints = percentile_ci)

( standard_error_ci <- get_ci(boot, type = "se", 
                            point_estimate = correlation_hat) )
lower	upper
0.858	0.9306
visualize(boot) +
  shade_confidence_interval(endpoints = standard_error_ci)

Two numerical vars - t
Not currently implemented since t could refer to standardized slope or standardized correlation.




<!--chapter:end:HypothesisTesting.Rmd-->

---
title: "Keras"
---


https://keras.io/






<!--chapter:end:keras.Rmd-->

---
title: "K Means Clustering"
---


https://datascienceplus.com/k-means-clustering-in-r/





<!--chapter:end:KMeansClustering.Rmd-->

---
title: "lessR"
---

lessRstats.com/lessR.r
tour of lessR functions for data analysis

Purpose: Provide basic statistical computations for the analyses presented in intro stat texts and more

Get R
http://r-project.org

one time only, to get the lessR functions onto your computer if asked to install into a personal library, say Yes

```{r eval=FALSE, include=FALSE, echo=TRUE}
install.packages("lessR")
```

Begin each R session by loading the lessR functions

```{r eval=FALSE, include=FALSE, echo=TRUE}
library(lessR)
```


help

```{r eval=FALSE, include=FALSE, echo=TRUE}
Help()   # list the lessR help values by subject
Help(lessR) #  access to the full lessR manual and News items
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
Help(Read)
```



read data from one of many different formats into a data table called mydata with the same Read statement: csv, tab-delimited text, Excel, SPSS, and SAS

Categorical: Gender coded M, F;  Dept coded ACCT, ADMN, FINC, MKTG, SALE Numeric: Salary and Years

`mydata <- Read()` or `{r # d()`, browse for text, Excel, SPSS, SAS, or R data file

```{r eval=FALSE, include=FALSE, echo=TRUE}
head(mydata) #  R function, see variable names and first 6 rows of data
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
mydata <- Read("http://lessRstats.com/data/employee.csv")   # read data from web
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
mydata <- Read("http://lessRstats.com/data/employee.csv", row.names=1) # row names
```


data included as part of lessR

```{r eval=FALSE, include=FALSE, echo=TRUE}
mydata <- rd("Employee", format="lessR")  # this example includes variable labels
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
details() #  provides full details of the data frame, defaults to mydata
```







```{r eval=FALSE, include=FALSE, echo=TRUE}




db()   brief version, same as provided by Read

 can also read a text file o fixed width formatted data
 example provided later

 distribution of a categorical variable
 ------------------------------------------------------------------------
 all the same
BarChart(Dept)    or bc(Dept)
bc(Dept)    all function calls have a 2 or 3 digit abbreviation
BarChart(Dept, data=mydata)    mydata is the default data frame (table)
bc(Dept, fill="colors")   specify a more colorful display
bc(Dept, rotate.x=45, offset=1)   value labels rotated and offset
 other summaries
PieChart(Dept)    or pc(Dept), provides a doughnut or ring chart
PieChart(Dept, hole=0)    full pie chart
Plot(Dept)   or Plot(Dept), 1-categorical variable bubble plot
SummaryStats(Dept)   or ss(Dept), no graphics

 read Mach IV data, integer Likert responses from 0 to 5
mydata <- Read("Mach4", format="lessR")   read a data file included with lessR
 form a Bubble Plot Frequency Matrix (BPFM) from a range of x-variables
 each line is a bubble plot of frequencies for a single variable
Plot(c(m06,m07,m09,m10))
 for each bubble, lighten fill color, make border black
Plot(m06:m12, trans=.8, coloe="gray50")
 create BPFM for entire Mach IV scale and store as a pdf file
LikertCats <- c("Strongly Disagree", "Disagree", "Slightly Disagree",
                     "Slightly Agree", "Agree", "Strongly Agree")
Plot(m01:m20, value.labels=LikertCats, pdf.file="MachFreqs.pdf")
Plot(m06)   Integrated Violin/Box/Scatterplot (VBS plot)

 back to the Employee data
mydata <- rd("Employee", format="lessR")   includes variable labels


 distribution of a numeric (continuous) variable
 ------------------------------------------------------------------------
Plot(Salary)    Integrated Violin/Box/Scatterplot (VBS plot)
Plot(Salary, by1=Gender)    Trellis VBS plots for Gender
Histogram(Salary)    or hs(Salary)
hs(Salary, bin.start=30000, bin.width=12000, bin.end=150000)   common options
Density(Salary)   or dn(Salary)
BoxPlot(Salary)   or bx(Salary), a call to Plot with vbs.plot="b"
SummaryStats(Salary)   or ss(Salary)
ss.brief(Salary)   brief output
hs(Salary, Rmd="Salary")   generate an R markdown file of all the methods
hs(Salry)    ERROR, misspelled variable name to show lessR error messages

 for ordered values, such as by time (just for illustration here)
LineChart(Salary)   or lc(Salary)
Plot(Salary, run=TRUE)
Plot(Salary, run=TRUE, center.line="off", show.runs=TRUE)
Plot(Salary, run=TRUE, lwd=0)   points only
Plot(Salary, run=TRUE, size=0)   line only
Plot(Salary, run=TRUE, area=TRUE)

 distributions for multiple variables (writes to pdf files)
 ------------------------------------------------------------------------
 the corresponding standard R functions do not take multiple variables
 specify multiple variables with c function or :
Histogram(c(Salary, Years))   c is R combine function
BarChart(Gender:Satisfaction)   : indicates a range of variables in R
Histogram()  a histogram for each numeric variable in mydata data table
CountAll()   or ca(), a bar chart or histogram for each variable in mydata

 relationship of two variables
 ------------------------------------------------------------------------
 two numeric variables yields traditional scatter plot
Plot(Years, Salary)    or Plot(Years, Salary)
Plot(Years, Salary, auto=TRUE)    or Plot(Years, Salary)
Plot(Years, Salary, ellipse=TRUE, fit="loess")    common options
Plot(Years, Salary, ellipse=c(.50,.75,.90))    3 data ellipses
Plot(Years, Salary, size=3)  bubbles

 classify by a 3rd variable, which is categorical
Plot(Years, Salary, by=Gender)
Plot(Years, Salary, by=Gender, color=c("green", "brown"))
Plot(Years, Salary, by=Gender, color=c("darkgreen", "brown"), shape=c("F","M"))
Plot(Years, Salary, by=Gender, color=c("darkgreen", "brown"), fit="ls")

 categorical with a numeric variable, shows scatter about each group mean
Plot(Dept, Salary)   Dept is categorical, Salary is numeric
SummaryStats(Salary, by=Dept)    stats, for each level of a grouping variable
ss.brief(Salary, by=Dept)    brief output

 two categorical variables (factors)
BarChart(Gender, by=Dept)   text from ss.brief
bc(Gender, by=Dept, beside=TRUE, horiz=TRUE)   common options
Plot(Gender, Dept)   scatter (bubble) plot in place of traditional bar chart
SummaryStats(Gender, Dept)   4 cross-tab tables
bc(Dept, by=Gender, beside=TRUE)
 the proportion of data values by fill variable within each group
bc(Dept, by=Gender, proportion=TRUE)

Plot(Dept, Salary)
 Plot refers to what is plotted in the scatter plot as the values
 Plot default for values is "data"
Plot(Dept, Salary, values="mean")
Plot(c(Pre, Post), Salary)
Plot(c(Pre, Post), Salary, fit="ls")



 Cleveland dot plot test
Plot(Salary, row.names)
Plot(c(Pre, Post), row.names)
Plot(row.names, Salary, means=FALSE, rotate.x=60, offset=1, xlab="")
Plot(Salary, row.names, fill="black", color="black", sort.y=TRUE)
style(panel.fill="off", grid.color="off")
Plot(Salary, row.names, sort.y=TRUE, segments.y=TRUE)
style()   return to default style


 set system values
 ------------------------------------------------------------------------
 color themes
Help(theme)
style("orange", sub.theme="black")
hs(Salary)
Plot(Years, Salary, ellipse=TRUE, fit="loess")    common options
style("darkred")
hs(Salary)
style("gray")
hs(Salary)
Plot(Years, Salary, ellipse=TRUE, fit="loess")    common options

 more system settings, can also be set by individual function calls
style(brief=TRUE)   partial text output
style(quiet=TRUE)   no text output
style(n.cat=6)   treat integer variables with <=6 unique values as categorical
style(brief=FALSE)  back to default
style(quiet=FALSE)
style(n.cat=0)


 analysis of a mean
 ------------------------------------------------------------------------
ttest(Salary)   or tt(Salary) for confidence interval
tt.brief(Salary, mu0=70000)   brief output, for CI and hypothesis test
tt.brief(n=37, m=63795.557, s=21799.533)   input summary stats instead of data
ttestPower(n=20, s=5)


 compare 2 groups, with graphics
 ------------------------------------------------------------------------
ttest(Salary ~ Gender)    or tt(Salary ~ Gender), compare means for two groups
tt(Gender ~ Salary)    ERROR, do it wrong to illustrate lessR error messages
tt.brief(Salary ~ Gender)    brief output
tt(n1=18, n2=19, m1=71147.458, m2=56830.598, s1=23128.436, s2=18438.456)  from stats
ttestPower(n1=14, n2=27, s1=4, s2=6, msmd=.5)

 Pre and Post are variables that are two matched sets of data values
ttest(Pre, Post)   t-test to compare group means, data entered as two vectors
ttest(Pre, Post, paired=TRUE)   t-test of paired differences


 analysis of variance, compare 2 or more groups
 ------------------------------------------------------------------------
 warpbreaks is a data set contained in R
ANOVA(breaks ~ tension, data=warpbreaks)   or av, one-way ANOVA

 two-factor between-groups ANOVA with replications and interaction
ANOVA(breaks ~ wool * tension, data=warpbreaks)

 randomized blocks design with the second term the blocking factor
ANOVA(breaks ~ wool + tension, data=warpbreaks)


 regression analysis
 ------------------------------------------------------------------------
 single predictor variable regression
reg(Salary ~ Years)   or reg, or reg.brief, equivalent of Excel regression
reg.brief(Salary ~ Years)   reg.brief equivalent of Excel regression
reg(Salary ~ 1)   null model
reg.brief(Salary ~ 1)   null model

 multiple regression
reg(Salary ~ Years + Pre + Post)   multiple reg

 output to an object
r <- reg(Salary ~ Years + Pre + Post)   multiple reg, save output to object r
r    see all the output
names(r)   see the output segments available for viewing or further analysis
r$out_estimates   see just the piece of the estimates

 create R markdown file that does full interpretation of the output
r <- reg(Salary ~ Years + Pre + Post, Rmd="MultReg")   R markdown

 nested models analysis
Nest(Salary, c(Years), c(Pre, Post))  compare reduced to full model, with common data

 logit analysis, with a classification table
Logit(Gender ~ Salary)


 transformations
 ------------------------------------------------------------------------
 use Recode to reverse score four Likert variables: m01, m02, m03, m10
mydata <- Read("Mach4", format="lessR")   read a data file included with lessR
mydata <- Recode(c(m01:m03,m10), old=0:5, new=5:0)   R has no recode function

mydata <- rd("Employee", format="lessR", quiet=TRUE)   internal data, no console output
mydata <- Transform(Salary = Salary / 1000)   transform by formula
males <- Subset(Gender=="M", columns=c(Years, Salary))   only Males, two vars
mydata <- Sort(Salary)   default from smallest to largest
mydata <- Sort(Salary, direction="-")   can also specify directiont
mydata <- Sort(row.names)   unlike standard R, can sort by row name
 also Merge for vertical or horizontal merge of two data tables


 variable labels
 ------------------------------------------------------------------------
 read data file w/o variable labels, and then separately read labels
mydata <- Read("http://lessRstats.com/data/employee.csv")
mylabels <- VariableLabels("http://lessRstats.com/data/employee_lbl.csv")

 read data file w/o variable labels, and then read labels from console
mydata <- Read("http://lessRstats.com/data/employee.csv")
lbl <- "
Years, time of company employment
Gender, Male or Female
Dept, department employed
Salary, annual salary
Satisfaction, satisfaction with work environment
HealthPlan, 1=GoodHealth 2=YellowCross 3=BestCare
Pre, Test score on legal issues before instruction
Post, Test score on legal issues after instruction
"
mylabels <- VariableLabels(lbl)

 modify/display a single variable label
mylabels <- VariableLabels(Salary, "Annual Salary (USD)")  add/modify 1 label
VariableLabels(Salary)   list the contents of a single variable label

 display all variable labels
db()   also the variable names and sample values
vl()


 write a data table to an external file
 ------------------------------------------------------------------------
Write("myfile")   write default mydata data table to myfile in csv format
Write("myfile", row.names=FALSE)   preferred if row names are only row numbers
Write("myfile", format="Excel", row.names=FALSE)   to Excel data table

 after reading original data and doing the transformations, save as is
Write("myfile", format="R")   native R format, straight copy of mydata


 correlation matrices and factor analysis
 ------------------------------------------------------------------------
mydata <- Read("Mach4", format="lessR")   read a data file included with lessR
 calculate the correlations and store in mycor
head(mydata)
mydata <- Recode(c(m03, m04, m06, m07, m09:m11, m14, m16, m17, m19), old=0:5, new=5:0)
mycor <- cr(m01:m20)
efa(n.factors=4)
 confirmatory factor analysis of 4-factor solution of Mach IV scale
 Hunter, Gerbing and Boster (1982)
MeasModel <-
  "
Deceit =~ m07 + m06 + m10 + m09
Trust =~ m12 + m05 + m13 + m01
Cynicism =~ m11 + m16 + m04
Flattery =~ m15 + m02
"
c <- cfa(MeasModel)
 view all the output
c
 names of each output segment
names(c)
 view just the scale reliabilities
c$out_reliability

 correlation matrix operations
mycor <- cr(m01:m20, graphics=TRUE)
mydata <- corReorder()    simple re-order algorithm by highest remaining cor
prop()
scree()


 read fixed width data, requires parameters: widths, color.names
 ------------------------------------------------------------------------
rep(1,20)   standard R function, here a 1 listed 20 times
to("m",20)   lessR, names sequential vars with same widths [unlike paste0("m", 1:20)]
 specify the width of each data column, then match with the column name
 the Mach4 scale is 6-pt Likert data, so each response is a single column
mydata <- Read("http://lessRstats.com/data/Mach4Plus.fwd",
               widths=c(4,2,1,5,1,rep(1,20)),
               col.names=c("ID", "Age", "Gender", "Code", "Form", to("m",20)))


```


<!--chapter:end:lessR.Rmd-->

---
title: "Linear Regression"
---

https://www.datacamp.com/community/tutorials/linear-regression-R

```{r, include=FALSE}
library(readxl)
ageandheight <- read_excel("data/ageandheight.xls", sheet = "Hoja2") 
lmHeight = lm(height~age, data = ageandheight)
summary(lmHeight)
```

```{r eval=FALSE, include=FALSE, echo=TRUE}
lmHeight2 = lm(height~age + no_siblings, data = ageandheight)
summary(lmHeight2)
```

```{r eval=FALSE, include=FALSE, echo=TRUE}
lmHeight2$coefficients
```

```{r eval=FALSE, include=FALSE, echo=TRUE}
lmHeight2$residuals
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
pressure <- read_excel("data/pressure.xlsx")
lmTemp <- lm(Pressure~Temperature, data = pressure)
summary(lmTemp)
plot(pressure, pch = 16, col = "blue")
abline(lmTemp)
```

```{r eval=FALSE, include=FALSE, echo=TRUE}
plot(lmTemp$residuals, pch = 16, col = "red")
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
lmTemp2 = lm(Pressure~Temperature + I(Temperature^2), data = pressure) #Create a linear regression with a quadratic coefficient
summary(lmTemp2) #Review the results
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
plot(lmTemp2$residuals, pch = 16, col = "red")
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
ageandheight[2, 2] = 7.7
head(ageandheight)
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
lmHeight3 = lm(height~age, data = ageandheight)#Create the linear regression
summary(lmHeight3)#Review the results
plot(cooks.distance(lmHeight3), pch = 16, col = "blue") #Plot the Cooks Distances.
```



```{r eval=FALSE, include=FALSE, echo=TRUE}
plot(cooks.distance(lmHeight), pch = 16, col = "blue")
```



<!--chapter:end:LinearRegression.Rmd-->

---
title: "Machine Learning"
---


# Code for Workshop: Introduction to Machine Learning with R


https://shirinsplayground.netlify.com/2018/06/intro_to_ml_workshop_heidelberg/




<!--chapter:end:MachineLearning.Rmd-->

---
title: R ile analize başlarken^[Bu bir derlemedir, mümkün mertebe alıntılara referans
  vermeye çalıştım.]
---





```
{r , echo=TRUE, cache=FALSE}
library(knitr)
library(rmdformats)

## Global options
options(max.print="75")
opts_chunk$set(echo=TRUE,
	             cache=TRUE,
               prompt=FALSE,
               tidy=TRUE,
               comment=NA,
               message=FALSE,
               warning=FALSE)
opts_knit$set(width=75)
```


<!-- Open all links in new tab-->  
<!-- <base target="_blank"/>    -->


<!-- Go to www.addthis.com/dashboard to customize your tools --> <script type="text/javascript" src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5bc36900a405090b">  
</script> 

# R hakkında


[![](http://res.cloudinary.com/dyd911kmh/image/upload/f_auto,q_auto:best/v1530113077/Image_2_vfy48b.png)](https://www.datacamp.com/community/tutorials/data-science-pitfalls)


- R generation

https://rss.onlinelibrary.wiley.com/doi/10.1111/j.1740-9713.2018.01169.x


# R yükleme

http://www.youtube.com/watch?v=XcBLEVknqvY

[![What is R?](http://img.youtube.com/vi/XcBLEVknqvY/0.jpg)](http://www.youtube.com/watch?v=XcBLEVknqvY)


## R-project

https://cran.r-project.org/

---

[![](https://ismayc.github.io/talks/ness-infer/img/engine.png)](https://ismayc.github.io/talks/ness-infer/slide_deck.html#6)

---

## RStudio

https://www.rstudio.com/

https://www.rstudio.com/products/rstudio/download/

https://moderndive.com/2-getting-started.html

---

[![](http://www-users.york.ac.uk/~er13/RStudio%20Anatomy.svg)](https://buzzrbeeline.blog/2018/07/04/rstudio-anatomy/)



---

### RStudio eklentileri

- Discover and install useful RStudio addins

https://cran.r-project.org/web/packages/addinslist/README.html

https://rstudio.github.io/rstudioaddins/


```{r eval=FALSE, include=FALSE, echo=TRUE}
# devtools::install_github("rstudio/addinexamples", type = "source")
```


---

## X11

https://www.xquartz.org/

---

## Java OS

https://support.apple.com/kb/dl1572

---


# R zor şeyler için kolay, kolay şeyler için zor


- [R makes easy things hard, and hard things easy](http://r4stats.com/articles/why-r-is-hard-to-learn/)


- Aynı şeyi çok fazla şekilde yapmak mümkün

R Syntax Comparison::CHEAT SHEET

https://www.amelia.mn/Syntax-cheatsheet.pdf



---


# R paketleri


## Neden paketler var

[![](https://ismayc.github.io/talks/ness-infer/img/appstore.png)](https://ismayc.github.io/talks/ness-infer/slide_deck.html#7)

---

<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script><blockquote class="twitter-tweet" data-lang="en"><p lang="en" dir="ltr">I love the <a href="https://twitter.com/hashtag/rstats?src=hash&amp;ref_src=twsrc%5Etfw">#rstats</a> community.<br>Someone is like, &quot;oh hey peeps, I saw a big need for this mundane but difficult task that I infrequently do, so I created a package that will literally scrape the last bits of peanut butter out of the jar for you. It&#39;s called pbplyr.&quot;<br>What a tribe.</p>&mdash; Frank Elavsky ᴰᵃᵗᵃ ᵂᶦᶻᵃʳᵈ (@Frankly_Data) <a href="https://twitter.com/Frankly_Data/status/1014189095294291968?ref_src=twsrc%5Etfw">July 3, 2018</a></blockquote>

---



https://blog.mitchelloharawild.com/blog/user-2018-feature-wall/

---

![](https://blog.mitchelloharawild.com/blog/2018-07-11-user-2018-feature-wall_files/final.jpg)

---

## Paketleri nereden bulabiliriz

- Available CRAN Packages By Name  
https://cran.r-project.org/web/packages/available_packages_by_name.html

- Bioconductor  
https://www.bioconductor.org

- RecommendR  
http://recommendr.info/

- pkgsearch  
CRAN package search  
https://github.com/metacran/pkgsearch

- Awesome R  
https://awesome-r.com/  


## Kendi paket evrenini oluştur

- pkgverse: Build a Meta-Package Universe  
https://cran.r-project.org/web/packages/pkgverse/index.html



---

## R için yardım bulma


```
# ?mean
# ??efetch
# help(merge)
# example(merge)
```



- Vignette

![](figures/vignette.png)

---

- RDocumentation
https://www.rdocumentation.org

- R Package Documentation
https://rdrr.io/

- GitHub

- Stackoverflow

https://stackoverflow.com/

- Google uygun anahtar kelime



<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script><blockquote class="twitter-tweet" data-lang="en"><p lang="en" dir="ltr">How I use <a href="https://twitter.com/hashtag/rstats?src=hash&amp;ref_src=twsrc%5Etfw">#rstats</a> <br>h/t <a href="https://twitter.com/ThePracticalDev?ref_src=twsrc%5Etfw">@ThePracticalDev</a> <a href="https://t.co/erRnTG0Ujr">pic.twitter.com/erRnTG0Ujr</a></p>&mdash; Emily Bovee (@ebovee09) <a href="https://twitter.com/ebovee09/status/1028037594947485696?ref_src=twsrc%5Etfw">August 10, 2018</a></blockquote>


---


![](figures/Google-package-name.png)

---



![](figures/Google-start-with-R.png)

---

- Awesome Cheatsheet
https://github.com/detailyang/awesome-cheatsheet

http://cran.r-project.org/doc/contrib/Baggott-refcard-v2.pdf

https://www.rstudio.com/resources/cheatsheets/


- Awesome R

https://github.com/qinwf/awesome-R#readme

https://awesome-r.com/




- Twitter

https://twitter.com/hashtag/rstats?src=hash


- Reproducible Examples

<blockquote class="twitter-tweet" data-lang="en"><p lang="en" dir="ltr">Got a question to ask on <a href="https://twitter.com/SlackHQ?ref_src=twsrc%5Etfw">@SlackHQ</a> or post on <a href="https://twitter.com/github?ref_src=twsrc%5Etfw">@github</a>? No time to read the long post on how to use reprex? Here is a 20-second gif for you to format your R codes nicely and for others to reproduce your problem. (An example from a talk given by <a href="https://twitter.com/JennyBryan?ref_src=twsrc%5Etfw">@JennyBryan</a>) <a href="https://twitter.com/hashtag/rstat?src=hash&amp;ref_src=twsrc%5Etfw">#rstat</a> <a href="https://t.co/gpuGXpFIsX">pic.twitter.com/gpuGXpFIsX</a></p>&mdash; ZhiYang (@zhiiiyang) <a href="https://twitter.com/zhiiiyang/status/1053006003711569920?ref_src=twsrc%5Etfw">October 18, 2018</a></blockquote><script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>





---

## R paket yükleme

```
install.packages("tidyverse", dependencies = TRUE)
install.packages("jmv", dependencies = TRUE)
install.packages("questionr", dependencies = TRUE)
install.packages("Rcmdr", dependencies = TRUE)
install.packages("summarytools")
```

```{r}
# install.packages("tidyverse", dependencies = TRUE)
# install.packages("jmv", dependencies = TRUE)
# install.packages("questionr", dependencies = TRUE)
# install.packages("Rcmdr", dependencies = TRUE)
# install.packages("summarytools")
```


```{r, error=FALSE, message = FALSE, warning = FALSE, eval = TRUE, include = TRUE}
# require(tidyverse)
# require(jmv)
# require(questionr)
# library(summarytools)
# library(gganimate)
```

---

# R studio ile proje oluşturma

https://support.rstudio.com/hc/en-us/articles/200526207-Using-Projects

![](http://www.rstudio.com/images/docs/projects_new.png)

---

# RStudio ile veri yükleme

https://support.rstudio.com/hc/en-us/articles/218611977-Importing-Data-with-RStudio

![](https://support.rstudio.com/hc/en-us/article_attachments/206277618/data-import-overview.gif)

---

## Excel

## SPSS

## csv


---

# Veriyi görüntüleme

<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script><blockquote class="twitter-tweet" data-lang="en"><p lang="en" dir="ltr">Spreadsheet users using <a href="https://twitter.com/hashtag/rstats?src=hash&amp;ref_src=twsrc%5Etfw">#rstats</a>:  where&#39;s the data?<a href="https://twitter.com/hashtag/rstats?src=hash&amp;ref_src=twsrc%5Etfw">#rstats</a> users using spreadsheets:  where&#39;s the code?</p>&mdash; Leonard Kiefer (@lenkiefer) <a href="https://twitter.com/lenkiefer/status/1015587475580956672?ref_src=twsrc%5Etfw">July 7, 2018</a></blockquote>



```{r, results="markup"}
# library(nycflights13)
# summary(flights)
```



```
View(data)
```


```
data
```


```
head
```


```
tail
```


```
glimpse
```


```
str
```


```
skimr::skim()
```

---


# Veriyi değiştirme

## Veriyi kod ile değiştirelim

## Veriyi eklentilerle değiştirme

![](figures/change_data.png)


---


## RStudio aracılığıyla recode

*questionr* paketi kullanılacak

![](figures/level_recode.png)


---



https://juba.github.io/questionr/articles/recoding_addins.html


![](https://raw.githubusercontent.com/juba/questionr/master/resources/screenshots/irec_1.png)


---

![](https://raw.githubusercontent.com/juba/questionr/master/resources/screenshots/irec_2.png)


---

![](https://raw.githubusercontent.com/juba/questionr/master/resources/screenshots/irec_3.png)


---

# Basit tanımlayıcı istatistikler

```
summary()
```

```
mean
```

```
median
```

```
min
```

```
max
```

```
sd
```

```
table()
```


```{r, echo=TRUE, include = TRUE}
library(readr)
irisdata <- read_csv("data/iris.csv")

jmv::descriptives(
    data = irisdata,
    vars = "Sepal.Length",
    splitBy = "Species",
    freq = TRUE,
    hist = TRUE,
    dens = TRUE,
    bar = TRUE,
    box = TRUE,
    violin = TRUE,
    dot = TRUE,
    mode = TRUE,
    sum = TRUE,
    sd = TRUE,
    variance = TRUE,
    range = TRUE,
    se = TRUE,
    skew = TRUE,
    kurt = TRUE,
    quart = TRUE,
    pcEqGr = TRUE)
```

---

```{r, echo=TRUE, include=FALSE}
# install.packages("scatr")

scatr::scat(
    data = irisdata,
    x = "Sepal.Length",
    y = "Sepal.Width",
    group = "Species",
    marg = "dens",
    line = "linear",
    se = TRUE)

```

## summarytools

https://cran.r-project.org/web/packages/summarytools/vignettes/Introduction.html

```{r, include=FALSE, comment=NA, prompt=FALSE, cache=FALSE, echo=TRUE, results='asis'}
library(summarytools)
summarytools::freq(iris$Species, style = "rmarkdown")
```

```{r, include=FALSE, comment=NA, prompt=FALSE, cache=FALSE, echo=TRUE, results='asis'}
summarytools::freq(iris$Species, report.nas = FALSE, style = "rmarkdown", headings = TRUE)
```


```{r, include=FALSE, comment=NA, prompt=FALSE, cache=FALSE, echo=TRUE, results='asis'}
with(tobacco, print(ctable(smoker, diseased), method = 'render'))
```


```{r, include=FALSE, comment=NA, prompt=FALSE, cache=FALSE, echo=TRUE, results='asis'}
with(tobacco, 
     print(ctable(smoker, diseased, prop = 'n', totals = FALSE), 
           headings = TRUE, method = "render"))
```



```{r, include=FALSE, comment=NA, prompt=FALSE, cache=FALSE, echo=TRUE, results='asis'}
summarytools::descr(iris, style = "rmarkdown")
```



```{r, include=FALSE, comment=NA, prompt=FALSE, cache=FALSE, echo=TRUE, results='asis'}
descr(iris, stats = c("mean", "sd", "min", "med", "max"), transpose = TRUE, 
      headings = TRUE, style = "rmarkdown")
```



```{r, include=FALSE, comment=NA, prompt=FALSE, cache=FALSE, echo=TRUE, results='asis'}
# view(dfSummary(iris))

```


![](figures/dfsummary.png)



```{r, include=FALSE, comment=NA, prompt=FALSE, cache=FALSE, echo=TRUE, results='asis'}
dfSummary(tobacco, plain.ascii = FALSE, style = "grid")
```


```{r, include=FALSE, comment=NA, prompt=FALSE, cache=FALSE, echo=TRUE, results='asis'}

# First save the results

iris_stats_by_species <- by(data = iris, 
                            INDICES = iris$Species, 
                            FUN = descr, stats = c("mean", "sd", "min", "med", "max"), 
                            transpose = TRUE)

# Then use view(), like so:

view(iris_stats_by_species, method = "pander", style = "rmarkdown")
```

```{r, include=FALSE, comment=NA, prompt=FALSE, cache=FALSE, echo=TRUE, results='asis'}
# view(iris_stats_by_species)
```

![](figures/DescriptiveStatistics.png)

---

```{r, include=FALSE, comment=NA, prompt=FALSE, cache=FALSE, echo=TRUE, results='asis'}
data(tobacco) # tobacco is an example dataframe included in the package
BMI_by_age <- with(tobacco, 
                   by(BMI, age.gr, descr, 
                      stats = c("mean", "sd", "min", "med", "max")))
view(BMI_by_age, "pander", style = "rmarkdown")
```

---

```{r, include=FALSE, comment=NA, prompt=FALSE, cache=FALSE, echo=TRUE, results='asis'}
BMI_by_age <- with(tobacco, 
                   by(BMI, age.gr, descr,  transpose = TRUE,
                      stats = c("mean", "sd", "min", "med", "max")))

view(BMI_by_age, "pander", style = "rmarkdown", headings = TRUE)
```

---

```{r, include=FALSE, comment=NA, prompt=FALSE, cache=FALSE, echo=TRUE, results='asis'}
tobacco_subset <- tobacco[ ,c("gender", "age.gr", "smoker")]
freq_tables <- lapply(tobacco_subset, freq)

# view(freq_tables, footnote = NA, file = 'freq-tables.html')
```

---

```{r, include=FALSE, comment=NA, prompt=FALSE, cache=FALSE, echo=TRUE, results='asis'}
what.is(iris)
```

---

```{r eval=FALSE, include=FALSE, echo=TRUE}
freq(tobacco$gender, style = 'rmarkdown')
```

---

```{r eval=FALSE, include=FALSE, echo=TRUE}
print(freq(tobacco$gender), method = 'render')
```

---

## skimr

```
library(skimr)
skim(df)
```

---

## DataExplorer

```
library(DataExplorer)
DataExplorer::create_report(df)
```


[![](https://static1.squarespace.com/static/58eef8846a4963e429687a4d/t/5bdfc2fb4d7a9c04ee50b7aa/1541391160702/dataExplorerGifLg.gif?format=1500w)](https://www.littlemissdata.com/blog/simple-eda)



---

## Grafikler

```{r eval=FALSE, include=FALSE, echo=TRUE}
# library(ggplot2)
# library(mosaic)
# mPlot(irisdata)
```

---

```{r eval=FALSE, include=FALSE, echo=TRUE}
ctable(tobacco$gender, tobacco$smoker, style = 'rmarkdown')
```

---

```{r eval=FALSE, include=FALSE, echo=TRUE}
print(ctable(tobacco$gender, tobacco$smoker), method = 'render')
```

```
descr(tobacco, style = 'rmarkdown')

print(descr(tobacco), method = 'render', table.classes = 'st-small')

dfSummary(tobacco, style = 'grid', plain.ascii = FALSE)

print(dfSummary(tobacco, graph.magnif = 0.75), method = 'render')
```


---



<blockquote class="twitter-tweet" data-lang="en"><p lang="en" dir="ltr">Here, building up a <a href="https://twitter.com/hashtag/ggplot2?src=hash&amp;ref_src=twsrc%5Etfw">#ggplot2</a> as slowly as possible, <a href="https://twitter.com/hashtag/rstats?src=hash&amp;ref_src=twsrc%5Etfw">#rstats</a>.  Incremental adjustments.  <a href="https://twitter.com/hashtag/rstatsteachingideas?src=hash&amp;ref_src=twsrc%5Etfw">#rstatsteachingideas</a> <a href="https://t.co/nUulQl8bPh">pic.twitter.com/nUulQl8bPh</a></p>&mdash; Gina Reynolds (@EvaMaeRey) <a href="https://twitter.com/EvaMaeRey/status/1029104656763572226?ref_src=twsrc%5Etfw">August 13, 2018</a></blockquote><script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>


---


[![](https://raw.githubusercontent.com/dreamRs/esquisse/master/man/figures/esquisse.gif)](https://github.com/dreamRs/esquisse)


<blockquote class="twitter-tweet" data-lang="en"><p lang="en" dir="ltr">Dreaming of a fancy <a href="https://twitter.com/hashtag/Rstats?src=hash&amp;ref_src=twsrc%5Etfw">#Rstats</a> <a href="https://twitter.com/hashtag/ggplot?src=hash&amp;ref_src=twsrc%5Etfw">#ggplot</a> <a href="https://twitter.com/hashtag/dataviz?src=hash&amp;ref_src=twsrc%5Etfw">#dataviz</a> but still scared of typing <a href="https://twitter.com/hashtag/code?src=hash&amp;ref_src=twsrc%5Etfw">#code</a>? <a href="https://twitter.com/_pvictorr?ref_src=twsrc%5Etfw">@_pvictorr</a> esquisse package has you covered <a href="https://t.co/1vIDXcVAAF">https://t.co/1vIDXcVAAF</a> <a href="https://t.co/RlTkptnrNv">pic.twitter.com/RlTkptnrNv</a></p>&mdash; Radoslaw Panczak (@RPanczak) <a href="https://twitter.com/RPanczak/status/1047019588658040832?ref_src=twsrc%5Etfw">October 2, 2018</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>










---

# Rcmdr

```
library(Rcmdr)

Rcmdr::Commander()

```


- A Comparative Review of the R Commander GUI for R

http://r4stats.com/articles/software-reviews/r-commander/


---

# jamovi

https://www.jamovi.org/

![![](https://www.jamovi.org/assets/main-screenshot.png)](https://www.jamovi.org/)


https://blog.jamovi.org/2018/07/30/rj.html

![![](https://blog.jamovi.org/assets/images/rj.png)](https://blog.jamovi.org/2018/07/30/rj.html)

---

# Sonraki Konular

- RStudio ile GitHub
- Hipotez testleri
- R Markdown ve R Notebook ile tekrarlanabilir rapor


---

# Diğer kodlar

- Diğer kodlar için bakınız: [https://sbalci.github.io/](https://sbalci.github.io/)


---

# Geri Bildirim

- Geri bildirim için tıklayınız: _[Geri bildirim formu](https://goo.gl/forms/YjGZ5DHgtPlR1RnB3)_


---

<script id="dsq-count-scr" src="//https-sbalci-github-io.disqus.com/count.js" async></script>

<div id="disqus_thread"></div>
<script>

/**
*  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
*  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/
/*
var disqus_config = function () {
this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};
*/
(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');
s.src = 'https://https-sbalci-github-io.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>

---






<!--chapter:end:material.Rmd-->

---
title: "21 Recipes for Mining Twitter Data with rtweet"
---

# 21 Recipes for Mining Twitter Data with rtweet

https://rud.is/books/21-recipes/

http://rtweet.info/index.html


```{r eval=FALSE, include=FALSE, echo=TRUE}
# devtools::install_github("mkearney/rtweet")
library(rtweet)
library(tidyverse)
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
(trends_avail <- trends_available())

```


```{r eval=FALSE, include=FALSE, echo=TRUE}
(us <- get_trends("united states"))
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
(tr <- get_trends("turkey"))
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
library(DBI)
library(RSQLite)
library(rtweet) # mkearney/rtweet

repeat {
  message("Retrieveing trends...") # optional
  us <- get_trends("united states")
  db_con <- dbConnect(RSQLite::SQLite(), "data/us-trends.db")
  dbWriteTable(db_con, "us_trends", us, append=TRUE) # append=TRUE will update the table vs overwrite and also create it on first run if it does not exist
  dbDisconnect(db_con)
  Sys.sleep(10 * 60) # sleep for 10 minutes
}
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
library(dplyr)

trends_db <- src_sqlite("data/us-trends.db")
us <- tbl(trends_db, "us_trends")
select(us, trend)
```



```{r eval=FALSE, include=FALSE, echo=TRUE}
(rstats <- search_tweets("#rstats", n=300)) # pull 300 tweets that used the "#rstats" hashtag

```


```{r eval=FALSE, include=FALSE, echo=TRUE}
glimpse(rstats)
```



```{r eval=FALSE, include=FALSE, echo=TRUE}
rstats %>% 
select(hashtags) %>% 
  unnest() %>% 
  mutate(hashtags = tolower(hashtags)) %>% 
  count(hashtags, sort=TRUE) %>% 
  filter(hashtags != "rstats") %>% 
  top_n(10)

```



```{r eval=FALSE, include=FALSE, echo=TRUE}
rstats <- search_tweets("#rstats -filter:retweets") %>%
  select(text)
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
rstats <- search_tweets("to:kearneymw") %>%
  select(text)
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
rstats <- search_tweets("#rstats url:github -#python") %>% 
  select(text)

```


```{r eval=FALSE, include=FALSE, echo=TRUE}
library(rtweet)
library(tidyverse)
rstats <- search_tweets("#rstats", n=500)

glimpse(rstats)

filter(rstats, retweet_count > 0) %>% 
  select(text, mentions_screen_name, retweet_count) %>% 
  mutate(text = substr(text, 1, 30)) %>% 
  unnest()
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
# regex mod from https://stackoverflow.com/questions/655903/python-regular-expression-for-retweets
filter(rstats, str_detect(text, "(RT|via)((?:[[:blank:]:]\\W*@\\w+)+)")) %>% 
  select(text, mentions_screen_name, retweet_count) %>% 
  mutate(extracted = str_match(text, "(RT|via)((?:[[:blank:]:]\\W*@\\w+)+)")[,3]) %>% 
  mutate(text = substr(text, 1, 30)) %>% 
  unnest()  
```




```{r eval=FALSE, include=FALSE, echo=TRUE}
library(rtweet)
library(igraph)
library(hrbrthemes)
library(tidyverse)
rstats <- search_tweets("#rstats", n=1500)

filter(rstats, retweet_count > 0) %>% 
  select(screen_name, mentions_screen_name) %>%
  unnest(mentions_screen_name) %>% 
  filter(!is.na(mentions_screen_name)) %>% 
  graph_from_data_frame() -> rt_g
  
summary(rt_g)
```



```{r eval=FALSE, include=FALSE, echo=TRUE}
ggplot(data_frame(y=degree_distribution(rt_g), x=1:length(y))) +
  geom_segment(aes(x, y, xend=x, yend=0), color="slateblue") +
  scale_y_continuous(expand=c(0,0), trans="sqrt") +
  labs(x="Degree", y="Density (sqrt scale)", title="#rstats Retweet Degree Distribution") +
  theme_ipsum_rc(grid="Y", axis="x")
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
V(rt_g)$node_label <- unname(ifelse(degree(rt_g)[V(rt_g)] > 20, names(V(rt_g)), "")) 
V(rt_g)$node_size <- unname(ifelse(degree(rt_g)[V(rt_g)] > 20, degree(rt_g), 0)) 
```



```{r eval=FALSE, include=FALSE, echo=TRUE}
library(ggraph)

ggraph(rt_g, layout = 'linear', circular = TRUE) + 
  geom_edge_arc(edge_width=0.125, aes(alpha=..index..)) +
  geom_node_label(aes(label=node_label, size=node_size),
                  label.size=0, fill="#ffffff66", segment.colour="springgreen",
                  color="slateblue", repel=TRUE, family=font_rc, fontface="bold") +
  coord_fixed() +
  scale_size_area(trans="sqrt") +
  labs(title="Retweet Relationships", subtitle="Most retweeted screen names labeled. Darkers edges == more retweets. Node size == larger degree") +
  theme_graph(base_family=font_rc) +
  theme(legend.position="none")
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
stream_tweets(
  lookup_coords("usa"), # handy helper function in rtweet
  verbose = FALSE,
  timeout = (60 * 1),
) -> usa
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
count(usa, place_full_name, sort=TRUE)
```



```{r eval=FALSE, include=FALSE, echo=TRUE}
unnest(usa, hashtags) %>% 
  count(hashtags, sort=TRUE) %>% 
  filter(!is.na(hashtags))
```




```{r eval=FALSE, include=FALSE, echo=TRUE}
count(usa, source, sort=TRUE)
```




```{r eval=FALSE, include=FALSE, echo=TRUE}
rtweet::write_as_csv()
```




```{r eval=FALSE, include=FALSE, echo=TRUE}
library(rtweet)
library(tidytext)
library(magick)
library(kumojars) # hrbrmstr/kumojars
library(kumo) # hrbrmstr/kumo
library(tidyverse)
```



scifi <- search_tweets("#NationalScienceFictionDay", n=1500)

data_frame(txt=str_replace_all(scifi$text, "#NationalScienceFictionDay", "")) %>% 
  unnest_tokens(word, txt) %>% 
  anti_join(stop_words, "word") %>% 
  anti_join(rtweet::stopwordslangs, "word") %>% 
  anti_join(data_frame(word=c("https", "t.co")), "word") %>% # need to make a more technical stopwords list or clean up the text better
  filter(nchar(word)>3) %>% 
  pull(word) %>% 
  paste0(collapse=" ") -> txt

cloud_img <- word_cloud(txt, width=800, height=500,  min_font_size=10, max_font_size=60, scale="log")

image_write(cloud_img, "data/wordcloud.png")




---



library(rtweet)
library(LSAfun)
library(jerichojars) # hrbrmstr/jerichojars
library(jericho) # hrbrmstr/jericho
library(tidyverse)
stiles <- get_timeline("stiles")

filter(stiles, str_detect(urls_expanded_url, "nyti|reut|wapo|lat\\.ms|53ei")) %>%  # only get tweets with news links
  pull(urls_expanded_url) %>% # extract the links
  flatten_chr() %>% # mush them into a nice character vector
  head(3) %>% # get the first 3
  map_chr(~{
    httr::GET(.x) %>% # get the URL (I'm lazily calling "fair use" here vs check robots.txt since I'm suggesting you do this for your benefit vs profit)
      httr::content(as="text") %>%  # extract the HTML
      jericho::html_to_text() %>% # strip away extraneous HTML tags
      LSAfun::genericSummary(k=3) %>% # summarise!
      paste0(collapse="\n\n") # easier to see
  }) %>%
  walk(cat)
  
  
---



library(rtweet)
library(tidyverse)
(brooke_followers <- rtweet::get_followers("gbwanderson"))


(brooke_friends <- rtweet::get_friends("gbwanderson"))










---


```{r eval=FALSE, include=FALSE, echo=TRUE}
library(rtweet)
library(tidyverse)
my_followers <- rtweet::get_followers("serdarbalci")
my_friends <- rtweet::get_friends("serdarbalci")
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
length(intersect(my_followers$user_id, my_friends$user_id))
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
length(setdiff(my_followers$user_id, my_friends$user_id))
```



```{r eval=FALSE, include=FALSE, echo=TRUE}
length(setdiff(my_friends$user_id, my_followers$user_id))
```



---


```{r eval=FALSE, include=FALSE, echo=TRUE}
rtweet::lookup_users(my_friends$user_id)
```





---

```{r eval=FALSE, include=FALSE, echo=TRUE}
library(rtweet)
library(hrbrthemes)
library(tidyverse)
influence_snapshot <- function(user, trans=c("log10", "identity")) {
  
  user <- user[1]
  trans <- match.arg(tolower(trimws(trans[1])), c("log10", "identity"))
  
  user_info <- lookup_users(user)
  
  user_followers <- get_followers(user_info$user_id)
  uf_details <- lookup_users(user_followers$user_id)
  
  primary_influence <- scales::comma(sum(c(uf_details$followers_count, user_info$followers_count)))
  
  filter(uf_details, followers_count > 0) %>% 
    ggplot(aes(followers_count)) +
    geom_density(aes(y=..count..), color="lightslategray", fill="lightslategray",
                 alpha=2/3, size=1) +
    scale_x_continuous(expand=c(0,0), trans="log10", labels=scales::comma) +
    scale_y_comma() +
    labs(
      x="Number of Followers of Followers (log scale)", 
      y="Number of Followers",
      title=sprintf("Follower chain distribution of %s (@%s)", user_info$name, user_info$screen_name),
      subtitle=sprintf("Follower count: %s; Primary influence/reach: %s", 
                       scales::comma(user_info$followers_count),
                       scales::comma(primary_influence))
    ) +
    theme_ipsum_rc(grid="XY") -> gg
  
  print(gg)
  
  return(invisible(list(user_info=user_info, follower_details=uf_details)))
  
}
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
juliasilge <- influence_snapshot("juliasilge")
```



---


```{r eval=FALSE, include=FALSE, echo=TRUE}
library(rtweet)
library(broom)
library(eechidna)
library(cartogram) # chxy/cartogram
library(hrbrthemes)
library(tidyverse)
# search twitter for tweets
rstats_us <- search_tweets("#rstats", 3000, geocode = "2.877742,-97.380979,3000mi") # geocode request isn't perfect but helps narrow down

# lookup each user (uniquely) so we can grab location information
user_info <- lookup_users(unique(rstats_us$user_id)) 
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
discard(user_info$location, `==`, "") %>% # ignore blank data
  str_match(sprintf("(%s)", paste0(state.abb, collapse="|"))) %>%  # try to match U.S. state abbreviations
  .[,2] %>% # the previous step creates a matrix with column 2 being the extracted information (if any)
  discard(is.na) %>%  # if no state match was found the value is NA so discard this one
  table() %>% # some habits are hard to break
  broom::tidy() %>% # but we can tidy them!
  set_names(c("state", "n")) %>% # these are more representative names
  tbl_df() %>% # not really necessary but I was printing this when testing
  arrange(desc(n)) %>% # same as ^^
  left_join(
    as_tibble(maps::state.carto.center) %>% # join state cartographic center data
      mutate(state=state.abb)
  ) -> for_dor 
# %>% 
  # the GitHub-only cartogram package nas a data structure which holds state adjacency information
  # by specifying that here, it will help make the force-directed cartogram circle positioning more precise (and pretty)
  # filter(state %in% names(cartogram::statenbrs)) -> for_dor 

glimpse(for_dor)
```




```{r eval=FALSE, include=FALSE, echo=TRUE}
par(family=font_rc, col="white")

eechidna:::dorling(
  for_dor$state, for_dor$x, for_dor$y, sqrt(for_dor$n),
  # nbr=cartogram::statenbrs,
  animation = FALSE, nbredge = TRUE, iteration=100, name.text=TRUE, dist.ratio=1.2,
  main="Dorling Cartogram of U.S. #rstats", xlab='', ylab='', col="lightslategray",
  frame=FALSE, asp=1, family=font_rc, cex.main=1.75, adj=0
) -> dor

par(family=font_rc, col="black")
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
library(rtweet)
library(ggmap)
library(tidyverse)
rstats_us <- search_tweets("#rstats", 300)

user_info <- lookup_users(unique(rstats_us$user_id))

discard(user_info$location, `==`, "") %>% 
  ggmap::geocode() -> coded

coded$location <- discard(user_info$location, `==`, "")

user_info <- left_join(user_info, coded, "location")
```




---


```{r eval=FALSE, include=FALSE, echo=TRUE}
library(rtweet)
library(tidyverse)
library(UpSetR)
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
# get a list of twitter handles you want to compare
rstaters <- c("dataandme", 
              "serdarbalci",
              "JennyBryan",
              "hrbrmstr",
              "xieyihui"
              # "drob", 
              # "juliasilge", 
              # "thomasp85"
              )

# scrape the user_id of all followers for each handle in the list and bind into 1 dataframe
followers <- rstaters %>%
  map_df(~ get_followers(.x, n = 200, retryonratelimit = TRUE) %>% 
           mutate(account = .x))

head(followers)

tail(followers)
```



```{r eval=FALSE, include=FALSE, echo=TRUE}
# get a de-duplicated list of all followers
aRdent_followers <- unique(followers$user_id)

# for each follower, get a binary indicator of whether they follow each tweeter or not and bind to one dataframe
binaries <- rstaters %>% 
  map_dfc(~ ifelse(aRdent_followers %in% filter(followers, account == .x)$user_id, 1, 0) %>% 
            as.data.frame) # UpSetR doesn't like tibbles

# set column names
names(binaries) <- rstaters

# have a look at the data
glimpse(binaries)
```





```{r eval=FALSE, include=FALSE, echo=TRUE}
# plot the sets with UpSetR
upset(binaries, nsets = 7, main.bar.color = "SteelBlue", sets.bar.color = "DarkCyan", 
      sets.x.label = "Follower Count", text.scale = c(rep(1.4, 5), 1), order.by = "freq")
```

























<!--chapter:end:Mining-Twitter-Data-rtweet.Rmd-->

---
title: "Multiple Pages"
---

```
output: flexdashboard::flex_dashboard
```


Page 1
=====================================  
    
Column {data-width=600}
-------------------------------------
    
### Chart 1
    
```{r eval=FALSE, include=FALSE, echo=TRUE}
```
   
Column {data-width=400}
-------------------------------------
   
### Chart 2

```{r eval=FALSE, include=FALSE, echo=TRUE}
```   
 
### Chart 3
    
```{r eval=FALSE, include=FALSE, echo=TRUE}
```

Page 2 {data-orientation=rows}
=====================================     
   
Row {data-height=600}
-------------------------------------

### Chart 1

```{r eval=FALSE, include=FALSE, echo=TRUE}
```

Row {data-height=400}
-------------------------------------
   
### Chart 2

```{r eval=FALSE, include=FALSE, echo=TRUE}
```   
    
### Chart 3

```{r eval=FALSE, include=FALSE, echo=TRUE}
```

<!--chapter:end:MultiplePages.Rmd-->

---
title: "mxnet"
---

https://mxnet.apache.org/versions/master/install/index.html?platform=MacOS&language=R&processor=CPU


```{r eval=FALSE, include=FALSE, echo=TRUE}
cran <- getOption("repos")
cran["dmlc"] <- "https://apache-mxnet.s3-accelerate.dualstack.amazonaws.com/R/CRAN/"
options(repos = cran)
install.packages("mxnet")
```

https://mxnet.apache.org/versions/master/api/r/index.html


https://mxnet.apache.org/versions/master/tutorials/index.html


R Tutorials

    Getting Started
        Basic Classification
        Using a pre-trained model for Image Classification

    Models
        MNIST Handwritten Digit Classification with Convolutional Network
        Shakespeare generation with Character-level RNN

    API Guides
        NDArray API
        Symbol API
        Callbacks
        Custom Data Iterators
        Custom Loss Functions


<!--chapter:end:mxnet.Rmd-->

---
title: "MyGIST"
---


https://gist.github.com/sbalci


https://docs.ropensci.org/gistr/

https://docs.ropensci.org/gistr/articles/gistr.html


```{r eval=FALSE, include=FALSE, echo=TRUE}
# install.packages("gistr")
# devtools::install_github("ropensci/gistr")
# renv::install("gistr")
library("gistr")
```



```{r eval=FALSE, include=FALSE, echo=TRUE}
(mygists <- gistr::gists('minepublic'))
```



```{r eval=FALSE, include=FALSE, echo=TRUE}
gistembedcodes <- purrr::map(.x = mygists, .f = gistr::embed)

gistembedcodes <- unlist(gistembedcodes)

gistembedcodes <- cat(gistembedcodes, sep = '\n\n\n')

```




```
`{r #  gistembedcodes`
```



`{r #  gistembedcodes`








```{r eval=FALSE, include=FALSE, echo=TRUE}
citation(package = 'gistr')
```




<!--chapter:end:MyGIST.Rmd-->

---
title: "News"
---


- Text Analysis of Newspaper News on Electoral Integrity and Electoral Violence in Turkey

http://rpubs.com/emretoros/dievt

<!--chapter:end:news.Rmd-->

---
title: "Presentation Ninja"
---

```
subtitle: "⚔<br/>with xaringan"
author: "Yihui Xie"
date: "2016/12/12 (updated: `{r #  Sys.Date()`)"
output:
  xaringan::moon_reader:
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
options(htmltools.dir.version = FALSE)
```

<!-- background-image: url(https://upload.wikimedia.org/wikipedia/commons/b/be/Sharingan_triple.svg) -->

???

Image credit: [Wikimedia Commons](https://commons.wikimedia.org/wiki/File:Sharingan_triple.svg)

---
class: center, middle

# xaringan

### /ʃæ.'riŋ.ɡæn/

---
class: inverse, center, middle

# Get Started

---

# Hello World

Install the **xaringan** package from [Github](https://github.com/yihui/xaringan):

```{r eval=FALSE, tidy=FALSE}
devtools::install_github("yihui/xaringan")
```

--

You are recommended to use the [RStudio IDE](https://www.rstudio.com/products/rstudio/), but you do not have to.

- Create a new R Markdown document from the menu `File -> New File -> R Markdown -> From Template -> Ninja Presentation`;<sup>1</sup>

--

- Click the `Knit` button to compile it;

--

- or use the [RStudio Addin](https://rstudio.github.io/rstudioaddins/)<sup>2</sup> "Infinite Moon Reader" to live preview the slides (every time you update and save the Rmd document, the slides will be automatically reloaded in RStudio Viewer.

.footnote[
[1] 中文用户请看[这份教程](http://slides.yihui.name/xaringan/zh-CN.html)

[2] See [#2](https://github.com/yihui/xaringan/issues/2) if you do not see the template or addin in RStudio.
]

---
background-image: url(`{r #  xaringan:::karl`)
background-position: 50% 50%
class: center, bottom, inverse

# You only live once!

---

# Hello Ninja

As a presentation ninja, you certainly should not be satisfied by the "Hello World" example. You need to understand more about two things:

1. The [remark.js](https://remarkjs.com) library;

1. The **xaringan** package;

Basically **xaringan** injected the chakra of R Markdown (minus Pandoc) into **remark.js**. The slides are rendered by remark.js in the web browser, and the Markdown source needed by remark.js is generated from R Markdown (**knitr**).

---

# remark.js

You can see an introduction of remark.js from [its homepage](https://remarkjs.com). You should read the [remark.js Wiki](https://github.com/gnab/remark/wiki) at least once to know how to

- create a new slide (Markdown syntax<sup>*</sup> and slide properties);

- format a slide (e.g. text alignment);

- configure the slideshow;

- and use the presentation (keyboard shortcuts).

It is important to be familiar with remark.js before you can understand the options in **xaringan**.

.footnote[[*] It is different with Pandoc's Markdown! It is limited but should be enough for presentation purposes. Come on... You do not need a slide for the Table of Contents! Well, the Markdown support in remark.js [may be improved](https://github.com/gnab/remark/issues/142) in the future.]

---
background-image: url(`{r #  xaringan:::karl`)
background-size: cover
class: center, bottom, inverse

# I was so happy to have discovered remark.js!

---
class: inverse, middle, center

# Using xaringan

---

# xaringan

Provides an R Markdown output format `xaringan::moon_reader` as a wrapper for remark.js, and you can use it in the YAML metadata, e.g.

```yaml
---
title: "A Cool Presentation"
output:
  xaringan::moon_reader:
    yolo: true
    nature:
      autoplay: 30000
---
```

See the help page `?xaringan::moon_reader` for all possible options that you can use.

---

# remark.js vs xaringan

Some differences between using remark.js (left) and using **xaringan** (right):

.pull-left[
1. Start with a boilerplate HTML file;

1. Plain Markdown;

1. Write JavaScript to autoplay slides;

1. Manually configure MathJax;

1. Highlight code with `*`;

1. Edit Markdown source and refresh browser to see updated slides;
]

.pull-right[
1. Start with an R Markdown document;

1. R Markdown (can embed R/other code chunks);

1. Provide an option `autoplay`;

1. MathJax just works;<sup>*</sup>

1. Highlight code with `{{}}`;

1. The RStudio addin "Infinite Moon Reader" automatically refreshes slides on changes;
]

.footnote[[*] Not really. See next page.]

---

# Math Expressions

You can write LaTeX math expressions inside a pair of dollar signs, e.g. &#36;\alpha+\beta$ renders $\alpha+\beta$. You can use the display style with double dollar signs:

```
$$\bar{X}=\frac{1}{n}\sum_{i=1}^nX_i$$
```

$$\bar{X}=\frac{1}{n}\sum_{i=1}^nX_i$$

Limitations:

1. The source code of a LaTeX math expression must be in one line, unless it is inside a pair of double dollar signs, in which case the starting `$$` must appear in the very beginning of a line, followed immediately by a non-space character, and the ending `$$` must be at the end of a line, led by a non-space character;

1. There should not be spaces after the opening `$` or before the closing `$`.

1. Math does not work on the title slide (see [#61](https://github.com/yihui/xaringan/issues/61) for a workaround).

---

# R Code

```{r comment='#'}
# a boring regression
fit = lm(dist ~ 1 + speed, data = cars)
coef(summary(fit))
dojutsu = c('地爆天星', '天照', '加具土命', '神威', '須佐能乎', '無限月読')
grep('天', dojutsu, value = TRUE)
```

---

# R Plots

```{r , fig.height=4, dev='svg'}
par(mar = c(4, 4, 1, .1))
plot(cars, pch = 19, col = 'darkgray', las = 1)
abline(fit, lwd = 2)
```

---

# Tables

If you want to generate a table, make sure it is in the HTML format (instead of Markdown or other formats), e.g.,

```{r eval=FALSE, include=FALSE, echo=TRUE}
knitr::kable(head(iris), format = 'html')
```

---

# HTML Widgets

I have not thoroughly tested HTML widgets against **xaringan**. Some may work well, and some may not. It is a little tricky.

Similarly, the Shiny mode (`{r # untime: shiny`) does not work. I might get these issues fixed in the future, but these are not of high priority to me. I never turn my presentation into a Shiny app. When I need to demonstrate more complicated examples, I just launch them separately. It is convenient to share slides with other people when they are plain HTML/JS applications.

See the next page for two HTML widgets.

---

```{r out.width='100%', fig.height=6, eval=require('leaflet')}
library(leaflet)
leaflet() %>% addTiles() %>% setView(-93.65, 42.0285, zoom = 17)
```

---

```{r eval=require('DT'), tidy=FALSE}
DT::datatable(
  head(iris, 10),
  fillContainer = FALSE, options = list(pageLength = 8)
)
```

---

# Some Tips

- When you use the "Infinite Moon Reader" addin in RStudio, your R session will be blocked by default. You can click the red button on the right of the console to stop serving the slides, or use the _daemonized_ mode so that it does not block your R session. To do the latter, you can set the option

    ```{r # 
    options(servr.daemon = TRUE)
    ```
    
    in your current R session, or in `~/.Rprofile` so that it is applied to all future R sessions. I do the latter by myself.
    
    To know more about the web server, see the [**servr**](https://github.com/yihui/servr) package.

--

- Do not forget to try the `yolo` option of `xaringan::moon_reader`.

    ```yaml
    output:
      xaringan::moon_reader:
        yolo: true
    ```

---

# Some Tips

- Slides can be automatically played if you set the `autoplay` option under `nature`, e.g. go to the next slide every 30 seconds in a lightning talk:

    ```yaml
    output:
      xaringan::moon_reader:
        nature:
          autoplay: 30000
    ```

--

- A countdown timer can be added to every page of the slides using the `countdown` option under `nature`, e.g. if you want to spend one minute on every page when you give the talk, you can set:

    ```yaml
    output:
      xaringan::moon_reader:
        nature:
          countdown: 60000
    ```

    Then you will see a timer counting down from `01:00`, to `00:59`, `00:58`, ... When the time is out, the timer will continue but the time turns red.
    
---

# Some Tips

- The title slide is created automatically by **xaringan**, but it is just another remark.js slide added before your other slides.

    The title slide is set to `class: center, middle, inverse, title-slide` by default. You can change the classes applied to the title slide with the `titleSlideClass` option of `nature` (`title-slide` is always applied).

    ```yaml
    output:
      xaringan::moon_reader:
        nature:
          titleSlideClass: [top, left, inverse]
    ```
    
--

- If you'd like to create your own title slide, disable **xaringan**'s title slide with the `seal = FALSE` option of `moon_reader`.

    ```yaml
    output:
      xaringan::moon_reader:
        seal: false
    ```

---

# Some Tips

- There are several ways to build incremental slides. See [this presentation](https://slides.yihui.name/xaringan/incremental.html) for examples.

- The option `highlightLines: true` of `nature` will highlight code lines that start with `*`, or are wrapped in `{{ }}`, or have trailing comments `#<<`;

    ```yaml
    output:
      xaringan::moon_reader:
        nature:
          highlightLines: true
    ```

    See examples on the next page.

---

# Some Tips


.pull-left[
An example using a leading `*`:

    ```{r # 
    if (TRUE) {
    ** message("Very important!")
    }
    ```
Output:
```{r # 
if (TRUE) {
* message("Very important!")
}
```

This is invalid R code, so it is a plain fenced code block that is not executed.
]

.pull-right[
An example using `{{}}`:

    `{r #  ''````{r tidy=FALSE}
    if (TRUE) {
    *{{ message("Very important!") }}
    }
    ```
Output:
```{r tidy=FALSE}
if (TRUE) {
{{ message("Very important!") }}
}
```

It is valid R code so you can run it. Note that `{{}}` can wrap an R expression of multiple lines.
]

---

# Some Tips

An example of using the trailing comment `#<<` to highlight lines:

````markdown
`{r #  ''````{r tidy=FALSE}
library(ggplot2)
ggplot(mtcars) + 
  aes(mpg, disp) + 
  geom_point() +   #<<
  geom_smooth()    #<<
```
````

Output:

```{r tidy=FALSE, eval=FALSE}
library(ggplot2)
ggplot(mtcars) + 
  aes(mpg, disp) + 
  geom_point() +   #<<
  geom_smooth()    #<<
```

---

# Some Tips

When you enable line-highlighting, you can also use the chunk option `highlight.output` to highlight specific lines of the text output from a code chunk. For example, `highlight.output = TRUE` means highlighting all lines, and `highlight.output = c(1, 3)` means highlighting the first and third line.

````md
`{r #  ''````{r, highlight.output=c(1, 3)}
head(iris)
```
````

```{r, highlight.output=c(1, 3), echo=TRUE}
head(iris)
```

Question: what does `highlight.output = c(TRUE, FALSE)` mean? (Hint: think about R's recycling of vectors)

---

# Some Tips

- To make slides work offline, you need to download a copy of remark.js in advance, because **xaringan** uses the online version by default (see the help page `?xaringan::moon_reader`).

- You can use `xaringan::summon_remark()` to download the latest or a specified version of remark.js. By default, it is downloaded to `libs/remark-latest.min.js`.

- Then change the `chakra` option in YAML to point to this file, e.g.

    ```yaml
    output:
      xaringan::moon_reader:
        chakra: libs/remark-latest.min.js
    ```

- If you used Google fonts in slides (the default theme uses _Yanone Kaffeesatz_, _Droid Serif_, and _Source Code Pro_), they won't work offline unless you download or install them locally. The Heroku app [google-webfonts-helper](https://google-webfonts-helper.herokuapp.com/fonts) can help you download fonts and generate the necessary CSS.

---

# Macros

- remark.js [allows users to define custom macros](https://github.com/yihui/xaringan/issues/80) (JS functions) that can be applied to Markdown text using the syntax `![:macroName arg1, arg2, ...]` or `![:macroName arg1, arg2, ...](this)`. For example, before remark.js initializes the slides, you can define a macro named `scale`:

    ```js
    remark.macros.scale = function (percentage) {
      var url = this;
      return '<img src="' + url + '" style="width: ' + percentage + '" />';
    };
    ```

    Then the Markdown text

    ```markdown
    ![:scale 50%](image.jpg)
    ```

    will be translated to
    
    ```html
    <img src="image.jpg" style="width: 50%" />
    ```

---

# Macros (continued)

- To insert macros in **xaringan** slides, you can use the option `beforeInit` under the option `nature`, e.g.,

    ```yaml
    output:
      xaringan::moon_reader:
        nature:
          beforeInit: "macros.js"
    ```

    You save your remark.js macros in the file `macros.js`.

- The `beforeInit` option can be used to insert arbitrary JS code before `{r # emark.create()`. Inserting macros is just one of its possible applications.

---

# CSS

Among all options in `xaringan::moon_reader`, the most challenging but perhaps also the most rewarding one is `css`, because it allows you to customize the appearance of your slides using any CSS rules or hacks you know.

You can see the default CSS file [here](https://github.com/yihui/xaringan/blob/master/inst/rmarkdown/templates/xaringan/resources/default.css). You can completely replace it with your own CSS files, or define new rules to override the default. See the help page `?xaringan::moon_reader` for more information.

---

# CSS

For example, suppose you want to change the font for code from the default "Source Code Pro" to "Ubuntu Mono". You can create a CSS file named, say, `ubuntu-mono.css`:

```css
@import url(https://fonts.googleapis.com/css?family=Ubuntu+Mono:400,700,400italic);

.remark-code, .remark-inline-code { font-family: 'Ubuntu Mono'; }
```

Then set the `css` option in the YAML metadata:

```yaml
output:
  xaringan::moon_reader:
    css: ["default", "ubuntu-mono.css"]
```

Here I assume `ubuntu-mono.css` is under the same directory as your Rmd.

See [yihui/xaringan#83](https://github.com/yihui/xaringan/issues/83) for an example of using the [Fira Code](https://github.com/tonsky/FiraCode) font, which supports ligatures in program code.

---

# Themes

Don't want to learn CSS? Okay, you can use some user-contributed themes. A theme typically consists of two CSS files `foo.css` and `foo-fonts.css`, where `foo` is the theme name. Below are some existing themes:

```{r eval=FALSE, include=FALSE, echo=TRUE}
names(xaringan:::list_css())
```

---

# Themes

To use a theme, you can specify the `css` option as an array of CSS filenames (without the `.css` extensions), e.g.,

```yaml
output:
  xaringan::moon_reader:
    css: [default, metropolis, metropolis-fonts]
```

If you want to contribute a theme to **xaringan**, please read [this blog post](https://yihui.name/en/2017/10/xaringan-themes).

---
class: inverse, middle, center
background-image: url(https://upload.wikimedia.org/wikipedia/commons/3/39/Naruto_Shiki_Fujin.svg)
background-size: contain

# Naruto

---
background-image: url(https://upload.wikimedia.org/wikipedia/commons/b/be/Sharingan_triple.svg)
background-size: 100px
background-position: 90% 8%

# Sharingan

The R package name **xaringan** was derived<sup>1</sup> from **Sharingan**, a dōjutsu in the Japanese anime _Naruto_ with two abilities:

- the "Eye of Insight"

- the "Eye of Hypnotism"

I think a presentation is basically a way to communicate insights to the audience, and a great presentation may even "hypnotize" the audience.<sup>2,3</sup>

.footnote[
[1] In Chinese, the pronounciation of _X_ is _Sh_ /ʃ/ (as in _shrimp_). Now you should have a better idea of how to pronounce my last name _Xie_.

[2] By comparison, bad presentations only put the audience to sleep.

[3] Personally I find that setting background images for slides is a killer feature of remark.js. It is an effective way to bring visual impact into your presentations.
]

---

# Naruto terminology

The **xaringan** package borrowed a few terms from Naruto, such as

- [Sharingan](http://naruto.wikia.com/wiki/Sharingan) (写輪眼; the package name)

- The [moon reader](http://naruto.wikia.com/wiki/Moon_Reader) (月読; an attractive R Markdown output format)

- [Chakra](http://naruto.wikia.com/wiki/Chakra) (查克拉; the path to the remark.js library, which is the power to drive the presentation)

- [Nature transformation](http://naruto.wikia.com/wiki/Nature_Transformation) (性質変化; transform the chakra by setting different options)

- The [infinite moon reader](http://naruto.wikia.com/wiki/Infinite_Tsukuyomi) (無限月読; start a local web server to continuously serve your slides)

- The [summoning technique](http://naruto.wikia.com/wiki/Summoning_Technique) (download remark.js from the web)

You can click the links to know more about them if you want. The jutsu "Moon Reader" may seem a little evil, but that does not mean your slides are evil.

---

class: center

# Hand seals (印)

Press `h` or `?` to see the possible ninjutsu you can use in remark.js.

![](https://upload.wikimedia.org/wikipedia/commons/7/7e/Mudra-Naruto-KageBunshin.svg)

---

class: center, middle

# Thanks!

Slides created via the R package [**xaringan**](https://github.com/yihui/xaringan).

The chakra comes from [remark.js](https://remarkjs.com), [**knitr**](http://yihui.name/knitr), and [R Markdown](https://rmarkdown.rstudio.com).



<!--chapter:end:Ninja-1.Rmd-->

---
title: "OpenCPU"
---

```{r eval=FALSE, include=FALSE, echo=TRUE}
# Install OpenCPU
# install.packages("opencpu")

# Run Apps directly from Github
library(opencpu)
ocpu_start_app("rwebapps/nabel")
ocpu_start_app("rwebapps/markdownapp")
ocpu_start_app("rwebapps/stockapp")

# Install / remove apps
# remove_apps("rwebapps/stockapp")

```



<!--chapter:end:OpenCPU.Rmd-->

---
title: "papeR"
---

copied from: https://cran.r-project.org/web/packages/papeR/vignettes/papeR_introduction.html


The package is intended to ease reporting of standard data analysis tasks such as descriptive statistics, simple test results, plots and to prettify the output of various statistical models.

https://github.com/hofnerb/papeR



```{r eval=FALSE, include=FALSE, echo=TRUE}
# install.packages("papeR")
# devtools::install_github("hofnerb/papeR")
library("papeR")
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
# install.packages("nlme")
data(Orthodont, package = "nlme")
Orthodont_orig <- Orthodont
Orthodont
```

```{r eval=FALSE, include=FALSE, echo=TRUE}
is.ldf(Orthodont)
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
labels(Orthodont)
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
labels(Orthodont) <- c("fissure distance (mm)", "age (years)", "Subject", "Sex")
Orthodont
```

```{r eval=FALSE, include=FALSE, echo=TRUE}
is.ldf(Orthodont)
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
class(Orthodont)
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
labels(Orthodont)
```

```{r eval=FALSE, include=FALSE, echo=TRUE}
labels(Orthodont, which = c("distance", "age")) <- c("Fissure distance (mm)", "Age (years)")
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
labels(Orthodont, which = "age")
```

```{r eval=FALSE, include=FALSE, echo=TRUE}
labels(Orthodont, which = 1:2)
```






































<!--chapter:end:papeR.Rmd-->

#### Conversion to labeled data frames

Instead of manually setting labels, we can simply convert a data frame to a
labeled data frame, either with the function `as.ldf()` or with `convert.labels()`.
Actually, both calls reference the same function (for an object of class `data.frame`).

While `as.ldf()` can be seen as the classical counterpart of `is.ldf()`, the
function name `convert.labels()` is inspired by the fact that these functions either
convert the variable names to labels or convert other variable labels to **papeR**-type
variable labels. Hence, these functions can, for example, be used to convert labels
from data sets which are  imported via the function `{r # ead.spss()` to **papeR**-type
variable labels.

If no variable labels are specified, the original variable names are used.
```{r eval=FALSE, include=FALSE, echo=TRUE}
Orthodont2 <- convert.labels(Orthodont_orig)
class(Orthodont2)
labels(Orthodont2)
```

### Plotting labeled data frames

For data frames of class `'ldf'`, there exist special plotting functions:
```{r plot_labeled_dataframe, eval=FALSE, include=FALSE}
par(mfrow = c(2, 2))
plot(Orthodont)
```

As one can see, the plot type is automatically determined
based on the data type and the axis label is defined by
the `labels()`.

To obtain group comparisons, we can use grouped plots. To plot all variable in the
groups of `Sex` one can use
```{r grouped_plot, eval=FALSE, include=FALSE}
par(mfrow = c(1, 3))
plot(Orthodont, by = "Sex")
```

We can as well plot everything against the metrical variable `distance`
```{r with_x, eval=FALSE, include=FALSE}
par(mfrow = c(1, 3))
plot(Orthodont, with = "distance")
```

To plot only a subset of the data, say all but `Subject`, against `distance` and
suppress the regression line we can use
```{r univariate_no_regressionline, eval=FALSE, include=FALSE}
par(mfrow = c(1, 2))
plot(Orthodont, variables = -3, with = "distance", regression.line = FALSE)
```

Note that again we can use either variable names or indices to specify the variables
which are to be plotted.


### Summary tables

One can use the command `summarize()` to automatically produce summary tables for
either numerical variables (i.e., variables where `is.numeric()` is `TRUE`) or
categorical variables (where `is.factor()` is `TRUE`). We now extract a summary
table for numerical variables of the `Orthodont` data set:

```{r eval=FALSE, include=FALSE, echo=TRUE}
data(Orthodont, package = "nlme")
summarize(Orthodont, type = "numeric")
```

Similarly, we can extract summaries for all factor variables. As one of the factors
is the `Subject` which has `{r #  nlevels(Orthodont$Subject)` levels, each with
`{r #  unique(table(Orthodont$Subject))` observations, we exclude this from the summary
table and only have a look at `Sex`
```{r eval=FALSE, include=FALSE, echo=TRUE}
summarize(Orthodont, type = "factor", variables = "Sex")
```

Again, as for the plots, one can specify `group`s to obtain grouped statistics:
```{r eval=FALSE, include=FALSE, echo=TRUE}
summarize(Orthodont, type = "numeric", group = "Sex", test = FALSE)
```

Per default, one also gets `test`s for group differences:
```{r eval=FALSE, include=FALSE, echo=TRUE}
summarize(Orthodont, type = "numeric", group = "Sex")
```

### Converting summaries to PDF

So far, we only got standard R output. Yet, any of these summary tables can be
easily converted to LaTeX code using the package **xtable**. In **papeR** two
special functions `xtable.summary()` and `print.xtable.summary()` are defined
for easy and pretty conversion. In `Sweave` we can use

```
{r # 
<<echo = TRUE, results = tex>>=
xtable(summarize(Orthodont, type = "numeric"))
xtable(summarize(Orthodont, type = "factor", variables = "Sex"))
xtable(summarize(Orthodont, type = "numeric", group = "Sex"))
@
```

and in **knitr** we can use

```
{r # 
<<echo = TRUE, results = 'asis'>>=
xtable(summarize(Orthodont, type = "numeric"))
xtable(summarize(Orthodont, type = "factor", variables = "Sex"))
xtable(summarize(Orthodont, type = "numeric", group = "Sex"))
@
```
to get the following PDF output

![LaTeX Output](tables.png)

Note that per default, `booktabs` is set to `TRUE` in `print.xtable.summary`, and
thus `\usepackage{booktabs}` is needed in the header of the LaTeX report. For details
on LaTeX summary tables see the dedicated vignette, which can be obtained, e.g., via
`vignette("papeR\_with\_latex", package = "papeR")`. See also there for more details
on summary tables in general.

### Converting summaries to Markdown

To obtain markdown output we can use, for example, the function `kable()` from
package **knitr** on the summary objects:

```{r , eval=FALSE, echo = TRUE, results = 'asis'}
library("knitr")
kable(summarize(Orthodont, type = "numeric"))
kable(summarize(Orthodont, type = "factor", variables = "Sex", cumulative = TRUE))
kable(summarize(Orthodont, type = "numeric", group = "Sex", test = FALSE))
```

which gives the following results

```{r, eval=FALSE, echo = TRUE, results = 'asis'}
library("knitr")
kable(summarize(Orthodont, type = "numeric"))
kable(summarize(Orthodont, type = "factor", variables = "Sex", cumulative = TRUE))
kable(summarize(Orthodont, type = "numeric", group = "Sex"))
```


### Prettify model output

To prettify the output of a linear model, one can use the function
`prettify()`. This function adds confidence intervals, properly
prints p-values, adds significance stars to the output (if desired)
and additionally adds pretty formatting for factors.

```
{r eval=FALSE, include=FALSE, echo=TRUE}
linmod <- lm(distance ~ age + Sex, data = Orthodont)
## Extract pretty summary
(pretty_lm <- prettify(summary(linmod)))
```

The resulting table can now be formatted for printing using packages like
**xtable** for LaTeX which can be used in `.Rnw` files with the option
`{r # esults='asis'` (in **knitr**) or `{r # esults = tex` (in `Sweave`)

```
{r, results='hide'}
xtable(pretty_lm)
```

In markdown files (`.Rmd`) one can instead use the function `kable()` with the
chunk option `{r # esults='asis'`. The result looks as follows:

```
{r, results='asis'}
kable(pretty_lm)
```

#### Supported objects

The function `prettify` is *currently* implemented for objects of the following classes:

* `lm` (linear models)
* `glm` (generalized linear models)
* `coxph` (Cox proportional hazards models)
* `lme` (linear mixed models; implemented in package **nlme**)
* `mer` (linear mixed models; implemented in package **lme4**, version < 1.0)
* `merMod` (linear mixed models; implemented in package **lme4**, version >= 1.0)
* `anova` (anova objects)

<!--chapter:end:papeR2.Rmd-->

---
title: "Power Analysis"
---


- How to calculate statistical power for your meta-analysis

https://towardsdatascience.com/how-to-calculate-statistical-power-for-your-meta-analysis-e108ee586ae8


<!--chapter:end:Power_Analysis.Rmd-->

---
title: "R scripts"
author: Marcello Gallucci, Giulio Constantini & Marco Perugini
---

```{r eval=FALSE, include=FALSE, echo=TRUE}
knitr::opts_chunk$set(echo = TRUE)
if(!require("pacman")) install.packages("pacman")
pacman::p_load("ggplot2", "pwr", "easypower", "powerMediation", "bmem")
```

# Introduction

In this page one can found all the R functions discussed in Perugini, Gallucci, Constantini (2018), A practical primer to power analysis for simple experimental designs. __International Review of Social Psychology__.  [rewf here](here). Examples are taken from the papers. In all the examples that require multiple lines of code the user needs to update only the variables listed before `### end of input ###` comment. The results are computed automatically for the required N. Small changes in the code are required to compute other power parameters.

# t-test
## Independent Samples

A very simple function for independent samples t-test. To use with Cohen's d, set `sd=1` and put `delta=d`, where `d` is your effect size.

```{r include=FALSE, eval=FALSE, echo = TRUE}

power.t.test(power=.80, sig.level=.05, delta=.5, sd=1)

```
Required n for each group should be rounded up to the first whole number. Exactly one of `n`, `delta`, `sd`, `power`, and `sig.level` must be NULL. The NULL parameter is estimated by the function. Thus, post-hoc analysis is done omitting `power` and including `n`. 

```{r eval=FALSE, include=FALSE, echo=TRUE}

power.t.test(n=64, sig.level=.05, delta=.5, sd=1)

```
`?power.t.test` for all the other options.


## Sensitivity analysis

In the paper we suggested to look at the relationship between total sample size and effect size for a given power level (.80 in the example) and alpha (.05 in the example). In R this can be done as follows:

```{r include=FALSE, eval=FALSE, echo = TRUE}
library("ggplot2")

N<-seq(20,140,by=20)

## end of input ##
dat<-data.frame(n=N)
for (i in seq(N)) {
   one.power<-power.t.test(power=.80, sig.level=.05, sd=1, n=N[i])
   dat$d[i]<-one.power$delta
}

plt<-ggplot(dat,aes(y=d,x=n, label = round(d,digits = 2))) +stat_smooth(method = "loess",se=FALSE)
plt<-plt+labs(x="Totale sample size",y="Effect size d")
plt<-plt+scale_x_continuous(breaks=seq(min(N),max(N),by=20))
plt<-plt+geom_label()
plt


```



## Paired Samples

Same function with the option `type = "paired"`
```{r include=FALSE, eval=FALSE, echo = TRUE}

power.t.test(power=.80, sig.level=.05, delta=0.527, sd=1,type = "paired")

```

# One way Analysis of Variance

For all F-test related power analysis, it is generally better to use `pwr.f2.test()` function from [pwr package](https://cran.r-project.org/web/packages/pwr/index.html). The function uses $f^2$ as effect size, which is the square of the $f$ used by GPower "ANOVA: Fixed effects, omnibus and one-way". Here we assume one has the $\eta_p^2$, which in one-way ANOVA is the $R^2$. For required N (total sample size) one needs to input also $k$, the number of groups defined by the independent variable. 

```{r include=FALSE, eval=FALSE, echo = TRUE}
library("pwr")

## input ##
etap<-.326
k<-8 # number of groups in the design
## end of input ##
f2=etap/(1-etap)
dfn<-k-1 # numerator degrees of freedom

(res<-pwr.f2.test(u = dfn,f2 = f2,sig.level = .05,power = .80))
## required N ##
ceiling(res$v)+res$u+1

```

# Factorial Designs

"Assume that in a 3 X 2 factorial design the researcher expects the interaction to explain around 10% of the variance. If the researcher expects no main effect, the proportion of residual variance is 1-.10=.90, so the pη2=.10." 

## method 1

Here we keep using `pwr.f2.test` because it can be used for any F-test in the linear model. However, for prospective power (required total N) the function returns the required error degrees of freedom, so the required N must be approximate adding the effects degrees of freedom to the error degrees of freedom. The following code does it automatically.

```{r include=FALSE, eval=FALSE, echo = TRUE}
library("pwr")

## input for interaction##
etap<-.10
k1<-3 # levels of the first factor
k2<-2 # levels of the second factor
## end of input ##

(f2=etap/(1-etap))
dfn<-(k1-1)*(k2-1) # df for the interaction

(res<-pwr.f2.test(u = dfn,f2 = f2,sig.level = .05,power = .80))

## required N ##
ceiling(res$v)+(k1*k2)-1


## input for main effects##
etap<-.10
k1<-3 # levels of the first factor
k2<-2 # levels of the second factor
## end of input ##

(f2=etap/(1-etap))
dfn<-(k1-1) # df for the main effect 1
(res<-pwr.f2.test(u = dfn,f2 = f2,sig.level = .05,power = .80))

## required N ##
ceiling(res$v)+(k1*k2)-1

dfn<-(k2-1) # df for the main effect 2
(res<-pwr.f2.test(u = dfn,f2 = f2,sig.level = .05,power = .80))

## required N ##
ceiling(res$v)+(k1*k2)-1


```

## Method 2

The package [easypower](https://cran.r-project.org/web/packages/easypower/index.html) provides short cuts for factorial ANOVA power analysis. It provides a dedicated function `n.multiway()` which simplifies computation of required N for main effects and interaction. It does not provide estimates for post-hoc power and other applications of power analysis.

```{r include=FALSE, eval=FALSE, echo = TRUE}

library("easypower")
## input ##
etap<-.10
k1<-3
k2<-2
## end of input ##

main.eff1 <- list(name = "A", levels = k1, eta.sq = etap)
main.eff2 <- list(name = "C", levels = k2, eta.sq = etap)
int.eff1 <- list(name = "A*C", eta.sq = etap)
n.multiway(iv1 = main.eff1, iv2 = main.eff2, int1 = int.eff1)


```
Results are the same than _method 1_ a part from rounding.

## Power analysis for contrasts

### Generic Contrasts

Main effects and interaction for example in Table 2. They are all equivalent, so we go for the interaction. We compute the $f$ as in the paper, but use $f^2$ as required by `pwr.f2.

```{r include=FALSE, eval=FALSE, echo = TRUE}

## input ##
means<-c(10,0,0,0)
sd=5
contInt<-c(1,-1,-1,1)

## end input ##

### contrast value
cvInt<-contInt%*%means

## Effect sizes ##

fInt<-cvInt/sqrt(length(means)*sum(contInt^2*sd^2))

#### expected N
res<-pwr.f2.test(u=1,power=.80,sig.level = .05, f2=fInt^2)
ceiling(res$v)+length(means)

```

## Guessing the interaction effect size from one-way designs

Consider the case in which the pattern of means in Table 3, case 1, in the paper is taken from a one-way design in which A1 and A2 show means equal to 5 and 2, respectively, and the same within groups variability (say equal to 1). Basically, the researcher observes in the literature a one-way design with only A as a factor and wishes to test the moderating effect of B in a 2 x 2 design. The B factor has two levels, that for simplicity we name “replicated” and “moderated”. The problem is to determine the power parameters of the expected interaction effect.

## Means based method

```{r include=FALSE, eval=FALSE, echo = TRUE}
# observed means #
A1<-c(5,2)
# expected means #
A2<-c(0,0)

sd=1
contInt<-c(1,-1,-1,1)

## end input ##

means<-c(A1,A2)
cvInt<-contInt%*%means

## Effect sizes ##

fInt<-cvInt/sqrt(length(means)*sum(contInt^2*sd^2))

#### expected N
res<-pwr.f2.test(u=1,power=.80,sig.level = .05, f2=fInt^2)
ceiling(res$v)+length(means)


```
### Percentage of  moderation

Researcher should estimate the percentage of moderation, where 100% means full suppression of the effec, 200% full reversing of the effect. 

```{r include=FALSE, eval=FALSE, echo = TRUE}
# observed means #
A1<-c(5,2)
cont1<-c(1,-1)
l<-2 #levels of moderators
sd=1
# percentage of moderation
pm<-100

## end of input ##

k1<-length(A1)
k2<-length(A1)*l
f0<-(A1%*%cont1)/(sqrt(length(A1)*sum(cont1^2)*sd^2))

f<-(pm/100)*f0*sqrt(length(cont1)/(length(cont1)*l^2))

#### expected N
(res<-pwr.f2.test(u=1,power=.80,sig.level = .05, f2=f^2))
ceiling(res$v)+length(cont1)*l

  
```
### Paper example "An example of a more complex design"

Consider a researcher who wishes to design a moderation study based on an one-way design with four conditions implementing an increasing intensity of a stimulus, such that the observed pattern of mean shows a linear trend. In particular, the observed linear trend contrast has an p=.184, corresponding to a f=0.475.


```{r include=FALSE, eval=FALSE, echo = TRUE}
### starting from eta-squared ###

# percentage of moderation
pm<-125
# observed parameters #
etap<-.184
cont<-c(-3,-1,1,3)
l<-2 # levels of moderator
### end of input ###

(f0<-sqrt(etap/(1-etap)))

(f<-(pm/100)*f0*sqrt(length(cont1)/(length(cont1)*l^2)))

#### expected N (total sample)
(res<-pwr.f2.test(u=1,power=.80,sig.level = .05, f2=f^2))
ceiling(res$v)+(length(cont)*l)

```

The same results can be obtained if one anticipates all expected means and pooled standard deviation (tedious method)

```{r include=FALSE, eval=FALSE, echo = TRUE}

Replicated<-c(5,9,16,20)
sd<-12.239
Expected<-c(5,3,2,1)
cont<-c(-3,-1,1,3)
## end of input ##
contInt<-c(cont,-cont)
means<-c(Replicated,Expected)

(f<-(contInt%*%means)/sqrt(length(contInt)*sum(contInt^2)*sd^2))

#### expected N
res<-pwr.f2.test(u=1,power=.80,sig.level = .05, f2=f^2)
ceiling(res$v)+length(cont1)*2

  
```

# Regression Analysis

Any effect in simple and the multiple regression for which the $\eta_p^2$ is available can be evaluated witht he following code. here is an example for a regression with three predictors and a small effect size, according to Cohen's (Cohen, 1988) classification.


```{r include=FALSE, eval=FALSE, echo = TRUE}
## input ##
etap<-0.02
k<-3 # number of predictors
## end of input ###
(f2<-etap/(1-etap))
res<-pwr.f2.test(u=1,power=.80,sig.level = .05, f2=f^2)
ceiling(res$v)+k

```

# Moderated regression 

## Interaction between continuous and dichotomous predictors.

The researcher needs to estimate the expected correlation between the continuous independent variable and the dependent variable for the two groups defined by the moderator.

```{r include=FALSE, eval=FALSE, echo = TRUE}
## input ##
ra<-.0
rb<-.50
k<-3 # number of coefficients in the regression (not considering the intercept)
## end of input ##

(bint<-abs(ra-rb))
(f2<-bint^2/(2*(2-ra^2-rb^2)))

## expected N ###
res<-pwr.f2.test(u=1,power=.80,sig.level = .05, f2=f2)
ceiling(res$v)+k+1


```
## Interaction between continuous predictors

The researcher needs to estimate the expected correlation between the continuous independent variables and the dependent variable, and the change of correlation from the average value of the moderator to one standard deviation above average of the moderator. We assume here that there

```{r include=FALSE, eval=FALSE, echo = TRUE}
## input ##
r_yx<-.35
r_ym<-.25
r_change<-.175
k<-3 # number of coefficients in the regression (not considering the intercept)
## end of input ##

(f2<-r_change^2/(1-r_yx^2-r_ym^2))

## expected N ###
res<-pwr.f2.test(u=1,power=.80,sig.level = .05, f2=f2)
ceiling(res$v)+k+1


```



# Mediation


### method based on Sobel test
```{r eval=FALSE, include=FALSE, echo=TRUE}
library("powerMediation")
### input ###
# a, b,  are the coefficients as in standard mediational path diagram
a<-.8186
b<-.4039
c<- .4334
### end of input ###

# compute sigma.epsilon
sigma.epsilon<-sqrt(1-(b^2+c^2+2*a*b*c))

## results: expected N
res<-ssMediation.Sobel(power = .80,  theta.1a = .8186, lambda.a = .4039, sigma.x = 1, sigma.m = 1, sigma.epsilon = .6020)
ceiling(res$n)

```

### method based on bootstrap analysis
The code below allows computing the power achieved with a certain sample size, that must be indicated as nobs below. The achieved power for the Indirect/Mediation effect (ab) using a sample of size nobs can be read under column "Power". 

The following code is based on boostrap and it takes a lot of time (several hours) to run. Be extremely patient (or leave it running at night). If you have a multi-core processor, function power.boot implements parallel processing to speed-up computations. See ?power.boot for details.

```{r eval=F}
library("bmem")
### input ###
# a, b,  are the coefficients as in standard mediational path diagram, n is the sample size-
a<-.8186
b<-.4039
c<- .4334
nobs <- 100

### end of input ###

model <- paste(c(
  'M ~ a*X + start(',a,')*X',
  'Y ~ b*M + c*X + start(',b,') * M + start(.4334)*X',
  'X ~~ start(1)*X',
  'M ~~ start(1)*M',
  'Y ~~ start(1)*Y'), collapse = "\n"
)
set.seed(1234)
power.result <- power.boot(model, indirect = 'ab := a*b', nobs = nobs) 
summary(power.result)
```



<!--chapter:end:power.Rmd-->

---
title: "Power Analysis"
---


- A Practical Primer To Power Analysis for Simple Experimental Designs. 
International Review of Social Psychology, 31(1), 20. 

DOI: http://doi.org/10.5334/irsp.181

https://www.rips-irsp.com/article/10.5334/irsp.181/

https://github.com/mcfanda/primerPowerIRSP



<!--chapter:end:PowerAnalysis.Rmd-->

---
title: "My R Codes For Data Analysis"
---


# Prepare Data for Analysis / Veriyi Analiz için hazırlamak

## The Quartz guide to bad data

>    An exhaustive reference to problems seen in real-world data along with suggestions on how to resolve them.

https://github.com/Quartz/bad-data-guide


## Kötü veri kılavuzu

Kötü veri kılavuzu

https://sbalci.github.io/Kotu-Veri-Kilavuzu/index.html

## data organization organizing data in spreadsheets

https://kbroman.org/dataorg/

Daniel Kaplan. (2018) Teaching Stats for Data Science. The American Statistician 72:1, pages 89-96. 

https://doi.org/10.1080/00031305.2017.1375989


## Tidy Data

Hadley Wickham.
Tidy data.
The Journal of Statistical Software, vol. 59, 2014.

http://vita.had.co.nz/papers/tidy-data.html
https://www.jstatsoft.org/article/view/v059i10
http://dx.doi.org/10.18637/jss.v059.i10


## The Ten Commandments for a well-formatted database

- The Ten Commandments for a well-formatted database

https://rtask.thinkr.fr/blog/the-ten-commandments-for-a-well-formatted-database/


## Software specific problems

### Keep SPSS labels

```{r eval=FALSE, include=FALSE, echo=TRUE}
library(foreign) # foreign paketi yükleniyor
```

read.spss komutu ile değer etiketlerini almasını ve bunu liste olarak değil de data.frame olarak kaydetmesini istiyoruz

```{r eval=FALSE, include=FALSE, echo=TRUE}
mydata <- read.spss("mydata.sav", use.value.labels = TRUE, to.data.frame = TRUE)
```

aktardığımız data.frame'in özellikleri (attr) içinde değişkenlerin etiketleri var, bunları dışarı çıkartıyoruz

```{r eval=FALSE, include=FALSE, echo=TRUE}
VariableLabels <- as.data.frame(attr(mydata, "variable.labels"))
```


elde ettiğimiz data.frame'deki satır isimleri değişkenlerin isimleri oluyor, karşılarında da değişken etiketleri var
satır isimlerini de dışarı çıkartıyoruz

```{r eval=FALSE, include=FALSE, echo=TRUE}
VariableLabels$original <- rownames(VariableLabels)
```

Değişken etiketi olanları etiketleri ile diğerlerini olduğu gibi saklıyoruz  

```{r eval=FALSE, include=FALSE, echo=TRUE}
VariableLabels$label[VariableLabels$label ==""] <- NA 
VariableLabels$colname <- VariableLabels$original
VariableLabels$colname[!is.na(VariableLabels$label)] <- as.vector(VariableLabels$label[!is.na(VariableLabels$label)])
```

son olarak da data.frame'deki sütun isimlerini değiştiriyoruz

```{r eval=FALSE, include=FALSE, echo=TRUE}
names(mydata) <- VariableLabels$colname
```


### Make both computer and human readible variable names

turkce karakter donusumu

```{r turkce karakter donusumu, eval=FALSE, include=FALSE}
# https://suatatan.wordpress.com/2017/10/07/bulk-replacing-turkish-characters-in-r/

to.plain <- function(s) {
        # 1 character substitutions
    old1 <- "çğşıüöÇĞŞİÖÜ"
    new1 <- "cgsiuocgsiou"
    s1 <- chartr(old1, new1, s)
    # 2 character substitutions
    old2 <- c("œ", "ß", "æ", "ø")
    new2 <- c("oe", "ss", "ae", "oe")
    s2 <- s1
    for(i in seq_along(old2)) s2 <- gsub(old2[i], new2[i], s2, fixed = TRUE)
    s2
}

names(df) <- make.names(to.plain(tolower(names(df))))

names(df) <- names(df) %>% 
    tolower() %>% 
    to.plain() %>% 
    make.names()

df$source=as.vector(sapply(df$source,to.plain))

make.names(tolower(names(df)))
to.plain(names(df))

purrr::map(df, to.plain)

```

### Anonimisation

### Add subject ID to the data / veriye ID ekleme

```{r eval=FALSE, include=FALSE, echo=TRUE}
df <- tibble::rowid_to_column(df, "subject")
```





## Data reorganization

```{r eval=FALSE, include=FALSE, echo=TRUE}
scabies <- read.csv(file = "http://datacompass.lshtm.ac.uk/607/2/S1-Dataset_CSV.csv", header = TRUE, sep = ",")

scabies$gender == "male"

scabies$age[scabies$gender == "male"]

mean(scabies$age[scabies$gender == "male"])


scabies %>% 
    filter(gender == "male") %>% 
    summarise_at("age" ,mean) 


```



```{r eval=FALSE, include=FALSE, echo=TRUE}

scabies$agegroups <- as.factor(cut(scabies$age, c(0,10,20,Inf), labels = c("0-10","11-20","21+"), include.lowest = TRUE)) 

scabies$house_cat <- as.factor(cut(scabies$house_inhabitants, c(0,5,10,Inf), labels = c("0-5","6-10","10+"), include.lowest = TRUE))


table(scabies$house_cat, scabies$house_inhabitants)

```

```{r eval=FALSE, include=FALSE, echo=TRUE}
ebola$status <- as.numeric(ebola$status) 

```


```{r eval=FALSE, include=FALSE, echo=TRUE}
ebola$transmission <- recode(ebola$transmission, syringe = "needle")

```

```{r eval=FALSE, include=FALSE, echo=TRUE}
scabies$house_cat <- relevel(scabies$house_cat, ref = "0-5")
#Make 0-5 household size the baseline group
```





```{r eval=FALSE, include=FALSE, echo=TRUE}
df <- data.frame(month=rep(1:3,2),
                 student=rep(c("Amy", "Bob"), each=3),
                 A=c(9, 7, 6, 8, 6, 9),
                 B=c(6, 7, 8, 5, 6, 7))
#Here we have a where each student is on a different row for each month
#The students took two tests A and B. 
#For each student/month combination we have a value for A and a value for B

df2 <- gather(df,test,score,A:B)
#Make a new datatable df2
#Have a column called "test". 
#This will have the value either A or B as these are the names of the columns we specified.
#Have a column called "score". 
#This will have the value previously in column A or B respectively for each row

df2
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
#Now we have a single row for each combination of month/student/test 
#Their score is in the score column

df3 <- spread(df2,test,score)
#Make a new datatable df3
#Make a column for each unique value in the test variable.
#Name each of these columns based on that unique value
#Under each column put the corresponding value that was in the score column

df3
```

```{r eval=FALSE, include=FALSE, echo=TRUE}
dt3 <- expandRows(dt, 2)
#Expand the original datatable. Replicate each row by the value in column 2. 
dt3
```










































<!-- # gerekli paketi yükleme -->
<!-- library(tidyverse) -->
<!-- library(haven) -->
<!-- # dosyayı yükleme -->
<!-- EDT256 <- read_sav("EDT256.sav") -->



<!-- # gerekli sütunları seçme -->
<!-- EDT256 <- EDT256 %>% -->
<!--     select(subject, tanı, starts_with("beck")) -->

<!-- # değişken değerlerini yeniden atama -->
<!-- EDT256$tanı <- as_factor(EDT256$tanı, labels = "values") -->

<!-- # yatay veriyi uzun hale getirme -->
<!-- EDT256_new <- EDT256 %>% -->
<!--     gather(- c("subject","tanı"), key = zaman, value = beck_int_score) -->


<!-- # değişkenleri rakamsal hale getirme -->
<!-- EDT256_new$zaman[which(EDT256_new$zaman == "beck_int_P")] <- 0 -->
<!-- EDT256_new$zaman[which(EDT256_new$zaman == "beck_int_A1")] <- 1 -->
<!-- EDT256_new$zaman[which(EDT256_new$zaman == "beck_int_A2")] <- 2 -->

<!-- # değişken değerlerini atama  -->
<!-- EDT256_new$zaman <- factor(EDT256_new$zaman, levels = c(0,1,2), -->
<!--                            labels = c("Tedavi Öncesi", "T. Sonrası 1. Hafta", -->
<!--                                       "T. Sonrası 2. Hafta")) -->


<!-- # nparLD analizi -->

<!-- # gerekli paketi yükleme -->
<!-- library(nparLD) -->

<!-- # model oluşturma -->
<!-- modelEDT256 <- nparLD(beck_int_score ~ tanı * zaman, data = EDT256_new, -->
<!--                       subject = "subject", -->
<!--                       plot.CI = TRUE, show.covariance = TRUE) -->

<!-- # model sonucunu yazdır -->
<!-- out <- capture.output(modelEDT256) -->
<!-- cat("EDT256 Model", out, file = "EDT256.txt", sep = "\n", append=TRUE) -->

<!-- # model grafiği -->
<!-- plot(modelEDT256) -->

<!-- # plot sonucunu yazdır -->
<!-- out2 <- capture.output(plot(modelEDT256)) -->
<!-- cat("\n","EDT256 Plot", out2, file = "EDT256.txt", sep = "\n", append = TRUE) -->

<!-- # grafiği yazdır -->
<!-- jpeg('modelEDT256.jpeg') -->
<!-- plot(modelEDT256) -->
<!-- dev.off() -->

<!-- # model özeti -->
<!-- summary(modelEDT256) -->

<!-- # model özeti sonucunu yazdır -->
<!-- out3 <- capture.output(modelEDT256) -->
<!-- cat("\n","EDT256 summary", out3, file = "EDT256.txt", sep = "\n", append=TRUE) -->


<!-- # ek olarak boxplot oluşturma -->
<!-- boxplot(beck_int_score ~ tanı * zaman, data = EDT256_new, -->
<!--         names = FALSE,col = c("grey",2),lwd = 2) -->
<!-- axis(1,at = 1.5,labels = "Tedavi Öncesi",font = 2,cex = 3) -->
<!-- axis(1,at = 3.5,labels = "T. Sonrası 1. Hafta",font = 2,cex = 3) -->
<!-- axis(1,at = 5.5,labels = "T. Sonrası 2. Hafta",font = 2,cex = 3) -->
<!-- legend(5,35,c("Tanı 1","Tanı 2"), -->
<!--        lwd = c(2,2), -->
<!--        col = c("grey",2),cex = 0.7) -->

<!-- # boxplot yazdır -->
<!-- jpeg('boxplotEDT256.jpeg') -->
<!-- boxplot(beck_int_score ~ tanı * zaman, data = EDT256_new, -->
<!--         names = FALSE,col = c("grey",2),lwd = 2) -->
<!-- axis(1,at = 1.5,labels = "Tedavi Öncesi",font = 2,cex = 3) -->
<!-- axis(1,at = 3.5,labels = "T. Sonrası 1. Hafta",font = 2,cex = 3) -->
<!-- axis(1,at = 5.5,labels = "T. Sonrası 2. Hafta",font = 2,cex = 3) -->
<!-- legend(5,35,c("Tanı 1","Tanı 2"), -->
<!--        lwd = c(2,2), -->
<!--        col = c("grey",2),cex = 0.7) -->
<!-- dev.off() -->


https://stackoverflow.com/questions/22353633/filter-for-complete-cases-in-data-frame-using-dplyr-case-wise-deletion


df %>% na.omit
or this:

df %>% filter(complete.cases(.))
or this:

library(tidyr)
df %>% drop_na
If you want to filter based on one variable's missingness, use a conditional:

df %>% filter(!is.na(x1))
or

df %>% drop_na(x1)
Other answers indicate that of the solutions above na.omit is much slower but that has to be balanced against the fact that it returns and row indices of the omitted rows as the na.action attribute whereas the other solutions above do not.

str(df %>% na.omit)
'data.frame':   2 obs. of  2 variables:
$ x1: num  1 2
$ x2: num  1 2
- attr(*, "na.action")= 'omit' Named int  3 4
    ..- attr(*, "names")= chr  "3" "4"



ozp <- veri %>% 
    select(RaporNo, Hasta, cinsiyet, Yas,
           ozp_parca, ozp_kaset, ozp_cap, ozp_tani, ozp_kod) %>% 
    filter(!is.na(ozp_parca) | !is.na(ozp_kaset) | !is.na(ozp_cap) | !is.na(ozp_tani) | !is.na(ozp_kod)
    )

ozp2 <- veri %>% 
    select(RaporNo, Hasta, cinsiyet, Yas,
           ozp_parca, ozp_kaset, ozp_cap, ozp_tani, ozp_kod) %>% 
    filter(complete.cases(ozp_parca, ozp_kaset, ozp_cap, ozp_tani, ozp_kod)
    )

ozp3 <- veri %>% 
    select(RaporNo, Hasta, cinsiyet, Yas,
           ozp_parca, ozp_kaset, ozp_cap, ozp_tani, ozp_kod) %>% 
    na.omit(ozp_parca, ozp_kaset, ozp_cap, ozp_tani, ozp_kod)
    


# xray

The R Package to Have X Ray Vision on your Datasets

https://blog.datascienceheroes.com/x-ray-vision-on-your-datasets/



```{r eval=FALSE, include=FALSE, echo=TRUE}
# install.packages("devtools")
devtools::install_github("sicarul/xray")
```



```{r eval=FALSE, include=FALSE, echo=TRUE}
data(longley)
badLongley <- longley
badLongley$GNP <- NA
xray::anomalies(badLongley)
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
distrLongley <- longley
distrLongley$testCategorical <- c(rep('One',7), rep('Two', 9))
xray::distributions(distrLongley)
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
dateLongley <- longley
dateLongley$Year <- as.Date(paste0(dateLongley$Year,'-01-01'))
dateLongley$Data <- 'Original'
ndateLongley <- dateLongley
ndateLongley$GNP <- dateLongley$GNP+10
ndateLongley$Data <- 'Offseted'
xray::timebased(rbind(dateLongley, ndateLongley), 'Year')
```


# convert all data.frame into character

df <- purrr::map_df(df, as.character)



# R:case4base - reshape data with base R

https://jozefhajnala.gitlab.io/r/r002-data-manipulation/

# R:case4base - data aggregation with base R

https://jozefhajnala.gitlab.io/r/r003-aggregation/

# tidyr

<blockquote class="twitter-tweet" data-lang="en"><p lang="en" dir="ltr">This week I&#39;m going to be looking at some <a href="https://twitter.com/hashtag/tidyr?src=hash&amp;ref_src=twsrc%5Etfw">#tidyr</a> functions! 💡 First is uncount() which might come in handy if you want to transform a summary table to individual rows. <a href="https://twitter.com/hashtag/rstats?src=hash&amp;ref_src=twsrc%5Etfw">#rstats</a> <a href="https://t.co/UZE0wUcEHC">pic.twitter.com/UZE0wUcEHC</a></p>&mdash; Nic Crane (@nic_crane) <a href="https://twitter.com/nic_crane/status/1066964776276893696?ref_src=twsrc%5Etfw">November 26, 2018</a></blockquote><script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>




---

```
df <- data.frame(
          V1 = c(0, 0, 0, 0, 1, 1, 1, 1, 0, 1),
          V2 = c(1, 1, 1, 0, 0, 0, 0, 0, 0, 1),
          V3 = c(0, 0, 0, 1, 1, 1, 1, 1, 0, 1)
)


df$V1_rec <- grepl(pattern = "0", x = df$V1)
df$V2_rec <- grepl(pattern = "0", x = df$V2)
df$V3_rec <- grepl(pattern = "0", x = df$V3)

df %>% 
    mutate(toplam = select(., V1_rec:V3_rec) %>% rowSums(na.rm = TRUE)
               )
```

---




















<!--chapter:end:PrepareData.Rmd-->

---
title: "Python Pandas"
---

http://nbviewer.jupyter.org/github/justmarkham/pandas-videos/blob/master/pandas.ipynb


https://www.youtube.com/watch?v=5_QXMwezPJE&list=PL5-da3qGB5ICCsgW1MxlZ0Hq8LL5U3u9y&index=2



https://developers.google.com/edu/python/

<!--chapter:end:PythonPandas.Rmd-->

---
title: "Text Data"
---

# quanteda: Quantitative Analysis of Textual Data

An R package for the Quantitative Analysis of Textual Data

https://quanteda.io

https://github.com/quanteda/quanteda



# Quick Start Guide


https://quanteda.io/articles/pkgdown/quickstart.html



```{r download package}
# devtools package required to install quanteda from Github 
# devtools::install_github("quanteda/quanteda") 
# install.packages("quanteda")
# install.packages("readtext")
```




# readtext: Import and handling for plain and formatted text files

https://readtext.quanteda.io

https://github.com/quanteda/readtext




# https://github.com/quanteda/readtext

# https://github.com/quanteda/stopwords

# https://github.com/quanteda/spacyr

# https://github.com/quanteda/quanteda.corpora

# https://github.com/kbenoit/quanteda.dictionaries

# https://quanteda.io/articles/quickstart.html

# https://quanteda.io/articles/pkgdown/comparison.html

# https://quanteda.io/articles/pkgdown/design.html

# https://quanteda.io/articles/pkgdown/examples/phrase.html

# https://quanteda.io/articles/pkgdown/examples/plotting.html

# https://quanteda.io/articles/pkgdown/examples/lsa.html

# https://quanteda.io/articles/pkgdown/examples/twitter.html

# https://quanteda.io/articles/pkgdown/replication/digital-humanities.html

# https://quanteda.io/articles/pkgdown/replication/text2vec.html

# https://quanteda.io/articles/pkgdown/replication/qss.html








# footer

```{r citation 2}
citation(package = "quanteda")
```







<!--chapter:end:quanteda.Rmd-->

---
title: R Arayüzler^[Bu bir derlemedir, mümkün mertebe alıntılara referans
  vermeye çalıştım.]
---

<!-- Open all links in new tab-->  
<base target="_blank"/>   


<!-- Go to www.addthis.com/dashboard to customize your tools --> <script type="text/javascript" src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5bc36900a405090b">  
</script> 



# R için arayüzler


## Analiz için genel GUI


### R Commander

https://socialsciences.mcmaster.ca/jfox/Misc/Rcmdr/


```
#### Obtain names of all packages on CRAN

names.available.packages <- rownames(available.packages())

#### Extract packages names that contain Rcmdr

Rcmdr.related.packages <- names.available.packages[grep("Rcmdr", names.available.packages)]
Rcmdr.related.packages

#### Install these packages
install.packages(pkgs = Rcmdr.related.packages)
```

```
# load library & run Rcmdr
library(Rcmdr)
# run Rcmdr
Rcmdr::Commander()
```



#### EZR

An extension for R Commander

http://www.jichi.ac.jp/saitama-sct/SaitamaHP.files/statmedEN.html

[Investigation of the freely available easy-to-use software ‘EZR’ for medical statistics](http://www.nature.com/bmt/journal/vaop/ncurrent/pdf/bmt2012244a.pdf)



### RKWard

https://rkward.kde.org/Main_Page


### Rattle


https://rattle.togaware.com/


### Jamovi

https://www.jamovi.org/


<!-- [![](https://www.jamovi.org/assets/main-screenshot.png)](https://www.jamovi.org/) -->


### JASP


https://jasp-stats.org/

<!-- [![](https://jasp-stats.org/wp-content/uploads/2018/12/start.png)](https://jasp-stats.org/) -->



### Blue Sky Statistics

https://www.blueskystatistics.com/


### R AnalyticFlow


https://r.analyticflow.com/en/



### Deducer

http://www.deducer.org/pmwiki/pmwiki.php?n=Main.DeducerManual

https://r4stats.com/2018/06/13/deducer/


```
install.packages(c("JGR","Deducer","DeducerExtras"))
```



```
library("JGR")
JGR()
```

### Radiant

https://radiant-rstats.github.io/docs/tutorials.html


### lessR

https://cran.r-project.org/web/packages/lessR/index.html

http://www.lessrstats.com/

http://www.lessrstats.com/videos.html


### Renjin

http://www.renjin.org/


### FastR


https://github.com/oracle/fastr


http://www.graalvm.org/docs/reference-manual/languages/r/


## Raporlama için


### Stencila


### Datazar


## Grafikler

### esquisse

https://github.com/dreamRs/esquisse

### ggExtra

https://github.com/daattali/ggExtra


### ggplotAssist

https://github.com/cardiomoon/ggplotAssist

### ggraptR

https://github.com/cargomoose/ggraptR


# Eclipse – an alternative to RStudio

https://datascienceplus.com/eclipse-an-alternative-to-rstudio-part-1/


---

# Diğer kodlar

- Diğer kodlar için bakınız: [https://sbalci.github.io/](https://sbalci.github.io/)


---

# Geri Bildirim

- Geri bildirim için tıklayınız: _[Geri bildirim formu](https://goo.gl/forms/YjGZ5DHgtPlR1RnB3)_

---

<script id="dsq-count-scr" src="//https-sbalci-github-io.disqus.com/count.js" async></script>

<div id="disqus_thread"></div>
<script>

/**
*  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
*  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/
/*
var disqus_config = function () {
this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};
*/
(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');
s.src = 'https://https-sbalci-github-io.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>

---


<!--chapter:end:R-Arayuzler.Rmd-->

---
title: "R drake"
---


Reproducible workflows at scale with drake

https://ropensci.org/commcalls/2019-09-24/









<!--chapter:end:R-drake.Rmd-->

---
title: R ile analize başlarken^[Bu bir derlemedir, mümkün mertebe alıntılara referans
  vermeye çalıştım.]
---

<!-- Open all links in new tab-->  
<base target="_blank"/>   

<!-- Go to www.addthis.com/dashboard to customize your tools --> <script type="text/javascript" src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5bc36900a405090b">  
</script>

<!-- [![](http://res.cloudinary.com/dyd911kmh/image/upload/f_auto,q_auto:best/v1530113077/Image_2_vfy48b.png)](https://www.datacamp.com/community/tutorials/data-science-pitfalls) -->


```{r setup global chunk settings, include=FALSE}
knitr::opts_chunk$set(
	message = FALSE,
	warning = FALSE,
	comment = NA,
	include = FALSE,
	tidy = TRUE
)
```


# R nerede kullanılır

- Veri düzenleme
- İstatistik analiz
- Web sayfası hazırlama (Statik/Dinamik)
https://sbalci.github.io/  
https://kevinrue.shinyapps.io/isee-shiny-contest/
- Sunum hazırlama (bu sunum)
- Programlama
https://serdarbalci.netlify.com/pathtweets/
- Otomatik, periodik ve tekrarlanabilir rapor hazırlama
https://sbalci.github.io/AutoJournalWatch/GallbladderRecent.html
- pdf, html, ppt oluşturma
- tez yazma
- kitap yazma
- CV oluşturma
https://rpubs.com/sbalci/CV   
- poster hazırlama
- rapor şablonu oluşturma
- Robot uygulamaları (Arudino kodu)
- ...



---

# R generation

R yıllar içinde çok fazla değişim gösterdi

https://rss.onlinelibrary.wiley.com/doi/10.1111/j.1740-9713.2018.01169.x

![](https://wol-prod-cdn.literatumonline.com/pb-assets/journal-banners/17409713-1501384756037.jpg)

![:scale 30%](https://wol-prod-cdn.literatumonline.com/cms/attachment/1a49c07b-b56f-4327-8e92-7827ef51a7bb/sign1169-gra-0002-m.jpg)


```{r eval=FALSE, include=FALSE, echo=TRUE}
knitr::include_graphics(path = "https://wol-prod-cdn.literatumonline.com/cms/attachment/1a49c07b-b56f-4327-8e92-7827ef51a7bb/sign1169-gra-0002-m.jpg")
```


---

# R yükleme

http://www.youtube.com/watch?v=XcBLEVknqvY


![What is R?](http://img.youtube.com/vi/XcBLEVknqvY/0.jpg)


---


## R-project

https://cran.r-project.org/

---

## RStudio


![](https://ismayc.github.io/talks/ness-infer/img/engine.png)

---

### RStudio

[https://www.rstudio.com/](https://www.rstudio.com/)

[https://www.rstudio.com/products/rstudio/download/](https://www.rstudio.com/products/rstudio/download/)

[https://moderndive.com/2-getting-started.html](https://moderndive.com/2-getting-started.html)

---

### RStudio


<!-- [![](http://www-users.york.ac.uk/~er13/RStudio%20Anatomy.svg)](https://buzzrbeeline.blog/2018/07/04/rstudio-anatomy/) -->



---

### RStudio eklentileri

- Discover and install useful RStudio addins

https://cran.r-project.org/web/packages/addinslist/README.html

https://rstudio.github.io/rstudioaddins/


```
devtools::install_github("rstudio/addinexamples", type = "source")
```


---

## MacOS için

### X11

https://www.xquartz.org/

### Java OS

https://support.apple.com/kb/dl1572

---


# R zor şeyler için kolay, kolay şeyler için zor


- [R makes easy things hard, and hard things easy](http://r4stats.com/articles/why-r-is-hard-to-learn/)


- Aynı şeyi çok fazla şekilde yapmak mümkün

R Syntax Comparison::CHEAT SHEET

https://www.amelia.mn/Syntax-cheatsheet.pdf


---


<blockquote class="twitter-tweet" data-lang="en"><p lang="en" dir="ltr"><a href="https://twitter.com/hashtag/RStats?src=hash&amp;ref_src=twsrc%5Etfw">#RStats</a> — There are always several ways to do the same thing... nice example on with the identity matrix by <a href="https://twitter.com/TeaStats?ref_src=twsrc%5Etfw">@TeaStats</a> <a href="https://t.co/O3GXdPiM32">https://t.co/O3GXdPiM32</a></p>&mdash; Colin Fay 🤘 (@_ColinFay) <a href="https://twitter.com/_ColinFay/status/1112746633467518977?ref_src=twsrc%5Etfw">April 1, 2019</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

---


# R paketleri


## Neden paketler var

[![](https://ismayc.github.io/talks/ness-infer/img/appstore.png)](https://ismayc.github.io/talks/ness-infer/slide_deck.html#7)

---

<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script><blockquote class="twitter-tweet" data-lang="en"><p lang="en" dir="ltr">I love the <a href="https://twitter.com/hashtag/rstats?src=hash&amp;ref_src=twsrc%5Etfw">#rstats</a> community.<br>Someone is like, &quot;oh hey peeps, I saw a big need for this mundane but difficult task that I infrequently do, so I created a package that will literally scrape the last bits of peanut butter out of the jar for you. It&#39;s called pbplyr.&quot;<br>What a tribe.</p>&mdash; Frank Elavsky ᴰᵃᵗᵃ ᵂᶦᶻᵃʳᵈ (@Frankly_Data) <a href="https://twitter.com/Frankly_Data/status/1014189095294291968?ref_src=twsrc%5Etfw">July 3, 2018</a></blockquote>

<!-- --- -->

<!-- https://blog.mitchelloharawild.com/blog/user-2018-feature-wall/ -->

---

![](https://blog.mitchelloharawild.com/blog/2018-07-11-user-2018-feature-wall_files/final.jpg)

---

## Paketleri nereden bulabiliriz

- Available CRAN Packages By Name  
https://cran.r-project.org/web/packages/available_packages_by_name.html

- CRAN Task Views  
https://cran.r-project.org/web/views/

- Bioconductor  
https://www.bioconductor.org

- RecommendR  
http://recommendr.info/

- pkgsearch  
CRAN package search  
https://github.com/metacran/pkgsearch

- CRANsearcher  
https://github.com/RhoInc/CRANsearcher  

- Awesome R  
https://awesome-r.com/  

---

## Kendi paket evrenini oluştur

- pkgverse: Build a Meta-Package Universe  
https://cran.r-project.org/web/packages/pkgverse/index.html


---

## R paket yükleme

```
{r # 
install.packages("tidyverse", dependencies = TRUE)
install.packages("jmv", dependencies = TRUE)
install.packages("questionr", dependencies = TRUE)
install.packages("Rcmdr", dependencies = TRUE)
install.packages("summarytools")
```

---

## Paket çağırma

```
{r # 
require(tidyverse)  
require(jmv)  
require(questionr)  
library(summarytools)  
library(gganimate)  
```



---

# R için yardım bulma


```
{r # 
?mean
??efetch
help(merge)
example(merge)
RSiteSearch("shiny")
```

---

- Vignette

![:scale 80%](figures/vignette.png)

---

- RDocumentation
https://www.rdocumentation.org

- R Package Documentation
https://rdrr.io/

- GitHub

- Stackoverflow

https://stackoverflow.com/

- Google uygun anahtar kelime

---

<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script><blockquote class="twitter-tweet" data-lang="en"><p lang="en" dir="ltr">How I use <a href="https://twitter.com/hashtag/rstats?src=hash&amp;ref_src=twsrc%5Etfw">#rstats</a> <br>h/t <a href="https://twitter.com/ThePracticalDev?ref_src=twsrc%5Etfw">@ThePracticalDev</a> <a href="https://t.co/erRnTG0Ujr">pic.twitter.com/erRnTG0Ujr</a></p>&mdash; Emily Bovee (@ebovee09) <a href="https://twitter.com/ebovee09/status/1028037594947485696?ref_src=twsrc%5Etfw">August 10, 2018</a></blockquote>


---


![](figures/Google-package-name.png)

---



![](figures/Google-start-with-R.png)


- Google'da ararken `[R]` yazmak da işe yarayabiliyor.


---

- searcher package 📦


[![](https://camo.githubusercontent.com/12f0e2d18047f1b5f36fbeb09a1d0e548236883f/68747470733a2f2f692e696d6775722e636f6d2f5a7132726736472e676966)](https://github.com/coatless/searcher)



---

- Awesome Cheatsheet
https://github.com/detailyang/awesome-cheatsheet

http://cran.r-project.org/doc/contrib/Baggott-refcard-v2.pdf

https://www.rstudio.com/resources/cheatsheets/


- Awesome R

https://github.com/qinwf/awesome-R#readme

https://awesome-r.com/


- Twitter

https://twitter.com/hashtag/rstats?src=hash

---


- Use Reproducible Examples When Asking  

<blockquote class="twitter-tweet" data-lang="en"><p lang="en" dir="ltr">Got a question to ask on <a href="https://twitter.com/SlackHQ?ref_src=twsrc%5Etfw">@SlackHQ</a> or post on <a href="https://twitter.com/github?ref_src=twsrc%5Etfw">@github</a>? No time to read the long post on how to use reprex? Here is a 20-second gif for you to format your R codes nicely and for others to reproduce your problem. (An example from a talk given by <a href="https://twitter.com/JennyBryan?ref_src=twsrc%5Etfw">@JennyBryan</a>) <a href="https://twitter.com/hashtag/rstat?src=hash&amp;ref_src=twsrc%5Etfw">#rstat</a> <a href="https://t.co/gpuGXpFIsX">pic.twitter.com/gpuGXpFIsX</a></p>&mdash; ZhiYang (@zhiiiyang) <a href="https://twitter.com/zhiiiyang/status/1053006003711569920?ref_src=twsrc%5Etfw">October 18, 2018</a></blockquote><script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>


- Keeping up to date with R news  
https://masalmon.eu/2019/01/25/uptodate/  

---

# Rstudio ile proje oluşturma

https://support.rstudio.com/hc/en-us/articles/200526207-Using-Projects

![](http://www.rstudio.com/images/docs/projects_new.png)

---

# RStudio ile veri yükleme

https://support.rstudio.com/hc/en-us/articles/218611977-Importing-Data-with-RStudio

![](https://support.rstudio.com/hc/en-us/article_attachments/206277618/data-import-overview.gif)

---

## Excel

## SPSS

## CSV


---

# Veriyi görüntüleme

<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script><blockquote class="twitter-tweet" data-lang="en"><p lang="en" dir="ltr">Spreadsheet users using <a href="https://twitter.com/hashtag/rstats?src=hash&amp;ref_src=twsrc%5Etfw">#rstats</a>:  where&#39;s the data?<a href="https://twitter.com/hashtag/rstats?src=hash&amp;ref_src=twsrc%5Etfw">#rstats</a> users using spreadsheets:  where&#39;s the code?</p>&mdash; Leonard Kiefer (@lenkiefer) <a href="https://twitter.com/lenkiefer/status/1015587475580956672?ref_src=twsrc%5Etfw">July 7, 2018</a></blockquote>

---

# Veriyi görüntüleme

```
{r # 
library(nycflights13)
summary(flights)
```

```
{r # 
View(data)
```

```
{r # 
data()
```


```
{r # 
head(data, n = 10)
```

```
{r # 
tail(data)
```

```
{r # 
glimpse(data)
```


```
{r # 
str(data)
```


```
{r # 
skimr::skim()
```

---


# Veriyi değiştirme

## Veriyi kod ile değiştirelim

## Veriyi eklentilerle değiştirme

![:scale 30%](figures/change_data.png)


---


## RStudio aracılığıyla recode

*questionr* paketi kullanılacak

![:scale 30%](figures/level_recode.png)


---



https://juba.github.io/questionr/articles/recoding_addins.html


![](https://raw.githubusercontent.com/juba/questionr/master/resources/screenshots/irec_1.png)


---

![](https://raw.githubusercontent.com/juba/questionr/master/resources/screenshots/irec_2.png)


---

![](https://raw.githubusercontent.com/juba/questionr/master/resources/screenshots/irec_3.png)


---

# Basit tanımlayıcı istatistikler

```
summary()
```

```
mean
```

```
median
```

```
min
```

```
max
```

```
sd
```

```
table()
```

---


```{r, echo=TRUE, include = TRUE}
library(readr)
irisdata <- read_csv("data/iris.csv")

jmv::descriptives(
    data = irisdata,
    vars = "Sepal.Length",
    splitBy = "Species",
    freq = TRUE,
    hist = TRUE,
    dens = TRUE,
    bar = TRUE,
    box = TRUE,
    violin = TRUE,
    dot = TRUE,
    mode = TRUE,
    sum = TRUE,
    sd = TRUE,
    variance = TRUE,
    range = TRUE,
    se = TRUE,
    skew = TRUE,
    kurt = TRUE,
    quart = TRUE,
    pcEqGr = TRUE)
```

---

```{r, echo=TRUE, include=FALSE}
# install.packages("scatr")
scatr::scat(
    data = irisdata,
    x = "Sepal.Length",
    y = "Sepal.Width",
    group = "Species",
    marg = "dens",
    line = "linear",
    se = TRUE)
```

---

## summarytools

https://cran.r-project.org/web/packages/summarytools/vignettes/Introduction.html

```{r, include=FALSE, comment=NA, prompt=FALSE, cache=FALSE, echo=TRUE, results='asis'}
library(summarytools)
summarytools::freq(iris$Species, style = "rmarkdown")
```


---

```{r, include=FALSE, comment=NA, prompt=FALSE, cache=FALSE, echo=TRUE, results='asis'}
summarytools::freq(iris$Species, report.nas = FALSE, style = "rmarkdown", headings = FALSE)
```


---


```{r, include=FALSE, comment=NA, prompt=FALSE, cache=FALSE, echo=TRUE, results='asis'}
with(tobacco, print(ctable(smoker, diseased), method = 'render'))
```


---


```{r, include=FALSE, comment=NA, prompt=FALSE, cache=FALSE, echo=TRUE, results='asis'}
with(tobacco,
     print(ctable(smoker, diseased, prop = 'n', totals = FALSE),
           headings = TRUE, method = "render"))
```

---

```{r, include=FALSE, comment=NA, prompt=FALSE, cache=FALSE, echo=TRUE, results='asis'}
summarytools::descr(iris, style = "rmarkdown")
```

---

```{r, include=FALSE, comment=NA, prompt=FALSE, cache=FALSE, echo=TRUE, results='asis'}
descr(iris,
      stats = c("mean", "sd", "min", "med", "max"),
      transpose = TRUE,
      headings = FALSE,
      style = "rmarkdown")
```

---

```{r, include=FALSE, comment=NA, prompt=FALSE, cache=FALSE, echo=TRUE, results='asis'}
# view(dfSummary(iris))

```


<!-- ![](figures/dfsummary.png) -->

<!-- --- -->

<!-- ```{r, include=FALSE, comment=NA, prompt=FALSE, cache=FALSE, echo=TRUE, results='asis'} -->
<!-- dfSummary(tobacco, -->
<!--           plain.ascii = FALSE, -->
<!--           style = "grid") -->
<!-- ``` -->


---

```{r, include=FALSE, comment=NA, prompt=FALSE, cache=FALSE, echo=TRUE, results='asis'}
# First save the results
iris_stats_by_species <- by(data = iris,
                            INDICES = iris$Species,
                            FUN = descr, stats = c("mean", "sd", "min", "med", "max"),
                            transpose = TRUE)
# Then use view(), like so:
view(iris_stats_by_species, method = "pander", style = "rmarkdown")
```


---


```{r, include=FALSE, comment=NA, prompt=FALSE, cache=FALSE, echo=TRUE, results='asis'}
# view(iris_stats_by_species)
```

![](figures/DescriptiveStatistics.png)

---

```{r, include=FALSE, comment=NA, prompt=FALSE, cache=FALSE, echo=TRUE, results='asis'}
data(tobacco) # tobacco is an example dataframe included in the package
BMI_by_age <- with(tobacco,
                   by(BMI, age.gr, descr,
                      stats = c("mean", "sd", "min", "med", "max")))
view(BMI_by_age, "pander", style = "rmarkdown")
```

---

```{r, include=FALSE, comment=NA, prompt=FALSE, cache=FALSE, echo=TRUE, results='asis'}
BMI_by_age <- with(tobacco,
                   by(BMI, age.gr, descr,  transpose = TRUE,
                      stats = c("mean", "sd", "min", "med", "max")))

view(BMI_by_age, "pander", style = "rmarkdown", headings = TRUE)
```

<!-- --- -->

<!-- ```{r, include=FALSE, comment=NA, prompt=FALSE, cache=FALSE, echo=TRUE, results='asis'} -->
<!-- tobacco_subset <- tobacco[ ,c("gender", "age.gr", "smoker")] -->
<!-- freq_tables <- lapply(tobacco_subset, freq) -->

<!-- # view(freq_tables, footnote = NA, file = 'freq-tables.html') -->
<!-- ``` -->

---

```{r, include=FALSE, comment=NA, prompt=FALSE, cache=FALSE, echo=TRUE, results='asis'}
what.is(iris)
```

<!-- --- -->

<!-- ```{r eval=FALSE, include=FALSE, echo=TRUE} -->
<!-- freq(tobacco$gender, style = 'rmarkdown') -->
<!-- ``` -->

<!-- --- -->

<!-- ```{r eval=FALSE, include=FALSE, echo=TRUE} -->
<!-- print(freq(tobacco$gender), method = 'render') -->
<!-- ``` -->

<!-- --- -->

<!-- ## skimr -->

<!-- ``` -->
<!-- library(skimr) -->
<!-- skim(df) -->
<!-- ``` -->

---

## DataExplorer

```
library(DataExplorer)
DataExplorer::create_report(df)
```


[![](https://static1.squarespace.com/static/58eef8846a4963e429687a4d/t/5bdfc2fb4d7a9c04ee50b7aa/1541391160702/dataExplorerGifLg.gif?format=1500w)](https://www.littlemissdata.com/blog/simple-eda)



---

## inspectdf

https://github.com/alastairrushworth/inspectdf


---

## Grafikler

```{r echo=TRUE}
# library(ggplot2)
# library(mosaic)
# mPlot(irisdata)
```

<!-- --- -->


<!-- ```{r, results="asis"} -->
<!-- ctable(tobacco$gender, tobacco$smoker, style = 'rmarkdown') -->
<!-- ``` -->

<!-- --- -->


<!-- ```{r eval=FALSE, include=FALSE, echo=TRUE} -->
<!-- print(ctable(tobacco$gender, tobacco$smoker), method = 'render') -->
<!-- ``` -->


<!-- --- -->

```
descr(tobacco, style = 'rmarkdown')

print(descr(tobacco), method = 'render', table.classes = 'st-small')

dfSummary(tobacco, style = 'grid', plain.ascii = FALSE)

print(dfSummary(tobacco, graph.magnif = 0.75), method = 'render')
```

---

A beginner kit for #rstats
The Landscape of R Packages for Automated Exploratory Data Analysis
https://journal.r-project.org/archive/2019/RJ-2019-033/


@article{RJ-2019-033,
  author = {Mateusz Staniak and Przemysław Biecek},
  title = {{The Landscape of R Packages for Automated Exploratory Data
          Analysis}},
  year = {2019},
  journal = {{The R Journal}},
  doi = {10.32614/RJ-2019-033},
  url = {https://journal.r-project.org/archive/2019/RJ-2019-033/index.html}
}



---



<blockquote class="twitter-tweet" data-lang="en"><p lang="en" dir="ltr">Here, building up a <a href="https://twitter.com/hashtag/ggplot2?src=hash&amp;ref_src=twsrc%5Etfw">#ggplot2</a> as slowly as possible, <a href="https://twitter.com/hashtag/rstats?src=hash&amp;ref_src=twsrc%5Etfw">#rstats</a>.  Incremental adjustments.  <a href="https://twitter.com/hashtag/rstatsteachingideas?src=hash&amp;ref_src=twsrc%5Etfw">#rstatsteachingideas</a> <a href="https://t.co/nUulQl8bPh">pic.twitter.com/nUulQl8bPh</a></p>&mdash; Gina Reynolds (@EvaMaeRey) <a href="https://twitter.com/EvaMaeRey/status/1029104656763572226?ref_src=twsrc%5Etfw">August 13, 2018</a></blockquote><script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>


---


[![](https://raw.githubusercontent.com/dreamRs/esquisse/master/man/figures/esquisse.gif)](https://github.com/dreamRs/esquisse)


<blockquote class="twitter-tweet" data-lang="en"><p lang="en" dir="ltr">Dreaming of a fancy <a href="https://twitter.com/hashtag/Rstats?src=hash&amp;ref_src=twsrc%5Etfw">#Rstats</a> <a href="https://twitter.com/hashtag/ggplot?src=hash&amp;ref_src=twsrc%5Etfw">#ggplot</a> <a href="https://twitter.com/hashtag/dataviz?src=hash&amp;ref_src=twsrc%5Etfw">#dataviz</a> but still scared of typing <a href="https://twitter.com/hashtag/code?src=hash&amp;ref_src=twsrc%5Etfw">#code</a>? <a href="https://twitter.com/_pvictorr?ref_src=twsrc%5Etfw">@_pvictorr</a> esquisse package has you covered <a href="https://t.co/1vIDXcVAAF">https://t.co/1vIDXcVAAF</a> <a href="https://t.co/RlTkptnrNv">pic.twitter.com/RlTkptnrNv</a></p>&mdash; Radoslaw Panczak (@RPanczak) <a href="https://twitter.com/RPanczak/status/1047019588658040832?ref_src=twsrc%5Etfw">October 2, 2018</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>



<!-- ## Tablolar -->


---


# Bazı arayüzler

https://sbalci.github.io/MyRCodesForDataAnalysis/R-Arayuzler.nb.html


---


## Rcmdr

```
library(Rcmdr)

Rcmdr::Commander()

```


- A Comparative Review of the R Commander GUI for R

http://r4stats.com/articles/software-reviews/r-commander/


---

## jamovi

https://www.jamovi.org/

<!-- ![![](https://www.jamovi.org/assets/main-screenshot.png)](https://www.jamovi.org/) -->


https://blog.jamovi.org/2018/07/30/rj.html

<!-- ![![](https://blog.jamovi.org/assets/images/rj.png)](https://blog.jamovi.org/2018/07/30/rj.html) -->

---

# R nereden öğrenilir

https://sbalci.github.io/MyRCodesForDataAnalysis/WhereToLearnR.nb.html

---

# Sonraki Konular

- RStudio ile GitHub kullanımı
- R Markdown ve R Notebook ile tekrarlanabilir rapor
- Hipotez testleri



-----


# Geri Bildirim

- Geri bildirim için tıklayınız: _[Geri bildirim formu](https://goo.gl/forms/YjGZ5DHgtPlR1RnB3)_


---


<script id="dsq-count-scr" src="//https-sbalci-github-io.disqus.com/count.js" async></script>

<div id="disqus_thread"></div>
<script>

/**
*  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
*  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/
/*
var disqus_config = function () {
this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};
*/
(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');
s.src = 'https://https-sbalci-github-io.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>


---


<!-- ``` -->
<!-- # Save Final Data -->

<!-- saved data after analysis to `Data-After-Analysis.xlsx`. -->

<!-- saveRDS(mydata, "Data-After-Analysis.rds") -->

<!-- writexl::write_xlsx(mydata, "Data-After-Analysis.xlsx") -->

<!-- file.info("Data-After-Analysis.xlsx")$ctime -->

<!-- ``` -->

<!-- --- -->


# Libraries Used

```{r eval=FALSE, include=FALSE, echo=TRUE}
citation()
```

```
citation("tidyverse")
citation("foreign")
citation("tidylog")
citation("janitor")
citation("jmv")
citation("tangram")
citation("finalfit")
citation("summarytools")
citation("ggstatplot")
citation("readxl")
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
citation("tidyverse")
citation("foreign")
citation("tidylog")
citation("janitor")
citation("jmv")
citation("tangram")
citation("finalfit")
citation("summarytools")
citation("ggstatplot")
citation("readxl")
```


<!-- --- -->


<!-- ```{r, results='asis'} -->
<!-- report::cite_packages(session = sessionInfo()) -->
<!-- ``` -->


<!-- --- -->

```{r eval=FALSE, include=FALSE, echo=TRUE}
sessionInfo()
```

---


# İletişim  

Completed on `{r #  Sys.time()`.  

Serdar Balci, MD, Pathologist  
drserdarbalci@gmail.com  

https://rpubs.com/sbalci/CV   
https://sbalci.github.io/  
https://github.com/sbalci  
https://twitter.com/serdarbalci

---

```{r eval=FALSE, include=FALSE, echo=TRUE}
CommitMessage <- paste("updated on ", Sys.time(), sep = "")

wd <- getwd()

gitCommand <- paste("cd ", wd, " \n git add . \n git commit --message '", CommitMessage, "' \n git push origin master \n", sep = "")

system(command = gitCommand, intern = TRUE
)

```









<!--chapter:end:R-Giris.Rmd-->

---
title: R, RStudio ve RMarkdown ile Tekrarlanabilir Rapor^[Bu bir derlemedir, mümkün mertebe alıntılara linklerle referans vermeye çalıştım.]
---

```
author: "[Serdar Balcı, MD, Pathologist](https://sbalci.github.io/)"
institute: "[serdarbalci.com](https://www.serdarbalci.com)"
date: "`{r #  format(Sys.Date())`"
output:
  revealjs::revealjs_presentation:
    incremental: true
    theme: sky
    highlight: pygments
    center: false
    smart: true
    transition: fade
    self_contained: true
    ig_width: 7
    fig_height: 6
    fig_caption: true
    reveal_options:
      slideNumber: true
      previewLinks: true
  rmdshower::shower_presentation:
  xaringan::moon_reader:
    lib_dir: libs
    nature:
      beforeInit: ["macros.js", "https://platform.twitter.com/widgets.js"]
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
    self_contained: true
  html_notebook:
    fig_caption: yes
    highlight: kate
    number_sections: yes
    theme: flatly
    toc: yes
    toc_depth: 5
    toc_float: yes
  prettydoc::html_pretty:
    theme: leonids
    highlight: github
  pdf_document:
    toc: yes
    toc_depth: '5'
  html_document:
    fig_caption: yes
    keep_md: yes
    toc: yes
    toc_depth: 5
    toc_float: yes
editor_options: 
  chunk_output_type: inline
```


<!-- Open all links in new tab-->  
<base target="_blank"/>  


<!-- Go to www.addthis.com/dashboard to customize your tools --> <script type="text/javascript" src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5bc36900a405090b">  
</script>



```{r eval=FALSE, include=FALSE, echo=TRUE}
knitr::opts_chunk$set(fig.width = 12, fig.height = 8, fig.path = 'Figs/', echo = TRUE, warning = FALSE, message = FALSE, error = FALSE, eval = TRUE, tidy = TRUE, comment = NA, cache = TRUE)
```



```{r eval=FALSE, message=FALSE, warning=FALSE, include=FALSE}
# xaringan::inf_mr()
# servr::daemon_stop(1)
```



# Tekrarlanabilir Analiz ve Rapor


[![](https://the-turing-way.netlify.com/figures/reproducibility/ReproducibleMatrix.jpg)](https://the-turing-way.netlify.com/reproducibility/03/definitions.html)




---


## Replication Crisis

![](figures/replicationCrisis.png)

https://en.wikipedia.org/wiki/Replication_crisis


---

## Replication Crisis Excel Version

[![](figures/geneNamesExcel.png)](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-016-1044-7)


---

# RStudio ile proje oluştur



![](images/RStudio-NewProject.gif)



---

# R Notebook  

## R Notebook dökümanı oluşturma 


![](images/RNotebook1.gif)

---

## R Notebook'tan html, pdf ve word oluşturma  


![](images/RNotebook2.gif)


---

## RNotebook vs RMarkdown  

<iframe width="560" height="315" src="https://www.youtube.com/embed/zNzZ1PfUDNk" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>  

https://youtu.be/zNzZ1PfUDNk


---

# R Markdown

## Hem kendi kodları hem de html kodları yazılabilir

https://rmarkdown.rstudio.com

<iframe src="https://player.vimeo.com/video/178485416?color=428bca&title=0&byline=0&portrait=0" width="640" height="400" frameborder="0" allow="autoplay; fullscreen" allowfullscreen></iframe>
<p><a href="https://vimeo.com/178485416">What is R Markdown?</a> from <a href="https://vimeo.com/rstudioinc">RStudio, Inc.</a> on <a href="https://vimeo.com">Vimeo</a>.</p>

---

## R Markdown: The Definitive Guide

https://bookdown.org/yihui/rmarkdown/


---

## R Markdown syntax

https://gist.github.com/MinhasKamal/7fdebb7c424d23149140


<script src="https://gist.github.com/MinhasKamal/7fdebb7c424d23149140.js"></script>



---

## Remedy Package  

[<img src="https://raw.githubusercontent.com/ThinkR-open/remedy/master/reference/figures/thinkr-hex-remedy.png" width=250px>](https://github.com/ThinkR-open/remedy)


---

### Remedy  

[<img src="https://raw.githubusercontent.com/ThinkR-open/remedy/master/reference/figures/remedy_example.gif" width=500px>](https://github.com/ThinkR-open/remedy)


---

## R Markdown paket ve şablonları  

https://bookdown.org/yihui/rmarkdown/document-templates.html


![](images/RMarkdownTemplates.gif)


---

## Render Markdown via code

*inside R*

```
markdown::markdownToHTML('markdown_example.md', 
'markdown_example.html')
```

*command line*

```
R -e "markdown::markdownToHTML('markdown_example.md',
'markdown_example.html')"
```

---


## pandoc Rstudio integration

*command line*

```
export PATH=$PATH:/Applications/RStudio.app/Contents/MacOS/pandoc
```


```
R -e "rmarkdown::render('markdown_example.md')"
```


---

# RMarkdown `chunk` içinde `R` kodlarını çalıştırma


```
{r, results='asis'}
 iris %>%
  tibble::as_tibble() %>%
  details::details(summary = 'tibble')
```

---

# Metin arasında `R` kodlarını çalıştırma


![](images/inlineRCode.png)


---

# Chunk Options

## Global Options

```
{r , include=FALSE}
knitr::opts_chunk$set(fig.width = 12,
                      fig.height = 8,
                      fig.path = 'Figs/',
                      echo = TRUE,
                      warning = FALSE,
                      message = FALSE,
                      error = FALSE,
                      eval = TRUE,
                      tidy = TRUE,
                      comment = NA)
```

---

## Other Code Languages


[![](https://d33wubrfki0l68.cloudfront.net/162347ef5afe219da22fb7d7d9a5989f2c3e5a85/59316/lesson-images/languages-1-demos.png)](https://rmarkdown.rstudio.com/lesson-5.html)


---


# R Markdown kod örneği  


```
{r}
data("cancer")
cancer
foreign::write.foreign(df = cancer,
                        datafile = "data/cancer.sav",
                        codefile = "data/cancer.spo",
                        package = "SPSS"
                        )
```


---

# R Markdown Paket Çağırma 📦    


```
{r}
suppressPackageStartupMessages(library("tidyverse"))
suppressPackageStartupMessages(library("survival"))
```

---

## Sık kullandığım paketler 📦  

{tidyverse}
{tidylog}

{lubridate}
{janitor}

{readxl}
{foreign}

{summarytools}
{ggstatsplot}
{tangram}
{finalfit}
{psycho}
{jmv}

{survival}
{survminer}

{report}
{kableExtra}

---

# R Markdown Veri Yükleme SPSS  

![](images/importSPSS.gif)


---

# R Markdown Veri Yükleme Excel  



![](images/importExcel.gif)


---

# Veri Görüntüleme


```
{r}
View(mydata)
glimpse(mydata)
```




---

# Veri Düzenleme

```
{r}
mydata <- janitor::clean_names(mydata)
```

```
{r}
mydata$sontarih <- janitor::excel_numeric_to_date(
  as.numeric(mydata$olum_tarihi)
  )
```


---

# Recode


```
{r}
mydata$Outcome <- "Dead"
mydata$Outcome[mydata$olum_tarihi == "yok"] <- "Alive"
```


```
{r}
## Recoding mydata$cinsiyet into mydata$Cinsiyet
mydata$Cinsiyet <- recode(mydata$cinsiyet,
               "K" = "Kadin",
               "E" = "Erkek")
mydata$Cinsiyet <- factor(mydata$Cinsiyet)
```


---

# Recode regular expression


```
{r recode TNM stage}
#pT2N0Mx -> 2
mydata$Tstage <- stringr::str_match(
  mydata$patolojik_evre, 
  paste('(.+)', "N", sep=''))[,2]
)
```


---

# Recode regular expression case_when

```
{r recode TNM2}
mydata <- mydata %>% 
    mutate(
        T_stage = case_when(
            grepl(pattern = "T1", x = .$Tstage) == TRUE ~ "T1",
            grepl(pattern = "T2", x = .$Tstage) == TRUE ~ "T2",
            grepl(pattern = "T3", x = .$Tstage) == TRUE ~ "T3",
            grepl(pattern = "T4", x = .$Tstage) == TRUE ~ "T4",
            TRUE ~ "Tx"
        )
    )
```

---

# Recode regular expression case_when

```
{r}
mydata <- mydata %>% 
    mutate(
TumorPDL1gr1 = case_when(
        t_pdl1 < 1 ~ "kucuk1",
        t_pdl1 >= 1 ~ "buyukesit1"
    )
    )
```

---

# R Markdown Tanımlayıcı İstatistikler  


```
{r}
library(summarytools)
view(dfSummary(colon_s))
```


---


A beginner kit for #rstats
The Landscape of R Packages for Automated Exploratory Data Analysis
https://journal.r-project.org/archive/2019/RJ-2019-033/



@article{RJ-2019-033,
  author = {Mateusz Staniak and Przemysław Biecek},
  title = {{The Landscape of R Packages for Automated Exploratory Data
          Analysis}},
  year = {2019},
  journal = {{The R Journal}},
  doi = {10.32614/RJ-2019-033},
  url = {https://journal.r-project.org/archive/2019/RJ-2019-033/index.html}
}



---


## Table One  


```
{r, results='asis'}
# cat(names(mydata), sep = " + \n")
library(arsenal)
tab1 <- tableby(~ Cinsiyet + 
Yas + 
TumorYerlesimi
                ,
                data = mydata)
summary(tab1)
```

---

## The Grammar of Tables


[tangram: The Grammar of Tables](https://cran.r-project.org/web/packages/tangram/)

[A grammar of tables](https://github.com/leeper/tttable)

[Grammar of Tables?](https://gist.github.com/leeper/f9cfbe6bd185763762e126a4d8d7c286)

[Easily generate information-rich, publication-quality tables from R](https://gt.rstudio.com)


---

## Kategorik Veriler

```
{r}
mydata %>% 
  janitor::tabyl(Categorical) %>%
  adorn_pct_formatting(rounding = 'half up',
                       digits = 1) %>%
  knitr::kable()
```

```
{r crosstable}
mydata %>%
    summary_factorlist(dependent = dependent, 
                       explanatory = explanatory,
                       total_col = TRUE,
                       p = TRUE,
                       add_dependent_label = TRUE) -> table
knitr::kable(table, row.names = FALSE, align = c('l', 'l', 'r', 'r', 'r'))
```

---

## Kategorik Veriler için Grafikler  

```
{r ggstatplot, layout='l-page'}
mydata %>% 
    ggstatsplot::ggbarstats(data = .,
                            main = Categorical_variable,
                            condition =  dependent_variable
                            )
```

---


## Continious Variables

```
{r}
mydata %>% 
jmv::descriptives(
    data = .,
    vars = c(yas),
    hist = TRUE,
    dens = TRUE,
    box = TRUE,
    violin = TRUE,
    dot = TRUE,
    mode = TRUE,
    sd = TRUE,
    variance = TRUE,
    skew = TRUE,
    kurt = TRUE,
    quart = TRUE)
```

---

# R Markdown örneği Çapraz Tablolar  

```
{r crosstable}
library(finalfit)
mydata %>%
    summary_factorlist(dependent = dependent, 
                       explanatory = explanatory,
                       column = TRUE,
                       total_col = TRUE,
                       p = TRUE,
                       add_dependent_label = TRUE,
                       na_include=FALSE
                       # catTest = catTestfisher
                       ) -> table
knitr::kable(table,
             row.names = FALSE,
             align = c('l', 'l', 'r', 'r', 'r'))
```

---

# R Markdown örneği Sağkalım  

- Drawing Survival Curves Using ggplot2  
https://rpkgs.datanovia.com/survminer/reference/ggsurvplot.html

## Sağkalım için veriyi düzenleme

```
{r define survival time}
mydata$int <- lubridate::interval(
  lubridate::ymd(mydata$CerrahiTarih),
  lubridate::ymd(mydata$SonTarih)
  )
mydata$OverallTime <- lubridate::time_length(mydata$int, "month")
mydata$OverallTime <- round(mydata$OverallTime, digits = 1)
```

```
{r}
## Recoding mydata$Outcome into mydata$Outcome2
mydata$Outcome2 <- recode(mydata$Outcome,
               "Alive" = "0",
               "Dead" = "1")
mydata$Outcome2 <- as.numeric(mydata$Outcome2)
```

---

## Kaplan-Meier

```
{r Kaplan-Meier}
mydata %>%
  finalfit::surv_plot(dependent,
                      explanatory,
                      xlab='Time (months)',
                      pval=TRUE,
                      legend = 'none',
                      break.time.by = 12,
                      xlim = c(0,60),
                      legend.labs = c('a','b')
)
```

---

## Sağkalım Tabloları

```
{r}
km_fit <- survfit(dependent ~ explanatory,
                  data = mydata)
km_fit
```

```
{r, eval=FALSE, include=FALSE, echo=TRUE}
library(survival)
km <- with(mydata, Surv(OverallTime, Outcome2))
# head(km,80)
# plot(km)
```

```
{r 1-3-5-yr}
summary(km_fit, times = c(12,36,60))
```

---

## Pairwise comparison

```
{r}
survminer::pairwise_survdiff(formula = Surv(time, Outcome) ~ Group, 
                             data = mydata,
                             p.adjust.method = "BH")
```

---

## Multivariate Analysis Survival

```
{r Multivariate Analysis, eval=FALSE, include=FALSE, echo=TRUE}
library(finalfit)
library(survival)
explanatoryMultivariate <- explanatoryKM
dependentMultivariate <- dependentKM
mydata %>%
  finalfit(dependentMultivariate, explanatoryMultivariate) -> tMultivariate
knitr::kable(tMultivariate, row.names=FALSE, align=c("l", "l", "r", "r", "r", "r"))
```

---

# jamovi

## jamovi ve R entegrasyonu

[Rj Editor – Analyse your data with R in jamovi](https://blog.jamovi.org/2018/07/30/rj.html)

![](images/jamoviRjEditor.gif)

---

## {jmv} paket kodları

[jamovi syntax mode](https://www.jamovi.org/user-manual.html#syntax-mode)


![](images/jamoviSyntaxMode.gif)



---

# Güncellemeler olunca kodlar çalışacak mı?


---

## Paket Kütüphaneleri

- packrat / renv

https://environments.rstudio.com

---

## Docker

- docker

---

### The Rocker Project

Docker Containers for the R Environment

```
docker run --rm -ti rocker/r-base
```

Or get started with an RStudio® instance:

```
docker run -e PASSWORD=yourpassword --rm -p 8787:8787 rocker/rstudio
```

and point your browser to [localhost:8787](localhost:8787)
Log in with user/password rstudio/yourpassword 

---

[![](images/TheRockerProject.png)](https://www.rocker-project.org)

[Managing containers](https://www.rocker-project.org/use/managing_containers/)


---

## Yeni R sürümleri 

- RSwitch

https://rud.is/rswitch/

- Using RSwitch

https://rud.is/rswitch/guide/


![: scale 30%](https://rud.is/rswitch/guide/menu-info.png)


---

# Yedeklemeyi nasıl yapacağız

## Projeyi düzgün organize edin

- pdf
- R
- images
- bib

```
{r load library}
source(file = here::here("R", "loadLibrary.R"))
```

---

## Save Final Data

```
{r}
saved data after analysis to `mydata.xlsx`.

save.image(file = here::here("data", "mydata_work_space.RData"))

readr::write_rds(x = mydata, path = here::here("data", "mydata_afteranalysis.rds"))

saveRDS(object = mydata, file = here::here("data", "mydata.rds"))

writexl::write_xlsx(mydata, here::here("data", "mydata.xlsx"))

paste0(rownames(file.info(here::here("data", "mydata.xlsx"))), " : ", file.info(here::here("data", "mydata.xlsx"))$ctime)

```

---

## GitHub  

```
{r github push}
CommitMessage <- paste("updated on ", Sys.time(), sep = "")
wd <- getwd()
gitCommand <- paste("cd ", 
                    wd,
                    " \n git add . \n git commit --message '",
                    CommitMessage,
                    "' \n git push origin master \n",
                    sep = ""
                    )
system(command = gitCommand,
       intern = TRUE
)
```

---

## GitHub Yedekleme

```{r github push, echo=TRUE}
CommitMessage <- paste("updated on ", Sys.time(), sep = "")
wd <- getwd()
gitCommand <- paste("cd ", 
                    wd,
                    " \n git add . \n git commit --message '",
                    CommitMessage,
                    "' \n git push origin master \n",
                    sep = ""
                    )
system(command = gitCommand,
       intern = TRUE
)
```

---

# Her dökümanın sonuna kullandığınız kütüphaneler için atıf yazdırabilirsiniz

```
{r}
citation()
```

---

## Libraries Used  

```{r  citation, echo=TRUE}
citation()
```

---

## Bu oturuma spesifik kullanılan paketler  

```{r  citation as report, echo=TRUE, results='asis'}
# report::cite_packages(session = sessionInfo())
```


---

## Tek tek paket atıfları


```
{r  citations}
citation("tidyverse")
citation("readxl")
citation("janitor")
citation("report")
citation("finalfit")
citation("ggstatplot")
```


---

## Jamovi ve R için atıf örneği

- The jamovi project (2019). jamovi. (Version 0.9) [Computer Software]. Retrieved from https://www.jamovi.org.

- R Core Team (2018). R: A Language and envionment for statistical computing. [Computer software]. Retrieved from https://cran.r-project.org/.

- Fox, J., & Weisberg, S. (2018). car: Companion to Applied Regression. [R package]. Retrieved from https://cran.r-project.org/package=car.



---

# Her dökümanın sonuna oturum detaylarınızı yazdırabilirsiniz  

```
{r session info, echo=TRUE}
sessionInfo()
```

---

## Session Info

```{r session info, echo=TRUE}
sessionInfo()
```

---

# Sonraki Konular

- RStudio ile GitHub kullanımı
- ...

---

# Önerilen Kaynaklar

- [Reproducible Templates for Analysis and Dissemination](https://www.coursera.org/learn/reproducible-templates-analysis/home/info)

- [Happy Git and GitHub for the useR](https://happygitwithr.com/)





---

# Sunum Linkleri

https://sbalci.github.io/MyRCodesForDataAnalysis/R-Markdown.nb.html
https://sbalci.github.io/MyRCodesForDataAnalysis/R-Markdown.html

https://forms.gle/UqGJBiAjB8uLPRon8

---

# Geri Bildirim

- Geri bildirim için tıklayınız: _[Geri bildirim formu](https://goo.gl/forms/YjGZ5DHgtPlR1RnB3)_


---

<script id="dsq-count-scr" src="//https-sbalci-github-io.disqus.com/count.js" async></script>

<div id="disqus_thread"></div>
<script>

/**
*  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
*  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/
/*
var disqus_config = function () {
this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};
*/
(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');
s.src = 'https://https-sbalci-github-io.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>

---

# İletişim  

Completed on `{r #  Sys.Date()`.  

Serdar Balci, MD, Pathologist  
drserdarbalci@gmail.com  

https://rpubs.com/sbalci/CV   
https://sbalci.github.io/  
https://github.com/sbalci  
https://twitter.com/serdarbalci


---

   



---


# Other Links


https://andrewbtran.github.io/NICAR/2018/workflow/docs/02-rmarkdown.html

- Troubleshooting in R Markdown

https://smithcollege-sds.github.io/sds-public/rmarkdown_problems.html


http://kbroman.org/knitr_knutshell/pages/Rmarkdown.html

https://kbroman.org/knitr_knutshell/pages/overview.html

https://kbroman.org/knitr_knutshell/pages/Rmarkdown.html

https://kbroman.org/knitr_knutshell/pages/markdown.html


https://onp4.com/



```
csv {headers: true, title: "**Drawing Tables In Markdown**"}
Name, Surname, Known As, Age
Marcelo, David, coldzera, 22
Oleksandr, Kostyliev, s1mple, 19
Nikola, Kovač, NiKo, 20
Richard, Papillon, shox, 25
Nicolai, Reedtz, dev1ce, 21
```

```
{pgn}
[Event "Bled-Zagreb-Belgrade Candidates"]
[Site "Bled, Zagreb & Belgrade YUG"]
[Date "1959.10.11"]
[Round "20"]
[Result "1-0"]
[White "Mikhail Tal"]
[Black "Robert James Fischer"]

1. d4 Nf6 2. c4 g6 3. Nc3 Bg7 4. e4 d6 5.
Be2 O-O 6. Nf3 e5 7. d5 Nbd7 8. Bg5 h6 9.
Bh4 a6 10. O-O Qe8 11. Nd2 Nh7 12. b4 Bf6
13. Bxf6 Nhxf6 14. Nb3 Qe7 15. Qd2 Kh7 16.
Qe3 Ng8 17. c5 f5 18. exf5 gxf5 19. f4 exf4
20. Qxf4 dxc5 21. Bd3 cxb4 22. Rae1 Qf6 23.
Re6 Qxc3 24. Bxf5+ Rxf5 25. Qxf5+ Kh8 26.
Rf3 Qb2 27. Re8 Nf6 28. Qxf6+ Qxf6 29. Rxf6
Kg7 30. Rff8 Ne7 31. Na5 h5 32. h4 Rb8 33.
Nc4 b5 34. Ne5 1-0
```


- Keeping Credentials Secret with Keyrings in R

https://ras44.github.io/blog/2019/01/19/keeping-credentials-secret-with-keyrings-in-r.html

- How to build a website with Blogdown in R

http://www.storybench.org/how-to-build-a-website-with-blogdown-in-r/

<!--chapter:end:R-Markdown.Rmd-->

---
title: "R-Tipps"
---

# readClipboard

setwd(readClipboard())


<blockquote class="twitter-tweet" data-lang="en"><p lang="en" dir="ltr"><a href="https://twitter.com/hashtag/Rstats?src=hash&amp;ref_src=twsrc%5Etfw">#Rstats</a>: When using setwd(), R expects forward-slashes in the directory name. But on Windows, copying the directory from folder gives back-slashes. To &#39;de-windowsify&#39; the path name, copy the windows directory <br>&amp; use:<br><br>setwd(readClipboard())<a href="https://twitter.com/hashtag/phdchat?src=hash&amp;ref_src=twsrc%5Etfw">#phdchat</a> <a href="https://twitter.com/hashtag/phd?src=hash&amp;ref_src=twsrc%5Etfw">#phd</a> <a href="https://t.co/cMbdmhNVuf">pic.twitter.com/cMbdmhNVuf</a></p>&mdash; Guy Prochilo 🏳️
🌈 (@GuyProchilo) <a href="https://twitter.com/GuyProchilo/status/1083117541210370048?ref_src=twsrc%5Etfw">January 9, 2019</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

---

# separate



<blockquote class="twitter-tweet" data-lang="en"><p lang="en" dir="ltr">One form of messy data is a table in which multiple variables are stored in a single column. The <a href="https://twitter.com/hashtag/tidyverse?src=hash&amp;ref_src=twsrc%5Etfw">#tidyverse</a> separate() function will transform this type of data into one column per variable by simply specifying the separator! <a href="https://twitter.com/hashtag/rstats?src=hash&amp;ref_src=twsrc%5Etfw">#rstats</a> <a href="https://twitter.com/hashtag/tidytuesday?src=hash&amp;ref_src=twsrc%5Etfw">#tidytuesday</a> <a href="https://twitter.com/hashtag/animation?src=hash&amp;ref_src=twsrc%5Etfw">#animation</a> <a href="https://twitter.com/hashtag/dataviz?src=hash&amp;ref_src=twsrc%5Etfw">#dataviz</a> <a href="https://t.co/jjVvqWToIg">pic.twitter.com/jjVvqWToIg</a></p>&mdash; Omni Analytics Group (@OmniAnalytics) <a href="https://twitter.com/OmniAnalytics/status/1087740310699032576?ref_src=twsrc%5Etfw">January 22, 2019</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>


---

# glue

<blockquote class="twitter-tweet" data-lang="en"><p lang="en" dir="ltr">🤩 glue:: finally clicked for me this AM! 🤓<br><br>I work with monthly data files and need to update filenames often. My go-to is paste0(). Gave glue() another try. Way less fiddling with commas &amp; quote marks, and it&#39;s easier to see the filename. <a href="https://twitter.com/hashtag/Rstats?src=hash&amp;ref_src=twsrc%5Etfw">#Rstats</a> <a href="https://twitter.com/hashtag/tidyverse?src=hash&amp;ref_src=twsrc%5Etfw">#tidyverse</a> <a href="https://t.co/EdpozMxrtM">pic.twitter.com/EdpozMxrtM</a></p>&mdash; Adam Stone (@foundinblank) <a href="https://twitter.com/foundinblank/status/1088025539191939073?ref_src=twsrc%5Etfw">January 23, 2019</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

![](images/DxlxObOW0AASiaP.png)

---


<blockquote class="twitter-tweet" data-lang="en"><p lang="en" dir="ltr"><a href="https://twitter.com/hashtag/RStats?src=hash&amp;ref_src=twsrc%5Etfw">#RStats</a> — {dplyr} debugging tip: put a browser() somewhere inside your mutate() call to have access to the intermediate elements and to the columns: <a href="https://t.co/KN53YJIMOe">pic.twitter.com/KN53YJIMOe</a></p>&mdash; Colin Fay 🤘 (@_ColinFay) <a href="https://twitter.com/_ColinFay/status/1088022736117645314?ref_src=twsrc%5Etfw">January 23, 2019</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

---

<blockquote class="twitter-tweet" data-lang="en"><p lang="en" dir="ltr">Accidentally discovered if you copy &amp; paste a file into your <a href="https://twitter.com/hashtag/R?src=hash&amp;ref_src=twsrc%5Etfw">#R</a> script you get the filepath including filename with / (not \) and you just delete &quot;file:///&quot; off the start and don&#39;t have to change \ to / <a href="https://twitter.com/RossGlvr?ref_src=twsrc%5Etfw">@RossGlvr</a> demanded I tweet this <a href="https://twitter.com/hashtag/rstats?src=hash&amp;ref_src=twsrc%5Etfw">#rstats</a></p>&mdash; Faye Jackson (@Faye_L_Jackson) <a href="https://twitter.com/Faye_L_Jackson/status/1086235176269688832?ref_src=twsrc%5Etfw">January 18, 2019</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>


---



<!--chapter:end:R-Tipps.Rmd-->

---
title: "radiant"
---



https://radiant-rstats.github.io/docs/




<!--chapter:end:radiant.Rmd-->

---
title: "rchess"
---



http://jkunst.com/rchess/

https://github.com/jbkunst/rchess



```{r eval=FALSE, include=FALSE, echo=TRUE}
install.packages("rchess")
```

```{r eval=FALSE, include=FALSE, echo=TRUE}
devtools::install_github("jbkunst/rchess")
```


<!--chapter:end:rchess.Rmd-->

---
title: "RCookbook"
---


https://rc2e.com/index.html


```{r eval=FALSE, include=FALSE, echo=TRUE}
data(cars)
library(purrr)
map_dbl(cars, mean)
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
map_dbl(cars, sd)
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
map_dbl(cars, median)
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
var(cars)
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
cor(cars)
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
cov(cars)

```



```{r eval=FALSE, include=FALSE, echo=TRUE}
v <- c(3, pi, 4)
any(v == pi) # Return TRUE if any element of v equals pi
all(v == 0) # Return TRUE if all elements of v are zero
```















<!--chapter:end:RCookbook.Rmd-->

---
title: R ile analize başlarken^[Bu bir derlemedir, mümkün mertebe alıntılara referans
  vermeye çalıştım.]
author: "Derleyen [Serdar Balcı, MD, Pathologist](https://sbalci.github.io/)"
date: "`{r #  format(Sys.Date())`"
output:
  rmdformats::readthedown:
    highlight: kate
  html_notebook:
    fig_caption: yes
    highlight: kate
    number_sections: yes
    theme: flatly
    toc: yes
    toc_depth: 5
    toc_float: yes
  pdf_document:
    toc: yes
    toc_depth: '5'
  html_document: 
    fig_caption: yes
    keep_md: yes
    toc: yes
    toc_depth: 5
    toc_float: yes
---



```{r , echo=TRUE, cache=FALSE}
library(knitr)
library(rmdformats)

## Global options
options(max.print="75")
opts_chunk$set(echo=TRUE,
	             cache=TRUE,
               prompt=FALSE,
               tidy=TRUE,
               comment=NA,
               message=FALSE,
               warning=FALSE)
opts_knit$set(width=75)
```



-

<!-- Open all links in new tab-->  
<base target="_blank"/>   


<!-- Go to www.addthis.com/dashboard to customize your tools --> <script type="text/javascript" src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5bc36900a405090b">  
</script> 




[![](http://res.cloudinary.com/dyd911kmh/image/upload/f_auto,q_auto:best/v1530113077/Image_2_vfy48b.png)](https://www.datacamp.com/community/tutorials/data-science-pitfalls)


- R generation

https://rss.onlinelibrary.wiley.com/doi/10.1111/j.1740-9713.2018.01169.x


# R yükleme

http://www.youtube.com/watch?v=XcBLEVknqvY

[![What is R?](http://img.youtube.com/vi/XcBLEVknqvY/0.jpg)](http://www.youtube.com/watch?v=XcBLEVknqvY)


## R-project

https://cran.r-project.org/

---

[![](https://ismayc.github.io/talks/ness-infer/img/engine.png)](https://ismayc.github.io/talks/ness-infer/slide_deck.html#6)

---

## RStudio

https://www.rstudio.com/

https://www.rstudio.com/products/rstudio/download/

https://moderndive.com/2-getting-started.html

---

[![](http://www-users.york.ac.uk/~er13/RStudio%20Anatomy.svg)](https://buzzrbeeline.blog/2018/07/04/rstudio-anatomy/)



---

### RStudio eklentileri

- Discover and install useful RStudio addins

https://cran.r-project.org/web/packages/addinslist/README.html

https://rstudio.github.io/rstudioaddins/


```{r eval=FALSE, include=FALSE, echo=TRUE}
# devtools::install_github("rstudio/addinexamples", type = "source")
```


---

## X11

https://www.xquartz.org/

---

## Java OS

https://support.apple.com/kb/dl1572

---


# R zor şeyler için kolay, kolay şeyler için zor


- [R makes easy things hard, and hard things easy](http://r4stats.com/articles/why-r-is-hard-to-learn/)


- Aynı şeyi çok fazla şekilde yapmak mümkün

R Syntax Comparison::CHEAT SHEET

https://www.amelia.mn/Syntax-cheatsheet.pdf



---


# R paketleri


## Neden paketler var

[![](https://ismayc.github.io/talks/ness-infer/img/appstore.png)](https://ismayc.github.io/talks/ness-infer/slide_deck.html#7)

---

<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script><blockquote class="twitter-tweet" data-lang="en"><p lang="en" dir="ltr">I love the <a href="https://twitter.com/hashtag/rstats?src=hash&amp;ref_src=twsrc%5Etfw">#rstats</a> community.<br>Someone is like, &quot;oh hey peeps, I saw a big need for this mundane but difficult task that I infrequently do, so I created a package that will literally scrape the last bits of peanut butter out of the jar for you. It&#39;s called pbplyr.&quot;<br>What a tribe.</p>&mdash; Frank Elavsky ᴰᵃᵗᵃ ᵂᶦᶻᵃʳᵈ (@Frankly_Data) <a href="https://twitter.com/Frankly_Data/status/1014189095294291968?ref_src=twsrc%5Etfw">July 3, 2018</a></blockquote>

---



https://blog.mitchelloharawild.com/blog/user-2018-feature-wall/

---

![](https://blog.mitchelloharawild.com/blog/2018-07-11-user-2018-feature-wall_files/final.jpg)

---

## Paketleri nereden bulabiliriz

- Available CRAN Packages By Name  
https://cran.r-project.org/web/packages/available_packages_by_name.html

- Bioconductor  
https://www.bioconductor.org

- RecommendR  
http://recommendr.info/

- pkgsearch  
CRAN package search  
https://github.com/metacran/pkgsearch

- Awesome R  
https://awesome-r.com/  


## Kendi paket evrenini oluştur

- pkgverse: Build a Meta-Package Universe  
https://cran.r-project.org/web/packages/pkgverse/index.html



---

## R için yardım bulma


```
# ?mean
# ??efetch
# help(merge)
# example(merge)
```



- Vignette

![](figures/vignette.png)

---

- RDocumentation
https://www.rdocumentation.org

- R Package Documentation
https://rdrr.io/

- GitHub

- Stackoverflow

https://stackoverflow.com/

- Google uygun anahtar kelime



<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script><blockquote class="twitter-tweet" data-lang="en"><p lang="en" dir="ltr">How I use <a href="https://twitter.com/hashtag/rstats?src=hash&amp;ref_src=twsrc%5Etfw">#rstats</a> <br>h/t <a href="https://twitter.com/ThePracticalDev?ref_src=twsrc%5Etfw">@ThePracticalDev</a> <a href="https://t.co/erRnTG0Ujr">pic.twitter.com/erRnTG0Ujr</a></p>&mdash; Emily Bovee (@ebovee09) <a href="https://twitter.com/ebovee09/status/1028037594947485696?ref_src=twsrc%5Etfw">August 10, 2018</a></blockquote>


---


![](figures/Google-package-name.png)

---



![](figures/Google-start-with-R.png)

---

- Awesome Cheatsheet
https://github.com/detailyang/awesome-cheatsheet

http://cran.r-project.org/doc/contrib/Baggott-refcard-v2.pdf

https://www.rstudio.com/resources/cheatsheets/


- Awesome R

https://github.com/qinwf/awesome-R#readme

https://awesome-r.com/




- Twitter

https://twitter.com/hashtag/rstats?src=hash


- Reproducible Examples

<blockquote class="twitter-tweet" data-lang="en"><p lang="en" dir="ltr">Got a question to ask on <a href="https://twitter.com/SlackHQ?ref_src=twsrc%5Etfw">@SlackHQ</a> or post on <a href="https://twitter.com/github?ref_src=twsrc%5Etfw">@github</a>? No time to read the long post on how to use reprex? Here is a 20-second gif for you to format your R codes nicely and for others to reproduce your problem. (An example from a talk given by <a href="https://twitter.com/JennyBryan?ref_src=twsrc%5Etfw">@JennyBryan</a>) <a href="https://twitter.com/hashtag/rstat?src=hash&amp;ref_src=twsrc%5Etfw">#rstat</a> <a href="https://t.co/gpuGXpFIsX">pic.twitter.com/gpuGXpFIsX</a></p>&mdash; ZhiYang (@zhiiiyang) <a href="https://twitter.com/zhiiiyang/status/1053006003711569920?ref_src=twsrc%5Etfw">October 18, 2018</a></blockquote><script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>





---

## R paket yükleme

```
install.packages("tidyverse", dependencies = TRUE)
install.packages("jmv", dependencies = TRUE)
install.packages("questionr", dependencies = TRUE)
install.packages("Rcmdr", dependencies = TRUE)
install.packages("summarytools")
```

```{r}
# install.packages("tidyverse", dependencies = TRUE)
# install.packages("jmv", dependencies = TRUE)
# install.packages("questionr", dependencies = TRUE)
# install.packages("Rcmdr", dependencies = TRUE)
# install.packages("summarytools")
```


```{r, error=FALSE, message = FALSE, warning = FALSE, eval = TRUE, include = TRUE}
# require(tidyverse)
# require(jmv)
# require(questionr)
# library(summarytools)
# library(gganimate)
```

---

# R studio ile proje oluşturma

https://support.rstudio.com/hc/en-us/articles/200526207-Using-Projects

![](http://www.rstudio.com/images/docs/projects_new.png)

---

# RStudio ile veri yükleme

https://support.rstudio.com/hc/en-us/articles/218611977-Importing-Data-with-RStudio

![](https://support.rstudio.com/hc/en-us/article_attachments/206277618/data-import-overview.gif)

---

## Excel

## SPSS

## csv


---

# Veriyi görüntüleme

<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script><blockquote class="twitter-tweet" data-lang="en"><p lang="en" dir="ltr">Spreadsheet users using <a href="https://twitter.com/hashtag/rstats?src=hash&amp;ref_src=twsrc%5Etfw">#rstats</a>:  where&#39;s the data?<a href="https://twitter.com/hashtag/rstats?src=hash&amp;ref_src=twsrc%5Etfw">#rstats</a> users using spreadsheets:  where&#39;s the code?</p>&mdash; Leonard Kiefer (@lenkiefer) <a href="https://twitter.com/lenkiefer/status/1015587475580956672?ref_src=twsrc%5Etfw">July 7, 2018</a></blockquote>



```{r, results="markup"}
# library(nycflights13)
# summary(flights)
```



```
View(data)
```


```
data
```


```
head
```


```
tail
```


```
glimpse
```


```
str
```


```
skimr::skim()
```

---


# Veriyi değiştirme

## Veriyi kod ile değiştirelim

## Veriyi eklentilerle değiştirme

![](figures/change_data.png)


---


## RStudio aracılığıyla recode

*questionr* paketi kullanılacak

![](figures/level_recode.png)


---



https://juba.github.io/questionr/articles/recoding_addins.html


![](https://raw.githubusercontent.com/juba/questionr/master/resources/screenshots/irec_1.png)


---

![](https://raw.githubusercontent.com/juba/questionr/master/resources/screenshots/irec_2.png)


---

![](https://raw.githubusercontent.com/juba/questionr/master/resources/screenshots/irec_3.png)


---

# Basit tanımlayıcı istatistikler

```
summary()
```

```
mean
```

```
median
```

```
min
```

```
max
```

```
sd
```

```
table()
```


```{r, echo=TRUE, include = TRUE}
library(readr)
irisdata <- read_csv("data/iris.csv")

jmv::descriptives(
    data = irisdata,
    vars = "Sepal.Length",
    splitBy = "Species",
    freq = TRUE,
    hist = TRUE,
    dens = TRUE,
    bar = TRUE,
    box = TRUE,
    violin = TRUE,
    dot = TRUE,
    mode = TRUE,
    sum = TRUE,
    sd = TRUE,
    variance = TRUE,
    range = TRUE,
    se = TRUE,
    skew = TRUE,
    kurt = TRUE,
    quart = TRUE,
    pcEqGr = TRUE)
```

---

```{r, echo=TRUE, include=FALSE}
# install.packages("scatr")

scatr::scat(
    data = irisdata,
    x = "Sepal.Length",
    y = "Sepal.Width",
    group = "Species",
    marg = "dens",
    line = "linear",
    se = TRUE)

```

## summarytools

https://cran.r-project.org/web/packages/summarytools/vignettes/Introduction.html

```{r, include=FALSE, comment=NA, prompt=FALSE, cache=FALSE, echo=TRUE, results='asis'}
library(summarytools)
summarytools::freq(iris$Species, style = "rmarkdown")
```

```{r, include=FALSE, comment=NA, prompt=FALSE, cache=FALSE, echo=TRUE, results='asis'}
summarytools::freq(iris$Species, report.nas = FALSE, style = "rmarkdown", headings = TRUE)
```

```{r, include=FALSE, comment=NA, prompt=FALSE, cache=FALSE, echo=TRUE, results='asis'}
with(tobacco, print(ctable(smoker, diseased), method = 'render'))
```


```{r, include=FALSE, comment=NA, prompt=FALSE, cache=FALSE, echo=TRUE, results='asis'}
with(tobacco, 
     print(ctable(smoker, diseased, prop = 'n', totals = FALSE), 
           headings = TRUE, method = "render"))
```



```{r, include=FALSE, comment=NA, prompt=FALSE, cache=FALSE, echo=TRUE, results='asis'}
summarytools::descr(iris, style = "rmarkdown")
```



```{r, include=FALSE, comment=NA, prompt=FALSE, cache=FALSE, echo=TRUE, results='asis'}
descr(iris, stats = c("mean", "sd", "min", "med", "max"), transpose = TRUE, 
      headings = TRUE, style = "rmarkdown")
```



```{r, include=FALSE, comment=NA, prompt=FALSE, cache=FALSE, echo=TRUE, results='asis'}
# view(dfSummary(iris))

```


![](figures/dfsummary.png)



```{r, include=FALSE, comment=NA, prompt=FALSE, cache=FALSE, echo=TRUE, results='asis'}
dfSummary(tobacco, plain.ascii = FALSE, style = "grid")
```


```{r, include=FALSE, comment=NA, prompt=FALSE, cache=FALSE, echo=TRUE, results='asis'}

# First save the results

iris_stats_by_species <- by(data = iris, 
                            INDICES = iris$Species, 
                            FUN = descr, stats = c("mean", "sd", "min", "med", "max"), 
                            transpose = TRUE)

# Then use view(), like so:

view(iris_stats_by_species, method = "pander", style = "rmarkdown")
```

```{r, include=FALSE, comment=NA, prompt=FALSE, cache=FALSE, echo=TRUE, results='asis'}
# view(iris_stats_by_species)
```

![](figures/DescriptiveStatistics.png)

---

```{r, include=FALSE, comment=NA, prompt=FALSE, cache=FALSE, echo=TRUE, results='asis'}
data(tobacco) # tobacco is an example dataframe included in the package
BMI_by_age <- with(tobacco, 
                   by(BMI, age.gr, descr, 
                      stats = c("mean", "sd", "min", "med", "max")))
view(BMI_by_age, "pander", style = "rmarkdown")
```

---

```{r, include=FALSE, comment=NA, prompt=FALSE, cache=FALSE, echo=TRUE, results='asis'}
BMI_by_age <- with(tobacco, 
                   by(BMI, age.gr, descr,  transpose = TRUE,
                      stats = c("mean", "sd", "min", "med", "max")))

view(BMI_by_age, "pander", style = "rmarkdown", headings = TRUE)
```

---

```{r, include=FALSE, comment=NA, prompt=FALSE, cache=FALSE, echo=TRUE, results='asis'}
tobacco_subset <- tobacco[ ,c("gender", "age.gr", "smoker")]
freq_tables <- lapply(tobacco_subset, freq)

# view(freq_tables, footnote = NA, file = 'freq-tables.html')
```

---

```{r, include=FALSE, comment=NA, prompt=FALSE, cache=FALSE, echo=TRUE, results='asis'}
what.is(iris)
```

---

```{r eval=FALSE, include=FALSE, echo=TRUE}
freq(tobacco$gender, style = 'rmarkdown')
```

---

```{r eval=FALSE, include=FALSE, echo=TRUE}
print(freq(tobacco$gender), method = 'render')
```

---

## skimr

```
library(skimr)
skim(df)
```

---

## DataExplorer

```
library(DataExplorer)
DataExplorer::create_report(df)
```


[![](https://static1.squarespace.com/static/58eef8846a4963e429687a4d/t/5bdfc2fb4d7a9c04ee50b7aa/1541391160702/dataExplorerGifLg.gif?format=1500w)](https://www.littlemissdata.com/blog/simple-eda)



---

## Grafikler

```{r eval=FALSE, include=FALSE, echo=TRUE}
# library(ggplot2)
# library(mosaic)
# mPlot(irisdata)
```

---

```{r eval=FALSE, include=FALSE, echo=TRUE}
ctable(tobacco$gender, tobacco$smoker, style = 'rmarkdown')
```

---

```{r eval=FALSE, include=FALSE, echo=TRUE}
print(ctable(tobacco$gender, tobacco$smoker), method = 'render')
```

```
descr(tobacco, style = 'rmarkdown')

print(descr(tobacco), method = 'render', table.classes = 'st-small')

dfSummary(tobacco, style = 'grid', plain.ascii = FALSE)

print(dfSummary(tobacco, graph.magnif = 0.75), method = 'render')
```


---



<blockquote class="twitter-tweet" data-lang="en"><p lang="en" dir="ltr">Here, building up a <a href="https://twitter.com/hashtag/ggplot2?src=hash&amp;ref_src=twsrc%5Etfw">#ggplot2</a> as slowly as possible, <a href="https://twitter.com/hashtag/rstats?src=hash&amp;ref_src=twsrc%5Etfw">#rstats</a>.  Incremental adjustments.  <a href="https://twitter.com/hashtag/rstatsteachingideas?src=hash&amp;ref_src=twsrc%5Etfw">#rstatsteachingideas</a> <a href="https://t.co/nUulQl8bPh">pic.twitter.com/nUulQl8bPh</a></p>&mdash; Gina Reynolds (@EvaMaeRey) <a href="https://twitter.com/EvaMaeRey/status/1029104656763572226?ref_src=twsrc%5Etfw">August 13, 2018</a></blockquote><script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>


---


[![](https://raw.githubusercontent.com/dreamRs/esquisse/master/man/figures/esquisse.gif)](https://github.com/dreamRs/esquisse)


<blockquote class="twitter-tweet" data-lang="en"><p lang="en" dir="ltr">Dreaming of a fancy <a href="https://twitter.com/hashtag/Rstats?src=hash&amp;ref_src=twsrc%5Etfw">#Rstats</a> <a href="https://twitter.com/hashtag/ggplot?src=hash&amp;ref_src=twsrc%5Etfw">#ggplot</a> <a href="https://twitter.com/hashtag/dataviz?src=hash&amp;ref_src=twsrc%5Etfw">#dataviz</a> but still scared of typing <a href="https://twitter.com/hashtag/code?src=hash&amp;ref_src=twsrc%5Etfw">#code</a>? <a href="https://twitter.com/_pvictorr?ref_src=twsrc%5Etfw">@_pvictorr</a> esquisse package has you covered <a href="https://t.co/1vIDXcVAAF">https://t.co/1vIDXcVAAF</a> <a href="https://t.co/RlTkptnrNv">pic.twitter.com/RlTkptnrNv</a></p>&mdash; Radoslaw Panczak (@RPanczak) <a href="https://twitter.com/RPanczak/status/1047019588658040832?ref_src=twsrc%5Etfw">October 2, 2018</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>










---

# Rcmdr

```
library(Rcmdr)

Rcmdr::Commander()

```


- A Comparative Review of the R Commander GUI for R

http://r4stats.com/articles/software-reviews/r-commander/


---

# jamovi

https://www.jamovi.org/

![![](https://www.jamovi.org/assets/main-screenshot.png)](https://www.jamovi.org/)


https://blog.jamovi.org/2018/07/30/rj.html

![![](https://blog.jamovi.org/assets/images/rj.png)](https://blog.jamovi.org/2018/07/30/rj.html)

---

# Sonraki Konular

- RStudio ile GitHub
- Hipotez testleri
- R Markdown ve R Notebook ile tekrarlanabilir rapor


---

# Diğer kodlar

- Diğer kodlar için bakınız: [https://sbalci.github.io/](https://sbalci.github.io/)


---

# Geri Bildirim

- Geri bildirim için tıklayınız: _[Geri bildirim formu](https://goo.gl/forms/YjGZ5DHgtPlR1RnB3)_


---

<script id="dsq-count-scr" src="//https-sbalci-github-io.disqus.com/count.js" async></script>

<div id="disqus_thread"></div>
<script>

/**
*  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
*  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/
/*
var disqus_config = function () {
this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};
*/
(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');
s.src = 'https://https-sbalci-github-io.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>

---








<!--chapter:end:readthedown.Rmd-->

---
output:
  rmarkdown::html_vignette: 
    css: 
    - !expr system.file("rmarkdown/templates/html_vignette/resources/vignette.css", package = "rmarkdown")
vignette: >
  %\VignetteIndexEntry{Recommendations for Rmarkdown}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r summarytool options 1, include=FALSE}
library(knitr)
opts_chunk$set(comment=NA, prompt=FALSE, cache=FALSE, echo=TRUE, results='asis')
```


```{r summarytool options 2, echo=TRUE}
library(summarytools)
st_css()
```


```{r summarytool options 3}
st_options(bootstrap.css     = FALSE,       # Already part of the theme so no need for it
           plain.ascii       = FALSE,       # One of the essential settings
           style             = "rmarkdown", # Idem.
           dfSummary.silent  = TRUE,        # Suppresses messages about temporary files
           footnote          = NA,          # Keeping the results minimalistic
           subtitle.emphasis = FALSE)       # For the vignette theme, this gives
                                            # much better results. Your mileage may vary.
``` 



```{r summarytools freq Rmarkdown Style}
freq(tobacco$gender, style = 'rmarkdown')
```


## HTML Rendering
```{r eval=FALSE, include=FALSE, echo=TRUE}
print(freq(tobacco$gender), method = 'render')
```

If you find the table too large, you can use `table.classes = 'st-small'` - an 
example is provided further below.

--------------------------------------------------------------------------------

<a href="#top">Back to top</a>

<!-- ctable() {#ctable} -->

## Rmarkdown Style

Tables with heading spanning over 2 rows are not fully supported in markdown 
(yet), but the result is getting close to acceptable. This, however, is not
true for all themes. That's why the rendering method is preferred.

```{r eval=FALSE, include=FALSE, echo=TRUE}
ctable(tobacco$gender, tobacco$smoker, style = 'rmarkdown')
```

## HTML Rendering

For best results, use this method.

```{r include=FALSE, eval=FALSE, echo = TRUE}
print(ctable(tobacco$gender, tobacco$smoker), method = 'render')
```

--------------------------------------------------------------------------------
 
<a href="#top">Back to top</a>
 
# descr() 2 {#descr2}

`descr()` is also best used with `style = 'rmarkdown'`, and HTML rendering is 
also supported.

## Rmarkdown Style
```{r eval=FALSE, include=FALSE, echo=TRUE}
descr(tobacco, style = 'rmarkdown')
```


## HTML Rendering

We'll use table.classes = 'st-small' to show how it affects the table's size
(compare to the `freq()` table rendered earlier).

```{r eval=FALSE, include=FALSE, echo=TRUE}
print(descr(tobacco), method = 'render', table.classes = 'st-small')
```
 
--------------------------------------------------------------------------------

<a href="#top">Back to top</a>

# dfSummary() 1 {#dfsummary1}

## Grid Style

This style gives good results, and since v0.9, the graphs are shown as true 
images. Don't forget to specify `plain.ascii = FALSE` (or set it as a global
option with `st_options(plain.ascii = FALSE)`), or you won't get good results.

```{r , eval=FALSE}
dfSummary(tobacco, style = 'grid', graph.magnif = 0.75, tmp.img.dir = "/tmp")
```




```{r eval=FALSE, include=FALSE, echo=TRUE}
print(dfSummary(tobacco, graph.magnif = 0.75), method = 'render')
```



<!--chapter:end:Recommendations-rmarkdown.Rmd-->

---
title: "Regression"
output: html_notebook
---

- Logistic Regression in R Tutorial

https://www.datacamp.com/community/tutorials/logistic-regression-R

<!--chapter:end:Regression.Rmd-->

---
title: "reprex"
author: "Serdar Balcı, MD, Pathologist"
date: "21 09 2018"
output: html_document
---

https://www.youtube.com/watch?v=MmTPhGQWPUo

https://reprex.tidyverse.org/index.html



```{r eval=FALSE, include=FALSE, echo=TRUE}
# install.packages("reprex")
# library(reprex)
```

``` r
(y <- 1:4)
#> [1] 1 2 3 4
mean(y)
#> [1] 2.5
```

<sup>Created on 2018-09-21 by the [reprex package](https://reprex.tidyverse.org) (v0.2.1)</sup>

<!--chapter:end:reprex.Rmd-->

---
title: "Reproducible Research"
output: html_notebook
---

- karthik/binder-test

Example repo for testing holepunch

https://github.com/karthik/binder-test

- Hole punch

https://karthik.github.io/holepunch/

- How to set up a My Binder for your R project

rstudio2019/binder-notes.md

https://github.com/karthik/rstudio2019/blob/master/binder-notes.md

    
- reproducibility guidelines

https://kbroman.org/blog/2019/04/01/reproducibility-guidelines/



- Disease risk modelling and visualization using R

http://manio.org/2015/06/22/my-approach-to-reproducible-research.html

- initial steps toward reproducible research

https://kbroman.org/steps2rr/


- Comments on reproducibility

https://kbroman.org/knitr_knutshell/pages/reproducible.html

- Reproducible science in R

https://grunwaldlab.github.io/Reproducible-science-in-R/index.html

- The Practice of Reproducible Research Case Studies and Lessons from the Data-Intensive Sciences

https://www.practicereproducibleresearch.org/


- papaja

papaja (Preparing APA Journal Articles) is an R package that provides document formats and helper functions to produce complete APA manscripts from RMarkdown-files (PDF and Word documents).

https://crsh.github.io/papaja/



-  Stencila

An open source office suite for reproducible research

http://stenci.la/


- redoc - reversible R Markdown/MS Word documents.

https://noamross.github.io/redoc/




```
---
output:
  redoc::rdocx_reversible:
    keep_md: TRUE
    highlight_outputs: TRUE
---
```


```
redoc::undoc("reversible2.docx")
redoc::undoc(file.choose())
```


```
redoc::redoc_extract_rmd("reversible.docx")
```



# docker

- An Introduction to Docker for R Users 

https://colinfay.me/docker-r-reproducibility/


# Naming Files

- How to name files  
https://speakerdeck.com/jennybc/how-to-name-files


# Example Articles

- Increasing the Transparency of Research Papers withExplorable Multiverse Analyses

https://hal.inria.fr/hal-01976951/document


- Replication Study: Transcriptional amplification in tumor cells with elevated c-Myc

https://repro.elifesciences.org/example.html

- Introducing eLife’s first computationally reproducible article

https://elifesciences.org/labs/ad58f08d/introducing-elife-s-first-computationally-reproducible-article

- Re-Evaluating the Efficiency of Physical Visualizations: A Simple Multiverse Analysis

https://explorablemultiverse.github.io/examples/frequentist/


- R Online

Try R
Run a piece of code against a specific version of R, from 3.6.0 to 3.1.0.

https://srv.colinfay.me:1001/#about

https://github.com/ColinFay/ronline


```
Sys.setenv(
  DSN = "database_name",
  UID = "User ID",
  PASS = "Password"
)

db <- DBI::dbConnect(
  drv = odbc::odbc(),
  dsn = Sys.getenv("DSN"),
  uid = Sys.getenv("UID"),
  pwd = Sys.getenv("PASS")
)
```









<!--chapter:end:ReproducibleResearch.Rmd-->

---
title: "R in Pathology Research"
output: html_notebook
---


- Progesterone Receptor Status Predicts Response to Progestin Therapy in Endometriosis

https://www.ncbi.nlm.nih.gov/pubmed/30357380

irr: Various Coefficients of Interrater Reliability and Agreement

https://cran.r-project.org/web/packages/irr/index.html

<!--chapter:end:RinPathologyResearch.Rmd-->

---
title: "R Notebook"
output: html_notebook
---

library(RISmed)
library(ggplot2)

query <- "(exome OR whole OR deep OR high-throughput OR (next AND generation) OR (massively AND parallel)) AND sequencing"
 
ngs_search <- EUtilsSummary(query, type="esearch",db = "pubmed",mindate=1980, maxdate=2013, retmax=30000)
QueryCount(ngs_search)
ngs_records <- EUtilsGet(ngs_search)
years <- Year(ngs_records)
ngs_pubs_count <- as.data.frame(table(years))
 
total <- NULL
for (i in 1980:2013){
peryear <- EUtilsSummary("", type="esearch", db="pubmed", mindate=i, maxdate=i)
total[i] <- QueryCount(peryear)
}
year <- 1980:2013
total_pubs_count<- as.data.frame(cbind(year,total[year]))
names(total_pubs_count) <- c("year","Total_publications")
names(ngs_pubs_count) <-  c("year","NGS_publications")
pubs_year <-  merge(ngs_pubs_count,total_pubs_count,by="year")
pubs_year$NGS_publications_normalized <-  pubs_year$NGS_publications *100000 / pubs_year$Total_publications

write.table(pubs_year,"NGS_publications_per_year.txt",quote=F,sep="\t",row.names=F)
 
 
journal <- MedlineTA(ngs_records)
ngs_journal_count <- as.data.frame(table(journal))
ngs_journal_count_top25 <- ngs_journal_count[order(-ngs_journal_count[,2]),][1:25,]
 
journal_names <- paste(ngs_journal_count_top25$journal,"[jo]",sep="")
 
total_journal <- NULL
for (i in journal_names){
perjournal <- EUtilsSummary(i, type='esearch', db='pubmed',mindate=1980, maxdate=2013)
total_journal[i] <- QueryCount(perjournal)
}
 
journal_ngs_total <- cbind(ngs_journal_count_top25,total_journal)
names(journal_ngs_total) <- c("journal","NGS_publications","Total_publications")
journal_ngs_total$NGS_publications_normalized <- journal_ngs_total$NGS_publications / journal_ngs_total$Total_publications
 
write.table(journal_ngs_total,"NGS_publications_per_journal.txt",quote=F,sep="\t",row.names=F)


pubs_per_year <- read.table("NGS_publications_per_year.txt",header = T,sep="\t")
pubs_per_journal <- read.table("NGS_publications_per_journal.txt",header = T,sep="\t")

ggplot(pubs_per_year,aes(year, NGS_publications_normalized)) + geom_line (colour="blue",size=2) +
xlab("Year") +
ylab("NGS/100000 articles")+
ggtitle("NGS PubMed articles")
 
ggplot(pubs_per_journal,aes(journal, NGS_publications,fill=journal)) + geom_bar(stat="identity")+
coord_flip()+
theme(legend.position="none")
 
ggplot(pubs_per_journal ,aes(journal, NGS_publications_normalized,fill=journal)) + geom_bar(stat="identity")+
coord_flip()+
theme(legend.position="none")


 

<!--chapter:end:RISmed.Rmd-->

---
title: "Creating websites in R"
author: "Emily C. Zabor"
output: 
    html_document:
        toc: TRUE
        toc_float: TRUE
---

http://www.emilyzabor.com/tutorials/rmarkdown_websites_tutorial.html

This tutorial provides an introduction to creating websites using R, R Markdown and GitHub pages. 

This tutorial was originally presented at the Memorial Sloan Kettering Cancer Center Department of Epidemiology and Biostatistics R User Group meeting on January 23, 2018.

The current version was updated and presented at the R Ladies NYC Meetup on February 15, 2018.


## Types of websites

The main types of websites you may want to create include:

1. Personal websites
2. Package websites
3. Project websites
4. Blogs


## R Markdown website basics

The minimum requirements for an R Markdown website are:

- `index.Rmd`: contains the content for the website homepage
- `_site.yml`: contains metadata for the website

A basic example of a `_site.yml` file for a website with two pages:

```{r eval = FALSE}
name: "my-website"
navbar:
  title: "My Website"
  left:
    - text: "Home"
      href: index.html
    - text: "About"
      href: about.html
```

And a basic `index.Rmd` to create the Home page:

```{r eval = FALSE}
---
title: "My Website"
---
    
Hello, Website! Welcome to the world.
```


You can find an overview of R Markdown website basics [here](http://rmarkdown.rstudio.com/rmarkdown_websites.html).


## GitHub

This tutorial will focus on hosting websites through GitHub pages. Hosting websites on GitHub pages is free.

If you don't have a GitHub account already, sign up for one at [https://github.com/join?source=header-home](https://github.com/join?source=header-home) with username YOUR_GH_NAME. I'll be referring to this username, YOUR_GH_NAME, as "your GitHub username" throughout this tutorial.

There are other free sites for website hosting, and another popular choice is [Netlify](https://www.netlify.com/).


## Personal websites

An example from the homepage of  [my personal website](http://www.emilyzabor.com/):

<img src="img/personal.png" style="border: #A9A9A9 1px solid; width:75%">


There are two main steps for creating a personal website that will be hosted on GitHub:

1. GitHub setup
2. Local setup


### GitHub setup

1. Create a GitHub repository ("repo") named YOUR_GH_NAME.github.io, where YOUR_GH_NAME is your GitHub username.
2. Initialize it with a README 
    - For the GitHub inexperienced: this can ease the process of cloning the repo by initializing the remote repo with a master branch


### Local setup

1. Clone this remote repository to a local directory with the same name, YOUR_GH_NAME.github.io
2. Add an R Project to this directory
3. Create a `_site.yml` and `index.Rmd` file in your new directory


### Why do I need an R Project?

The R Project is useful because RStudio will recognize your project as a website, and provide appropriate build tools. 

Note: After creating the R Project and initial files, you may need to close the project and reopen it before R will recognize it as a website and show the appropriate build tools.


### Create content

Edit the `_site.yml` file to change the metadata, layout, and  theme of your website. Preview Jekyll themes [here](http://jekyllthemes.org/) and play around with different options. Themes are easy to change even after you have added content. 

For example, the `_site.yml` for my personal website looks like this:

```{r eval = FALSE}
name: "Emily C. Zabor"
output_dir: "."
navbar:
  title: "Emily C. Zabor"
  left:
    - text: "Writing"
      href: research.html
    - text: "Speaking"
      href: talks.html
    - text: "Programming"
      href: software.html
    - text: "Teaching"
      href: teaching.html
  right:
    - icon: fa-envelope fa-lg
      href: contact.html
    - icon: fa-github fa-lg
      href: http://github.com/zabore
    - icon: fa-twitter fa-lg
      href: https://twitter.com/zabormetrics
    - icon: fa-linkedin fa-lg
      href: https://www.linkedin.com/in/emily-zabor-59b902b7/
output:
  html_document:
    theme: paper
    css: 'styles.css'
```

Edit and create `.Rmd` files that contain your website content, which will produce the html pages of your website when you knit them. 

For example, the `index.Rmd` file for my personal website homepage looks like this:

```{r eval = FALSE}
---
---

<link rel="stylesheet" href="styles.css" type="text/css">

<img src="images/emily_2.jpg" style="width:25%; border:10px solid; margin-right: 20px" align="left">

I like to analyze data to answer research questions and test hypotheses. Currently I investigate questions related to breast cancer through my work as a Research Biostatistician at [Memorial Sloan Kettering Cancer Center](https://www.mskcc.org/departments/epidemiology-biostatistics) in the department of Epidemiology & Biostatistics. 

I graduated from the [University of Minnesota](http://www.sph.umn.edu/academics/divisions/biostatistics/) with a MS in biostatistics in 2010. In 2012 I began working toward my DrPH in biostatistics as a part-time student at [Columbia University](https://www.mailman.columbia.edu/become-student/departments/biostatistics), where I am investigating statistical methods for the study of etiologic heterogeneity in epidemiologic studies under the advisement of [Dr. Shuang Wang](https://www.mailman.columbia.edu/people/our-faculty/sw2206) at Columbia University and [Dr. Colin Begg](https://www.mskcc.org/profile/colin-begg) at Memorial Sloan Kettering Cancer Center. I expect to graduate by the end of 2018.

I am a well-known R enthusiast, including serving on the board and being an active member of [R Ladies NYC](http://www.rladiesnyc.org/). 

My full CV is available [here](files/Zabor_CV_2017_Q4.pdf).
```

Once you have your content written and the layout setup, on the Build tab in RStudio, select "Build Website":

<img src="img/build.png" style="border: #A9A9A9 1px solid; width:75%">

Now your local directory contains all of the files needed for your website:

<img src="img/directory.png" style="border: #A9A9A9 1px solid; width:75%">


### Deploy website

Basic approach:

- Select "Upload files" from the main page of your GitHub repo:

<img src="img/uploadbutton.png" style="border: #A9A9A9 1px solid; width:75%">


- And simply drag or select the files from the local repository:


<img src="img/upload.png" style="border: #A9A9A9 1px solid; width:75%">

Advanced approach (recommended):

- use Git from the shell, from a Git client, or from within RStudio (another great reason to use an R Project!)

<img src="img/rstudiogit.png" style="border: #A9A9A9 1px solid; width:75%">

- But this is not a Git/GitHub tutorial. If you want to learn more about Git/GitHub, which I encourage you to do, here's a great resource to get you started: [http://happygitwithr.com/](http://happygitwithr.com/)


### Custom domains

The default is for your site to be hosted at http://YOUR_GH_NAME.github.io, but you can add a custom domain name as well. There are two steps:

1. In your GitHub repository YOUR_GH_NAME.github.io, go to Settings > GitHub pages. Type your domain name in the box under Custom domain and hit Save.

<img src="img/domain.png" style="border: #A9A9A9 1px solid; width:75%">

2. Add a CNAME file to your GitHub repsitory YOUR_GH_NAME.github.io. 

It will appear like this in your repository:

<img src="img/cname1.png" style="border: #A9A9A9 1px solid; width:75%">

And inside the file you will simply have your domain name:

<img src="img/cname2.png" style="border: #A9A9A9 1px solid; width:75%">


## Package websites

An example from the [website](http://www.emilyzabor.com/ezfun/) for my package `ezfun`:

<img src="img/package.png" style="border: #A9A9A9 1px solid; width:75%">

Use Hadley Wickham's great package `pkgdown` to easily build a website from your package that is hosted on GitHub. Details of `pkgdown` can be found on [the pkgdown website](http://pkgdown.r-lib.org/), which was also created using `pkgdown`.

This assumes you already have an R package with a local directory and a GitHub repository.

From within your package directory run: 

```{r eval = FALSE}
devtools::install_github("hadley/pkgdown")
pkgdown::build_site()
```

- This will add a folder called `docs` to the local directory for your package 

- Upload/push these changes to the GitHub repository for your package

- In the GitHub repository for your package go to Settings > GitHub pages. Select "master branch/docs folder" as the source and hit Save

<img src="img/ghsource.png" style="border: #A9A9A9 1px solid; width:75%">

- The page will be added as to your personal website as YOUR_GH_NAME.github.io/repo_name

    - The Home page of the site will be pulled from the README file on your package repository
    - The Reference page of the site lists the included functions with their description
    - Each function can be clicked through to see the help page, if any
    - Would also build pages for any available vignettes

And you're done, it's that easy.


## Project websites

You can create a website for a non-package repository as well. For example, I have [a page](http://www.emilyzabor.com/tutorials/) on my website linking to the repository in which this tutorial is stored.

<img src="img/project.png" style="border: #A9A9A9 1px solid; width:75%">


### Local setup

From within the local directory of the project of interest:

1. Create a `_site.yml` and `index.Rmd` file in your new directory
2. Edit these files to create content and manage layout, as before for personal websites


### GitHub setup

- Upload/push these new files to the GitHub repository for your project

- Enable GitHub pages for the repository by going to Settings > GitHub Pages, where you'll select the "master branch" folder and hit Save

<img src="img/ghpages.png" style="border: #A9A9A9 1px solid; width:75%">


## Blogs

R Markdown websites are simple to create and deploy, but can become cumbersome if you make frequent updates or additions to the website, as in the case of a blog. Luckily, the R package `blogdown` exists just for this purpose. `blogdown` is an R package that allows you to create static websites, which means that the deployed version of the website only consists of JavaScript, HTML, CSS, and images. Luckily the `blogdown` package makes it so that you don't have to know any of those things to create a beautiful website for your blog, powered by Hugo.

For a complete resource on using the `blogdown` website, checkout this [short blogdown book](https://bookdown.org/yihui/blogdown/).

I don't have a personal blog, so let's look at the website I built to feature the events and blog of the [R-Ladies NYC](http://www.rladiesnyc.org/) organization as an example.

<img src="img/rladiesnychome.png" style="border: #A9A9A9 1px solid; width:75%">


### Setup

The first three steps are similar to those from creating a basic R Markdown website:

1. Create a GitHub repository named YOUR_GH_NAME.github.io, where YOUR_GH_NAME is your GitHub username, initialized with a README file
2. Clone the GitHub repo to a local directory with the same name
3. Add an R Project to the local directoroy

Next we get started with `blogdown`.

4. Install `blogdown` and Hugo

```{r eval = FALSE}
install.packages("blogdown")
blogdown::install_hugo()
```

5. Choose a [theme](https://themes.gohugo.io/) and find the link to the theme's GitHub repository. In this case themes aren't quite as easy to change as with basic R Markdown websites, so choose carefully.

6. Within your project session, generate a new site. The option `theme_example = TRUE` will obtain the files for an example site that you can then customize for your needs. Below "user/repo" refers to the GitHub username and GitHub repository for your selected theme.

```{r eval = FALSE}
blogdown::new_site(theme = "user/repo", theme_example = TRUE)
```

This will generate all of the file structure for your new blog.

<img src="img/blogdirectory.png" style="border: #A9A9A9 1px solid; width:75%">

After this is complete, you should quit and then reopen the project. Upon reopening, RStudio will recognize the project as a website.


### Customizing the appearance

Make changes to the `config.toml` file (equivalent to the `_site.yml` from basic R Markdown websites) to change the layout and appearance of your website. The available features of the `config.toml` file will differ depending on your theme, and most theme examples come with a well annotated `config.toml` that you can use as a template.

Once you have customized your website features, click on the RStudio addin "Serve Site" to preview the site locally.

<img src="img/servesite.png" style="border: #A9A9A9 1px solid; width:75%">


### Writing a new blog post

There are several ways to create a new post for your site, but the easiest is using the RStudio addin "New Post":

<img src="img/newpost.png" style="border: #A9A9A9 1px solid; width:75%">

This opens a pop-up where you can enter the meta-data for a new post:

<img src="img/newpostbox.png" style="border: #A9A9A9 1px solid; width:75%">

In addition to setting the Title, Author and Date of the post, you can additionally create categories, which will organize your posts in folders, and can add tags to posts, which can make them searchable within your site's content. Be aware that the functioning of these features will vary by theme. Dates can be in the future to allow future release of a post.

Notice at the bottom that you can select whether to use a regular markdown (`.md`) or R markdown (`.Rmd`) file. `.Rmd` files will have to be rendered before generating html pages so it is best practice to limit their use to cases where R code is included. 

A file name and slug will automatically be generated based on the other metadata. The slug is a URL friendly title of your post.

<img src="img/newpostboxfilled.png" style="border: #A9A9A9 1px solid; width:75%">


### Hosting

A `blogdown` site is a bit more cumbersome both to build and to host on GitHub as compared to a regular R Markdown website, and as compared to what I described above. 

*Problem 1*: Because it is a static site, upon building, the files needed to generate the site online are automatically created in a separate subdirectory called `public` within your local directory. However this will cause problems with GitHub hosting since the files to host need to be in the local YOUR_GH_NAME.github.io directory

My solution:

1. Maintain separate directories for the source files (I named this directory "source") and for the static files (the directory YOUR_GH_NAME.github.io) that will be generated on build. The "source" folder is where your R project and `config.toml` files will live.

<img src="img/blogfolders.png" style="border: #A9A9A9 1px solid; width:75%">

2. In your `config.toml` use the option `publishDir = ` to customize `blogdown` to publish to the YOUR_GH_NAME.github.io folder, rather than to the default local location

<img src="img/publishdir.png" style="border: #A9A9A9 1px solid; width:75%">

*Problem 2*: GitHub defaults to using Jekyll with website content, and this needs to be disabled since `blogdown` sites are built with Hugo

To get around this, you need to include an empty file named `.nojekyll` in your GitHub repo YOUR_GH_NAME.github.io, prior to publishing.

<img src="img/nojekyll.png" style="border: #A9A9A9 1px solid; width:75%">


## Additional resources

A compiled list of the additional resources/links presented throughout this tutorial:

- [http://rmarkdown.rstudio.com/rmarkdown_websites.html](http://rmarkdown.rstudio.com/rmarkdown_websites.html): an overview of R Markdown website basics
- [http://jekyllthemes.org/](http://jekyllthemes.org/): Jekyll themes for use with your R Markdown website
- [http://happygitwithr.com/](http://happygitwithr.com/): an introduction to Git/GitHub
- [http://pkgdown.r-lib.org/](http://pkgdown.r-lib.org/): Hadley Wickham's `pkgdown` website
- [https://bookdown.org/yihui/blogdown/](https://bookdown.org/yihui/blogdown/): Yihui Xie's blogdown book
- [https://themes.gohugo.io/](https://themes.gohugo.io/): Hugo themes for use with your `blogdown` website


## Demo

 








<!--chapter:end:rmarkdown_websites_tutorial.Rmd-->

---
title: "Creating websites in R"
author: "Emily C. Zabor"
output: 
    html_document:
        toc: TRUE
        toc_float: TRUE
---

This tutorial provides an introduction to creating websites using R, R Markdown and GitHub pages. 

This tutorial was originally presented at the Memorial Sloan Kettering Cancer Center Department of Epidemiology and Biostatistics R User Group meeting on January 23, 2018.

The current version was updated and presented at the R Ladies NYC Meetup on February 15, 2018.


## Types of websites

The main types of websites you may want to create include:

1. Personal websites
2. Package websites
3. Project websites
4. Blogs


## R Markdown website basics

The minimum requirements for an R Markdown website are:

- `index.Rmd`: contains the content for the website homepage
- `_site.yml`: contains metadata for the website

A basic example of a `_site.yml` file for a website with two pages:

```{r eval = FALSE}
name: "my-website"
navbar:
  title: "My Website"
  left:
    - text: "Home"
      href: index.html
    - text: "About"
      href: about.html
```

And a basic `index.Rmd` to create the Home page:

```{r eval = FALSE}
---
title: "My Website"
---
    
Hello, Website! Welcome to the world.
```


You can find an overview of R Markdown website basics [here](http://rmarkdown.rstudio.com/rmarkdown_websites.html).


## GitHub

This tutorial will focus on hosting websites through GitHub pages. Hosting websites on GitHub pages is free.

If you don't have a GitHub account already, sign up for one at [https://github.com/join?source=header-home](https://github.com/join?source=header-home) with username YOUR_GH_NAME. I'll be referring to this username, YOUR_GH_NAME, as "your GitHub username" throughout this tutorial.

There are other free sites for website hosting, and another popular choice is [Netlify](https://www.netlify.com/).


## Personal websites

An example from the homepage of  [my personal website](http://www.emilyzabor.com/):

<img src="img/personal.png" style="border: #A9A9A9 1px solid; width:75%">


There are two main steps for creating a personal website that will be hosted on GitHub:

1. GitHub setup
2. Local setup


### GitHub setup

1. Create a GitHub repository ("repo") named YOUR_GH_NAME.github.io, where YOUR_GH_NAME is your GitHub username.
2. Initialize it with a README 
    - For the GitHub inexperienced: this can ease the process of cloning the repo by initializing the remote repo with a master branch


### Local setup

1. Clone this remote repository to a local directory with the same name, YOUR_GH_NAME.github.io
2. Add an R Project to this directory
3. Create a `_site.yml` and `index.Rmd` file in your new directory


### Why do I need an R Project?

The R Project is useful because RStudio will recognize your project as a website, and provide appropriate build tools. 

Note: After creating the R Project and initial files, you may need to close the project and reopen it before R will recognize it as a website and show the appropriate build tools.


### Create content

Edit the `_site.yml` file to change the metadata, layout, and  theme of your website. Preview Jekyll themes [here](http://jekyllthemes.org/) and play around with different options. Themes are easy to change even after you have added content. 

For example, the `_site.yml` for my personal website looks like this:

```{r eval = FALSE}
name: "Emily C. Zabor"
output_dir: "."
navbar:
  title: "Emily C. Zabor"
  left:
    - text: "Writing"
      href: research.html
    - text: "Speaking"
      href: talks.html
    - text: "Programming"
      href: software.html
    - text: "Teaching"
      href: teaching.html
  right:
    - icon: fa-envelope fa-lg
      href: contact.html
    - icon: fa-github fa-lg
      href: http://github.com/zabore
    - icon: fa-twitter fa-lg
      href: https://twitter.com/zabormetrics
    - icon: fa-linkedin fa-lg
      href: https://www.linkedin.com/in/emily-zabor-59b902b7/
output:
  html_document:
    theme: paper
    css: 'styles.css'
```

Edit and create `.Rmd` files that contain your website content, which will produce the html pages of your website when you knit them. 

For example, the `index.Rmd` file for my personal website homepage looks like this:

```{r eval = FALSE}
---
---

<link rel="stylesheet" href="styles.css" type="text/css">

<img src="images/emily_2.jpg" style="width:25%; border:10px solid; margin-right: 20px" align="left">

I like to analyze data to answer research questions and test hypotheses. Currently I investigate questions related to breast cancer through my work as a Research Biostatistician at [Memorial Sloan Kettering Cancer Center](https://www.mskcc.org/departments/epidemiology-biostatistics) in the department of Epidemiology & Biostatistics. 

I graduated from the [University of Minnesota](http://www.sph.umn.edu/academics/divisions/biostatistics/) with a MS in biostatistics in 2010. In 2012 I began working toward my DrPH in biostatistics as a part-time student at [Columbia University](https://www.mailman.columbia.edu/become-student/departments/biostatistics), where I am investigating statistical methods for the study of etiologic heterogeneity in epidemiologic studies under the advisement of [Dr. Shuang Wang](https://www.mailman.columbia.edu/people/our-faculty/sw2206) at Columbia University and [Dr. Colin Begg](https://www.mskcc.org/profile/colin-begg) at Memorial Sloan Kettering Cancer Center. I expect to graduate by the end of 2018.

I am a well-known R enthusiast, including serving on the board and being an active member of [R Ladies NYC](http://www.rladiesnyc.org/). 

My full CV is available [here](files/Zabor_CV_2017_Q4.pdf).
```

Once you have your content written and the layout setup, on the Build tab in RStudio, select "Build Website":

<img src="img/build.png" style="border: #A9A9A9 1px solid; width:75%">

Now your local directory contains all of the files needed for your website:

<img src="img/directory.png" style="border: #A9A9A9 1px solid; width:75%">


### Deploy website

Basic approach:

- Select "Upload files" from the main page of your GitHub repo:

<img src="img/uploadbutton.png" style="border: #A9A9A9 1px solid; width:75%">


- And simply drag or select the files from the local repository:


<img src="img/upload.png" style="border: #A9A9A9 1px solid; width:75%">

Advanced approach (recommended):

- use Git from the shell, from a Git client, or from within RStudio (another great reason to use an R Project!)

<img src="img/rstudiogit.png" style="border: #A9A9A9 1px solid; width:75%">

- But this is not a Git/GitHub tutorial. If you want to learn more about Git/GitHub, which I encourage you to do, here's a great resource to get you started: [http://happygitwithr.com/](http://happygitwithr.com/)


### Custom domains

The default is for your site to be hosted at http://YOUR_GH_NAME.github.io, but you can add a custom domain name as well. There are two steps:

1. In your GitHub repository YOUR_GH_NAME.github.io, go to Settings > GitHub pages. Type your domain name in the box under Custom domain and hit Save.

<img src="img/domain.png" style="border: #A9A9A9 1px solid; width:75%">

2. Add a CNAME file to your GitHub repsitory YOUR_GH_NAME.github.io. 

It will appear like this in your repository:

<img src="img/cname1.png" style="border: #A9A9A9 1px solid; width:75%">

And inside the file you will simply have your domain name:

<img src="img/cname2.png" style="border: #A9A9A9 1px solid; width:75%">


## Package websites

An example from the [website](http://www.emilyzabor.com/ezfun/) for my package `ezfun`:

<img src="img/package.png" style="border: #A9A9A9 1px solid; width:75%">

Use Hadley Wickham's great package `pkgdown` to easily build a website from your package that is hosted on GitHub. Details of `pkgdown` can be found on [the pkgdown website](http://pkgdown.r-lib.org/), which was also created using `pkgdown`.

This assumes you already have an R package with a local directory and a GitHub repository.

From within your package directory run: 

```{r eval = FALSE}
devtools::install_github("hadley/pkgdown")
pkgdown::build_site()
```

- This will add a folder called `docs` to the local directory for your package 

- Upload/push these changes to the GitHub repository for your package

- In the GitHub repository for your package go to Settings > GitHub pages. Select "master branch/docs folder" as the source and hit Save

<img src="img/ghsource.png" style="border: #A9A9A9 1px solid; width:75%">

- The page will be added as to your personal website as YOUR_GH_NAME.github.io/repo_name

    - The Home page of the site will be pulled from the README file on your package repository
    - The Reference page of the site lists the included functions with their description
    - Each function can be clicked through to see the help page, if any
    - Would also build pages for any available vignettes

And you're done, it's that easy.


## Project websites

You can create a website for a non-package repository as well. For example, I have [a page](http://www.emilyzabor.com/tutorials/) on my website linking to the repository in which this tutorial is stored.

<img src="img/project.png" style="border: #A9A9A9 1px solid; width:75%">


### Local setup

From within the local directory of the project of interest:

1. Create a `_site.yml` and `index.Rmd` file in your new directory
2. Edit these files to create content and manage layout, as before for personal websites


### GitHub setup

- Upload/push these new files to the GitHub repository for your project

- Enable GitHub pages for the repository by going to Settings > GitHub Pages, where you'll select the "master branch" folder and hit Save

<img src="img/ghpages.png" style="border: #A9A9A9 1px solid; width:75%">


## Blogs

R Markdown websites are simple to create and deploy, but can become cumbersome if you make frequent updates or additions to the website, as in the case of a blog. Luckily, the R package `blogdown` exists just for this purpose. `blogdown` is an R package that allows you to create static websites, which means that the deployed version of the website only consists of JavaScript, HTML, CSS, and images. Luckily the `blogdown` package makes it so that you don't have to know any of those things to create a beautiful website for your blog, powered by Hugo.

For a complete resource on using the `blogdown` website, checkout this [short blogdown book](https://bookdown.org/yihui/blogdown/).

I don't have a personal blog, so let's look at the website I built to feature the events and blog of the [R-Ladies NYC](http://www.rladiesnyc.org/) organization as an example.

<img src="img/rladiesnychome.png" style="border: #A9A9A9 1px solid; width:75%">


### Setup

The first three steps are similar to those from creating a basic R Markdown website:

1. Create a GitHub repository named YOUR_GH_NAME.github.io, where YOUR_GH_NAME is your GitHub username, initialized with a README file
2. Clone the GitHub repo to a local directory with the same name
3. Add an R Project to the local directoroy

Next we get started with `blogdown`.

4. Install `blogdown` and Hugo

```{r eval = FALSE}
install.packages("blogdown")
blogdown::install_hugo()
```

5. Choose a [theme](https://themes.gohugo.io/) and find the link to the theme's GitHub repository. In this case themes aren't quite as easy to change as with basic R Markdown websites, so choose carefully.

6. Within your project session, generate a new site. The option `theme_example = TRUE` will obtain the files for an example site that you can then customize for your needs. Below "user/repo" refers to the GitHub username and GitHub repository for your selected theme.

```{r eval = FALSE}
blogdown::new_site(theme = "user/repo", theme_example = TRUE)
```

This will generate all of the file structure for your new blog.

<img src="img/blogdirectory.png" style="border: #A9A9A9 1px solid; width:75%">

After this is complete, you should quit and then reopen the project. Upon reopening, RStudio will recognize the project as a website.


### Customizing the appearance

Make changes to the `config.toml` file (equivalent to the `_site.yml` from basic R Markdown websites) to change the layout and appearance of your website. The available features of the `config.toml` file will differ depending on your theme, and most theme examples come with a well annotated `config.toml` that you can use as a template.

Once you have customized your website features, click on the RStudio addin "Serve Site" to preview the site locally.

<img src="img/servesite.png" style="border: #A9A9A9 1px solid; width:75%">


### Writing a new blog post

There are several ways to create a new post for your site, but the easiest is using the RStudio addin "New Post":

<img src="img/newpost.png" style="border: #A9A9A9 1px solid; width:75%">

This opens a pop-up where you can enter the meta-data for a new post:

<img src="img/newpostbox.png" style="border: #A9A9A9 1px solid; width:75%">

In addition to setting the Title, Author and Date of the post, you can additionally create categories, which will organize your posts in folders, and can add tags to posts, which can make them searchable within your site's content. Be aware that the functioning of these features will vary by theme. Dates can be in the future to allow future release of a post.

Notice at the bottom that you can select whether to use a regular markdown (`.md`) or R markdown (`.Rmd`) file. `.Rmd` files will have to be rendered before generating html pages so it is best practice to limit their use to cases where R code is included. 

A file name and slug will automatically be generated based on the other metadata. The slug is a URL friendly title of your post.

<img src="img/newpostboxfilled.png" style="border: #A9A9A9 1px solid; width:75%">


### Hosting

A `blogdown` site is a bit more cumbersome both to build and to host on GitHub as compared to a regular R Markdown website, and as compared to what I described above. 

*Problem 1*: Because it is a static site, upon building, the files needed to generate the site online are automatically created in a separate subdirectory called `public` within your local directory. However this will cause problems with GitHub hosting since the files to host need to be in the local YOUR_GH_NAME.github.io directory

My solution:

1. Maintain separate directories for the source files (I named this directory "source") and for the static files (the directory YOUR_GH_NAME.github.io) that will be generated on build. The "source" folder is where your R project and `config.toml` files will live.

<img src="img/blogfolders.png" style="border: #A9A9A9 1px solid; width:75%">

2. In your `config.toml` use the option `publishDir = ` to customize `blogdown` to publish to the YOUR_GH_NAME.github.io folder, rather than to the default local location

<img src="img/publishdir.png" style="border: #A9A9A9 1px solid; width:75%">

*Problem 2*: GitHub defaults to using Jekyll with website content, and this needs to be disabled since `blogdown` sites are built with Hugo

To get around this, you need to include an empty file named `.nojekyll` in your GitHub repo YOUR_GH_NAME.github.io, prior to publishing.

<img src="img/nojekyll.png" style="border: #A9A9A9 1px solid; width:75%">


## Additional resources

A compiled list of the additional resources/links presented throughout this tutorial:

- [http://rmarkdown.rstudio.com/rmarkdown_websites.html](http://rmarkdown.rstudio.com/rmarkdown_websites.html): an overview of R Markdown website basics
- [http://jekyllthemes.org/](http://jekyllthemes.org/): Jekyll themes for use with your R Markdown website
- [http://happygitwithr.com/](http://happygitwithr.com/): an introduction to Git/GitHub
- [http://pkgdown.r-lib.org/](http://pkgdown.r-lib.org/): Hadley Wickham's `pkgdown` website
- [https://bookdown.org/yihui/blogdown/](https://bookdown.org/yihui/blogdown/): Yihui Xie's blogdown book
- [https://themes.gohugo.io/](https://themes.gohugo.io/): Hugo themes for use with your `blogdown` website


## Demo

 








<!--chapter:end:rmarkdown_websites_tutorial2.Rmd-->

---
title: "ROC"
output: html_notebook
---

# tidyroc

https://github.com/dariyasydykova/tidyroc


# 









<!--chapter:end:ROC.Rmd-->

---
title: "ORCID"
output: html_notebook
---



- Introduction to ORCID Researcher Identifiers in R with rorcid

https://www.pauloldham.net/introduction-to-orcid-with-rorcid/







<!--chapter:end:rorcid.Rmd-->

---
title: "R Packages Used"
---

<!-- ```{r , cache = F, warning = FALSE} -->

<!-- knitr::opts_chunk$set( -->
<!-- 	error = TRUE, -->
<!-- 	message = FALSE, -->
<!-- 	warning = FALSE, -->
<!-- 	eval = FALSE, -->
<!-- 	include = TRUE -->
<!-- ) -->
<!-- ``` -->


---


# R package development  

- Analyses as Packages  
http://rmflight.github.io/posts/2014/07/analyses_as_packages.html  

- Creating an analysis as a package and vignette 
http://rmflight.github.io/posts/2014/07/vignetteAnalysis.html

- pkgdown  
https://pkgdown.r-lib.org/index.html

- Writing an R package from scratch  
https://hilaryparker.com/2014/04/29/writing-an-r-package-from-scratch/

- R packages  
http://r-pkgs.had.co.nz/intro.html  
https://r-pkgs.org  


- devtools  
https://github.com/r-lib/devtools


---





# purrr

- Learning Functional Programming & purrr

https://paulvanderlaken.com/2018/12/05/learning-functional-programming-purrr/







# DiagrammeR

http://rich-iannone.github.io/DiagrammeR/


# gggenes

https://cran.r-project.org/web/packages/gggenes/vignettes/introduction-to-gggenes.html


# textmineR

https://cran.r-project.org/web/packages/textmineR/vignettes/a_start_here.html

https://cran.r-project.org/web/packages/textmineR/vignettes/b_document_clustering.html

https://cran.r-project.org/web/packages/textmineR/vignettes/c_topic_modeling.html

https://cran.r-project.org/web/packages/textmineR/vignettes/d_text_embeddings.html

https://cran.r-project.org/web/packages/textmineR/vignettes/e_doc_summarization.html




# tidyshiny

https://github.com/MangoTheCat/tidyshiny/


# rio

https://cran.r-project.org/web/packages/rio/README.html

install.packages("rio")
install_formats()


# addinplots

RStudio Addins for plotting

https://github.com/homerhanumat/addinplots/





# citr

citr: RStudio Addin to Insert Markdown Citations

https://github.com/crsh/citr




# ggplotAssist

https://github.com/cardiomoon/ggplotAssist/blob/master/README.md


# Radiant

Radiant – Business analytics using R and Shiny

https://radiant-rstats.github.io/docs/tutorials.html

# rspivot

https://ryantimpe.github.io/rspivot/index.html







# finalfit

http://www.datasurg.net/2018/05/22/finalfit-knitr-and-r-markdown-for-quick-results/



# ggraptR

https://github.com/cargomoose/ggraptR

[![](https://raw.githubusercontent.com/cargomoose/ggraptR/master/inst/ggraptR/www/demo.gif)](https://github.com/cargomoose/ggraptR)



# bookdownThemeEditor

https://github.com/hebrewseniorlife/bookdownThemeEditor


# esquisse


https://github.com/dreamRs/esquisse


# ggtextures  

https://github.com/clauswilke/ggtextures



# ggstatsplot

ggplot2 Based Plots with Statistical Details

https://indrajeetpatil.github.io/ggstatsplot/


## ggcoefstats

https://indrajeetpatil.github.io/ggstatsplot/articles/ggcoefstats.html

# Choroplethr

https://arilamstein.com/open-source/



# purrr

https://colinfay.me/happy-dev-purrr/

# knitcitations

http://www.carlboettiger.info/2012/05/30/knitcitations.html

# infer

Tidy Statistical Inference  
https://cran.r-project.org/web/packages/infer/index.html

https://kbroman.org/pkg_primer/

## Packages to be studied


```{r eval=FALSE, include=FALSE, echo=TRUE}
packagefinder::findPackage(c("pubmed", "bibliography"))
```

https://cran.rstudio.com/web/packages/sys/

# scales

Scale Functions for Visualization

https://cran.r-project.org/web/packages/scales/index.html

```{r eval=FALSE, include=FALSE, echo=TRUE}
paste0(round(.20394*100, 1), "%")
scales::percent(.20394)
```


# animation
A Gallery of Animations in Statistics and Utilities to Create Animations
https://cran.r-project.org/web/packages/animation/

# gganimate
A Grammar of Animated Graphics
https://github.com/thomasp85/gganimate


https://cran.r-project.org/web/packages/naniar/index.html

https://hughjonesd.github.io/anim.plots/anim.plots.html

You Can Design a Good Chart with R
https://towardsdatascience.com/you-can-design-a-good-chart-with-r-5d00ed7dd18e

Bubble-grid-maps
https://jschoeley.github.io/2018/06/30/bubble-grid-map.html

Taking control of animations in R and demystifying them in the process 
https://www.data-imaginist.com/2017/animating-the-logo/?utm_content=bufferd418f&utm_medium=social&utm_source=twitter.com&utm_campaign=buffer
 
https://cran.r-project.org/web/packages/rticles/rticles.pdf



https://colinfay.me/build-api-wrapper-package-r/

https://www.datacamp.com/community/tutorials/setup-data-science-environment

https://cran.r-project.org/web/packages/kableExtra/vignettes/awesome_table_in_html.html####installation

https://cran.r-project.org/web/packages/expss/vignettes/tables-with-labels.html

https://cran.r-project.org/web/packages/tabulizer/vignettes/tabulizer.html

https://github.com/benjaminrich/table1

https://cran.r-project.org/web/packages/tableone/vignettes/introduction.html

https://twitter.com/mitchoharawild/status/1007297976711110659?s=12

https://twitter.com/mf_viz/status/1004954297962917891?s=12

https://github.com/cmap/morpheus.R

https://github.com/talgalili/heatmaply/

https://github.com/rstudio/d3heatmap

http://worksmarter.pl/en/post/webscraping-case-study-1/

http://worksmarter.pl/en/post/job-automation-r-1/

http://www.brodrigues.co/blog/2018-06-10-scraping_pdfs/

https://www.r-bloggers.com/how-to-plot-with-ggiraph/

https://www.r-bloggers.com/understanding-pca-using-stack-overflow-data/

https://www.r-bloggers.com/beautiful-and-powerful-correlation-tables-in-r/

http://journals.plos.org/plosone/article?id=10.1371%2Fjournal.pone.0188299

https://github.com/trinker/reports

Animated Directional Chord Diagrams
https://guyabel.com/post/animated-directional-chord-diagrams/


modelDown: a website generator for your predictive models
http://smarterpoland.pl/index.php/2018/06/modeldown-a-website-generator-for-your-predictive-models/

# drake
https://ropensci.github.io/drake/


## chromoMap

An R package for Interactive visualization and mapping of human chromosomes

https://cran.r-project.org/web/packages/chromoMap/vignettes/chromoMap.html

## tabplot

Tableplot, a Visualization of Large Datasets

https://cran.r-project.org/web/packages/tabplot/index.html

https://cran.r-project.org/web/packages/tabplot/vignettes/tabplot-vignette.html

https://cran.r-project.org/web/packages/tabplot/vignettes/tabplot-timings.html

## treemap

Treemap Visualization

https://cran.r-project.org/web/packages/treemap/index.html

## VIM

Visualization and Imputation of Missing Values

https://cran.r-project.org/web/packages/VIM/index.html

## survey

https://cran.r-project.org/web/packages/survey/index.html

## tidytext

https://cran.r-project.org/web/packages/tidytext/index.html

https://cran.r-project.org/web/packages/tidytext/vignettes/tf_idf.html

https://cran.r-project.org/web/packages/tidytext/vignettes/tidying_casting.html

https://cran.r-project.org/web/packages/tidytext/vignettes/tidytext.html

https://cran.r-project.org/web/packages/tidytext/vignettes/topic_modeling.html


## rapport  an R templating system

http://rapport-package.info/

## ztable

https://github.com/cardiomoon/ztable

## texreg

https://cran.r-project.org/web/packages/texreg/vignettes/texreg.pdf

## SortableHTMLTables

https://cran.r-project.org/web/packages/SortableHTMLTables/index.html

## hwriter

https://cran.r-project.org/web/packages/hwriter/vignettes/hwriter.pdf

## HTMLUtils

https://cran.r-project.org/web/packages/HTMLUtils/index.html

## htmlTable
https://cran.r-project.org/web/packages/htmlTable/index.html

## formattable

https://cran.r-project.org/web/packages/formattable/index.html

## DT

DT: An R interface to the DataTables library

https://rstudio.github.io/DT/

## stargazer

Well-Formatted Regression and Summary Statistics Tables

https://cran.r-project.org/web/packages/stargazer/index.html




## conflicted

## psycho

```{r, eval=FALSE, include=FALSE}
install.packages("devtools")
library("devtools")
install_github("neuropsychology/psycho.R")
library("psycho")
```


## styler

## tidyverse

## splitstackshape

- The “splitstackshape” package for R

https://www.r-bloggers.com/the-splitstackshape-package-for-r/

## epiR

Basic Epi

install.packages("epiR")

## lmtest

Some useful epitools especially 2*2 tables and stratified MH Odds

install.packages("lmtest")

## popEpi

install.packages("popEpi")

## Extra tools to go with 'Epi' and make nice rate tables

## survival

install.packages("survival")

for survial analysis, kaplan-meier plots and cox regression

## survminer

install.packages("survminer")

Really nice KM plots!

## Random-Effects Modeks

install.packages("lme4")

install.packages("sjstats")


## Multiple Imputation

install.packages("mice")


install.packages("arsenal")

## Really nice summmary tables

## Complex survey data analysis

install.packages("survey")

## For handling complex survey data

install.packages("ICC")


## Alternative methods for ICC calculation from survey data

## Meta-analysis

install.packages("meta")


install.packages("metafor")


install.packages("workflowr")


install.packages("DiagrammeR")


install.packages("tangram")


devtools::install_github("lbusett/insert_table")


```{r eval=FALSE, include=FALSE, echo=TRUE}
#### Obtain names of all packages on CRAN
names.available.packages <- rownames(available.packages())

#### Extract packages names that contain Rcmdr
Rcmdr.related.packages <- names.available.packages[grep("Rcmdr", names.available.packages)]
Rcmdr.related.packages

#### Install these packages
install.packages(pkgs = Rcmdr.related.packages)
```



```{r eval=FALSE, include=FALSE, echo=TRUE}
usePackage <- function(p) {
  if (!is.element(p, installed.packages()[, 1])) {
    install.packages(p, dep = TRUE)
  }
  require(p, character.only = TRUE)
}
```



```{r eval=FALSE, include=FALSE, echo=TRUE}
file.edit("~/Desktop/foo/.Rprofile")
## This opens up a script window, within which you can enter in your library commands
library(ggplot2)
library(scales)
library(plyr)
library(reshape2)
```






## abind



## acepack



## AlgDesign



## AnnotationDbi



## aplpack



## arm



## assertthat



## backports



## base



## base64enc



## BCA



## BH



## BiasedUrn



## bindr



## bindrcpp



## Biobase



## BiocGenerics



## BiocInstaller



## bit



## bit64



## bitops



## blob



## boot



## broom



## BsMD



## ca



## car



## caTools



## cellranger



## checkmate



## class



## cluster



## clv



## coda



## codetools



## coin



## colorspace



## combinat



## compiler



## conf.design



## curl



## data.table



## datasets



## date



## DBI



## deldir



## depthTools



## DiceDesign



## dichromat



## digest



## DoE.base



## DoE.wrapper



## doParallel



## dplyr



## e1071



## effects



## ENmisc



## epiR



## estimability



## evaluate



## ez



## flexclust



## forcats



## foreach



## foreign



## formatR



## Formula



## fracdiff



## FrF2



## gdata



## ggplot2

http://www.cookbook-r.com/Graphs/

### ggplot2 extensions

http://www.ggplot2-exts.org/gallery/

http://corybrunson.github.io/ggalluvial/

http://www.sthda.com/english/rpkgs/survminer/

## ggthemes



## ggvis



## glue



## goftest



## graphics



## grDevices



## grid



## gridBase



## gridExtra



## gtable



## gtools



## haven



## highr



## Hmisc



## hms



## htmlTable



## htmltools



## htmlwidgets



## httpuv



## httr



## igraph



## IRanges



## irlba



## iterators



## jsonlite



## KernSmooth



## knitr



## labeling



## lattice



## latticeExtra



## lazyeval



## leaps



## lhs



## lme4



## lmtest



## lsmeans



## lubridate



## magrittr



## markdown



## MASS



## Matrix



## matrixcalc



## MatrixModels



## memoise



## methods



## mgcv



## mi



## mime



## minqa



## mnormt



## modelr



## modeltools



## multcomp



## munsell



## mvtnorm



## nlme



## nloptr



## NLP



## NMF



## nnet



## nortest



## openssl



## ordinal



## orloca



## orloca.es



## parallel



## pbkrtest



## pkgconfig



## pkgmaker



## plogr



## plyr



## polyclip



## psych



## purrr

https://colinfay.me/happy-dev-purrr/

## quadprog



## quantmod



## quantreg



## R2HTML



## R6



## randtests



## Rcmdr



## RcmdrMisc



## RcmdrPlugin.BCA



## RcmdrPlugin.coin



## RcmdrPlugin.depthTools



## RcmdrPlugin.DoE



## RcmdrPlugin.doex



## RcmdrPlugin.EACSPIR



## RcmdrPlugin.EBM



## RcmdrPlugin.epack



## RcmdrPlugin.EZR



## RcmdrPlugin.KMggplot2



## RcmdrPlugin.mosaic



## RcmdrPlugin.MPAStats



## RcmdrPlugin.orloca



## RcmdrPlugin.plotByGroup



## RcmdrPlugin.qual



## RcmdrPlugin.SCDA

## RcmdrPlugin.sampling

Tools for sampling in Official Statistical Surveys

https://cran.r-project.org/web/packages/RcmdrPlugin.sampling/index.html


## RcmdrPlugin.seeg



## RcmdrPlugin.SLC



## RcmdrPlugin.SM



## RcmdrPlugin.steepness



## RcmdrPlugin.survival



## RcmdrPlugin.TeachingDemos



## RcmdrPlugin.temis



## RcmdrPlugin.UCA



## RColorBrewer



## Rcpp



## RcppArmadillo



## RcppEigen



## readr



## readxl



## registry



## relimp



## rematch



## reshape



## reshape2



## rgl



## RISmed



## rJava



## rlang



## rmarkdown



## RMySQL



## rngtools



## rpart



## rpart.plot



## rprojroot



## rsm


## rticles tufte


https://github.com/rstudio/rticles

https://www.coursera.org/learn/reproducible-templates-analysis/lecture/QaDy7/building-a-document-template-part-2


https://www.coursera.org/learn/reproducible-templates-analysis/supplement/GQz0G/lecture-prep-code-file


https://github.com/rstudio/tufte



## RSQLite



## rvest



## S4Vectors



## sandwich



## scales



## scatterplot3d



## SCMA



## SCRT



## SCVA



## seeg



## selectr



## sem



## sfsmisc



## sgeostat



## shiny



## shinythemes



## slam



## SLC



## sourcetools



## SparseM



## spatial



## spatstat



## spatstat.utils



## splines



## stats



## stats4



## steepness



## stringi



## stringr



## survival



## tcltk



## tcltk2



## TeachingDemos



## tensor



## TH.data



## tibble



## tidyr



## tidyverse



## timeDate



## tkrplot



## tm



## tools



## tseries



## TTR



## ucminf



## utils



## vcd



## viridis



## viridisLite



## XLConnect



## XLConnectJars



## xml2



## xtable



## xts



## yaml



## zoo















install.packages("stargazer")

install.packages("pander")

install.packages("tables")

install.packages("ascii")

install.packages("xtable")

devtools::install_github("hadley/dplyr")


install.packages("gapminder")

install.packages("alluvial")


vignette("databases", package = "dplyr")

install.packages("robustbase")

install.packages("insuranceData")

install.packages("Lahman")


install.packages("tidyverse")

install.packages("qdap")

install.packages("openxlsx")


devtools::install_github("kassambara/r2excel")

install.packages("WriteXLS")

install.packages("XLConnect")

library(XLConnect)


```{r eval=FALSE, include=FALSE, echo=TRUE}
if (!require(wordcloud)) {
  install.packages("wordcloud")
  library(wordcloud)
}
```



```{r eval=FALSE, include=FALSE, echo=TRUE}
installed <- as.data.frame(installed.packages())
write.csv(installed, "installed_previously.csv")

installedPreviously <- read.csv("installed_previously.csv")
baseR <- as.data.frame(installed.packages())
toInstall <- setdiff(installedPreviously, baseR)

install.packages(toInstall)


packagelist <- installed.packages()
class(packagelist)
packagelist[, 1]
```




## Rcmdr

```{r eval=FALSE, include=FALSE, echo=TRUE}
install.packages("Rcmdr")
install.packages("RcmdrPlugin.BCA")
install.packages("RcmdrPlugin.coin")
install.packages("RcmdrPlugin.depthTools")
install.packages("RcmdrPlugin.doBy")
install.packages("RcmdrPlugin.DoE")
install.packages("RcmdrPlugin.doex")
install.packages("RcmdrPlugin.EACSPIR")
install.packages("RcmdrPlugin.EBM")
install.packages("RcmdrPlugin.epack")
install.packages("RcmdrPlugin.EZR")
install.packages("RcmdrPlugin.HH")
install.packages("RcmdrPlugin.IPSUR")
install.packages("RcmdrPlugin.KMggplot2")
install.packages("RcmdrPlugin.mosaic")
install.packages("RcmdrPlugin.MPAStats")
install.packages("RcmdrPlugin.orloca")
install.packages("RcmdrPlugin.plotByGroup")
install.packages("RcmdrPlugin.qcc")
install.packages("RcmdrPlugin.qual")
install.packages("RcmdrPlugin.SCDA")
install.packages("RcmdrPlugin.seeg")
install.packages("RcmdrPlugin.SLC")
install.packages("RcmdrPlugin.SM")
install.packages("RcmdrPlugin.StatisticalURV")
install.packages("RcmdrPlugin.steepness")
install.packages("RcmdrPlugin.survival")
install.packages("RcmdrPlugin.TeachingDemos")
install.packages("RcmdrPlugin.temis")
install.packages("RcmdrPlugin.UCA")
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
source("https://bioconductor.org/biocLite.R")
biocLite()
```


install.packages("tidyverse")

install.packages("devtools")

devtools::install_github("hadley/colformat")

devtools::install_github("ropenscilabs/skimr")

install.packages("hflights")

install.packages("dbplyr")

install.packages("xlsx")

require(devtools)

install_github("ProjectMOSAIC/mosaic")

install.packages("jmv")


install.packages("wordcloud")

install.packages("https://cran.r-project.org/src/contrib/Archive/tm/tm_0.6.tar.gz", repos = NULL)

install.packages("SnowballC")

install.packages("RColorBrewer")

install.packages("tidytext")

install.packages("packrat")

install.packages("testthat")

install.packages("prettydoc")

install.packages("rmdformats")

install.packages(c("fivethirtyeight", "tidyverse", "knitr", "kableExtra", "ggthemes"))

library(devtools)

install_github("SPSStoR", username = "lebebr01")

library(SPSStoR)


```{r eval=FALSE, include=FALSE, echo=TRUE}
ip <- installed.packages()
pkgs.to.remove <- ip[!(ip[, "Priority"] %in% c("base", "recommended")), 1]
sapply(pkgs.to.remove, remove.packages)
sapply(pkgs.to.remove, install.packages)
```




```{r eval=FALSE, include=FALSE, echo=TRUE}
install.packages("devtools")
devtools::install_github("AndreaCirilloAC/updateR")
library(updateR)
updateR(admin_password = "") ## Where "PASSWORD" stands for your system password

```






survival

survminer

install.packages("tutorial")


devtools::install_github("romainfrancois/highlight")

install.packages("highr", repos = "http://rforge.net", type = "source")



install.packages("xplain")





```{r remove all user installed packages in R, eval=FALSE, include=FALSE}
## How to remove all user installed packages in R
## https://www.r-bloggers.com/how-to-remove-all-user-installed-packages-in-r/

## create a list of all installed packages
ip <- as.data.frame(installed.packages())
head(ip)
## if you use MRO, make sure that no packages in this library will be removed
ip <- subset(ip, !grepl("MRO", ip$LibPath))
## we don't want to remove base or recommended packages either\
ip <- ip[!(ip[,"Priority"] %in% c("base", "recommended")),]
## determine the library where the packages are installed
path.lib <- unique(ip$LibPath)
## create a vector with all the names of the packages you want to remove
pkgs.to.remove <- ip[,1]
head(pkgs.to.remove)
## remove the packages
sapply(pkgs.to.remove, remove.packages, lib = path.lib)
```


## Schedule R Script

library(knitr)

library(markdown)
 

knit("SchedulePubMedAnalysis.Rmd")

markdownToHTML("SchedulePubMedAnalysis.md", "docs/SchedulePubMedAnalysis.html")

setwd("~")

file.choose()

library(rmarkdown)


Preperation Packages Schedule R Script

 https://github.com/jkeirstead/scholar

 http://rogiersbart.blogspot.com.tr/2015/05/put-google-scholar-citations-on-your.html

 http://tuxette.nathalievilla.org/?p=1682

 https://github.com/sunitj/scholar

 https://tgmstat.wordpress.com/2013/09/11/schedule-rscript-with-cron/

 https://github.com/ropensci/git2r
     

 library(knitr)

 library(markdown)
 
 install.packages('data.table')

 install.packages('knitr')

 install.packages('miniUI')

 install.packages('shiny')

 install.packages('shinyFiles')
 
 install.packages("taskscheduleR", repos = "http://www.datatailor.be/rcube", type = "source")
 
 devtools::install_github("bnosac/cronR")

 install.packages('scholar')
 
 
 install.packages("git2r")


## flatxml

http://www.zuckarelli.de/flatxml/articles/example/example.html

```{r eval=FALSE, include=FALSE, echo=TRUE}
xmldata <- flatxml::fxml_importXMLFlat("data/PathologyTurkey.xml")

```


## roomba

General purpose API response tidier

https://github.com/ropenscilabs/roomba/

```{r eval=FALSE, include=FALSE, echo=TRUE}
roomba::shiny_roomba()
```



## tufterhandout

Tufte-style html document format for rmarkdown

https://cran.r-project.org/web/packages/tufterhandout/index.html

```{r eval=FALSE, include=FALSE, echo=TRUE}
## install.packages("tufterhandout")
library(tufterhandout)
```

## papeR

[https://sbalci.github.io/MyRCodesForDataAnalysis/papeR.nb.html](https://sbalci.github.io/MyRCodesForDataAnalysis/papeR.nb.html)

## tm

install.packages("tm")

https://cran.r-project.org/web/packages/tm/vignettes/tm.pdf

---

https://atom.io/packages/hydrogen


---

R Package Documentation
https://rdrr.io/
R Packages Search and Statistics
https://www.rpackages.io/
ReporteRs is an R package for creating Microsoft Word and Powerpoint documents.
https://davidgohel.github.io/ReporteRs/
officer Manipulation of Microsoft Word and PowerPoint Documents
https://cran.r-project.org/web/packages/officer/
pubh
https://cran.rstudio.com/web/packages/pubh/vignettes/introduction.html
What does visdat do?
Initially inspired by csv-fingerprint, vis_dat helps you visualise a dataframe and “get a look at the data” by displaying the variable classes in a dataframe as a plot with vis_dat, and getting a brief look into missing data patterns using vis_miss.
https://www.tidyverse.org/
https://cran.r-project.org/web/packages/shiny/index.html
https://rmarkdown.rstudio.com/
http://ggplot2.org/
https://yihui.name/knitr/
http://vita.had.co.nz/papers/tidy-data.html
https://blog.rstudio.com/2015/04/09/readr-0-1-0/
http://readxl.tidyverse.org/
https://cran.r-project.org/web/packages/lubridate/vignettes/lubridate.html
https://github.com/r-lib/devtools
https://cran.r-project.org/web/packages/magrittr/index.html
http://rstudio.github.io/packrat/
https://cran.r-project.org/web/packages/stringr/vignettes/stringr.html
https://github.com/tidyverse/dplyr
https://cran.r-project.org/web/packages/broom/vignettes/broom.html
https://cran.r-project.org/web/packages/roxygen2/vignettes/roxygen2.html
http://tibble.tidyverse.org/
https://blog.rstudio.com/2016/03/29/feather/
https://blog.rstudio.com/2016/08/31/forcats-0-1-0/
https://github.com/tidyverse/hms
https://github.com/trestletech/plumber
https://github.com/r-lib/testthat
http://purrr.tidyverse.org/


# broom

summarizes key information about models

https://broom.tidyverse.org/index.html












## abind


## acepack


## addinexamples


## additivityTests


## ade4


## AER


## afex


## AlgDesign


## alluvial


## alr4


## animation


## ANTsR


## ANTsRCore


## anytime


## apaTables


## aplpack


## arm


## arsenal


## ash


## assertthat


## aws.s3


## aws.signature


## backports


## base


## base64enc


## BayesFactor


## bayesplot


## BCA


## BcDiag


## BDgraph


## bdsmatrix


## BH


## BiasedUrn


## BiBitR


## bibtex


## biclust


## bindr


## bindrcpp


## BiocGenerics


## BiocInstaller


## biostatmethods


## bit


## bit64


## bitops


## blob


## blogdown


## bookdown


## boot


## brew


## bridgesampling


## brms


## Brobdingnag


## broom


## BsMD


## ca


## cairoDevice


## callr


## car


## carData


## carbonate

https://yonicd.github.io/carbonate/

https://carbon.now.sh/

## caTools


## cellranger


## checkmate


## class


## classInt


## cli


## clipr


## clisymbols


## clue


## cluster


## clv


## cmaker


## cmprsk


## coauthornetwork


## cobs


## coda


## CodeDepends


## codetools


## coin


## colorspace


## colourpicker


## combinat


## commonmark


## compiler


## conf.design


## config


## conflicted


## contfrac


## corpcor


## covr


## cowplot


## coxme


## cramer


## crayon


## cronR


## crosstalk


## csvy


## curl


## d3Network


## data.table


## data.tree


## datapasta

https://github.com/MilesMcBain/datapasta

## datasets


## date


## DBI


## dbplyr


## debugme


## Deducer


## DeducerExtras


## demography


## dendextend


## DEoptimR


## depthTools


## desc


## descr1


## DescTools


## deSolve


## devtools


## DiagrammeR


## DiceDesign


## dichromat


## digest


## diptest


## DoE.base


## DoE.wrapper


## doParallel


## downloader


## dplyr


## DT


## dtplyr


## dunn.test


## dygraphs


## e1071


## EcoVirtual


## effects


## effsize


## ellipse


## elliptic


## emmeans


## enc


## ENmisc


## Epi


## epibasix


## epiDisplay


## epiR


## epitools


## errorist

Automatic Error and Warning Search

https://github.com/coatless/errorist


## estimability


## etm


## evaluate


## exact2x2


## exactci


## exactRankTests


## experiment


## expm


## extrantsr


## ez


## FactoMineR


## Fahrmeir


## fansi


## fastmatch


## fBasics


## fda


## fdrtool


## feather


## ff


## ffbase


## finalfit


## fit.models


## fivethirtyeight


## flashClust


## flatxml


## flexclust


## flexmix


## flextable


## FNN


## forcats


## foreach


## forecast


## forecastHybrid


## foreign


## formatR


## formattable


## Formula


## fpc


## fracdiff


## FrF2


## fslr


## fst


## ftsa


## gdata


## gdtools


## gee


## getPass


## GGally


## ggcorrplot


## ggdendro


## ggExtra


## ggforce


## ggformula


## ggfortify


## ggm


## ggplot2


## ggplot2movies


## ggpubr


## ggraph


## ggrepel


## ggridges


## ggsci


## ggsignif


## ggstance


## ggstatsplot


## ggthemes


## ggvis


## gh


## git2r


## glasso


## glmmTMB


## glue


## gmailr


## gmodels


## gmp


## gnm


## googledrive

https://googledrive.tidyverse.org/

## markdrive

https://github.com/MilesMcBain/markdrive


## googleVis


## GPArotation


## gplots


## graph


## graphics


## graphTweets


## grDevices


## grid


## gridExtra


## gsl


## gss


## gtable


## gtools


## gWidgets


## gWidgetstcltk


## GWRM


## h2o


## hash


## haven


## hdrcde


## hexbin


## HH


## highlight


## highr


## Hmisc


## hms


## htmlTable


## htmltools


## htmlwidgets


## httpuv


## httr


## huge


## hunspell


## hypergeo


## ICC


## IRdisplay


## IRkernel


## ISLR


## ISwR


## ITKR


## igraph


## influenceR


## ini


## inline


## ipred


## irlba


## irr


## iterators


## JavaGD


## JGR


## jmv


## jmvcore


## jmvconnect

https://cran.r-project.org/package=jmvconnect


## jomo


## jose


## jpeg


## jsonlite


## kableExtra


## kernlab


## KernSmooth


## klaR


## km.ci


## KMsurv


## knitr


## kohonen


## ks


## labeling


## labelled


## Lahman


## later


## lattice


## latticeExtra


## lava


## lavaan


## lazyeval


## leaps


## lfstat


## lhs


## lintr


## lisrelToR


## lme4


## lmerTest


## lmom


## lmomRFA


## lmtest


## locfit


## loo


## lpSolve


## lubridate


## MAd


## magic


## magicfor


https://cran.r-project.org/web/packages/magicfor/vignettes/magicfor.html


```{r eval=FALSE, include=FALSE, echo=TRUE}
magicfor::magic_for(silent = TRUE)

for (i in 1:3) {
  squared <- i ^ 2
  cubed <- i ^ 3
  put(squared, cubed)
}

magicfor::magic_result_as_dataframe()
```



## magrittr


## manipulate


## manipulateWidget


## mapproj


## maps


## maptools


## markdown


## MASS


## Matrix


## matrixcalc


## MatrixModels


## matrixStats


## maxstat


## MBESS


## mc2d


## mclust


## memisc


## memoise


## meta


## metafor


## methods


## mgcv


## mi


## mice

http://www.stefvanbuuren.nl/mi/FIMD.html

- Flexible Imputation of Missing Data

https://stefvanbuuren.name/fimd/

## microbenchmark


## mime


## miniUI


## minpack.lm


## minqa


## misc3d


## mitml


## mixlm


## mixOmics


## mlbench


## mnormt


## modelr


## modeltools


## mosaic

https://github.com/ProjectMOSAIC/mosaic/blob/master/vignettes/mosaic-resources.Rmd

http://mosaic-web.org/


## mosaicCore


## mosaicData


## multcomp


## multcompView


## multicool


## MuMIn


## munsell


## mvnormtest


## mvtnorm


## network


## networkD3


## neurobase


## neuroim


## nFactors


## nleqslv


## nlme


## nloptr


## NLP


## nnet


## nortest


## nparLD


## numbers


## numDeriv


## nycflights13


## OceanView


## officer


## OpenMx


## openssl


## openxlsx


## OptimClassifier


## ordinal


## orloca


## orloca.es


## oro.dicom


## oro.nifti


## packagefinder


## packrat


## pacman


## pan


## pander


## papeR


## parallel


## party


## pbapply


## pbdZMQ


## pbivnorm


## pbkrtest


## pcaPP


## permute


## phia


## pillar


## pkgbuild


## pkgconfig


## pkgload


## PKI


## plogr


## plot3D


## plot3Drgl


## plotly


## pls


## plyr


## PMCMR


## png


## popEpi


## ppcor


## prabclus


## pracma


## praise


## prediction


## prettydoc


## prettyR


## prettyunits


## pROC


## processx


## prodlim


## progress


## promises


## pspearman


## psych


## psycho


## pubh


## purrr


## purrrlyr


## pwr


## qgraph


## quadprog


## quantmod


## quantreg


## questionr


## qvcalc


## R.cache


## R.matlab


## R.methodsS3


## R.oo


## R.utils


## R2HTML


## R6


## rainbow


## rainfreq


## randomcoloR


## randtests


## ranger


## RApiDatetime


## rappdirs


## rARPACK


## raster


## rattle


## rCharts


## rcmdcheck


## Rcmdr


## RcmdrMisc


## RcmdrPlugin.aRnova


## RcmdrPlugin.BCA


## RcmdrPlugin.BiclustGUI


## RcmdrPlugin.coin


## RcmdrPlugin.depthTools


## RcmdrPlugin.DoE


## RcmdrPlugin.doex


## RcmdrPlugin.EACSPIR


## RcmdrPlugin.EBM


## RcmdrPlugin.EcoVirtual


## RcmdrPlugin.epack


## RcmdrPlugin.Export


## RcmdrPlugin.EZR


## RcmdrPlugin.FactoMineR


## RcmdrPlugin.FuzzyClust


## RcmdrPlugin.GWRM


## RcmdrPlugin.HH


## RcmdrPlugin.IPSUR


## RcmdrPlugin.KMggplot2


## RcmdrPlugin.lfstat


## RcmdrPlugin.MA


## RcmdrPlugin.mosaic


## RcmdrPlugin.MPAStats


## RcmdrPlugin.NMBU


## RcmdrPlugin.OptimClassifier


## RcmdrPlugin.orloca


## RcmdrPlugin.PcaRobust


## RcmdrPlugin.plotByGroup


## RcmdrPlugin.pointG


## RcmdrPlugin.qual


## RcmdrPlugin.RiskDemo


## RcmdrPlugin.RMTCJags


## RcmdrPlugin.ROC


## RcmdrPlugin.sampling


## RcmdrPlugin.SCDA


## RcmdrPlugin.SLC


## RcmdrPlugin.SM


## RcmdrPlugin.sos


## RcmdrPlugin.steepness


## RcmdrPlugin.survival


## RcmdrPlugin.sutteForecastR


## RcmdrPlugin.TeachingDemos


## RcmdrPlugin.temis


## RcmdrPlugin.UCA


## RColorBrewer


## Rcpp


## RcppArmadillo


## RcppEigen


## RCurl


## readODS


## readr


## readstata13


## readxl


## RefManageR


## relimp


## rematch


## rematch2


## remotes


## repr


## reprex


## reshape


## reshape2


## ResourceSelection


## rex


## rgexf


## rgl


## RGtk2


## rhandsontable


## RISmed


## rio


## rjags


## rJava


## rjson


## RJSONIO


## rlang


## rmarkdown


## rmatio


## rmdformats


## rmeta


## Rmpfr


## RMySQL


## RNifti


## robets


## robust


## robustbase


## rockchalk


## ROCR


## Rook


## roomba


## roxygen2


## rpart


## rpart.plot


## rpf


## RPostgreSQL


## rprojroot


## rrcov


## rsconnect


## rsm


## RSpectra


## RSQLite


## rstan


## rstanarm


## rstantools


## rstudioapi


## RStudioConsoleRender


## rsvd


## Rtsne


## rtweet


## runjags


## RVAideMemoire


## rversions


## rvest


## rvg

https://github.com/davidgohel/rvg


## s4vd


## sampling


## sandwich


## scales


## scatr


## scatterplot3d


## scholar


## SCMA


## SCRT


## SCVA


## sde


## SDMTools


## selectr


## sem


## SemiCompRisks


## semPlot


## semTools


## sendmailR


## servr


## sessioninfo


## sf


## sfsmisc


## shape


## shiny


## shinyFiles


## shinyjs


## shinystan


## shinythemes


## sjlabelled


## sjmisc


## sjstats


## skimr


## slam


## SLC


## Sleuth2


## smcure


## sna


## snakecase


## som


## sos


## sourcetools


## sp


## sparklyr


## SparseM


## spatial


## spData


## splines


## splitstackshape


## SQUAREM


## ssanv


## ssgraph


## stabledist


## StanHeaders


## stapler


## stargazer


## statisticalModeling


## statnet.common


## stats


## stats4


## steepness


## stencila


## stringdist


## stringi


## stringr


## strucchange


## styler


## superbiclust


## SuppDists


## survey


## survival


## survminer


## survMisc


## sutteForecastR


## svglite


## tableone

https://cran.r-project.org/web/packages/tableone/vignettes/introduction.html



## tables


## tabplot


## tangram

https://github.com/spgarbet/tangram

https://cran.r-project.org/web/packages/tangram/index.html

https://cran.r-project.org/web/packages/tangram/vignettes/summary-example.html


- Global Style for Rmd Example


     <style type="text/css">
     \```{r, results='asis'}
     cat(custom_css("lancet.css"))
     ```
     .figbody thead
     {
       background: #aaffff !important;
     }
     .figbody tbody .odd
     {
       background: #aaffff !important;
     }
     </style>
 


https://cran.r-project.org/web/packages/tangram/vignettes/fda-example.html

https://cran.r-project.org/web/packages/tangram/vignettes/example.html


 

https://cran.r-project.org/web/packages/tangram/vignettes/single-style.html


## tcltk


## tcltk2


## TeachingDemos


## testit


## testthat


## TH.data


## threejs


## tibble


## tidybayes

Bayesian analysis + tidy data + geoms

http://mjskay.github.io/tidybayes/


## tidygraph


## tidyr


## tidyselect


## tidyverse


## timeDate


## timeSeries


## tinytex


## tkrplot


## tm


## TMB


## tools


## tree


## triebeard


## trimcluster


## tseries


## TTR


## tufterhandout


## tutorial


## tweenr


## ucminf


## units


## updateR


## urca


## urltools


## uroot


## userfriendlyscience


## usethis


## utf8


## utils


## uuid


## V8


## vcd


## vcdExtra


## vegan


## VGAM


## ViewPipeSteps

https://github.com/daranzolin/ViewPipeSteps



## viridis


## viridisLite


## visdat


## visNetwork


## visreg


## wbstats


## webshot


## whisker


## WhiteStripe


## withr


## wordcloud


## workflowr


## writexl


## WRS2


## xfun


## XLConnect


## XLConnectJars


## xlsx


## xlsxjars


## XML


## xml2


## xplain


## xray


## xtable


## xts


## yaImpute


## yaml


## zip


## zoo



## Effsize

a package for efficient effect size computation

https://github.com/mtorchiano/effsize


## keyring

https://www.infoworld.com/video/91987/r-tip-keep-passwords-and-tokens-secure-with-the-keyring-package

## testthat

R tip: Test your code with testthat

https://www.infoworld.com/video/87735/r-tip-test-your-code-with-testthat


## cronR

Schedule R scripts on a Mac

https://www.infoworld.com/video/90629/r-tip-schedule-r-scripts-on-a-mac



<!--chapter:end:RPackagesUsed.Rmd-->

---
title: "Sankey Diagrams"
output: html_notebook
---

> https://www.r-bloggers.com/creating-custom-sankey-diagrams-using-r/

```{r eval=FALSE, include=FALSE, echo=TRUE}
library(networkD3)
nodes = data.frame("name" = 
 c("Node A", # Node 0
 "Node B", # Node 1
 "Node C", # Node 2
 "Node D"))# Node 3
links = as.data.frame(matrix(c(
 0, 1, 10, # Each row represents a link. The first number
 0, 2, 20, # represents the node being conntected from. 
 1, 3, 30, # the second number represents the node connected to.
 2, 3, 40),# The third number is the value of the node
 byrow = TRUE, ncol = 3))
names(links) = c("source", "target", "value")
sankeyNetwork(Links = links, Nodes = nodes,
 Source = "source", Target = "target",
 Value = "value", NodeID = "name",
 fontSize= 12, nodeWidth = 30)
```


> https://plot.ly/r/sankey-diagram/

> https://cran.r-project.org/web/packages/riverplot/riverplot.pdf



<!--chapter:end:SankeyDiagrams.Rmd-->

---
title: "Sensitivity Specificity"
output: html_notebook
---


# Classification And REgression Training

http://topepo.github.io/caret/index.html

## Calculate Sensitivity, Specificity And Predictive Values

https://www.rdocumentation.org/packages/caret/versions/3.51/topics/sensitivity

```
sensitivity(data, reference, positive = levels(reference)[1])
specificity(data, reference, negative = levels(reference)[2])
posPredValue(data, reference, positive = levels(reference)[1])
negPredValue(data, reference, negative = levels(reference)[2])
```








<!--chapter:end:SensitivitySpecificity.Rmd-->

---
title: "Shiny"
output: html_notebook
---

- Some notes on my first shiny app

https://ggvy.cl/post/some-notes-on-my-first-shiny-app/


<!--chapter:end:shiny-1.Rmd-->

---
title: "Shiny"
output: html_notebook
---


# Shiny Online Documentation 


https://shiny.rstudio.com



# Mastering Shiny

https://mastering-shiny.org/










<!--chapter:end:Shiny.Rmd-->

---
title: "ShinyCodes"
output: html_notebook
---


```{r eval=FALSE, include=FALSE, echo=TRUE}
# from CRAN
# install.packages("bs4Dash")
# latest devel version
# devtools::install_github("RinteRface/bs4Dash")
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
library(bs4Dash)
# classic theme
# bs4DashGallery()
# old_school theme
# bs4DashGallery(theme = "old_school")
```





```{r eval=FALSE, include=FALSE, echo=TRUE}
# install.packages("argonR")
# devtools::install_github("RinteRface/argonDash")
library(argonR)
```



```{r eval=FALSE, include=FALSE, echo=TRUE}

```





<!--chapter:end:ShinyCodes.Rmd-->

---
title: "snahelper"
output: html_notebook
---


```{r eval=FALSE, include=FALSE, echo=TRUE}
# devtools::install_github("schochastics/snahelper")
# devtools::install_github("schochastics/smglr")
```



http://blog.schochastics.net/post/an-rstudio-addin-for-network-analysis-and-visualization/


![](http://blog.schochastics.net/img/example.png)













<!--chapter:end:snahelper.Rmd-->

---
title: "Introduction to summarytools"
author: "Dominic Comtois"
date: "`{r #  Sys.Date()`"
output: 
  rmarkdown::html_vignette: 
    css: 
    - !expr system.file("rmarkdown/templates/html_vignette/resources/vignette.css", package = "rmarkdown")
    - !expr system.file("includes/stylesheets/summarytools.css", package = "summarytools")
vignette: >
  %\VignetteIndexEntry{Introduction to summarytools 0.8}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r eval=FALSE, include=FALSE, echo=TRUE}
library(knitr)
opts_chunk$set(comment=NA, prompt=FALSE, cache=FALSE, results='asis')
library(summarytools)
st_options('footnote', NA)
```

*summarytools* is an *R* package providing tools
to *neatly and quickly summarize data*.  It can also make _R_ a little easier to 
learn and use. Four functions are at the core of the package:

  - `freq()` : **frequency tables** with proportions, cumulative
    proportions and missing data information.
  - `ctable()` : **cross-tabulations** between two factors or any
    discrete data, with total, rows or columns proportions, as well
    as marginal totals.
  - `descr()` : **descriptive (univariate) statistics** for numerical data.
  - `dfSummary()` : Extensive **data frame summaries** that facilitate
    data cleaning and firsthand evaluation.

An emphasis has been put on both _what_ and _how_ results are presented, so that the
package can serve both as a data exploration _and_ reporting tool, which can be used
either on its own for minimal reports, or along with larger sets of tools such as
RStudio's for [rmarkdown](http://rmarkdown.rstudio.com/), and [knitr](https://yihui.name/knitr/).

**Building on the strengths of [pander](https://github.com/Rapporter/pander) and
[htmltools](https://CRAN.R-project.org/package=htmltools)**, the 
outputs produced by summarytools can be:

  - Displayed in plain text in the *R* console (default behaviour)
  - Used in *Rmardown* documents and *knitted* along with other text and
    *R* output
  - Written to *html* files that fire up in
    [*RStudio*](http://www.rstudio.com/)’s Viewer pane or in your
    system’s default browser
  - Written to plain text files / *Rmarkdown* text files

### Latest Improvements

Version 0.8.3 brings several improvements to *summarytools*, notably:

 - Introduction of global settings (customizable defaults)
 - Options to make content fit more naturally in _shiny_ apps or _Rmarkdown_ documents
 - A better handling of "split-group" statistics with `by()`
 - A more thorough documentation

## summarytool's Core Functions

## 1 - freq() : Frequency Tables

The `freq()` function generates a table of frequencies with counts and
proportions. Since this page use *markdown* rendering, we’ll set `style
= 'rmarkdown'` to take advantage of it.

```{r eval=FALSE, include=FALSE, echo=TRUE}
library(summarytools)
freq(iris$Species, style = "rmarkdown")
```

If we do not worry about missing data, we can set `{r # eport.nas = FALSE`:

```{r eval=FALSE, include=FALSE, echo=TRUE}
freq(iris$Species, report.nas = FALSE, style = "rmarkdown", headings = TRUE)
```

We could simplify further and omit the _Totals_ row by setting `totals = FALSE`.

To get familiar with the output styles, try different values for `style=`
and see how results look in the console. 

## 2 - ctable() : Cross-Tabulations

We’ll now use a sample data frame called *tobacco*, which is included in
the package. We want to cross-tabulate the two categorical variables
`smoker` and `diseased`. By default, `ctable()` gives row proportions,
but we’ll include the full syntax anyway.

Since *markdown* has not support (yet) for multi-line headings, we’ll
show an image of the resulting html table.

```{r eval=FALSE, include=FALSE, echo=TRUE}
with(tobacco, print(ctable(smoker, diseased), method = 'render'))
```

<!-- ![Example of ctable() output](ctable-with-row-props.png) -->
  
  
Notice that instead of `ctable(tobacco$smoker, tobacco$diseased, ...)`,
we used the `with()` function, making the syntax less redundant.

It is possible to display *column*, *total*, or no proportions at all.
We can also omit the marginal totals to have a simple “2 x 2” table.

```{r eval=FALSE, include=FALSE, echo=TRUE}
with(tobacco, 
     print(ctable(smoker, diseased, prop = 'n', totals = FALSE), 
           headings = TRUE, method = "render"))
```

## 3 - descr() : Descriptive Univariate Stats 

The `descr()` function generates common central tendency statistics and
measures of dispersion for numerical data. It can handle single vectors
as well as data frames, in which case it just ignores non-numerical
columns (and displays a message to that effect).

```{r eval=FALSE, include=FALSE, echo=TRUE}
descr(iris, style = "rmarkdown")
```

### Transposing and selecting only the stats you need

If your eyes/brain prefer seeing things the other way around, just use
`transpose = TRUE`. Here, we also select only the statistics we wish to
see, and specify `headings = TRUE` to avoid reprinting the same
information as above.

```{r eval=FALSE, include=FALSE, echo=TRUE}
descr(iris, stats = c("mean", "sd", "min", "med", "max"), transpose = TRUE, 
      headings = TRUE, style = "rmarkdown")
```

## 4 - dfSummary() : Data Frame Summaries

`dfSummary()` collects information about all variables
in a data frame and displays it in a singe, legible table.

To generate a summary report and have it displayed in
[*RStudio*](http://www.rstudio.com/)’s Viewer pane (or in your default
Web Browser if working with another interface), we simply do like this:

```{r, eval=FALSE}
view(dfSummary(iris))
```


It is also possible to use `dfSummary()` in *Rmarkdown* documents. In this next example, 
note that due to rmarkdown compatibility issues, histograms are not shown. We’re working
on this. Further down, we'll see how tu use _html_ rendering to go around this problem.

```{r eval=FALSE, include=FALSE, echo=TRUE}
dfSummary(tobacco, plain.ascii = FALSE, style = "grid")
```

## The print() and view() Functions

*summarytools* has a generic `print` method, `print.summarytools()`. By
default, its `method` argument is set to `'pander'`. One of the ways in
which `view()` is useful is that we can use it to easily display *html*
outputs in *RStudio*’s Viewer. In this case, the `view()` function
simply acts as a wrapper around the generic `print()` function,
specifying the `method = 'viewer'` for us. When used outside *RStudio*,
the `method` falls back on `'browser'` and the report is fired up in the
system’s default browser.

## Using by() to Show Results By Groups

With `freq()` and `descr()`, you can use *R*’s base function `by()` to
show statistics split by a ventilation / categorical variable. *R*’s
`by()` function returns a `list` containing as many *summarytools* objects
as there are categories in our ventilation variable. 

To propertly display the content present in that list, **we use
the `view()` function**. Using `print()`, while technically possible,
will not give as much satisfactory results.

#### Example 

Using the _iris_ data frame, we will display descriptive statistics
broken down by Species.

```{r eval=FALSE, include=FALSE, echo=TRUE}
# First save the results
iris_stats_by_species <- by(data = iris, 
                            INDICES = iris$Species, 
                            FUN = descr, stats = c("mean", "sd", "min", "med", "max"), 
                            transpose = TRUE)

# Then use view(), like so:
view(iris_stats_by_species, method = "pander", style = "rmarkdown")
```

To see an *html* version of these results, we'd simply do this (results not shown):

```{r, eval=FALSE}
view(iris_stats_by_species)
```

### Special Case - Using descr() With by() For A Single Variable

Instead of showing several tables having only one column each, the
`view()` function will assemble the results into a single table:

```{r eval=FALSE, include=FALSE, echo=TRUE}
data(tobacco) # tobacco is an example dataframe included in the package
BMI_by_age <- with(tobacco, 
                   by(BMI, age.gr, descr, 
                      stats = c("mean", "sd", "min", "med", "max")))
view(BMI_by_age, "pander", style = "rmarkdown")
```

The transposed version looks like this:

```{r eval=FALSE, include=FALSE, echo=TRUE}
BMI_by_age <- with(tobacco, 
                   by(BMI, age.gr, descr,  transpose = TRUE,
                      stats = c("mean", "sd", "min", "med", "max")))
view(BMI_by_age, "pander", style = "rmarkdown", headings = TRUE)
```

## Using lapply() to Show Several freq() tables at once

As is the case for `by()`, the `view()` function is essential for making
results nice and tidy.


```{r, eval=FALSE}
tobacco_subset <- tobacco[ ,c("gender", "age.gr", "smoker")]
freq_tables <- lapply(tobacco_subset, freq)
view(freq_tables, footnote = NA, file = 'freq-tables.html')
```

## Using summarytools in Rmarkdown documents

As we have seen, *summarytools* can generate both text (including
rmarkdown) and html results. Both can be used in Rmarkdown, according to
your preferences. There is a vignette dedicated to this, which gives
several examples, but if you’re in a hurry, here are a few tips to
get started:

 - Always set the `knitr` chunk option `{r # esults = 'asis'`. You can do
   this on a chunk-by-chunk basis, but here is how to do it globally:

```{r, eval=FALSE, tidy=FALSE}
    knitr::opts_chunk$set(echo = TRUE, results = 'asis')
```

\ \ \ \ \ \ \ \ \ \ Refer to [this page](https://yihui.name/knitr/options/) for
more on _knitr_'s options.

  - To get better results when using html (with `method = 'render'`),
    set up your .Rmd document so it includes *summarytool*’s css.

#### Example

````
# ---
# title: "RMarkdown using summarytools"
# output: 
#   html_document: 
#     css: C:/R/win-library/3.4/summarytools/includes/stylesheets/summarytools.css
# ---
````

For more details on using *summarytools* in *Rmarkdown* documents, please refer to
the corresponding vignette.

## Writing Output to Files

The console will always tell you the location of the temporary *html*
file that is created in the process. However, you can specify the name
and location of that file explicitly if you need to reuse it later on:

```{r, eval=FALSE}
view(iris_stats_by_species, file = "~/iris_stats_by_species.html")
```
Based on the file extension you provide (_.html_ vs others), _summarytools_ will
use the appropriate method; there is no need to specify the `method` argument.

### Appending output files

There is also an `append =` logical argument for adding content to
existing reports, both text/Rmarkdown and html. This is useful if you want to
quickly include several statistical tables in a single file. It is fast 
alternative to creating an _.Rmd_ document if you don't need the extra
content that the latter allows.

## Global options

Version 0.8.3 introduced the following set of global options:

  - `{r # ound.digits` = `2`
  - `plain.ascii` = `TRUE`
  - `headings` = `FALSE` (if using in a markdown document or a
    shiny app, setting this to `TRUE` might be preferable
  - `footnote` = `'default'` (set to empty string or `NA` to omit
    footnote)
  - `display.labels` = `TRUE`
  - `freq.totals` = `TRUE`
  - `freq.display.nas` = `TRUE`
  - `ctable.totals` = `TRUE`
  - `ctable.prop` = `'r'` (display *r*ow proportions by default)
  - `descr.stats` = `'all'`
  - `descr.transpose` = `FALSE`
  - `bootstrap.css` = `TRUE` (if using in a markdown document or a shiny
    app, setting this to `FALSE` might be preferable
  - `custom.css` = `NA`
  - `escape.pipe` = `FALSE`

#### Examples

```{r, eval=FALSE}
st_options()                      # display all global options' values
st_options('round.digits')        # display only one option
st_options('headings', TRUE) # change an option's value
st_options('footnote', NA)        # Turn off the footnote on all outputs.
                                  # This option was used prior to generating
                                  # the present document.
```

## Overriding formatting attributes

When a *summarytools* object is stored, its formatting attributes are
stored with it. However, you can override most of them when using the
`print()` and `view()`
functions.


#### Example 

```{r eval=FALSE, include=FALSE, echo=TRUE}
age_stats <- freq(tobacco$age.gr)  # age_stats contains a regular output for freq 
                                   # including headings, NA counts, and Totals
print(age_stats, style = "rmarkdown", report.nas = FALSE, 
                 totals = FALSE, headings = TRUE)
```

Note that the omitted attributes are stil part of the _age\_stats_ object.

## Order of Priority for Options / Parameters

1.  Options over-ridden explicitly with `print()` or `view()` have
    precendence
2.  options specified as explicit arguments to `freq() / ctable() /
    descr() / dfSummary()` come second
3.  Global options, which can be set with `st_options`, come third

## Customizing looks with CSS

Version 0.8 of *summarytools* uses *RStudio*’s [htmltools
package](https://CRAN.R-project.org/package=htmltools)
and version 4 of [Bootstrap](https://getbootstrap.com/)’s cascading
stylesheets.

It is possible to include your own *css* if you wish to customize the
look of the output tables. See the documentation for the package’s
`print.summarytools()` function for details, but here is a quick example
to give you the gist of it.

#### Example

Say you need to make the font size really small, smaller than by using
the `st-small` class as seen in a previous example. For this, you would
create a CSS file - let’s call it “custom.css” - containing a class such
as the following:

```css
.table-condensed {
  font-size: 8px;
}
```

Then, to apply it to a *summarytools* object and display it in your browser:

```{r, eval=FALSE}
view(dfSummary(tobacco), custom.css = 'path/to/custom.css', 
     table.classes = 'table-condensed')
```

## Working with _shiny_ apps

To include _summarytools_ functions into _shiny_ apps, it is recommended that you:

 - set `bootstrap.css = FASE` to avoid interacting with the app's layout  
 - adjust the size of the graphs in `dfSummary()` 
 - set `headings` to `TRUE`
 
#### Example

```{r # 
print(dfSummary(somedata, graph.magnif = 0.8), 
      method = 'render',
      headings = TRUE,
      bootstrap.css = FALSE)
```

## Getting Most Properties of an Object With `what.is()`

When developing, we often use a number of functions to obtain an object's properties.
`what.is()` proposes to lump together the results of most of these functions (`class()`,
`typeof()`, `attributes()` and others).

```{r what_is, warning=FALSE, results='markup'}
what.is(iris)
```

## Limitations

  - The text histograms in dfSummary are not yet supported in `{r # markdown`. Better use the
  'render' method. 
  - It is not possible to control the heading levels of individual items. You can however 
  choose to `headings` altogether.

## Stay Up-to-date

Check out the [project's page](https://github.com/dcomtois/summarytools) - 
from there you can see the latest updates and also submit feature requests.

To install the version of summarytools that is on CRAN, but that might have
benefited from quick fixes:

```{r # 
install.packages('devtools')
library(devtools)
install_github('dcomtois/summarytools')
```

To install the package in its development version, use

```{r # 
install_github('dcomtois/summarytools', ref='dev-current')
```

# Final notes

The package comes with no guarantees. It is a work in progress and
feedback / feature requests are welcome. Just send me an email
(dominic.comtois (at) gmail.com), or open an
[Issue on GitHub](https://github.com/dcomtois/summarytools/issues) if you find a
bug.

<!--chapter:end:summarytools_introduction.Rmd-->

---
title: "Recommendations for Using summarytools With Rmarkdown"
author: "Dominic Comtois"
date: "`{r #  Sys.Date()`"
output: 
  rmarkdown::html_vignette: 
    css: 
    - !expr system.file("rmarkdown/templates/html_vignette/resources/vignette.css", package = "rmarkdown")
    - !expr system.file("includes/stylesheets/summarytools.css", package = "summarytools")
vignette: >
  %\VignetteIndexEntry{Recommendations for Rmarkdown}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}

---

```{r, include=FALSE}
library(knitr)
opts_chunk$set(comment=NA, prompt=FALSE, cache=FALSE, echo=TRUE, results='asis')
library(summarytools)
```

This document uses theme `{r # markdown::html_vignette`.

Below are examples using recommended styles for *Rmarkdown* rendering. Available styles in
summarytools are the same as `pander`'s: 

- _simple_ (default)  
- _rmarkdown_  
- _grid_  
- _multiline_  
 
For `freq()`, `descr()` (and `ctable()`, although with caveats), _rmarkdown_ style is recommended. For
`dfSummary()`, _grid_ is recommended.
 
## Important Note

`knitr` option `{r # esults = 'asis'` must be specified to get good results. This can be done globally via
`opts_chunk$set(results='asis')`, or in the individual chunks.

The following summarytools global options have been set:
```{r eval=FALSE, include=FALSE, echo=TRUE}
#st_options('headings', TRUE)
st_options('bootstrap.css', FALSE)
st_options('footnote', NA)
``` 

## Using method = 'render'

To generate tables using summarytool's own html rendering, the _.Rmd_ document's configuration part
(yaml) must point to the package's `summarytools.css` file. This can be achieved in several ways;
the current vignette uses this configuration:

````
output: 
  rmarkdown::html_vignette: 
    css: 
    - !expr system.file("rmarkdown/templates/html_vignette/resources/vignette.css", package = "rmarkdown")
    - !expr system.file("includes/stylesheets/summarytools.css", package = "summarytools")
````

An alternative is to point to the directory on your system containing _summarytools.css_:

````
---
title: "RMarkdown using summarytools"
output: 
  html_document: 
    css: C:/R/win-library/3.4/summarytools/includes/stylesheets/summarytools.css
---
````

Starting with `freq()`, we'll review the recommended methods and styles to get going with _summarytools_
in _Rmarkdown_ documents.

Jump to...

- [ctable()](#ctable)  
- [descr()](#descr)  
- [dfSummary()](#dfsummary)  


--------------------------------------------------------------------------------

# freq()

`freq()` is best used with `style = 'rmarkdown'; html rendering is also possible.

## Rmarkdown Style
```{r eval=FALSE, include=FALSE, echo=TRUE}
freq(tobacco$gender, style = 'rmarkdown')
```


## HTML Rendering
```{r eval=FALSE, include=FALSE, echo=TRUE}
print(freq(tobacco$gender), method = 'render')
```

If you find the table too large, you can use `table.classes = 'st-small'` - an example is provided 
further below.

--------------------------------------------------------------------------------

<a href="#top">Back to top</a>

# ctable() {#ctable}

## Rmarkdown Style

Tables with heading spanning over 2 rows are not fully supported in markdown (yet), but the result is 
getting close to acceptable. 

```{r eval=FALSE, include=FALSE, echo=TRUE}
ctable(tobacco$gender, tobacco$smoker, style = 'rmarkdown')
```

## HTML Rendering

For best results, use this method.

```{r include=FALSE, eval=FALSE, echo = TRUE}
print(ctable(tobacco$gender, tobacco$smoker), method = 'render')
```

--------------------------------------------------------------------------------------------------
 
<a href="#top">Back to top</a>
 
# descr() 3 {#descr3}

`descr()` is also best used with `style = 'rmarkdown'`, and HTML rendering is also supported.

## Rmarkdown Style
```{r eval=FALSE, include=FALSE, echo=TRUE}
descr(tobacco, style = 'rmarkdown')
```


## HTML Rendering

We'll use table.classes = 'st-small' to show how it affects the table's size (compare to the `freq()`
table rendered earlier).

```{r eval=FALSE, include=FALSE, echo=TRUE}
print(descr(tobacco), method = 'render', table.classes = 'st-small')
```
 
--------------------------------------------------------------------------------

<a href="#top">Back to top</a>

# dfSummary() {#dfsummary}

## Grid Style

This gives good results, although the histograms are not shown. This has to do with an unresolved issue,
but we're working hard to figure out a solution. Don't forget to specify `plain.ascii = FALSE`, or
you won't get good results.

```{r , results='asis'}
dfSummary(tobacco, style = 'grid', plain.ascii = FALSE)
```

## HTML Rendering

Although the results are not as neat as they are when simply generating an html report from 
the R interpreter -- the transparency of the graphs is lost in translation --, this is the
best method still.

```{r eval=FALSE, include=FALSE, echo=TRUE}
print(dfSummary(tobacco, graph.magnif = 0.75), method = 'render')
```

<a href="#top">Back to top</a>

<!--chapter:end:summarytools_markdown.Rmd-->

---
title: "R-project giriş"
subtitle: "⚔<br/>with xaringan"
author: "Yihui Xie"
institute: "RStudio, Inc."
date: "2016/12/12 (updated: `{r #  Sys.Date()`)"
output:
  xaringan::moon_reader:
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---

background-image: url(https://upload.wikimedia.org/wikipedia/commons/b/be/Sharingan_triple.svg)

```{r eval=FALSE, include=FALSE, echo=TRUE}
options(htmltools.dir.version = FALSE)
```

???

Image credit: [Wikimedia Commons](https://commons.wikimedia.org/wiki/File:Sharingan_triple.svg)

---
class: center, middle

# xaringan

### /ʃaː.'riŋ.ɡan/

---
class: inverse, center, middle

# Get Started

---

# Hello World

Install the **xaringan** package from [Github](https://github.com/yihui/xaringan):

```{r eval=FALSE, tidy=FALSE}
devtools::install_github("yihui/xaringan")
```

--

You are recommended to use the [RStudio IDE](https://www.rstudio.com/products/rstudio/), but you do not have to.

- Create a new R Markdown document from the menu `File -> New File -> R Markdown -> From Template -> Ninja Presentation`;<sup>1</sup>

--

- Click the `Knit` button to compile it;

--

- or use the [RStudio Addin](https://rstudio.github.io/rstudioaddins/)<sup>2</sup> "Infinite Moon Reader" to live preview the slides (every time you update and save the Rmd document, the slides will be automatically reloaded in RStudio Viewer.

.footnote[
[1] 中文用户请看[这份教程](http://slides.yihui.name/xaringan/zh-CN.html)

[2] See [#2](https://github.com/yihui/xaringan/issues/2) if you do not see the template or addin in RStudio.
]

---
background-image: url(`{r #  xaringan:::karl`)
background-position: 50% 50%
class: center, bottom, inverse

# You only live once!

---

# Hello Ninja

As a presentation ninja, you certainly should not be satisfied by the "Hello World" example. You need to understand more about two things:

1. The [remark.js](https://remarkjs.com) library;

1. The **xaringan** package;

Basically **xaringan** injected the chakra of R Markdown (minus Pandoc) into **remark.js**. The slides are rendered by remark.js in the web browser, and the Markdown source needed by remark.js is generated from R Markdown (**knitr**).

---

# remark.js

You can see an introduction of remark.js from [its homepage](https://remarkjs.com). You should read the [remark.js Wiki](https://github.com/gnab/remark/wiki) at least once to know how to

- create a new slide (Markdown syntax<sup>*</sup> and slide properties);

- format a slide (e.g. text alignment);

- configure the slideshow;

- and use the presentation (keyboard shortcuts).

It is important to be familiar with remark.js before you can understand the options in **xaringan**.

.footnote[[*] It is different with Pandoc's Markdown! It is limited but should be enough for presentation purposes. Come on... You do not need a slide for the Table of Contents! Well, the Markdown support in remark.js [may be improved](https://github.com/gnab/remark/issues/142) in the future.]

---
background-image: url(`{r #  xaringan:::karl`)
background-size: cover
class: center, bottom, inverse

# I was so happy to have discovered remark.js!

---
class: inverse, middle, center

# Using xaringan

---

# xaringan

Provides an R Markdown output format `xaringan::moon_reader` as a wrapper for remark.js, and you can use it in the YAML metadata, e.g.

```yaml
---
title: "A Cool Presentation"
output:
  xaringan::moon_reader:
    yolo: true
    nature:
      autoplay: 30000
---
```

See the help page `?xaringan::moon_reader` for all possible options that you can use.

---

# remark.js vs xaringan

Some differences between using remark.js (left) and using **xaringan** (right):

.pull-left[
1. Start with a boilerplate HTML file;

1. Plain Markdown;

1. Write JavaScript to autoplay slides;

1. Manually configure MathJax;

1. Highlight code with `*`;

1. Edit Markdown source and refresh browser to see updated slides;
]

.pull-right[
1. Start with an R Markdown document;

1. R Markdown (can embed R/other code chunks);

1. Provide an option `autoplay`;

1. MathJax just works;<sup>*</sup>

1. Highlight code with `{{}}`;

1. The RStudio addin "Infinite Moon Reader" automatically refreshes slides on changes;
]

.footnote[[*] Not really. See next page.]

---

# Math Expressions

You can write LaTeX math expressions inside a pair of dollar signs, e.g. &#36;\alpha+\beta$ renders $\alpha+\beta$. You can use the display style with double dollar signs:

```
$$\bar{X}=\frac{1}{n}\sum_{i=1}^nX_i$$
```

$$\bar{X}=\frac{1}{n}\sum_{i=1}^nX_i$$

Limitations:

1. The source code of a LaTeX math expression must be in one line, unless it is inside a pair of double dollar signs, in which case the starting `$$` must appear in the very beginning of a line, followed immediately by a non-space character, and the ending `$$` must be at the end of a line, led by a non-space character;

1. There should not be spaces after the opening `$` or before the closing `$`.

1. Math does not work on the title slide (see [#61](https://github.com/yihui/xaringan/issues/61) for a workaround).

---

# R Code

```{r comment='#'}
# a boring regression
fit = lm(dist ~ 1 + speed, data = cars)
coef(summary(fit))
dojutsu = c('地爆天星', '天照', '加具土命', '神威', '須佐能乎', '無限月読')
grep('天', dojutsu, value = TRUE)
```

---

# R Plots

```{r , fig.height=4, dev='svg'}
par(mar = c(4, 4, 1, .1))
plot(cars, pch = 19, col = 'darkgray', las = 1)
abline(fit, lwd = 2)
```

---

# Tables

If you want to generate a table, make sure it is in the HTML format (instead of Markdown or other formats), e.g.,

```{r eval=FALSE, include=FALSE, echo=TRUE}
knitr::kable(head(iris), format = 'html')
```

---

# HTML Widgets

I have not thoroughly tested HTML widgets against **xaringan**. Some may work well, and some may not. It is a little tricky.

Similarly, the Shiny mode (`{r # untime: shiny`) does not work. I might get these issues fixed in the future, but these are not of high priority to me. I never turn my presentation into a Shiny app. When I need to demonstrate more complicated examples, I just launch them separately. It is convenient to share slides with other people when they are plain HTML/JS applications.

See the next page for two HTML widgets.

---

```{r out.width='100%', fig.height=6, eval=require('leaflet')}
library(leaflet)
leaflet() %>% addTiles() %>% setView(-93.65, 42.0285, zoom = 17)
```

---

```{r eval=require('DT'), tidy=FALSE}
DT::datatable(
  head(iris, 10),
  fillContainer = FALSE, options = list(pageLength = 8)
)
```

---

# Some Tips

- When you use the "Infinite Moon Reader" addin in RStudio, your R session will be blocked by default. You can click the red button on the right of the console to stop serving the slides, or use the _daemonized_ mode so that it does not block your R session. To do the latter, you can set the option

    ```{r # 
    options(servr.daemon = TRUE)
    ```
    
    in your current R session, or in `~/.Rprofile` so that it is applied to all future R sessions. I do the latter by myself.
    
    To know more about the web server, see the [**servr**](https://github.com/yihui/servr) package.

--

- Do not forget to try the `yolo` option of `xaringan::moon_reader`.

    ```yaml
    output:
      xaringan::moon_reader:
        yolo: true
    ```

---

# Some Tips

- Slides can be automatically played if you set the `autoplay` option under `nature`, e.g. go to the next slide every 30 seconds in a lightning talk:

    ```yaml
    output:
      xaringan::moon_reader:
        nature:
          autoplay: 30000
    ```

--

- A countdown timer can be added to every page of the slides using the `countdown` option under `nature`, e.g. if you want to spend one minute on every page when you give the talk, you can set:

    ```yaml
    output:
      xaringan::moon_reader:
        nature:
          countdown: 60000
    ```

    Then you will see a timer counting down from `01:00`, to `00:59`, `00:58`, ... When the time is out, the timer will continue but the time turns red.
    
---

# Some Tips

- The title slide is created automatically by **xaringan**, but it is just another remark.js slide added before your other slides.

    The title slide is set to `class: center, middle, inverse, title-slide` by default. You can change the classes applied to the title slide with the `titleSlideClass` option of `nature` (`title-slide` is always applied).

    ```yaml
    output:
      xaringan::moon_reader:
        nature:
          titleSlideClass: [top, left, inverse]
    ```
    
--

- If you'd like to create your own title slide, disable **xaringan**'s title slide with the `seal = FALSE` option of `moon_reader`.

    ```yaml
    output:
      xaringan::moon_reader:
        seal: false
    ```

---

# Some Tips

- There are several ways to build incremental slides. See [this presentation](https://slides.yihui.name/xaringan/incremental.html) for examples.

- The option `highlightLines: true` of `nature` will highlight code lines that start with `*`, or are wrapped in `{{ }}`, or have trailing comments `#<<`;

    ```yaml
    output:
      xaringan::moon_reader:
        nature:
          highlightLines: true
    ```

    See examples on the next page.

---

# Some Tips


.pull-left[
An example using a leading `*`:

    ```{r # 
    if (TRUE) {
    ** message("Very important!")
    }
    ```
Output:
```{r # 
if (TRUE) {
* message("Very important!")
}
```

This is invalid R code, so it is a plain fenced code block that is not executed.
]

.pull-right[
An example using `{{}}`:

````
`{r #  ''````{r tidy=FALSE}
if (TRUE) {
*{{ message("Very important!") }}
}
```
````
Output:
```{r tidy=FALSE}
if (TRUE) {
{{ message("Very important!") }}
}
```

It is valid R code so you can run it. Note that `{{}}` can wrap an R expression of multiple lines.
]

---

# Some Tips

An example of using the trailing comment `#<<` to highlight lines:

````markdown
`{r #  ''````{r tidy=FALSE}
library(ggplot2)
ggplot(mtcars) + 
  aes(mpg, disp) + 
  geom_point() +   #<<
  geom_smooth()    #<<
```
````

Output:

```{r tidy=FALSE, eval=FALSE}
library(ggplot2)
ggplot(mtcars) + 
  aes(mpg, disp) + 
  geom_point() +   #<<
  geom_smooth()    #<<
```

---

# Some Tips

When you enable line-highlighting, you can also use the chunk option `highlight.output` to highlight specific lines of the text output from a code chunk. For example, `highlight.output = TRUE` means highlighting all lines, and `highlight.output = c(1, 3)` means highlighting the first and third line.

````md
`{r #  ''````{r, highlight.output=c(1, 3)}
head(iris)
```
````

```{r, highlight.output=c(1, 3), echo=TRUE}
head(iris)
```

Question: what does `highlight.output = c(TRUE, FALSE)` mean? (Hint: think about R's recycling of vectors)

---

# Some Tips

- To make slides work offline, you need to download a copy of remark.js in advance, because **xaringan** uses the online version by default (see the help page `?xaringan::moon_reader`).

- You can use `xaringan::summon_remark()` to download the latest or a specified version of remark.js. By default, it is downloaded to `libs/remark-latest.min.js`.

- Then change the `chakra` option in YAML to point to this file, e.g.

    ```yaml
    output:
      xaringan::moon_reader:
        chakra: libs/remark-latest.min.js
    ```

- If you used Google fonts in slides (the default theme uses _Yanone Kaffeesatz_, _Droid Serif_, and _Source Code Pro_), they won't work offline unless you download or install them locally. The Heroku app [google-webfonts-helper](https://google-webfonts-helper.herokuapp.com/fonts) can help you download fonts and generate the necessary CSS.

---

# Macros

- remark.js [allows users to define custom macros](https://github.com/yihui/xaringan/issues/80) (JS functions) that can be applied to Markdown text using the syntax `![:macroName arg1, arg2, ...]` or `![:macroName arg1, arg2, ...](this)`. For example, before remark.js initializes the slides, you can define a macro named `scale`:

    ```js
    remark.macros.scale = function (percentage) {
      var url = this;
      return '<img src="' + url + '" style="width: ' + percentage + '" />';
    };
    ```

    Then the Markdown text

    ```markdown
    ![:scale 50%](image.jpg)
    ```

    will be translated to
    
    ```html
    <img src="image.jpg" style="width: 50%" />
    ```

---

# Macros (continued)

- To insert macros in **xaringan** slides, you can use the option `beforeInit` under the option `nature`, e.g.,

    ```yaml
    output:
      xaringan::moon_reader:
        nature:
          beforeInit: "macros.js"
    ```

    You save your remark.js macros in the file `macros.js`.

- The `beforeInit` option can be used to insert arbitrary JS code before `{r # emark.create()`. Inserting macros is just one of its possible applications.

---

# CSS

Among all options in `xaringan::moon_reader`, the most challenging but perhaps also the most rewarding one is `css`, because it allows you to customize the appearance of your slides using any CSS rules or hacks you know.

You can see the default CSS file [here](https://github.com/yihui/xaringan/blob/master/inst/rmarkdown/templates/xaringan/resources/default.css). You can completely replace it with your own CSS files, or define new rules to override the default. See the help page `?xaringan::moon_reader` for more information.

---

# CSS

For example, suppose you want to change the font for code from the default "Source Code Pro" to "Ubuntu Mono". You can create a CSS file named, say, `ubuntu-mono.css`:

```css
@import url(https://fonts.googleapis.com/css?family=Ubuntu+Mono:400,700,400italic);

.remark-code, .remark-inline-code { font-family: 'Ubuntu Mono'; }
```

Then set the `css` option in the YAML metadata:

```yaml
output:
  xaringan::moon_reader:
    css: ["default", "ubuntu-mono.css"]
```

Here I assume `ubuntu-mono.css` is under the same directory as your Rmd.

See [yihui/xaringan#83](https://github.com/yihui/xaringan/issues/83) for an example of using the [Fira Code](https://github.com/tonsky/FiraCode) font, which supports ligatures in program code.

---

# Themes

Don't want to learn CSS? Okay, you can use some user-contributed themes. A theme typically consists of two CSS files `foo.css` and `foo-fonts.css`, where `foo` is the theme name. Below are some existing themes:

```{r eval=FALSE, include=FALSE, echo=TRUE}
names(xaringan:::list_css())
```

---

# Themes

To use a theme, you can specify the `css` option as an array of CSS filenames (without the `.css` extensions), e.g.,

```yaml
output:
  xaringan::moon_reader:
    css: [default, metropolis, metropolis-fonts]
```

If you want to contribute a theme to **xaringan**, please read [this blog post](https://yihui.name/en/2017/10/xaringan-themes).

---
class: inverse, middle, center
background-image: url(https://upload.wikimedia.org/wikipedia/commons/3/39/Naruto_Shiki_Fujin.svg)
background-size: contain

# Naruto

---
background-image: url(https://upload.wikimedia.org/wikipedia/commons/b/be/Sharingan_triple.svg)
background-size: 100px
background-position: 90% 8%

# Sharingan

The R package name **xaringan** was derived<sup>1</sup> from **Sharingan**, a dōjutsu in the Japanese anime _Naruto_ with two abilities:

- the "Eye of Insight"

- the "Eye of Hypnotism"

I think a presentation is basically a way to communicate insights to the audience, and a great presentation may even "hypnotize" the audience.<sup>2,3</sup>

.footnote[
[1] In Chinese, the pronounciation of _X_ is _Sh_ /ʃ/ (as in _shrimp_). Now you should have a better idea of how to pronounce my last name _Xie_.

[2] By comparison, bad presentations only put the audience to sleep.

[3] Personally I find that setting background images for slides is a killer feature of remark.js. It is an effective way to bring visual impact into your presentations.
]

---

# Naruto terminology

The **xaringan** package borrowed a few terms from Naruto, such as

- [Sharingan](http://naruto.wikia.com/wiki/Sharingan) (写輪眼; the package name)

- The [moon reader](http://naruto.wikia.com/wiki/Moon_Reader) (月読; an attractive R Markdown output format)

- [Chakra](http://naruto.wikia.com/wiki/Chakra) (查克拉; the path to the remark.js library, which is the power to drive the presentation)

- [Nature transformation](http://naruto.wikia.com/wiki/Nature_Transformation) (性質変化; transform the chakra by setting different options)

- The [infinite moon reader](http://naruto.wikia.com/wiki/Infinite_Tsukuyomi) (無限月読; start a local web server to continuously serve your slides)

- The [summoning technique](http://naruto.wikia.com/wiki/Summoning_Technique) (download remark.js from the web)

You can click the links to know more about them if you want. The jutsu "Moon Reader" may seem a little evil, but that does not mean your slides are evil.

---

class: center

# Hand seals (印)

Press `h` or `?` to see the possible ninjutsu you can use in remark.js.

![](https://upload.wikimedia.org/wikipedia/commons/7/7e/Mudra-Naruto-KageBunshin.svg)

---

class: center, middle

# Thanks!

Slides created via the R package [**xaringan**](https://github.com/yihui/xaringan).

The chakra comes from [remark.js](https://remarkjs.com), [**knitr**](http://yihui.name/knitr), and [R Markdown](https://rmarkdown.rstudio.com).

<!--chapter:end:sunum2.Rmd-->

---
title: "Survival Analysis in R"
output: 
    html_document:
        toc: TRUE
        toc_float: TRUE
---

http://www.emilyzabor.com/tutorials/survival_analysis_in_r_tutorial.html

This tutorial provides an introduction to survival analysis, and to conducting a survival analysis in R.

This tutorial was originally presented at the Memorial Sloan Kettering Cancer Center R-Presenters series on August 30, 2018.


```{r eval=FALSE, include=FALSE, echo=TRUE}
# load packages
library(knitr)
library(tidyverse)
library(broom)

# load my package to access msk_palette with MSK brand colors
devtools::install_github("zabore/ezfun")
```


```{r loaddata, echo = TRUE}
# load(here::here("data", "survival_data_example.RData"))
```


# Introduction {.center}


## What is survival data?

Time-to-event data that consists of a distinct start time and end time.

Examples from cancer:

- Time from surgery to death
- Time from start of treatment to progression
- Time from response to recurrence


## Examples from other fields

Time-to-event data is common in many fields including, but not limited to:

- Time from HIV infection to development of AIDS
- Time to heart attack
- Time to onset of substance abuse
- Time to initiation of sexual activity
- Time to machine malfunction


## Aliases for survival analysis

Because survival analysis is common in many other fields, it also goes by other names:

- Reliability analysis
- Duration analysis
- Event history analysis
- Time-to-event analysis


## Questions of interest

The two most common questions I encounter related to survival analysis are:

1. What is the probability of survival to a certain point in time?
2. What is the average survival time?


# Censoring {.center}


## What is censoring?

In survival analysis it is common for the exact event time to be unknown, or unobserved, which is called censoring. A subject may be censored due to:

- Loss to follow-up
- Withdrawal from study
- No event by end of fixed study period

Specifically these are examples of **right** censoring. Other common types of censoring include:

- Left
- Interval


## Censored survival data

When the exact event time is unknown then some patients are censored, and survival analysis methods are required.

```{r swimmer, eval=FALSE, include=FALSE}
# make fake data
set.seed(20180809)
fkdt <- data_frame(Subject = as.factor(1:10), 
                   Years = sample(4:20, 10, replace = T),
                   censor = sample(c("Censor", rep("Event", 2)), 10, 
                                   replace = T)) 
# %>% mutate(Subject = fct_reorder(Subject, Years, desc = TRUE))

# plot with shapes to indicate censoring or event
ggplot(fkdt, aes(Subject, Years)) + 
    geom_bar(stat = "identity", width = 0.5, 
             fill = ezfun::msk_palette("main")[3]) + 
    geom_point(data = fkdt, 
               aes(Subject, Years, color = censor, shape = censor), 
               size = 6) +
    scale_color_manual(values = ezfun::msk_palette("contrast")[2:3]) +
    coord_flip() +
    theme_minimal() + 
    theme(legend.title = element_blank(),
          legend.position = "bottom")
```

In this example, how would we compute the proportion who are event-free at 10 years?

- Subjects 2, 3, 5, 6, 8, 9, and 10 were all event-free at 10 years.
- Subjects 4 and 7 had the event before 10 years.
- Subject 1 was censored before 10 years, so we don't know whether they had the event or not by 10 years. How do we incorporate this subject into our estimate?


## We can incorporate censored data using survival analysis techniques

Toy example of a Kaplan-Meier curve for this simple data (details to follow):

```{r eval=FALSE, include=FALSE}
library(survival)
plot(survfit(Surv(Years, ifelse(censor == "Event", 1, 0)) ~ 1, data = fkdt), 
     xlab = "Years", 
     ylab = "Survival probability", 
     mark.time = T, 
     conf.int = FALSE)
```

- Horizontal lines represent survival duration for the interval
- An interval is terminated by an event
- The height of vertical lines show the change in cumulative probability
- Censored observations, indicated by tick marks, reduces the cumulative survival between intervals


## Danger of ignoring censoring

[Case study: musicians and mortality](https://callingbullshit.org/case_studies/case_study_musician_mortality.html)

Conclusion: Musical genre is associated with early death among musicians.

Problem: this graph does not account for the right-censored nature of the data.

```{r include=FALSE, eval=FALSE, echo = TRUE}
# include_graphics("./img/musician_death_graph.jpg")
```


## Components of survival data

For subject $i$:

1. Event time $T_i$
2. Censoring time $C_i$
3. Event indicator $\delta_i$: 

    - 1 if event observed (i.e. $T_i \leq C_i$)
    - 0 if censored (i.e. $T_i > C_i$)
    
4. Observed time $Y_i = \min(T_i, C_i)$


# Data example {.center}


## Research question of interest

Investigating the "obesity paradox" in kidney cancer patients.

- Increased BMI associated with risk of kidney cancer
- Is increased BMI also associated with worse prognosis among kidney cancer patients?


## Data structure

- `{r #  nrow(df)` kidney cancer patients 
- Outcome: overall survival
- Predictor: BMI

```{r peekdata, eval=FALSE, include=FALSE}
df[, c("os_yrs", "os", "bmi_cat")] %>% 
    head
```

Variables:

- "os_yrs": the observed time $Y_i = min(T_i, C_i)$
- "os": the event indicator $\delta_i$
- "bmi_cat": 1 = Normal BMI < 25, 2 = Overweight BMI 25-30, 3 = Obese BMI > 30 


# Preparing data for analysis {.center}


## Dates

Data will often come with start and end dates rather than pre-calculated survival times. Our data example includes the following variables:

1. "proc_date": Date of surgery
2. "last_status_date": Date of death or last follow-up
3. "last_status": Character variable denoting whether the patient is alive, and the cause of death if dead

The first step is to make sure these are formatted as dates in R. 


## Formatting dates - base R

First let's look at the current format of our surgery date:

```{r eval=FALSE, include=FALSE, echo=TRUE}
str(df$proc_date)
```

We see this is a character variable in a certain format, but we need it to be formatted as a Date.

```{r format_date1, eval=FALSE, include=FALSE}
df <- df %>% 
    mutate(proc_date_format1 = as.Date(proc_date, format = "%d%b%Y"))
```

And after formatting we see that surgery date has Date format now:

```{r eval=FALSE, include=FALSE, echo=TRUE}
str(df$proc_date_format1)
```


- Note that in base R the format must include the separator as well as the symbol. e.g. if your date is in format m/d/Y then you would need `format = "%m/%d/%Y"`

- See a full list of date format symbols at [https://www.statmethods.net/input/dates.html](https://www.statmethods.net/input/dates.html)


## Formatting dates - lubridate

We can also use the `lubridate` package to format dates. Again we look at our original surgery date variable:

```{r eval=FALSE, include=FALSE, echo=TRUE}
str(df$proc_date)
```

And now use the `lubridate::dmy` function to format this into a Date:

```{r format_date2, eval=FALSE, include=FALSE}
df <- df %>% 
    mutate(proc_date_format2 = lubridate::dmy(proc_date))
```

And again we see that `R` now recognizes surgery date as a Date format:

```{r eval=FALSE, include=FALSE, echo=TRUE}
str(df$proc_date_format2)
```

- The help page for `?ymd` will show all format options.

- Note that unlike the base R option, the separators do not need to be specified


## Event indicator

Most functions used in survival analysis will also require a binary indicator of event that is:

- 0 for no event
- 1 for event

Currently our data example contains a character variable indicating whehter the patient is alive, and if not indicating the cause of death, so we must create a binary indicator.

```{r eval=FALSE, include=FALSE, echo=TRUE}
table(df$last_status, useNA = 'ifany')

df <- df %>% 
    mutate(os_event = ifelse(last_status == "Alive", 0, 1))

table(df$os_event)
```


## Calculating survival times - base R

Now that we have our dates formatted, we need to calculate the difference between start and end time in some units, usually months or years.

A base `R` solution to calculate the number of years from surgery to death:

```{r include=FALSE, eval=FALSE, echo = TRUE}
# need to actually properly format the real dates for use
df <- df %>% 
    mutate(proc_date = as.Date(proc_date, format = "%d%b%Y"),
           last_status_date = as.Date(last_status_date, format = "%d%b%Y"))
```


```{r difftime_ex1, eval=FALSE, include=FALSE}
df <- df %>% 
    mutate(os_yrs_opt1 = as.numeric(difftime(last_status_date, proc_date, 
                                             units = "days")) / 365.25)
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
summary(df$os_yrs_opt1)
```


- Here we use `difftime` to calculate the number of days between our two dates and convert it to a numeric value using `as.numeric`. We then convert to years by dividing by 365.25, the average number of days in a year.

- *Sidenote*: the >0 nature of survival times is another reason why standard regression techniques such as linear regression would not be an appropriate way to analyze time-to-event data


## Calculating survival times - lubridate

We can also use the `lubridate` package to calculate the number of years from surgery to death:

```
{r difftime_ex2, message = FALSE, warning = FALSE}
library(lubridate)

df <- df %>% 
    mutate(os_yrs_opt2 = 
               as.duration(proc_date %--% last_status_date) /
               dyears(1))
```


```
{r eval=FALSE, include=FALSE, echo=TRUE}
summary(df$os_yrs_opt2)
```


- Here the operator `%--%` is used to designate a time interval, which is then converted to the number of elapsed seconds using `as.duration` and finally converted to years by dividing by `dyears(1)`, which gives the number of seconds in a year.
- If you look closely you will see the result differs slightly from the previous result due to rounding differences, but nothing that would impact our results
- *Note*: we need to load the `lubridate` package using a call to `library` in order to be able to access the special operators (similar to situation with pipes)


# Analyzing survival data {.center}


## Questions of interest

Recall the questions of interest:

1. What is the probability of surviving to a certain point in time?
2. What is the average survival time?


## Creating survival objects

The Kaplan-Meier method is the most common way to estimate survival times and probabilities. It is a non-parametric approach that results in a step function, where there is a step down each time an event occurs.

- The `Surv` function from the `survival` package creates a survival object for use as the response in a model formula. There will be one entry for each subject that is the survival time, which is followed by a `+` if the subject was censored. Let's look at the first 10 observations:

```{r eval=FALSE, include=FALSE, echo=TRUE}
survival::Surv(df$os_yrs, df$os)[1:10]
```


## Estimating survival curves with the Kaplan-Meier method

- The `survival::survfit` function creates survival curves based on a formula. Let's generate the overall survival curve for the entire cohort, assign it to object `f1`, and look at the `names` of that object:

```{r eval=FALSE, include=FALSE, echo=TRUE}
f1 <- survival::survfit(survival::Surv(os_yrs, os) ~ 1, data = df)
names(f1)
```

Some key components of this `survfit` object that will be used to create survival curves include:

- `time`, which contains the start and endpoints of each time interval
- `surv`, which contains the survival probability corresponding to each `time`


## Kaplan-Meier plot - base R

Now we plot the `survfit` object in base `R` to get the Kaplan-Meier plot:

```{r eval=FALSE, include=FALSE, echo=TRUE}
plot(survival::survfit(survival::Surv(os_yrs, os) ~ 1, data = df), 
     xlab = "Years", 
     ylab = "Overall survival probability")
```

- The default plot in base `R` shows the step function (solid line) with associated confidence intervals (dotted lines). Note that the tick marks for censored patients are not shown by default, but could be added using `mark.time = TRUE`


## Kaplan-Meier plot - ggsurvplot

Alternatively, the `ggsurvplot` function from the `survminer` package is built on `ggplot2`, and can be used to create Kaplan-Meier plots:

```
{r eval=FALSE, message=FALSE, warning=FALSE, include=FALSE}
survminer::ggsurvplot(
    fit = survival::survfit(survival::Surv(os_yrs, os) ~ 1, data = df), 
    xlab = "Years", 
    ylab = "Overall survival probability")
```

- The default plot using `survminer::ggsurvplot` shows the step function (solid line) with associated confidence bands (shaded area). The tick marks for censored patients are shown by default, somewhat obscuring the line itself in this example, and could be supressed using `censor = FALSE`

## Estimating $x$-year survival

One quantity often of interest in a survival analysis is the probability of surviving a certain number ($x$) of years.

For example, to estimate the probability of survivng to 5 years, use `summary` with the `times` argument:

```{r eval=FALSE, include=FALSE, echo=TRUE}
summary(survival::survfit(survival::Surv(os_yrs, os) ~ 1, data = df), 
        times = 5)
```

We find that the 5-year probability of survival in this study is `{r #  round(summary(f1, times = 5)$surv * 100)`%. The associated lower and upper bounds of the 95\% confidence interval are also displayed.


## $x$-year survival and the survival curve

The 5-year survival probability is the point on the y-axis that corresponds to 5 years on the x-axis for the survival curve.

```
{r eval=FALSE, message=FALSE, include=FALSE}
survplot1 <- survminer::ggsurvplot(survival::survfit(
    survival::Surv(os_yrs, os) ~ 1, data = df), 
           xlab = "Years",
           ylab = "Overall survival probability",
           break.x.by = 5, 
           palette = ezfun::msk_palette("main")) +
    geom_segment(x = 5, xend = 5, y = -1, yend = 0.808, col = 2, lwd = 2) +
    geom_segment(x = 5, xend = -0.7, y = 0.808, yend = 0.808, 
                 arrow = arrow(length = unit(0.3, "inches")), col = 2, 
                 lwd = 2)

survplot1$plot <- survplot1$plot +
    annotate("text", x = 0.2, y = 0.7, label = "81%", size = 6, col = 2)

survplot1
```


## $x$-year survival is often estimated incorrectly 

What happens if you use a "naive" estimate? 

`{r #  table(df$os[df$os_yrs <= 5])[2]` of the `{r #  nrow(df)` patients died by 5 years so:

$$\Big(1 - \frac{297}{2119}\Big) \times 100 = 86\%$$

- You get an **incorrect** estimate of the 5-year probability of survival when you ignore the fact that `{r #  table(df$os[df$os_yrs <= 5])[1]` patients were censored before 5 years.

- Recall the **correct** estimate of the 5-year probability of survival was `{r #  round(summary(f1, times = 5)$surv * 100)`%.


## Estimating median survival time

Another quantity often of interest in a survival analysis is the average survival time, which we quantify using the median (survival times are not expected to be normally distributed so the mean is not an appropriate summary). 

We can obtain this directly from our `survfit` object:

```
{r eval=FALSE, include=FALSE, echo=TRUE}
survival::survfit(survival::Surv(os_yrs, os) ~ 1, data = df)
```

We see the median survival time is `{r #  round(summary(f1)$table["median"], 1)` years. The lower and upper bounds of the 95\% confidence interval are also displayed.


## Median survival time and the survival curve

Median survival is the time corresponding to a survival probability of 0.5: 

```
{r eval=FALSE, message=FALSE, include=FALSE}
survplot2 <- survminer::ggsurvplot(survival::survfit(
    survival::Surv(os_yrs, os) ~ 1, data = df), 
           xlab = "Years",
           ylab = "Overall survival probability",
           break.x.by = 5, 
           palette = ezfun::msk_palette("main")) +
    geom_segment(x = -1, xend = 12.6794, y = 0.5, yend = 0.5, 
                 col = 2, lwd = 2) +
    geom_segment(x = 12.6794, xend = 12.6794, y = 0.5, yend = -0.02, 
                 arrow = arrow(length = unit(0.3, "inches")), col = 2, 
                 lwd = 2)

survplot2$plot <- survplot2$plot +
    annotate("text", x = 9.5, y = 0.02, label = "12.7 years", size = 6, col = 2)

survplot2
```


## Median survival is often estimated incorrectly

What happens if you use a "naive" estimate? 

Summarize the median survival time among the `{r #  table(df$os)[2]` patients who died:

```{r eval=FALSE, include=FALSE, echo=TRUE}
df$os_yrs[df$os == 1] %>% 
    median
```

- You get an **incorrect** estimate of median survival time of `{r #  round(median(df$os_yrs[df$os == 1]), 1)` years when you ignore the fact that censored patients also contribute follow-up time.

- Recall the **correct** estimate of median survival time is `{r #  round(summary(f1)$table["median"], 1)` years.


# Comparing survival times between groups {.center}


## Questions of interest with respect to between-group differences

Is there a difference in survival probability between groups?

From our example: does the probability of survival differ according to BMI among kidney cancer patients?


## Kaplan-Meier plot by group

We can add a covariate to the right-hand side of the `survival::survfit` object to obtain a stratified Kaplan-Meier plot. 

Let's also look at some other customization we can do with `survminer::ggsurvplot`.

```
{r fig.height = 6}
survminer::ggsurvplot(
    fit = survival::survfit(survival::Surv(os_yrs, os) ~ bmi_cat, data = df), 
    xlab = "Years",
    ylab = "Overall survival probability",
    legend.title = "BMI",
    legend.labs = c("Normal", "Overweight", "Obese"),
    break.x.by = 5, 
    palette = ezfun::msk_palette("contrast"), 
    censor = FALSE,
    risk.table = TRUE,
    risk.table.y.text = FALSE)
```

- By looking at the curves, we can see that normal BMI patients have the lowest overall survival probability, followed by overweight BMI patients and obese BMI patients.
- The risk table below the plot shows us the number of patients at risk at certain time points, which can give an idea of how much information is being used to calculate the estimates at each time


## $x$-year survival probability by group

As before, we can get an estimate of, for example, 5-year survival by using `summary` with the `times` argument in our `survival::survfit` object:

```{r eval=FALSE, include=FALSE, echo=TRUE}
summary(survival::survfit(survival::Surv(os_yrs, os) ~ bmi_cat, data = df), 
        times = 5)
```

To summarize:

```{r eval=FALSE, include=FALSE}
library(survival)
tab <- ezfun::uvsurv(NULL, "bmi_cat", "os", "os_yrs", 5, df)[-1, c(1, 3)]
colnames(tab) <- c("BMI", "5-year estimate (95% CI)")
tab[, 1] <- c("Normal", "Overweight", "Obese")
kable(tab, row.names = FALSE)
```


## Log-rank test for between-group significance test

- We can conduct between-group significance tests using a log-rank test. 
- The log-rank test equally weights observations over the entire follow-up time and is the most common way to compare survival times between groups.
- There are versions that more heavily weight the early or late follow-up that could be more appropriate depending on the research question.

We get the log-rank p-value using the `survival::survdiff` function:

```{r eval=FALSE, include=FALSE, echo=TRUE}
survival::survdiff(survival::Surv(df$os_yrs, df$os) ~ df$bmi_cat)
```

And we see that the p-value is <.001, indicating a significant difference in overall survival according to BMI.


# Regression {.center}


## The Cox regression model

We may want to quantify an effect size for a single variable, or include more than one variable into a regression model to account for the effects of multiple variables.

The Cox regression model is a semi-parametric model that can be used to fit univariable and multivariable regression models that have survival outcomes.

Some key assumptions of the model:

- non-informative censoring
- proportional hazards

*Note*: parametric regression models for survival outcomes are also available, but I won't cover them in detail here.


## Cox regression example using a single covariate

We can fit regression models for survival data using the `survival::coxph` function, which takes a `survival::Surv` object on the left hand side and has standard syntax for regression formulas in `R` on the right hand side.

```{r eval = FALSE}
survival::coxph(survival::Surv(os_yrs, os) ~ factor(bmi_cat), data = df)
```

We can see a tidy version of the output using the `tidy` function from the `broom` package:

```{r eval=FALSE, include=FALSE, echo=TRUE}
broom::tidy(survival::coxph(survival::Surv(os_yrs, os) ~ factor(bmi_cat), 
                            data = df))
```


## Hazard ratios

The quantity of interest from a Cox regression model is a **hazard ratio (HR)**.

If you have a regression parameter $\beta$ (from column `estimate` in our `survival::coxph`) then HR = $\exp(\beta)$.

For example, from our example we obtain the regression parameter $\beta_1=-0.4441733$ for obese vs normal BMI, so we have HR = $\exp(\beta_1)=0.64$. 

A HR < 1 indicates reduced hazard of death whereas a HR > 1 indicates an increased hazard of death.

So we would say that obese BMI kidney cancer patients have 0.64 times reduced hazard of death as compared to normal BMI kidney cancer patients.


## Summary

- Time-to-event data is common
- Survival analysis techniques are required to account for censored data
- The `survival` package provides tools for survival analysis, including the `Surv` and `survfit` functions
- The `survminer` package allows for customization of Kaplan-Meier plots based on `ggplot2`
- Between-group comparisons can be made with the log-rank test using `survival::survdiff`
- Multiavariable Cox regression analysis can be accomplished using `survival::coxph` 

<!--chapter:end:survival_analysis_in_r_tutorial.Rmd-->

---
title: "Survival Analysis in R"
output: 
    html_document:
        toc: TRUE
        toc_float: TRUE
---

This tutorial provides an introduction to survival analysis, and to conducting a survival analysis in R.

This tutorial was originally presented at the Memorial Sloan Kettering Cancer Center R-Presenters series on August 30, 2018.


```{r eval=FALSE, include=FALSE, echo=TRUE}
# load packages
library(knitr)
library(tidyverse)
library(broom)

# load my package to access msk_palette with MSK brand colors
devtools::install_github("zabore/ezfun")
```


```{r loaddata 2, echo = TRUE}
# load(here::here("data", "survival_data_example.RData"))
```


# Introduction {.center}


## What is survival data?

Time-to-event data that consists of a distinct start time and end time.

Examples from cancer:

- Time from surgery to death
- Time from start of treatment to progression
- Time from response to recurrence


## Examples from other fields

Time-to-event data is common in many fields including, but not limited to:

- Time from HIV infection to development of AIDS
- Time to heart attack
- Time to onset of substance abuse
- Time to initiation of sexual activity
- Time to machine malfunction


## Aliases for survival analysis

Because survival analysis is common in many other fields, it also goes by other names:

- Reliability analysis
- Duration analysis
- Event history analysis
- Time-to-event analysis


## Questions of interest

The two most common questions I encounter related to survival analysis are:

1. What is the probability of survival to a certain point in time?
2. What is the average survival time?


# Censoring {.center}


## What is censoring?

In survival analysis it is common for the exact event time to be unknown, or unobserved, which is called censoring. A subject may be censored due to:

- Loss to follow-up
- Withdrawal from study
- No event by end of fixed study period

Specifically these are examples of **right** censoring. Other common types of censoring include:

- Left
- Interval


## Censored survival data

When the exact event time is unknown then some patients are censored, and survival analysis methods are required.

```{r swimmer 2, eval=FALSE, include=FALSE}
# make fake data
set.seed(20180809)
fkdt <- data_frame(Subject = as.factor(1:10), 
                   Years = sample(4:20, 10, replace = T),
                   censor = sample(c("Censor", rep("Event", 2)), 10, 
                                   replace = T)) 
# %>% mutate(Subject = fct_reorder(Subject, Years, desc = TRUE))

# plot with shapes to indicate censoring or event
ggplot(fkdt, aes(Subject, Years)) + 
    geom_bar(stat = "identity", width = 0.5, 
             fill = ezfun::msk_palette("main")[3]) + 
    geom_point(data = fkdt, 
               aes(Subject, Years, color = censor, shape = censor), 
               size = 6) +
    scale_color_manual(values = ezfun::msk_palette("contrast")[2:3]) +
    coord_flip() +
    theme_minimal() + 
    theme(legend.title = element_blank(),
          legend.position = "bottom")
```

In this example, how would we compute the proportion who are event-free at 10 years?

- Subjects 2, 3, 5, 6, 8, 9, and 10 were all event-free at 10 years.
- Subjects 4 and 7 had the event before 10 years.
- Subject 1 was censored before 10 years, so we don't know whether they had the event or not by 10 years. How do we incorporate this subject into our estimate?


## We can incorporate censored data using survival analysis techniques

Toy example of a Kaplan-Meier curve for this simple data (details to follow):

```{r eval=FALSE, include=FALSE}
library(survival)
plot(survfit(Surv(Years, ifelse(censor == "Event", 1, 0)) ~ 1, data = fkdt), 
     xlab = "Years", 
     ylab = "Survival probability", 
     mark.time = T, 
     conf.int = FALSE)
```

- Horizontal lines represent survival duration for the interval
- An interval is terminated by an event
- The height of vertical lines show the change in cumulative probability
- Censored observations, indicated by tick marks, reduces the cumulative survival between intervals


## Danger of ignoring censoring

[Case study: musicians and mortality](https://callingbullshit.org/case_studies/case_study_musician_mortality.html)

Conclusion: Musical genre is associated with early death among musicians.

Problem: this graph does not account for the right-censored nature of the data.

```{r include=FALSE, eval=FALSE, echo = TRUE}
# include_graphics("./img/musician_death_graph.jpg")
```


## Components of survival data

For subject $i$:

1. Event time $T_i$
2. Censoring time $C_i$
3. Event indicator $\delta_i$: 

    - 1 if event observed (i.e. $T_i \leq C_i$)
    - 0 if censored (i.e. $T_i > C_i$)
    
4. Observed time $Y_i = \min(T_i, C_i)$


# Data example {.center}


## Research question of interest

Investigating the "obesity paradox" in kidney cancer patients.

- Increased BMI associated with risk of kidney cancer
- Is increased BMI also associated with worse prognosis among kidney cancer patients?


## Data structure

- `{r #  nrow(df)` kidney cancer patients 
- Outcome: overall survival
- Predictor: BMI

```{r peekdata 2, eval=FALSE, include=FALSE}
df[, c("os_yrs", "os", "bmi_cat")] %>% 
    head
```

Variables:

- "os_yrs": the observed time $Y_i = min(T_i, C_i)$
- "os": the event indicator $\delta_i$
- "bmi_cat": 1 = Normal BMI < 25, 2 = Overweight BMI 25-30, 3 = Obese BMI > 30 


# Preparing data for analysis {.center}


## Dates

Data will often come with start and end dates rather than pre-calculated survival times. Our data example includes the following variables:

1. "proc_date": Date of surgery
2. "last_status_date": Date of death or last follow-up
3. "last_status": Character variable denoting whether the patient is alive, and the cause of death if dead

The first step is to make sure these are formatted as dates in R. 


## Formatting dates - base R

First let's look at the current format of our surgery date:

```{r eval=FALSE, include=FALSE, echo=TRUE}
str(df$proc_date)
```

We see this is a character variable in a certain format, but we need it to be formatted as a Date.

```{r format_date1 2, eval=FALSE, include=FALSE}
df <- df %>% 
    mutate(proc_date_format1 = as.Date(proc_date, format = "%d%b%Y"))
```

And after formatting we see that surgery date has Date format now:

```{r eval=FALSE, include=FALSE, echo=TRUE}
str(df$proc_date_format1)
```


- Note that in base R the format must include the separator as well as the symbol. e.g. if your date is in format m/d/Y then you would need `format = "%m/%d/%Y"`

- See a full list of date format symbols at [https://www.statmethods.net/input/dates.html](https://www.statmethods.net/input/dates.html)


## Formatting dates - lubridate

We can also use the `lubridate` package to format dates. Again we look at our original surgery date variable:

```{r eval=FALSE, include=FALSE, echo=TRUE}
str(df$proc_date)
```

And now use the `lubridate::dmy` function to format this into a Date:

```
{r format_date2 2}
df <- df %>% 
    mutate(proc_date_format2 = lubridate::dmy(proc_date))
```

And again we see that `R` now recognizes surgery date as a Date format:

```{r eval=FALSE, include=FALSE, echo=TRUE}
str(df$proc_date_format2)
```

- The help page for `?ymd` will show all format options.

- Note that unlike the base R option, the separators do not need to be specified


## Event indicator

Most functions used in survival analysis will also require a binary indicator of event that is:

- 0 for no event
- 1 for event

Currently our data example contains a character variable indicating whehter the patient is alive, and if not indicating the cause of death, so we must create a binary indicator.

```{r eval=FALSE, include=FALSE, echo=TRUE}
table(df$last_status, useNA = 'ifany')

df <- df %>% 
    mutate(os_event = ifelse(last_status == "Alive", 0, 1))

table(df$os_event)
```


## Calculating survival times - base R

Now that we have our dates formatted, we need to calculate the difference between start and end time in some units, usually months or years.

A base `R` solution to calculate the number of years from surgery to death:

```{r include=FALSE, eval=FALSE, echo = TRUE}
# need to actually properly format the real dates for use
df <- df %>% 
    mutate(proc_date = as.Date(proc_date, format = "%d%b%Y"),
           last_status_date = as.Date(last_status_date, format = "%d%b%Y"))
```


```
{r difftime_ex1 2}
df <- df %>% 
    mutate(os_yrs_opt1 = as.numeric(difftime(last_status_date, proc_date, 
                                             units = "days")) / 365.25)
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
summary(df$os_yrs_opt1)
```


- Here we use `difftime` to calculate the number of days between our two dates and convert it to a numeric value using `as.numeric`. We then convert to years by dividing by 365.25, the average number of days in a year.

- *Sidenote*: the >0 nature of survival times is another reason why standard regression techniques such as linear regression would not be an appropriate way to analyze time-to-event data


## Calculating survival times - lubridate

We can also use the `lubridate` package to calculate the number of years from surgery to death:

```
{r difftime_ex2 2, message = FALSE, warning = FALSE}
library(lubridate)

df <- df %>% 
    mutate(os_yrs_opt2 = 
               as.duration(proc_date %--% last_status_date) /
               dyears(1))
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
summary(df$os_yrs_opt2)
```


- Here the operator `%--%` is used to designate a time interval, which is then converted to the number of elapsed seconds using `as.duration` and finally converted to years by dividing by `dyears(1)`, which gives the number of seconds in a year.
- If you look closely you will see the result differs slightly from the previous result due to rounding differences, but nothing that would impact our results
- *Note*: we need to load the `lubridate` package using a call to `library` in order to be able to access the special operators (similar to situation with pipes)


# Analyzing survival data {.center}


## Questions of interest

Recall the questions of interest:

1. What is the probability of surviving to a certain point in time?
2. What is the average survival time?


## Creating survival objects

The Kaplan-Meier method is the most common way to estimate survival times and probabilities. It is a non-parametric approach that results in a step function, where there is a step down each time an event occurs.

- The `Surv` function from the `survival` package creates a survival object for use as the response in a model formula. There will be one entry for each subject that is the survival time, which is followed by a `+` if the subject was censored. Let's look at the first 10 observations:

```{r eval=FALSE, include=FALSE, echo=TRUE}
survival::Surv(df$os_yrs, df$os)[1:10]
```


## Estimating survival curves with the Kaplan-Meier method

- The `survival::survfit` function creates survival curves based on a formula. Let's generate the overall survival curve for the entire cohort, assign it to object `f1`, and look at the `names` of that object:

```{r eval=FALSE, include=FALSE, echo=TRUE}
f1 <- survival::survfit(survival::Surv(os_yrs, os) ~ 1, data = df)
names(f1)
```

Some key components of this `survfit` object that will be used to create survival curves include:

- `time`, which contains the start and endpoints of each time interval
- `surv`, which contains the survival probability corresponding to each `time`


## Kaplan-Meier plot - base R

Now we plot the `survfit` object in base `R` to get the Kaplan-Meier plot:

```{r eval=FALSE, include=FALSE, echo=TRUE}
plot(survival::survfit(survival::Surv(os_yrs, os) ~ 1, data = df), 
     xlab = "Years", 
     ylab = "Overall survival probability")
```

- The default plot in base `R` shows the step function (solid line) with associated confidence intervals (dotted lines). Note that the tick marks for censored patients are not shown by default, but could be added using `mark.time = TRUE`


## Kaplan-Meier plot - ggsurvplot

Alternatively, the `ggsurvplot` function from the `survminer` package is built on `ggplot2`, and can be used to create Kaplan-Meier plots:

```
{r, message = FALSE, warning = FALSE}
survminer::ggsurvplot(
    fit = survival::survfit(survival::Surv(os_yrs, os) ~ 1, data = df), 
    xlab = "Years", 
    ylab = "Overall survival probability")
```

- The default plot using `survminer::ggsurvplot` shows the step function (solid line) with associated confidence bands (shaded area). The tick marks for censored patients are shown by default, somewhat obscuring the line itself in this example, and could be supressed using `censor = FALSE`

## Estimating $x$-year survival

One quantity often of interest in a survival analysis is the probability of surviving a certain number ($x$) of years.

For example, to estimate the probability of survivng to 5 years, use `summary` with the `times` argument:

```{r eval=FALSE, include=FALSE, echo=TRUE}
summary(survival::survfit(survival::Surv(os_yrs, os) ~ 1, data = df), 
        times = 5)
```

We find that the 5-year probability of survival in this study is `{r #  round(summary(f1, times = 5)$surv * 100)`%. The associated lower and upper bounds of the 95\% confidence interval are also displayed.


## $x$-year survival and the survival curve

The 5-year survival probability is the point on the y-axis that corresponds to 5 years on the x-axis for the survival curve.

```
{r, message = FALSE, echo = TRUE}
survplot1 <- survminer::ggsurvplot(survival::survfit(
    survival::Surv(os_yrs, os) ~ 1, data = df), 
           xlab = "Years",
           ylab = "Overall survival probability",
           break.x.by = 5, 
           palette = ezfun::msk_palette("main")) +
    geom_segment(x = 5, xend = 5, y = -1, yend = 0.808, col = 2, lwd = 2) +
    geom_segment(x = 5, xend = -0.7, y = 0.808, yend = 0.808, 
                 arrow = arrow(length = unit(0.3, "inches")), col = 2, 
                 lwd = 2)

survplot1$plot <- survplot1$plot +
    annotate("text", x = 0.2, y = 0.7, label = "81%", size = 6, col = 2)

survplot1
```


## $x$-year survival is often estimated incorrectly 

What happens if you use a "naive" estimate? 

`{r #  table(df$os[df$os_yrs <= 5])[2]` of the `{r #  nrow(df)` patients died by 5 years so:

$$\Big(1 - \frac{297}{2119}\Big) \times 100 = 86\%$$

- You get an **incorrect** estimate of the 5-year probability of survival when you ignore the fact that `{r #  table(df$os[df$os_yrs <= 5])[1]` patients were censored before 5 years.

- Recall the **correct** estimate of the 5-year probability of survival was `{r #  round(summary(f1, times = 5)$surv * 100)`%.


## Estimating median survival time

Another quantity often of interest in a survival analysis is the average survival time, which we quantify using the median (survival times are not expected to be normally distributed so the mean is not an appropriate summary). 

We can obtain this directly from our `survfit` object:

```{r eval=FALSE, include=FALSE, echo=TRUE}
survival::survfit(survival::Surv(os_yrs, os) ~ 1, data = df)
```

We see the median survival time is `{r #  round(summary(f1)$table["median"], 1)` years. The lower and upper bounds of the 95\% confidence interval are also displayed.


## Median survival time and the survival curve

Median survival is the time corresponding to a survival probability of 0.5: 

```
{r, message = FALSE, echo = TRUE}
survplot2 <- survminer::ggsurvplot(survival::survfit(
    survival::Surv(os_yrs, os) ~ 1, data = df), 
           xlab = "Years",
           ylab = "Overall survival probability",
           break.x.by = 5, 
           palette = ezfun::msk_palette("main")) +
    geom_segment(x = -1, xend = 12.6794, y = 0.5, yend = 0.5, 
                 col = 2, lwd = 2) +
    geom_segment(x = 12.6794, xend = 12.6794, y = 0.5, yend = -0.02, 
                 arrow = arrow(length = unit(0.3, "inches")), col = 2, 
                 lwd = 2)

survplot2$plot <- survplot2$plot +
    annotate("text", x = 9.5, y = 0.02, label = "12.7 years", size = 6, col = 2)

survplot2
```


## Median survival is often estimated incorrectly

What happens if you use a "naive" estimate? 

Summarize the median survival time among the `{r #  table(df$os)[2]` patients who died:

```{r eval=FALSE, include=FALSE, echo=TRUE}
df$os_yrs[df$os == 1] %>% 
    median
```

- You get an **incorrect** estimate of median survival time of `{r #  round(median(df$os_yrs[df$os == 1]), 1)` years when you ignore the fact that censored patients also contribute follow-up time.

- Recall the **correct** estimate of median survival time is `{r #  round(summary(f1)$table["median"], 1)` years.


# Comparing survival times between groups {.center}


## Questions of interest with respect to between-group differences

Is there a difference in survival probability between groups?

From our example: does the probability of survival differ according to BMI among kidney cancer patients?


## Kaplan-Meier plot by group

We can add a covariate to the right-hand side of the `survival::survfit` object to obtain a stratified Kaplan-Meier plot. 

Let's also look at some other customization we can do with `survminer::ggsurvplot`.

```
{r fig.height = 6}
survminer::ggsurvplot(
    fit = survival::survfit(survival::Surv(os_yrs, os) ~ bmi_cat, data = df), 
    xlab = "Years",
    ylab = "Overall survival probability",
    legend.title = "BMI",
    legend.labs = c("Normal", "Overweight", "Obese"),
    break.x.by = 5, 
    palette = ezfun::msk_palette("contrast"), 
    censor = FALSE,
    risk.table = TRUE,
    risk.table.y.text = FALSE)
```

- By looking at the curves, we can see that normal BMI patients have the lowest overall survival probability, followed by overweight BMI patients and obese BMI patients.
- The risk table below the plot shows us the number of patients at risk at certain time points, which can give an idea of how much information is being used to calculate the estimates at each time


## $x$-year survival probability by group

As before, we can get an estimate of, for example, 5-year survival by using `summary` with the `times` argument in our `survival::survfit` object:

```{r eval=FALSE, include=FALSE, echo=TRUE}
summary(survival::survfit(survival::Surv(os_yrs, os) ~ bmi_cat, data = df), 
        times = 5)
```

To summarize:

```{r eval=FALSE, include=FALSE}
library(survival)
tab <- ezfun::uvsurv(NULL, "bmi_cat", "os", "os_yrs", 5, df)[-1, c(1, 3)]
colnames(tab) <- c("BMI", "5-year estimate (95% CI)")
tab[, 1] <- c("Normal", "Overweight", "Obese")
kable(tab, row.names = FALSE)
```


## Log-rank test for between-group significance test

- We can conduct between-group significance tests using a log-rank test. 
- The log-rank test equally weights observations over the entire follow-up time and is the most common way to compare survival times between groups.
- There are versions that more heavily weight the early or late follow-up that could be more appropriate depending on the research question.

We get the log-rank p-value using the `survival::survdiff` function:

```{r eval=FALSE, include=FALSE, echo=TRUE}
survival::survdiff(survival::Surv(df$os_yrs, df$os) ~ df$bmi_cat)
```

And we see that the p-value is <.001, indicating a significant difference in overall survival according to BMI.


# Regression {.center}


## The Cox regression model

We may want to quantify an effect size for a single variable, or include more than one variable into a regression model to account for the effects of multiple variables.

The Cox regression model is a semi-parametric model that can be used to fit univariable and multivariable regression models that have survival outcomes.

Some key assumptions of the model:

- non-informative censoring
- proportional hazards

*Note*: parametric regression models for survival outcomes are also available, but I won't cover them in detail here.


## Cox regression example using a single covariate

We can fit regression models for survival data using the `survival::coxph` function, which takes a `survival::Surv` object on the left hand side and has standard syntax for regression formulas in `R` on the right hand side.

```
{r eval = FALSE}
survival::coxph(survival::Surv(os_yrs, os) ~ factor(bmi_cat), data = df)
```

We can see a tidy version of the output using the `tidy` function from the `broom` package:

```{r eval=FALSE, include=FALSE, echo=TRUE}
broom::tidy(survival::coxph(survival::Surv(os_yrs, os) ~ factor(bmi_cat), 
                            data = df))
```


## Hazard ratios

The quantity of interest from a Cox regression model is a **hazard ratio (HR)**.

If you have a regression parameter $\beta$ (from column `estimate` in our `survival::coxph`) then HR = $\exp(\beta)$.

For example, from our example we obtain the regression parameter $\beta_1=-0.4441733$ for obese vs normal BMI, so we have HR = $\exp(\beta_1)=0.64$. 

A HR < 1 indicates reduced hazard of death whereas a HR > 1 indicates an increased hazard of death.

So we would say that obese BMI kidney cancer patients have 0.64 times reduced hazard of death as compared to normal BMI kidney cancer patients.


## Summary

- Time-to-event data is common
- Survival analysis techniques are required to account for censored data
- The `survival` package provides tools for survival analysis, including the `Surv` and `survfit` functions
- The `survminer` package allows for customization of Kaplan-Meier plots based on `ggplot2`
- Between-group comparisons can be made with the log-rank test using `survival::survdiff`
- Multiavariable Cox regression analysis can be accomplished using `survival::coxph` 

<!--chapter:end:survival_analysis_in_r_tutorial2.Rmd-->

---
title: "My R Codes For Data Analysis"
subtitle: "Survival Analysis"
author: "[Serdar Balcı, MD, Pathologist](https://www.serdarbalci.com/)"
date: '`{r #  format(Sys.Date())`'
output: 
  html_notebook: 
    fig_caption: yes
    highlight: tango
    number_sections: yes
    theme: paper
    toc: yes
    toc_depth: 5
    toc_float: yes
  html_document: 
    code_folding: hide
    df_print: kable
    keep_md: yes
    number_sections: yes
    theme: cerulean
    toc: yes
    toc_float: yes
    highlight: kate
---

## Survival Analysis in R


https://rviews.rstudio.com/2017/09/25/survival-analysis-with-r/


```{r eval=FALSE, include=FALSE, echo=TRUE}
library(survival)
library(ranger)
library(ggplot2)
library(dplyr)
library(ggfortify)
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
data(veteran)
head(veteran)
```

### Censored Data

```{r eval=FALSE, include=FALSE, echo=TRUE}
km <- with(veteran, Surv(time, status))

head(km,80)

```


### Kaplan Meier 

```{r eval=FALSE, include=FALSE, echo=TRUE}
km_fit <- survfit(Surv(time, status) ~ 1, data = veteran)

km_fit
```


### Life Table

```{r eval=FALSE, include=FALSE, echo=TRUE}
summary(km_fit, times = c(1,30,60,90*(1:10)))

```

### KM Graph Overall

```{r eval=FALSE, include=FALSE, echo=TRUE}
plot(km_fit, xlab="Days", main = 'Kaplan Meyer Plot') #base graphics is always ready

```


```{r eval=FALSE, include=FALSE, echo=TRUE}
autoplot(km_fit)

```


### KM per treatment group


```{r eval=FALSE, include=FALSE, echo=TRUE}
km_trt_fit <- survfit(Surv(time, status) ~ trt, data = veteran)
km_trt_fit

```


### KM Graph per treatment group


```{r eval=FALSE, include=FALSE, echo=TRUE}
autoplot(km_trt_fit)
```




# survminer

http://www.sthda.com/english/rpkgs/survminer/#uber-platinum-customized-survival-curves





<!-- vet <- mutate(veteran, AG = ifelse((age < 60), "LT60", "OV60"), -->
<!--               AG = factor(AG), -->
<!--               trt = factor(trt,labels=c("standard","test")), -->
<!--               prior = factor(prior,labels=c("N0","Yes"))) -->

<!-- km_AG_fit <- survfit(Surv(time, status) ~ AG, data = vet) -->
<!-- km_AG_fit -->

<!-- autoplot(km_AG_fit) -->


<!-- Fit Cox Model -->
<!-- cox <- coxph(Surv(time, status) ~ trt + celltype + karno + diagtime + age + prior , data = vet) -->
<!-- cox -->
<!-- summary(cox) -->

<!-- cox_fit <- survfit(cox) -->
<!-- cox_fit -->
<!-- plot(cox_fit, main = "cph model", xlab="Days") -->
<!-- autoplot(cox_fit) -->

<!-- aa_fit <-aareg(Surv(time, status) ~ trt + celltype + -->
<!--                    karno + diagtime + age + prior ,  -->
<!--                data = vet) -->
<!-- aa_fit -->


<!-- #summary(aa_fit)  # provides a more complete summary of results -->
<!-- autoplot(aa_fit) -->


<!-- # ranger model -->
<!-- r_fit <- ranger(Surv(time, status) ~ trt + celltype +  -->
<!--                     karno + diagtime + age + prior, -->
<!--                 data = vet, -->
<!--                 mtry = 4, -->
<!--                 importance = "permutation", -->
<!--                 splitrule = "extratrees", -->
<!--                 verbose = TRUE) -->


<!-- # Average the survival models -->
<!-- death_times <- r_fit$unique.death.times  -->
<!-- surv_prob <- data.frame(r_fit$survival) -->
<!-- avg_prob <- sapply(surv_prob,mean) -->

<!-- # Plot the survival models for each patient -->
<!-- plot(r_fit$unique.death.times,r_fit$survival[1,],  -->
<!--      type = "l",  -->
<!--      ylim = c(0,1), -->
<!--      col = "red", -->
<!--      xlab = "Days", -->
<!--      ylab = "survival", -->
<!--      main = "Patient Survival Curves") -->

<!-- # -->
<!-- cols <- colors() -->
<!-- for (n in sample(c(2:dim(vet)[1]), 20)){ -->
<!--     lines(r_fit$unique.death.times, r_fit$survival[n,], type = "l", col = cols[n]) -->
<!-- } -->
<!-- lines(death_times, avg_prob, lwd = 2) -->
<!-- legend(500, 0.7, legend = c('Average = black')) -->


<!-- vi <- data.frame(sort(round(r_fit$variable.importance, 4), decreasing = TRUE)) -->
<!-- names(vi) <- "importance" -->
<!-- head(vi) -->


<!-- cat("Prediction Error = 1 - Harrell's c-index = ", r_fit$prediction.error) -->


<!-- # Set up for ggplot -->
<!-- kmi <- rep("KM",length(km_fit$time)) -->
<!-- km_df <- data.frame(km_fit$time,km_fit$surv,kmi) -->
<!-- names(km_df) <- c("Time","Surv","Model") -->

<!-- coxi <- rep("Cox",length(cox_fit$time)) -->
<!-- cox_df <- data.frame(cox_fit$time,cox_fit$surv,coxi) -->
<!-- names(cox_df) <- c("Time","Surv","Model") -->

<!-- rfi <- rep("RF",length(r_fit$unique.death.times)) -->
<!-- rf_df <- data.frame(r_fit$unique.death.times,avg_prob,rfi) -->
<!-- names(rf_df) <- c("Time","Surv","Model") -->

<!-- plot_df <- rbind(km_df,cox_df,rf_df) -->

<!-- p <- ggplot(plot_df, aes(x = Time, y = Surv, color = Model)) -->
<!-- p + geom_line() -->




<!-- ############ -->





<!-- install.packages("OIsurv") -->
<!-- library(OIsurv) -->

<!-- library(help=KMsurv) -->

<!-- data(aids) -->
<!-- glimpse(aids) -->


<!-- data(tongue) -->

<!-- glimpse(tongue) -->

<!-- my.surv.object <- Surv(tongue$time[tongue$type==1], tongue$delta[tongue$type==1]) -->

<!-- my.surv.object -->

<!-- data(psych) -->

<!-- my.surv.object2 <- Surv(psych$age, psych$age+psych$time, psych$death) -->

<!-- my.surv.object2 -->



<!-- ```{r eval=FALSE, include=FALSE, echo=TRUE} -->
<!-- library("survival") -->
<!-- fit <- survfit(Surv(time,status) -->
<!-- ~ sex, data = lung) -->
<!-- class(fit) -->
<!-- ## [1] "survfit" -->
<!-- library("survminer") -->
<!-- ggsurvplot(fit, data = lung) -->
<!-- ``` -->


<!-- ```{r eval=FALSE, include=FALSE, echo=TRUE} -->
<!-- ggsurvplot(fit, data = lung, fun = "event") -->

<!-- ``` -->


<!-- ```{r eval=FALSE, include=FALSE, echo=TRUE} -->
<!-- ggsurvplot(fit, data = lung, fun = "cumhaz") -->
<!-- ``` -->

<!-- ```{r eval=FALSE, include=FALSE, echo=TRUE} -->
<!-- ggsurvplot(fit, data = lung, -->
<!-- conf.int = TRUE, -->
<!-- pval = TRUE, -->
<!-- fun = "pct", -->
<!-- risk.table = TRUE, -->
<!-- tables.height = 0.3, -->
<!-- size = 0.8, -->
<!-- linetype = "strata", -->
<!-- palette = c("#E7B800", -->
<!-- "#2E9FDF"), -->
<!-- legend = "top", -->
<!-- legend.title = "Sex", -->
<!-- legend.labs = c("Male", -->
<!-- "Female"), -->
<!-- cex = 0.2 -->
<!-- ) -->
<!-- ``` -->

<!-- ```{r eval=FALSE, include=FALSE, echo=TRUE} -->
<!-- library("survival") -->
<!-- fit <- coxph(Surv(time, status) ~ sex + age, data = lung) -->
<!-- ftest <- cox.zph(fit) -->
<!-- ftest -->
<!-- ``` -->

<!-- ```{r eval=FALSE, include=FALSE, echo=TRUE} -->
<!-- library("survminer") -->
<!-- ggcoxzph(ftest) -->
<!-- ``` -->


<!-- ```{r eval=FALSE, include=FALSE, echo=TRUE} -->
<!-- library("survival") -->
<!-- library("survminer") -->
<!-- fit <- coxph(Surv(time, status) ~ -->
<!-- sex + age, data = lung) -->

<!-- ``` -->




<!-- ```{r eval=FALSE, include=FALSE, echo=TRUE} -->
<!-- ggcoxdiagnostics(fit, -->
<!-- type = "deviance", -->
<!-- ox.scale = "linear.predictions") -->

<!-- ``` -->



<!-- ```{r eval=FALSE, include=FALSE, echo=TRUE} -->
<!-- ggcoxdiagnostics(fit, -->
<!-- type = "schoenfeld", -->
<!-- ox.scale = "time") -->
<!-- ``` -->




<!-- # Analysis -->

<!-- ## Survival Analysis -->

<!-- https://github.com/datacamp/tutorial -->

<!-- Based on https://www.datacamp.com/community/tutorials/survival-analysis-R -->


<!-- ```{r eval=FALSE, include=FALSE, echo=TRUE} -->
<!-- library(tutorial) -->
<!-- library(survival) -->
<!-- library(survminer) -->
<!-- library(tidyverse) -->
<!-- library(jmv) -->
<!-- ``` -->


<!-- ```{r eval=FALSE, include=FALSE, echo=TRUE} -->
<!-- data(ovarian) -->
<!-- glimpse(ovarian) -->
<!-- help(ovarian) -->
<!-- ``` -->


<!-- ```{r eval=FALSE, include=FALSE, echo=TRUE} -->
<!-- summary(ovarian) -->
<!-- ``` -->


<!-- ```{r eval=FALSE, include=FALSE, echo=TRUE} -->
<!-- varOvarian <- names(ovarian) -->

<!-- ovarian2 <- ovarian %>%  -->
<!--     map_df(as.factor) -->

<!-- jmv::descriptives(ovarian, varOvarian, freq = TRUE) -->

<!-- jmv::descriptives(ovarian2, varOvarian, freq = TRUE) -->

<!-- ``` -->


<!-- ```{r eval=FALSE, include=FALSE, echo=TRUE} -->
<!-- # Dichotomize age and change data labels -->
<!-- ovarian$rx <- factor(ovarian$rx,  -->
<!--                      levels = c("1", "2"),  -->
<!--                      labels = c("A", "B")) -->
<!-- ovarian$resid.ds <- factor(ovarian$resid.ds,  -->
<!--                            levels = c("1", "2"),  -->
<!--                            labels = c("no", "yes")) -->
<!-- ovarian$ecog.ps <- factor(ovarian$ecog.ps,  -->
<!--                           levels = c("1", "2"),  -->
<!--                           labels = c("good", "bad")) -->

<!-- jmv::descriptives(ovarian, varOvarian, freq = TRUE) -->

<!-- ``` -->


<!-- ```{r eval=FALSE, include=FALSE, echo=TRUE} -->
<!-- # Data seems to be bimodal -->
<!-- hist(ovarian$age) -->
<!-- ``` -->


<!-- ```{r eval=FALSE, include=FALSE, echo=TRUE} -->
<!-- ovarian <- ovarian %>% mutate(age_group = ifelse(age >= 50, "old", "young")) -->
<!-- ovarian$age_group <- factor(ovarian$age_group) -->

<!-- varOvarian <- names(ovarian) -->

<!-- jmv::descriptives(ovarian, varOvarian, freq = TRUE) -->
<!-- ``` -->


<!-- ```{r eval=FALSE, include=FALSE, echo=TRUE} -->
<!-- # Fit survival data using the Kaplan-Meier method -->
<!-- surv_object <- Surv(time = ovarian$futime, event = ovarian$fustat) -->
<!-- surv_object -->
<!-- ``` -->

<!-- ```{r eval=FALSE, include=FALSE, echo=TRUE} -->
<!-- fit1 <- survfit(surv_object ~ rx, data = ovarian) -->
<!-- summary(fit1) -->
<!-- ``` -->


<!-- ```{r eval=FALSE, include=FALSE, echo=TRUE} -->
<!-- ggsurvplot(fit1, data = ovarian, pval = TRUE) -->
<!-- ``` -->


<!-- ```{r eval=FALSE, include=FALSE, echo=TRUE} -->
<!-- # Examine prdictive value of residual disease status -->
<!-- fit2 <- survfit(surv_object ~ resid.ds, data = ovarian) -->
<!-- ggsurvplot(fit2, data = ovarian, pval = TRUE) -->
<!-- ``` -->

<!-- ```{r eval=FALSE, include=FALSE, echo=TRUE} -->
<!-- # Fit a Cox proportional hazards model -->
<!-- fit.coxph <- coxph(surv_object ~ rx + resid.ds + age_group + ecog.ps,  -->
<!--                    data = ovarian) -->
<!-- ggforest(fit.coxph, data = ovarian) + -->
<!--     geom_errorbar(na.rm = TRUE) -->
<!-- ``` -->



# OpenIntro Statistics 

https://www.openintro.org/stat/surv.php

https://www.openintro.org/download.php?file=survival_analysis_in_R&referrer=/stat/surv.php

https://www.openintro.org/download.php?file=survival_analysis_in_R_code&referrer=/stat/surv.php

https://www.openintro.org/download.php?file=survival_analysis_in_R_code_df-cp&referrer=/stat/surv.php


```{r eval=FALSE, include=FALSE, echo=TRUE}
# install.packages("OIsurv")
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
# library(OIsurv)
library(survival)
library(splines)
library(KMsurv)
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
library(help=KMsurv)
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
data(aids)
aids
```

```{r eval=FALSE, include=FALSE, echo=TRUE}
 data(tongue)
tongue
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
my.surv.object <- Surv(tongue$time[tongue$type==1], tongue$delta[tongue$type==1])
my.surv.object
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
data(psych)
psych
my.surv.object <- Surv(psych$age, psych$age + psych$time, psych$death)
my.surv.object

```








```{r eval=FALSE, include=FALSE, echo=TRUE}
rm(list=ls())

#===> loading packages and such <===#
#install.packages("OIsurv")
library(OIsurv)
data(aids)
aids
attach(aids)
infect
detach(aids)
aids$infect

#===> survival object <===#
data(tongue); attach(tongue)   # the following will not affect computations
# create a subset for just the first group by using [type==1]
my.surv.object <- Surv(time[type==1], delta[type==1])
my.surv.object
detach(tongue)

data(psych); attach(psych)
my.surv.object <- Surv(age, age+time, death)
my.surv.object
detach(psych)

#===> K-M Estimate <===#
data(tongue); attach(tongue)
my.surv <- Surv(time[type==1], delta[type==1])
survfit(my.surv ~ 1)
my.fit <- survfit(my.surv ~ 1)
summary(my.fit)$surv     # returns the Kaplan-Meier estimate at each t_i
summary(my.fit)$time     # {t_i}
summary(my.fit)$n.risk   # {Y_i}
summary(my.fit)$n.event  # {d_i}
summary(my.fit)$std.err  # standard error of the K-M estimate at {t_i}
summary(my.fit)$lower    # lower pointwise estimates (alternatively, $upper)
str(my.fit)              # full summary of the my.fit object
str(summary(my.fit))     # full summary of the my.fit object
pdf("../figures/kmPlot.pdf", 7, 4.5)
plot(my.fit, main="Kaplan-Meier estimate with 95% confidence bounds",
   xlab="time", ylab="survival function")
dev.off()
my.fit1 <- survfit( Surv(time, delta) ~ type )   # here the key is "type"
detach(tongue)

#===> confidence bands <===#
data(bmt); attach(bmt)
my.surv <- Surv(t2[group==1], d3[group==1])
my.cb <- confBands(my.surv, confLevel=0.95, type="hall")
pdf("../figures/confBand.pdf", 8, 5)
plot(survfit(my.surv ~ 1), xlim=c(100, 600), xlab="time",
  ylab="Estimated Survival Function",
  main="Reproducing Confidence Bands for Example 4.2 in Klein/Moeschberger")

lines(my.cb$time, my.cb$lower, lty=3, type="s")
lines(my.cb$time, my.cb$upper, lty=3, type="s")
legend(100, 0.3, legend=c("K-M survival estimate",
  "pointwise intervals","confidence bands"), lty=1:3)
dev.off()
detach(bmt)


#===> cumulative hazard <===#
data(tongue); attach(tongue)
my.surv <- Surv(time[type==1], delta[type==1])
my.fit  <- summary(survfit(my.surv ~ 1))
H.hat   <- -log(my.fit$surv)
H.hat   <- c(H.hat, tail(H.hat, 1))
h.sort.of <- my.fit$n.event / my.fit$n.risk
H.tilde   <- cumsum(h.sort.of)
H.tilde   <- c(H.tilde, tail(H.tilde, 1))
pdf("../figures/cumHazard.pdf", 6, 4)
plot(c(my.fit$time, 250), H.hat, xlab="time", ylab="cumulative hazard",
  main="comparing cumulative hazards", ylim=range(c(H.hat, H.tilde)), type="s")
points(c(my.fit$time, 250), H.tilde, lty=2, type="s")
legend("topleft", legend=c("H.hat","H.tilde"), lty=1:2)
dev.off()
detach(tongue)


#===> mean/median <===#
data(drug6mp); attach(drug6mp)
my.surv <- Surv(t1, rep(1, 21))   # all placebo patients observed
survfit(my.surv ~ 1)
print(survfit(my.surv ~ 1), print.rmean=TRUE)
detach(drug6mp)

#===> test for 2+ samples <===#
data(btrial); attach(btrial)
survdiff(Surv(time, death) ~ im)   # output omitted
survdiff(Surv(time, death) ~ im, rho=1)   # output omitted
detach(btrial)

#===> coxph, time-independent <===#
data(burn); attach(burn)
my.surv   <- Surv(T1, D1)
coxph.fit <- coxph(my.surv ~ Z1 + as.factor(Z11), method="breslow")
coxph.fit
co <- coxph.fit$coefficients  # may use coxph.fit$coeff instead
va <- coxph.fit$var           # I^(-1), estimated cov matrix of the estimates
ll <- coxph.fit$loglik        # log-likelihood for alt and null MLEs, resp.
my.survfit.object <- survfit(coxph.fit)
hold <- survfit(my.surv ~ 1)
#source("http://www.stat.ucla.edu/~david/teac/surv/local-coxph-test.R")
coxph.fit
C   <- matrix(c(0, 1, 0, 0,
                0, 0, 1, 0,
                0, 0, 0, 1), nrow=3, byrow=TRUE)
d   <- rep(0, 3)
t1  <- C %*% co - d
t2  <- C %*% va %*% t(C)
XW2 <- c(t(t1) %*% solve(t2) %*% t1)
pchisq(XW2, 3, lower.tail=FALSE)
#local.coxph.test(coxph.fit, 2:4)
my.survfit.object <- survfit(coxph.fit)
detach(burn)


#===> coxph, time-dependent <===#
data(relapse)
relapse

N  <- dim(relapse)[1]
t1 <- rep(0, N+sum(!is.na(relapse$int)))  # initialize start time at 0
t2 <- rep(-1, length(t1))                 # build vector for end times
d  <- rep(-1, length(t1))                 # whether event was censored
g  <- rep(-1, length(t1))                 # gender covariate
i  <- rep(FALSE, length(t1))              # initialize intervention at FALSE

j  <- 1
for(ii in 1:dim(relapse)[1]){
  if(is.na(relapse$int[ii])){      # no intervention, copy survival record
    t2[j] <- relapse$event[ii]
    d[j]  <- relapse$delta[ii]
    g[j]  <- relapse$gender[ii]
    j <- j+1
  } else {                         # intervention, split records
    g[j+0:1] <- relapse$gender[ii] # gender is same for each time
    d[j]     <- 0                  # no relapse observed pre-intervention
    d[j+1]   <- relapse$delta[ii]  # relapse occur post-intervention?
    i[j+1]   <- TRUE               # intervention covariate, post-intervention
    t2[j]    <- relapse$int[ii]-1  # end of pre-intervention
    t1[j+1]  <- relapse$int[ii]-1  # start of post-intervention
    t2[j+1]  <- relapse$event[ii]  # end of post-intervention
    j <- j+2                       # two records added
  }
}

mySurv <- Surv(t1, t2, d)        # pg 3 discusses left-trunc. right-cens. data
myCPH  <- coxph(mySurv ~ g + i)

#data(burn); attach(burn)
##source("http://www.stat.ucla.edu/~david/teac/surv/time-dep-coxph.R")
#td.coxph <- timeDepCoxph(burn, "T1", "D1", 2:4, "Z1", verbose=FALSE)
#td.coxph   # some model output is omitted for brevity
#detach(burn)

#===> AFT models <===#
data(larynx)
attach(larynx)
srFit <- survreg(Surv(time, delta) ~ as.factor(stage) + age, dist="weibull")
summary(srFit)
srFitExp <- survreg(Surv(time, delta) ~ as.factor(stage) + age, dist="exponential")
summary(srFitExp)
srFitExp$coeff    # covariate coefficients
srFitExp$icoef    # intercept and scale coefficients
srFitExp$var      # variance-covariance matrix
srFitExp$loglik   # log-likelihood
srFit$scale       # not using srFitExp (defaulted to 1)
detach(larynx)

```




```{r eval=FALSE, include=FALSE, echo=TRUE}

# Posted with permission of the code author:
# Beau Benjamin Bruce
# Author email: bbbruce@emory.edu
# 
# This code is available under GPL-2
# For license information, see
# http://cran.r-project.org/web/licenses/GPL-2

library(KMsurv)
data(burn)

#=====> Convert a data frame to a counting process version <=====#
#=====> Allows for time dependent variables to be introduced <=====#
df.cp = function(data,t.var,status.var,covars=setdiff(names(data),c(t.var,status.var))) {
  # data: data frame that represents the dataset
  # t.var: the column name of "data" that represents the survival time variable
  # status.var: the column name of "data" that represents the status variable
  # covars: list other covariates to retain
  
  # Returned object: a data frame that breaks each event down to a counting process
  
  # sorted times, append to 0
  t.sort <- c(0,sort(unlist(unique(data[t.var]))))
  
  # for each data point find times less than or equal to the obs' time
  t.list <- lapply(unlist(data[t.var]),function(x) t.sort[t.sort<=x])
  
  # create a list of datasets with covariates and all relevant start/stop times
  # use with to set the environment to the correct list item
  df.list <- lapply(seq_along(t.list),function(i) cbind(with(list(x=t.list[[i]]),
                     # start by removing one from end of x, stop by removing first of x
                     # include the status variable in the dataframe - helpful later                                        
                     data.frame(start=head(x,-1),stop=tail(x,-1),data[i,c(status.var,covars)]))))

  # do.call uses df.list as the argument for rbind
  df <- do.call(rbind,df.list)

  # create the correct status need last time for each
  # subject with status=1 to to be status=1 but all others status=0
  #
  # the lapply creates vectors 0,0,0,...,1 based on length of t.list
  # need to substract 2 because the lag takes one away, then need one for the 1 at end
  # do.call with c binds them together into a single vector
  # this is then multiplied by status to correct it
  keep.status <- do.call(c,lapply(t.list,function(x) c(rep(0,length(x)-2),1)))
  df[status.var] <- df[status.var] * keep.status
  df
}


#=====> Create the counting process data frame <=====#
burn.cp <- df.cp(burn,'T1','D1')
burn.cp <- within(burn.cp,{ T1Z1 <- log(stop)*Z1; }) 

#=====> Apply the Cox Proportional Hazards model <=====#
coxph(Surv(start,stop,D1) ~ Z1+Z2+Z3+T1Z1,data=burn.cp)


```


---

# survsup

- Plotting survival curves with the survsup package  

https://cran.r-project.org/web/packages/survsup/vignettes/survsup_intro.html


http://github.com/dlindholm/survsup/


```{r eval=FALSE, include=FALSE, echo=TRUE}
# install.packages("survsup")
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
library(survsup)
library(ggplot2)
library(survival)
library(dplyr)
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
lung
```



```{r eval=FALSE, include=FALSE, echo=TRUE}
fit <- survfit(Surv(time, status) ~ 1, data = lung)
plot_survfit(fit)
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
lung %>%
  survfit(Surv(time, status) ~ 1, data = .) %>%
  plot_survfit()
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
lung %>%
  survfit(Surv(time, status) ~ 1, data = .) %>%
  plot_survfit(cuminc = FALSE)
```



```{r eval=FALSE, include=FALSE, echo=TRUE}
lung %>%
  survfit(Surv(time, status) ~ sex, data = .) %>%
  plot_survfit(cuminc = FALSE)
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
lung %>%
  survfit(Surv(time, status) ~ sex, data = .) %>%
  plot_survfit(cuminc = FALSE, ci = TRUE)
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
lung %>%
  survfit(Surv(time, status) ~ sex, data = .) %>%
  plot_survfit(cuminc = FALSE, ci = TRUE) + # <--- NOTE!
  labs(x = "Time (days)", y = "Survival (%)")
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
lung %>%
  survfit(Surv(time, status) ~ sex, data = .) %>%
  plot_survfit(cuminc = FALSE) %>%
  nar()
```



```{r eval=FALSE, include=FALSE, echo=TRUE}
lung %>%
  survfit(Surv(time, status) ~ sex, data = .) %>%
  plot_survfit(cuminc = FALSE) %>%
  nar(size = 3)
```



```{r eval=FALSE, include=FALSE, echo=TRUE}
colon
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
colon %>%
  survfit(Surv(time, status) ~ rx, data = .) %>%
  plot_survfit() %>%
  nar() +
  scale_color_manual(values = c("darkorange", "steelblue", "darkred"))
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
colon %>%
  survfit(Surv(time, status) ~ extent, data = .) %>%
  plot_survfit() %>%
  nar() %>%
  skislopes()
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
colon %>%
  survfit(Surv(time, status) ~ extent, data = .) %>%
  plot_survfit() %>%
  nar() %>%
  skislopes(reverse = TRUE)
```



```{r eval=FALSE, include=FALSE, echo=TRUE}
colon %>%
  survfit(Surv(time, status) ~ extent, data = .) %>%
  plot_survfit() %>%
  nar() %>%
  cat4()
```




```{r eval=FALSE, include=FALSE, echo=TRUE}
colon %>%
  survfit(Surv(time, status) ~ extent, data = .) %>%
  plot_survfit() %>%
  nar() %>%
  hcl_rainbow()
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
colon %>%
  survfit(Surv(time, status) ~ extent, data = .) %>%
  plot_survfit() %>%
  nar() %>%
  hcl_rainbow(reverse = TRUE)
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
colon %>%
  survfit(Surv(time, status) ~ extent, data = .) %>%
  plot_survfit(lwd = 2)
```



```{r eval=FALSE, include=FALSE, echo=TRUE}
colon %>%
  survfit(Surv(time, status) ~ extent, data = .) %>%
  plot_survfit(lwd = 0.5)
```



```{r eval=FALSE, include=FALSE, echo=TRUE}
colon %>%
  survfit(Surv(time, status) ~ extent, data = .) %>%
  plot_survfit(legend.title = "Extent of disease")
```




```{r eval=FALSE, include=FALSE, echo=TRUE}
colon %>%
  survfit(Surv(time, status) ~ extent, data = .) %>%
  plot_survfit(ylim = c(0, 100)) %>%
  nar()
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
colon %>%
  survfit(Surv(time, status) ~ extent, data = .) %>%
  plot_survfit(xmax = 2000) %>%
  nar()
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
colon %>%
  survfit(Surv(time, status) ~ extent, data = .) %>%
  plot_survfit(xmax = 2000, xbreaks = c(0, 1000, 2000)) %>%
  nar()
```



```{r eval=FALSE, include=FALSE, echo=TRUE}
colon %>%
  survfit(Surv(time, status) ~ sex, data = .) %>%
  plot_survfit() %>%
  nar()
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
colon %>%
  survfit(Surv(time, status) ~ sex, data = .) %>%
  plot_survfit() %>%
  nar(flip = TRUE)
```



```{r eval=FALSE, include=FALSE, echo=TRUE}
colon %>%
  survfit(Surv(time, status) ~ sex, data = .) %>%
  plot_survfit() %>%
  nar(size = 5, flip = TRUE)
```



```{r eval=FALSE, include=FALSE, echo=TRUE}
colon %>%
  survfit(Surv(time, status) ~ sex, data = .) %>%
  plot_survfit() %>%
  nar(size = 2, flip = TRUE)
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
colon %>%
  survfit(Surv(time, status) ~ sex, data = .) %>%
  plot_survfit() %>%
  nar(y_offset = 0.1, flip = TRUE)
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
colon %>%
  survfit(Surv(time, status) ~ sex, data = .) %>%
  plot_survfit() %>%
  nar(y_offset = 0.03, flip = TRUE)
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
colon %>%
  survfit(Surv(time, status) ~ sex, data = .) %>%
  plot_survfit() %>%
  nar(separator = FALSE)
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
colon %>%
  survfit(Surv(time, status) ~ sex, data = .) %>%
  plot_survfit() %>%
  nar(sep_color = "grey90", sep_lwd = 1.5)
```



https://cran.r-project.org/package=gridExtra


```{r eval=FALSE, include=FALSE, echo=TRUE}
library(gridExtra)
```
  

```{r eval=FALSE, include=FALSE, echo=TRUE}
p <- list(
  p1 = colon %>%
    survfit(Surv(time, status) ~ sex, data = .) %>%
    plot_survfit(ylim = c(0, 100)) %>%
    nar() +
    labs(tag = "A"),

  p2 = colon %>%
    survfit(Surv(time, status) ~ node4, data = .) %>%
    plot_survfit(ylim = c(0, 100)) %>%
    nar() +
    labs(tag = "B")
)

grid.arrange(grobs = p, ncol = 2)
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
# Store plots in a list
p <- list(
  p1 = colon %>%
    survfit(Surv(time, status) ~ 1, data = .) %>%
    plot_survfit(ylim = c(0, 100)) +
    labs(tag = "A"),

  p2 = colon %>%
    survfit(Surv(time, status) ~ rx, data = .) %>%
    plot_survfit(ylim = c(0, 100)) %>%
    nar(2, separator = FALSE) +
    labs(tag = "B"),

  p3 = colon %>%
    survfit(Surv(time, status) ~ extent, data = .) %>%
    plot_survfit(ylim = c(0, 100)) %>%
    nar(2, separator = FALSE) +
    labs(tag = "C"),

  p4 = colon %>%
    survfit(Surv(time, status) ~ sex, data = .) %>%
    plot_survfit(ylim = c(0, 100)) %>%
    nar(2, separator = FALSE) +
    labs(tag = "D"),

  p5 = colon %>%
    survfit(Surv(time, status) ~ node4, data = .) %>%
    plot_survfit(ylim = c(0, 100)) %>%
    nar(2, separator = FALSE) +
    labs(tag = "E"),

  p6 = colon %>%
    survfit(Surv(time, status) ~ surg, data = .) %>%
    plot_survfit(ylim = c(0, 100)) %>%
    nar(2, separator = FALSE) +
    labs(tag = "F")

)

#Define layout matrix
lay <- rbind(c(1,1,2),
             c(1,1,3),
             c(4,5,6))

#Plot it all!
grid.arrange(grobs = p, layout_matrix = lay)
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
p[["p3"]]
```



# survivalAnalysis


## Univariate Survival Analysis


https://cran.r-project.org/web/packages/survivalAnalysis/vignettes/univariate.html


# breast cancer wisconsin 

```{r eval=FALSE, include=FALSE, echo=TRUE}
library(tidyverse)
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
UCI_data_URL <- RCurl::getURL('https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data')




```


# Lung Cancer


```{r eval=FALSE, include=FALSE, echo=TRUE}
library(survival)
km_fit <- survfit(Surv(time, status) ~ sex, data = lung)
km_fit
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
summary(km_fit, times = c(12,36,60))
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
# View fit
View(km_fit)

# Get formula
km_fit[["call"]][["formula"]]

# Get formula 2
km_fit[["call"]]
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
library(tidyverse)
# make a dataframe from fit list
fitDF <- bind_cols(
time = km_fit[["time"]],
n.risk = km_fit[["n.risk"]],
n.event = km_fit[["n.event"]],
n.censor = km_fit[["n.censor"]],
surv = km_fit[["surv"]],
lower = km_fit[["lower"]],
upper = km_fit[["upper"]],
) %>% 
  mutate(
    timeyear = time / 365.25
  )
```



```{r eval=FALSE, include=FALSE, echo=TRUE}
# this gives the result for the time = 0.5
fitDF[which.min(abs(fitDF$timeyear - 0.5)),]

# this gives the result for the time = 1
fitDF[which.min(abs(fitDF$timeyear - 1)),]

# this gives the result for the time = 2
fitDF[which.min(abs(fitDF$timeyear - 2)),]
```




```{r eval=FALSE, include=FALSE, echo=TRUE}
fitDF[which.min(abs(fitDF$timeyear - 0.5)),]

fitDF[which.min(abs(fitDF$timeyear - 1)),]

fitDF[which.min(abs(fitDF$timeyear - 2)),]
```



```{r eval=FALSE, include=FALSE, echo=TRUE}
# https://stackoverflow.com/questions/43419385/how-to-export-survfit-output-as-a-csv-table
res <- summary(km_fit, times = c(12,36,60))
save.df <- as.data.frame(res[c("strata", "time", "n.risk", "n.event", "surv", "std.err", "lower", "upper")])
# write.csv(save.df, file = "./file.csv")
```










<!--chapter:end:SurvivalAnalysis.Rmd-->

---
title: "Syncing a GitHub Fork"
---

# Configuring a remote for a fork

https://help.github.com/articles/configuring-a-remote-for-a-fork/

## Open Terminal.

## List the current configured remote repository for your fork.

```
{bash}
git remote -v
```


```
origin  https://github.com/YOUR_USERNAME/YOUR_FORK.git (fetch)
origin  https://github.com/YOUR_USERNAME/YOUR_FORK.git (push)
```


## Specify a new remote upstream repository that will be synced with the fork.


```
{bash}
git remote add upstream https://github.com/BIOP/IPA4LSx.git
```



```
git remote add upstream https://github.com/ORIGINAL_OWNER/ORIGINAL_REPOSITORY.git

git remote add upstream https://github.com/BIOP/IPA4LSx.git
```

## Verify the new upstream repository you've specified for your fork.

```
{bash}
git remote -v
```



```
origin    https://github.com/YOUR_USERNAME/YOUR_FORK.git (fetch)
origin    https://github.com/YOUR_USERNAME/YOUR_FORK.git (push)
upstream  https://github.com/ORIGINAL_OWNER/ORIGINAL_REPOSITORY.git (fetch)
upstream  https://github.com/ORIGINAL_OWNER/ORIGINAL_REPOSITORY.git (push)
```

# Syncing a fork

https://help.github.com/articles/syncing-a-fork/

## Open Terminal.

## Change the current working directory to your local project.

## Fetch the branches and their respective commits from the upstream repository. Commits to master will be stored in a local branch, upstream/master.

```
{bash}
git fetch upstream
```


```
git fetch upstream
remote: Counting objects: 75, done.
remote: Compressing objects: 100% (53/53), done.
remote: Total 62 (delta 27), reused 44 (delta 9)
Unpacking objects: 100% (62/62), done.
From https://github.com/ORIGINAL_OWNER/ORIGINAL_REPOSITORY
 * [new branch]      master     -> upstream/master
```

## Check out your fork's local master branch.


```
{bash}
git checkout master
```


```
git checkout master
Switched to branch 'master'
```

## Merge the changes from upstream/master into your local master branch. This brings your fork's master branch into sync with the upstream repository, without losing your local changes.


```
{bash}
git merge upstream/master
```



```
git merge upstream/master
Updating a422352..5fdff0f
Fast-forward
 README                    |    9 -------
 README.md                 |    7 ++++++
 2 files changed, 7 insertions(+), 9 deletions(-)
 delete mode 100644 README
 create mode 100644 README.md
```


## If your local branch didn't have any unique commits, Git will instead perform a "fast-forward":


```
{bash}
git merge upstream/master
```



```
git merge upstream/master
Updating 34e91da..16c56ad
Fast-forward
 README.md                 |    5 +++--
 1 file changed, 3 insertions(+), 2 deletions(-)
```

**Tip: Syncing your fork only updates your local copy of the repository. To update your fork on GitHub, you must push your changes.**


<!--chapter:end:SyncingGitHubFork.Rmd-->

---
title: "My R Codes For Data Analysis"
---

- finalfit


http://www.datasurg.net/2018/07/12/finalfit-now-includes-bootstrap-simulation-for-model-prediction/

- 

https://kbroman.org/knitr_knutshell/pages/figs_tables.html

## Tables

```{r eval=FALSE, include=FALSE, echo=TRUE}
library(xtable)
xtable(BMT)
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
library(kableExtra)
kable(BMT)
```


```{r display results as table, eval=FALSE, include=FALSE, echo=TRUE, warning=FALSE, message=FALSE}

require(rhandsontable)
rhandsontable(BMT)

```

## tttable

https://github.com/leeper/tttable







### - renderers

####  - xtable

https://rdrr.io/cran/xtable/man/xtable.html










####  - flextable

https://davidgohel.github.io/flextable/index.html

```{r eval=FALSE, include=FALSE, echo=TRUE}

```


https://cran.r-project.org/web/packages/flextable/vignettes/overview.html

```{r eval=FALSE, include=FALSE, echo=TRUE}
library(flextable)
library(officer)
```

```{r eval=FALSE, include=FALSE, echo=TRUE}
data <- iris[c(1:3, 51:53, 101:104),]

data

```


```{r eval=FALSE, include=FALSE, echo=TRUE}
flextable::regulartable(data)

```

```{r eval=FALSE, include=FALSE, echo=TRUE}
myft <- regulartable(
  head(mtcars), 
  col_keys = c("am", "carb", "gear", "mpg", "drat" ))
myft
```

```{r eval=FALSE, include=FALSE, echo=TRUE}
myft <- theme_vanilla(myft)
myft
```

```{r eval=FALSE, include=FALSE, echo=TRUE}
myft <- merge_v(myft, j = c("am", "carb") )
myft
```

```{r eval=FALSE, include=FALSE, echo=TRUE}
myft <- set_header_labels( myft, carb = "# carb." )
myft <- width(myft, width = .75) # set width of all columns to .75 in
myft
```

```{r eval=FALSE, include=FALSE, echo=TRUE}
myft <- autofit(myft)
myft
```

```{r eval=FALSE, include=FALSE, echo=TRUE}
myft <- italic(myft, j = 1)
myft <- bg(myft, bg = "#C90000", part = "header")
myft <- color(myft, color = "white", part = "header")
myft
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
myft <- color(myft, ~ drat > 3.5, ~ drat, color = "red")
myft <- bold(myft, ~ drat > 3.5, ~ drat, bold = TRUE)
myft <- autofit(myft)

myft
```

```{r eval=FALSE, include=FALSE, echo=TRUE}
library(officer)


ft <- regulartable(head(mtcars))
ft <- theme_booktabs(ft)
ft <- autofit(ft)

ppt <- read_pptx()
ppt <- add_slide(ppt, layout = "Title and Content", master = "Office Theme")
ppt <- ph_with_flextable(ppt, value = ft, type = "body") 

print(ppt, target = "output/example.pptx")

# https://view.officeapps.live.com/op/view.aspx?src=https://davidgohel.github.io/flextable/articles/assets/pptx/example.pptx

```





```{r eval=FALSE, include=FALSE, echo=TRUE}
doc <- read_docx()
doc <- body_add_flextable(doc, value = ft)
print(doc, target = "output/example.docx")
```






- Make Beautiful Tables with the Formattable Package

https://www.displayr.com/formattable/?utm_medium=Feed&utm_source=Syndication



















####  - knitr::kable()


####  - formatttable

```{r eval=FALSE, include=FALSE, echo=TRUE}
library(formattable)

p <- percent(c(0.1, 0.02, 0.03, 0.12))
p
p + 0.05
max(p)

```

```{r eval=FALSE, include=FALSE, echo=TRUE}
balance <- accounting(c(1000, 500, 200, -150, 0, 1200))
balance

balance + 1000

```


```{r eval=FALSE, include=FALSE, echo=TRUE}
p <- data.frame(
  id = c(1, 2, 3, 4, 5), 
  name = c("A1", "A2", "B1", "B2", "C1"),
  balance = accounting(c(52500, 36150, 25000, 18300, 7600), format = "d"),
  growth = percent(c(0.3, 0.3, 0.1, 0.15, 0.15), format = "d"),
  ready = formattable(c(TRUE, TRUE, FALSE, FALSE, TRUE), "yes", "no"))
p
print.AsIs(p)
```

```{r eval=FALSE, include=FALSE, echo=TRUE}
df <- data.frame(
  id = 1:10,
  name = c("Bob", "Ashley", "James", "David", "Jenny", 
    "Hans", "Leo", "John", "Emily", "Lee"), 
  age = c(28, 27, 30, 28, 29, 29, 27, 27, 31, 30),
  grade = c("C", "A", "A", "C", "B", "B", "B", "A", "C", "C"),
  test1_score = c(8.9, 9.5, 9.6, 8.9, 9.1, 9.3, 9.3, 9.9, 8.5, 8.6),
  test2_score = c(9.1, 9.1, 9.2, 9.1, 8.9, 8.5, 9.2, 9.3, 9.1, 8.8),
  final_score = c(9, 9.3, 9.4, 9, 9, 8.9, 9.25, 9.6, 8.8, 8.7),
  registered = c(TRUE, FALSE, TRUE, FALSE, TRUE, TRUE, TRUE, FALSE, FALSE, FALSE),
  stringsAsFactors = FALSE)
```









```{r eval=FALSE, include=FALSE, echo=TRUE}


library(formattable)

formattable(df, list(
  age = color_tile("white", "orange"),
  grade = formatter("span", style = x ~ ifelse(x == "A", 
    style(color = "green", font.weight = "bold"), NA)),
  area(col = c(test1_score, test2_score)) ~ normalize_bar("pink", 0.2),
  final_score = formatter("span",
    style = x ~ style(color = ifelse(rank(-x) <= 3, "green", "gray")),
    x ~ sprintf("%.2f (rank: %02d)", x, rank(-x))),
  registered = formatter("span",
    style = x ~ style(color = ifelse(x, "green", "red")),
    x ~ icontext(ifelse(x, "ok", "remove"), ifelse(x, "Yes", "No")))
))
```



####  - knitLatex



####  - htmlTable


####  - psytabs


####  - SortableHTMLTables


####  - tablaxlsx


####  - table1xls


####  - tableHTML


####  - TableMonster


####  - texreg


####  - ztable


####  - apaStyle


####  - apaTables


####  - apsrtable


####  - DT

https://www.infoworld.com/video/91607/r-tip-quick-interactive-tables


https://rstudio.github.io/DT/


https://datatables.net/


```{r eval=FALSE, include=FALSE, echo=TRUE}
# install.packages('DT')
```


#### - gtsummary

https://github.com/vincentarelbundock/gtsummary





### - higher-level functionality

#### - finalfit




https://github.com/ewenharrison/finalfit

http://www.datasurg.net/2018/05/16/elegant-regression-results-tables-and-plots-the-finalfit-package/


```{r eval=FALSE, include=FALSE, echo=TRUE}
# devtools::install_github("ewenharrison/finalfit")
# install.packages("rstan")
# install.packages("boot")

library(tidyverse)
library(finalfit)
library(rstan)
library(boot)

```




```{r eval=FALSE, include=FALSE, echo=TRUE}
# Load example dataset, modified version of survival::colon
data(colon_s)



colon_s2$age <- as.numeric(colon_s$age)
colon_s2$age.factor <- as_factor(colon_s$age.factor)
colon_s2$sex.factor <- as_factor(colon_s$sex.factor)
colon_s2$obstruct.factor <- as_factor(colon_s$obstruct.factor)
colon_s2$perfor.factor <- as_factor(colon_s$perfor.factor)
colon_s2$mort_5yr <- as_factor(colon_s$mort_5yr)
colon_s2$hospital <- as_factor(colon_s$hospital)
colon_s2$status <- as.numeric(colon_s$status)
colon_s2$time <- as.numeric(colon_s$time)
colon_s2


```


```{r eval=FALSE, include=FALSE, echo=TRUE}
# Table 1 - Patient demographics by variable of interest ----
explanatory = c("age", "age.factor", 
  "sex.factor", "obstruct.factor")
dependent = "perfor.factor" # Bowel perforation
table <- colon_s2 %>%
  summary_factorlist(dependent, explanatory,
  p=TRUE, add_dependent_label=TRUE, total_col = TRUE)

print.AsIs(table)



```




```{r eval=FALSE, include=FALSE, echo=TRUE}
# Table 2 - 5 yr mortality ----
explanatory = c("age.factor", 
  "sex.factor",
  "obstruct.factor")
dependent = 'mort_5yr'
table <- colon_s %>%
  summary_factorlist(dependent, explanatory, 
  p=TRUE, add_dependent_label=TRUE)

print.AsIs(table)

```




```{r results='asis', eval=FALSE, include=FALSE, echo=TRUE}
knitr::kable(table, row.names=FALSE, 
    align=c("l", "l", "r", "r", "r", "r"))
```



```{r eval=FALSE, include=FALSE, echo=TRUE}
explanatory = c("age.factor", "sex.factor", 
  "obstruct.factor", "perfor.factor")
dependent = 'mort_5yr'
table_surv <- colon_s2 %>%
  finalfit(dependent, explanatory)
```




```{r results='asis', eval=FALSE, include=FALSE, echo=TRUE}
knitr::kable(table_surv, row.names = FALSE, 
    align = c("l", "l", "r", "r", "r", "r"))
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
explanatory = c("age.factor", "sex.factor", 
  "obstruct.factor", "perfor.factor")
explanatory_multi = c("age.factor", 
  "obstruct.factor")
dependent = 'mort_5yr'
colon_s2 %>%
  finalfit(dependent, explanatory, 
  explanatory_multi)
```




```{r eval=FALSE, include=FALSE, echo=TRUE}
explanatory = c("age.factor", "sex.factor", 
  "obstruct.factor", "perfor.factor")
explanatory_multi = c("age.factor", "obstruct.factor")
random_effect = "hospital"
dependent = 'mort_5yr'
colon_s2 %>%
  finalfit(dependent, explanatory, 
  explanatory_multi, random_effect)
```




```{r eval=FALSE, include=FALSE, echo=TRUE}

explanatory = c("age.factor", "sex.factor", 
"obstruct.factor", "perfor.factor")
dependent = "Surv(time, status)"
colon_s2 %>%
  finalfit(dependent, explanatory)
```





```{r eval=FALSE, include=FALSE, echo=TRUE}
explanatory = c("age.factor", "sex.factor", 
  "obstruct.factor", "perfor.factor")
dependent = 'mort_5yr'
table7 <- colon_s2 %>%
  finalfit(dependent, explanatory, 
  metrics=TRUE)
```





```{r, eval=FALSE, include=FALSE, echo=TRUE, results="asis"}
knitr::kable(table7[[1]], row.names=FALSE, align=c("l", "l", "r", "r", "r"))
knitr::kable(table7[[2]], row.names=FALSE)
```





```{r eval=FALSE, include=FALSE, echo=TRUE}
explanatory = c("age.factor", "sex.factor", 
  "obstruct.factor", "perfor.factor")
explanatory_multi = c("age.factor", "obstruct.factor")
random_effect = "hospital"
dependent = 'mort_5yr'

# Separate tables
colon_s2 %>%
  summary_factorlist(dependent, 
  explanatory, fit_id=TRUE) -> example.summary

colon_s2 %>%
  glmuni(dependent, explanatory) %>%
  fit2df(estimate_suffix=" (univariable)") -> example.univariable

colon_s2 %>%
  glmmulti(dependent, explanatory) %>%
  fit2df(estimate_suffix=" (multivariable)") -> example.multivariable

colon_s2 %>%
  glmmixed(dependent, explanatory, random_effect) %>%
  fit2df(estimate_suffix=" (multilevel)") -> example.multilevel

# Pipe together
example.summary %>%
  finalfit_merge(example.univariable) %>%
  finalfit_merge(example.multivariable) %>%
  finalfit_merge(example.multilevel) %>%
  select(-c(fit_id, index)) %>% # remove unnecessary columns
  dependent_label(colon_s2, dependent, prefix="") # place dependent variable label
```







```{r fig.height=5, fig.width=6, eval=FALSE, include=FALSE, echo=TRUE, warning=FALSE, message=FALSE}
# OR plot
explanatory = c("age.factor", "sex.factor", 
  "obstruct.factor", "perfor.factor")
dependent = 'mort_5yr'
colon_s2 %>%
  or_plot(dependent, explanatory)
# Previously fitted models (`glmmulti()` or 
# `glmmixed()`) can be provided directly to `glmfit`
```


```{r fig.height=5, fig.width=6, eval=FALSE, include=FALSE, echo=TRUE}
# HR plot
explanatory = c("age.factor", "sex.factor", 
  "obstruct.factor", "perfor.factor")
dependent = "Surv(time, status)"
colon_s2 %>%
  hr_plot(dependent, explanatory, dependent_label = "Survival")
# Previously fitted models (`coxphmulti`) can be provided directly using `coxfit`
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
# KM plot
explanatory = c("perfor.factor")
dependent = "Surv(time, status)"

plotKM <- colon_s2 %>%
  surv_plot(dependent, explanatory, 
  xlab = "Time (days)", pval = TRUE, legend = "none")

plotKM

```




####   - janitor


####   - huxtable

https://hughjonesd.github.io/huxtable/design-principles.html



####   - tables


####   - stargazer


####   - pixiedust

pixiedust: Tables so Beautifully Fine-Tuned You Will Believe It's Magic

https://cran.r-project.org/web/packages/pixiedust/vignettes/pixiedust.html

https://cran.r-project.org/web/packages/pixiedust/vignettes/advancedMagic.html






####   - reporttools


####   - rtable


####   - summarytools


####   - tab


####   - tableone


####   - carpenter


https://cran.r-project.org/web/packages/carpenter/vignettes/carpenter.html



####   - dtables


####   - etable

####   - tangram
tangram (grammar of tables)

https://github.com/spgarbet/tangram
http://htmlpreview.github.io/?https://github.com/spgarbet/tg/blob/master/vignettes/example.html

https://github.com/spgarbet/tangram/issues/36


<!-- library(tangram) -->
<!-- library(Hmisc) -->
<!-- getHdata(pbc) -->
<!-- View(pbc) -->
<!-- table <- tangram(drug ~ bili + albumin + stage + protime + sex + age + spiders, data = pbc) -->

<!-- table -->
<!-- html5(table) -->
<!-- latex(table) -->
<!-- index(table) -->

<!-- write( -->
<!-- html5(tangram("drug ~ bili[2] + albumin + stage::Categorical + protime + sex + age + spiders", pbc, msd=TRUE, quant=seq(0, 1, 0.25)), -->
<!--       fragment=TRUE, inline="hmisc.css", caption = "HTML5 Table Hmisc Style", id="tbl2"), -->
<!-- "tangram1.html") -->

<!-- write( -->
<!-- html5(tangram("drug ~ bili[2] + albumin + stage::Categorical + protime + sex + age + spiders", pbc), -->
<!--       fragment=TRUE, inline="nejm.css", caption = "HTML5 Table NEJM Style", id="tbl3"), -->
<!-- "tangram_nejm.html") -->


<!-- tbl <- tangram("drug ~ bili[2] + albumin + stage::Categorical[1] + protime + sex[1] + age + spiders[1]",  -->
<!--                data=pbc, -->
<!--                pformat = 5) -->
<!-- write(html5(tbl, -->
<!--       fragment=TRUE, -->
<!--       inline="lancet.css", -->
<!--       caption = "HTML5 Table Lancet Style", id="tbl4" -->
<!-- ), -->
<!-- "tangram_lancet.html") -->

<!-- index(tangram("drug ~ bili + albumin + stage::Categorical + protime + sex + age + spiders", pbc))[1:20,] -->


<!-- library(readxl) -->
<!-- MDL307_Data <- read_excel("MDL307 - Data.xlsx") -->

<!-- MDL307_Data <- as.data.frame(MDL307_Data) -->

<!-- names(MDL307_Data) -->

<!-- View(MDL307_Data) -->

<!-- MDL307_Data$biyokimyasalrekurrens <- as.factor(MDL307_Data$biyokimyasalrekurrens) -->
<!-- levels(MDL307_Data$biyokimyasalrekurrens)[1] <- "yok" -->
<!-- levels(MDL307_Data$biyokimyasalrekurrens)[2] <- "var" -->

<!-- collist <- c("gleasonskor", -->
<!--                  "tersiyer", -->
<!--                  "kribriform", -->
<!--                  "cerrahisinir", -->
<!--                  "ekstaprostatik", -->
<!--                  "lenfnodu", -->
<!--                  "seminalvezikul" -->
<!--                  ) -->


<!-- MDL307_Data[collist] <- lapply(MDL307_Data[collist], as.factor) -->


<!-- table <- tangram(biyokimyasalrekurrens ~ yas + -->
<!--                  gleasonskor + -->
<!--                  tersiyer + -->
<!--                  kribriform + -->
<!--                  kribriformyuzde + -->
<!--                  cerrahisinir + -->
<!--                  ekstaprostatik + -->
<!--                  lenfnodu + -->
<!--                  seminalvezikul + -->
<!--                  biyokimyasalrekurrens, -->
<!--                  data = MDL307_Data) -->
<!-- table -->


<!-- ## SemiCompRisks -->

<!-- install.packages("SemiCompRisks") -->
<!-- library(SemiCompRisks) -->
<!-- data(BMT) -->

<!-- ## smcure -->

<!-- install.packages("smcure") -->
<!-- library(smcure) -->
<!-- data(e1684) -->




### - pivots


####   - rpivotttable

### - other


####   - gtable


#### - sjtlm
https://strengejacke.github.io/sjPlot/articles/sjtlm.html


#### - arsenal

to compare data frames: https://cran.r-project.org/web/packages/arsenal/vignettes/compare.html

##### - arsenal::paired

summary statistics for a set of variables paired across two time points: 

https://cran.r-project.org/web/packages/arsenal/vignettes/paired.html


```{r eval=FALSE, include=FALSE, echo=TRUE}
library(arsenal)
dat <- data.frame(
  tp = paste0("Time Point ", c(1, 2, 1, 2, 1, 2, 1, 2, 1, 2)),
  id = c(1, 1, 2, 2, 3, 3, 4, 4, 5, 6),
  Cat = c("A", "A", "A", "B", "B", "B", "B", "A", NA, "B"),
  Fac = factor(c("A", "B", "C", "A", "B", "C", "A", "B", "C", "A")),
  Num = c(1, 2, 3, 4, 4, 3, 3, 4, 0, NA),
  Ord = ordered(c("I", "II", "II", "III", "III", "III", "I", "III", "II", "I")),
  Lgl = c(TRUE, TRUE, FALSE, TRUE, FALSE, TRUE, TRUE, FALSE, FALSE, FALSE),
  Dat = as.Date("2018-05-01") + c(1, 1, 2, 2, 3, 4, 5, 6, 3, 4),
  stringsAsFactors = FALSE
)
```

```{r eval=FALSE, include=FALSE, echo=TRUE}
dat
```


```{r include=FALSE, eval=FALSE, comment=NA, prompt=FALSE, cache=FALSE, echo=TRUE, results='asis'}
p <- paired(tp ~ Cat + Fac + Num + Ord + Lgl + Dat, data = dat, id = id, signed.rank.exact = FALSE)
summary(p)
```


##### - arsenal::freqlist

https://cran.r-project.org/web/packages/arsenal/vignettes/freqlist.html

```{r eval=FALSE, include=FALSE, echo=TRUE}
require(arsenal)
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
# load the data
data(mockstudy)

# retain NAs when creating the table using the useNA argument
tab.ex <- table(mockstudy[, c("arm", "sex", "mdquality.s")], useNA = "ifany")
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
tab.ex
```

```{r eval=FALSE, include=FALSE, echo=TRUE}
noby <- freqlist(tab.ex)

str(noby)
```

```{r eval=FALSE, include=FALSE, echo=TRUE}
# view the data frame portion of freqlist output
head(noby[["freqlist"]])  ## or use as.data.frame(noby)
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
summary(noby)
```


summary(noby, title = "Basic freqlist output")

summary(freqlist(~ arm + sex + mdquality.s, data = mockstudy, addNA = TRUE))

summary(freqlist(~arm + sex + addNA(mdquality.s), data = mockstudy))

summary(freqlist(~arm + sex + includeNA(mdquality.s, "Missing"), data = mockstudy))

```
withnames <- freqlist(tab.ex, labelTranslations = c("Treatment Arm", "Gender", "LASA QOL"), digits = 0)
summary(withnames)
```



##### - arsenal::tableby

https://cran.r-project.org/web/packages/arsenal/vignettes/tableby.html


##### - arsenal::modelsum

https://cran.r-project.org/web/packages/arsenal/vignettes/modelsum.html


#### - desctable


https://cran.r-project.org/web/packages/desctable/vignettes/desctable.html


# GT

https://github.com/rstudio/gt


```{r eval=FALSE, include=FALSE, echo=TRUE}
install.packages("devtools")
remotes::install_github("rstudio/gt")
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
library(gt)
library(tidyverse)
library(glue)

# Define the start and end dates for the data range
start_date <- "2010-06-07"
end_date <- "2010-06-14"

# Create a gt table based on preprocessed
# `sp500` table data
sp500 %>%
  dplyr::filter(date >= start_date & date <= end_date) %>%
  dplyr::select(-adj_close) %>%
  dplyr::mutate(date = as.character(date)) %>%
  gt() %>%
  tab_header(
    title = "S&P 500",
    subtitle = glue::glue("{start_date} to {end_date}")
  ) %>%
  fmt_date(
    columns = vars(date),
    date_style = 3
  ) %>%
  fmt_currency(
    columns = vars(open, high, low, close),
    currency = "USD"
  ) %>%
  fmt_number(
    columns = vars(volume),
    scale_by = 1 / 1E9,
    pattern = "{x}B"
  )
```






# sparklines

R tip: Sparklines in HTML tables  
https://www.infoworld.com/video/91867/r-tip-sparklines-in-html-tables  





<!--chapter:end:Table.Rmd-->

---
title: "Tensorflow"
output: html_notebook
---


https://ai.google/education/

 

https://github.com/tensorflow/workshops

 

https://experiments.withgoogle.com/collection/ai

 

https://developers.google.com/machine-learning/crash-course/

 

https://ai.google/education/responsible-ai-practices

 

https://research.google.com/seedbank/

 

https://codelabs.developers.google.com/codelabs/end-to-end-ml/index.html#0



<!--chapter:end:tensorflow.Rmd-->

---
title: "Text Mining"
output: html_notebook
---


https://regexr.com/

https://regex101.com/

# Read text files with readtext()

https://cran.r-project.org/web/packages/readtext/vignettes/readtext_vignette.html


# qdapRegex

```{r eval=FALSE, include=FALSE, echo=TRUE}
library(qdapRegex)
```


https://github.com/trinker/qdapRegex

http://trinker.github.io/qdapRegex_dev/index.html

# stringr

https://cran.r-project.org/web/packages/stringr/vignettes/stringr.html

```
stringr::str_view()
```


# RegExplain

https://www.garrickadenbuie.com/project/regexplain/


```{r eval=FALSE, include=FALSE, echo=TRUE}
# devtools::install_github("gadenbuie/regexplain")
# source("https://install-github.me/gadenbuie/regexplain")
```



# A Tidy Text Analysis of My Google Search History

https://www.r-bloggers.com/a-tidy-text-analysis-of-my-google-search-history/



# Text Mining with R A Tidy Approach

https://www.tidytextmining.com/

# Pride and Prejudice

https://juliasilge.com/blog/tidy-text-classification/

https://juliasilge.com/blog/if-i-loved-nlp-less/


# cleanNLP

https://statsmaths.github.io/cleanNLP/

# Text analysis of holy books

## quRan 

https://github.com/andrewheiss/quRan

https://github.com/andrewheiss/quRan/blob/master/data-raw/clean_data.R


### Tidy text, parts of speech, and unique words in the Qur'an

https://www.andrewheiss.com/blog/2018/12/28/tidytext-pos-arabic/


## sacred

http://sacred.john-coene.com/

https://github.com/JohnCoene/sacred

## scriptuRs 

https://github.com/andrewheiss/scriptuRs


### Tidy text, parts of speech, and unique words in the Bible

https://www.andrewheiss.com/blog/2018/12/26/tidytext-pos-john/



```{r eval=FALSE, include=FALSE, echo=TRUE}
# devtools::install_github("andrewheiss/scriptuRs")
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
library(tidyverse)  # For dplyr, ggplot2, and friends
library(scriptuRs)  # For full text of bible
library(tidytext)   # For analyzing text
library(cleanNLP)   # For fancier natural language processing

# Load data
gospels <- kjv_bible() %>% 
  filter(book_title %in% c("Matthew", "Mark", "Luke", "John"))
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
# Set up NLP backend
# reticulate::use_python("/Users/serdarbalciold/.conda")  # I use homebrew python3
# reticulate::use_condaenv(condaenv = "/anaconda3/envs", conda = "auto", required = FALSE)
# cnlp_init_spacy()  # Use spaCy
cnlp_init_udpipe()  # Or use this R-only one without external dependencies
```



```{r eval=FALSE, include=FALSE, echo=TRUE}
# Determine the parts of speech of the "text" column and use "verse_title" as the id
gospels_annotated <- cnlp_annotate(gospels, as_strings = TRUE,
                                   text_var = "text", doc_var = "verse_title")
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
gospels_annotated
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
glimpse(gospels_annotated)
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
gospel_terms <- gospels_annotated %>% 
  cnlp_get_token()
head(gospel_terms)
```

```{r eval=FALSE, include=FALSE, echo=TRUE}
gospels_lookup <- gospels %>% 
  select(verse_title, book_title, chapter_number, verse_number)

gospel_terms <- gospel_terms %>% 
  left_join(gospels_lookup, by = c("id" = "verse_title"))

glimpse(gospel_terms)
```


# WORD ASSOCIATIONS FROM THE SMALL WORLD OF WORDS

https://juliasilge.com/blog/word-associations/


# 










<!--chapter:end:TextMining.Rmd-->

---
title: "The Lesser Known Stars of the Tidyverse"
---

```
slug: the-lesser-known-stars-of-the-tidyverse
tags:
- tidyverse
- R
- Code
categories: []
output: html_notebook
```


*I copied this code just to learn myself. See original links below:*


# Webinar: Tidyverse Exploratory Analysis (Emily Robinson)

<iframe src="https://www.facebook.com/plugins/video.php?href=https%3A%2F%2Fwww.facebook.com%2F726282547396228%2Fvideos%2F584417861986887%2F&show_text=1&width=560" width="560" height="529" style="border:none;overflow:hidden" scrolling="no" frameborder="0" allowTransparency="true" allow="encrypted-media" allowFullScreen="true"></iframe>  


<iframe width="560" height="315" src="https://www.youtube.com/embed/uG3igAGX7UE" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>  



<iframe width="560" height="315" src="https://www.youtube.com/embed/ax4LXQ5t38k" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>  


https://hookedondata.org/the-lesser-known-stars-of-the-tidyverse/  

https://www.rstudio.com/resources/videos/the-lesser-known-stars-of-the-tidyverse/  

https://github.com/robinsones/robinsones_blog/blob/master/content/post/multipleChoiceResponses.csv  

https://github.com/robinsones/robinsones_blog/blob/master/content/post/2018-11-16-the-lesser-known-stars-of-the-tidyverse.Rmd  


## Reading in the data

```
{r eval=FALSE, include=FALSE, echo=TRUE}
knitr::opts_chunk$set(echo = TRUE, warning = TRUE, message = FALSE, error = TRUE)

library(tidyverse)
library(magrittr)
theme_set(theme_bw())
```




```
{r eval=FALSE, include=FALSE, echo=TRUE}
multiple_choice_responses_base <- read.csv("data/multipleChoiceResponses.csv")
```



```
{r eval=FALSE, include=FALSE, echo=TRUE}
# for one column
sum(is.na(multiple_choice_responses_base$Country))
```




```
{r eval=FALSE, include=FALSE, echo=TRUE}
# for five columns
multiple_choice_responses_base %>%
  summarise_at(1:5, ~sum(is.na(.))) 

multiple_choice_responses_base %>%
  summarise_at(vars(GenderSelect:StudentStatus), ~sum(is.na(.))) 

multiple_choice_responses_base %>%
  summarise_at(vars(GenderSelect, Age), ~sum(is.na(.))) 

```



```
{r eval=FALSE, include=FALSE, echo=TRUE}
multiple_choice_responses_base %>%
  count(StudentStatus) 
```

Yep. We see here we have a lot of `""` entries instead of NAs. We can correct this with `na_if` from `dplyr`, which takes as an argument what we want to turn into NAs. We can also use `%<>%`, which is a reassignment pipe. While this is nice to save some typing, it can make it confusing when reading a script, so use with caution. 

```
{r eval=FALSE, include=FALSE, echo=TRUE}
multiple_choice_responses_base %<>%
  na_if("")

## is the same as: 

multiple_choice_responses_base <- multiple_choice_responses_base %>%
  na_if("")
```

Now let's count the NAs again. 

```
{r eval=FALSE, include=FALSE, echo=TRUE}
multiple_choice_responses_base %>%
  summarise_at(1:5, ~sum(is.na(.))) 
```

And it's fixed! 

How could we have avoided this all in the first place? By using `{r # eadr::read_csv` instead of `{r # ead.csv`.

If you're not familiar with `::`, it's for explicitly setting what package you're getting the function on the right from. This is helpful in three ways:

  1) There can be name conflicts, where two packages have functions with the same name. Using `::` ensures you're getting the function you want. 
  2) if you only want to use one function from a package, you can use `::` to skip the library call. As long as you've installed the package, you don't need to have loaded it to get the function. 
  3) For teaching purposes, it's nice to remind people where the function is coming from. 

```
{r eval=FALSE, include=FALSE, echo=TRUE}
multiple_choice_responses <- readr::read_csv("multipleChoiceResponses.csv")
```


It's definitely faster, but it seems we have some errors. Let's inspect them. 

```
{r eval=FALSE, include=FALSE, echo=TRUE}
problems(multiple_choice_responses)
```

We see each row and column where a problem occurs. What's happening is that `{r # ead_csv` uses the first 1000 rows of a column to guess its type. But in some cases, it's guessing the column is an integer, because the first 1000 rows are whole numbers, when actually it should be double, as some entries have decimal points. We can fix this by changing the number of rows `{r # ead_csv` uses to guess the column type (with the `guess_max` argument) to the number of rows in the data set. 

```
{r eval=FALSE, include=FALSE, echo=TRUE}
multiple_choice_responses <- readr::read_csv("multipleChoiceResponses.csv", 
                                             guess_max = nrow(multiple_choice_responses))
```

Error-free!

## Initial examination 

Let's see what we can glean from the column names themselves. I'll only look at the first 20 since there are so many. 

```
{r eval=FALSE, include=FALSE, echo=TRUE}
colnames(multiple_choice_responses) %>%
  head(20)
```

We can see that there were categories of questions, like "LearningPlatform," with each platform having its own column.

Now let's take a look at our numeric columns with [skimr](https://github.com/ropensci/skimr). Skimr is a package from [rOpenSci](https://ropensci.org/) that allows you to quickly view summaries of your data. We  can use `select_if`  to select only columns where a certain condition, in this case whether it's a numeric column, is true.  

```
{r eval=FALSE, include=FALSE, echo=TRUE}
multiple_choice_responses %>%
  select_if(is.numeric) %>%
  skimr::skim()
```

I love the histograms. We can quickly see from them that people self teach a lot and spend a good amount of time building models and gathering data, compared to visualizing data or working in production.   

Let's see how many distinct answers we have for each question. We can use `n_distinct()`, a shorter and faster version of `length(unique())`. We'll use `summarise_all`, which is the same as `summarise_at` except that you don't select a group of columns and so it applies to every one in the dataset.

```
{r eval=FALSE, include=FALSE, echo=TRUE}
multiple_choice_responses %>%
  summarise_all(n_distinct) %>%
  select(1:10)
```

This data would be more helpful if it was tidy and had two columns, `question` and `num_distinct_answers`. We can use `tidyr::gather` to change our data from "wide" to "long" format and then `arrange` it so we can see the columns with the most distinct answers first. If you've used (or are still using!) reshape2, check out tidyr; reshape2 is retired and is only updated with changes necessary for it to remain on CRAN. While not exactly equivalent, `tidyr::spread` replaces `{r # eshape2::dcast`, `tidyr::separate` `{r # eshape2::colsplit`, and `tidyr::gather` `{r # eshape2::melt`. 

```
{r eval=FALSE, include=FALSE, echo=TRUE}
multiple_choice_responses %>%
  summarise_all(n_distinct) %>%
  tidyr::gather(question, num_distinct_answers) %>%
  arrange(desc(num_distinct_answers))
```

Let's take a look at the question with the most distinct answers, WorkMethodsSelect. 

```
{r eval=FALSE, include=FALSE, echo=TRUE}
multiple_choice_responses %>%
  count(WorkMethodsSelect, sort = TRUE)
```

We can see this is a multiple select question, where if a person selected multiple answers they're listed as one entry, separated by commas. Let's tidy it up. 

First, let's get rid of the NAs. We can use `!is.na(WorkMethodsSelect)`, short for `is.na(WorkMethodsSelect) == FALSE`, to filter out NAs. We then use `str_split`, from stringr, to divide the entries up. `str_split(WorkMethodsSelect, ",")`  says "Take this string and split it into a list by dividing it where there are `,`s." 

```
{r eval=FALSE, include=FALSE, echo=TRUE}
nested_workmethods <- multiple_choice_responses %>%
  select(WorkMethodsSelect) %>%
  filter(!is.na(WorkMethodsSelect)) %>%
  mutate(work_method = str_split(WorkMethodsSelect, ",")) 

nested_workmethods %>%
  select(work_method)
```

Now we have a list column, with each entry in the list being one work method. We can `unnest` this so we can get back a tidy dataframe. 

```
{r eval=FALSE, include=FALSE, echo=TRUE}
unnested_workmethods <- nested_workmethods %>%
  tidyr::unnest(work_method) %>%
  select(work_method)

unnested_workmethods
```

Great! As a last step, let's `count` this data so we can find which are the most common work methods people use. 

```
{r eval=FALSE, include=FALSE, echo=TRUE}
unnested_workmethods %>%
  count(work_method, sort = TRUE)
```

We see the classic methods of data visualization, logistic regression, and cross-validation lead the pack. 

### Graphing Frequency of Different Work Challenges 

Now let's move on to understanding what challenges people face at work. This was one of those categories where there were multiple questions asked, all having names starting with `WorkChallengeFrequency` and ending with the challenge (e.g "DirtyData"). 

We can find the relevant columns by using the dplyr `select` helper `contains`. We then use `gather` to tidy the data for analysis, filter for only the non-NAs, and remove the `WorkChallengeFrequency` from each question using `stringr::str_remove`. 

```
{r eval=FALSE, include=FALSE, echo=TRUE}
WorkChallenges <- multiple_choice_responses %>%
  select(contains("WorkChallengeFrequency")) %>%
  gather(question, response) %>%
  filter(!is.na(response)) %>%
  mutate(question = stringr::str_remove(question, "WorkChallengeFrequency")) 

WorkChallenges
```

Let's make a facet bar plot, one for each question with the frequency of responses.To make the x-axis tick labels readable, we'll change them to be vertical instead of horizontal. 

```
{r WorkChallenges_graph1, fig.width = 9, fig.height = 6}
ggplot(WorkChallenges, aes(x = response)) + 
  geom_bar() + 
  facet_wrap(~question) + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

This graph has two main problems. First, there are too many histograms for it to be really useful. But second, the order of the x-axis is wrong. We want it to go from least often to most, but instead `{r # arely` is in the middle. We can manually reorder the level of this variable using `forcats::fct_relevel`. 

```
{r WorkChallenges_graph2, fig.width = 9, fig.height = 6}
WorkChallenges %>%
  mutate(response = fct_relevel(response, "Rarely", "Sometimes", 
                                "Often", "Most of the time")) %>%
  ggplot(aes(x = response)) + 
  geom_bar() + 
  facet_wrap(~question) + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

Now we've got the x-axis in the order we want it. Let's try dichotomizing the variable by grouping "most of the time" and "often" together as the person considering something a challenge. We can use `if_else` and `%in%`. `%in%` is equivalent to `{r # esponse == "Most of the time" | response == "Often"` and can save you a lot of typing if you have a bunch of variables to match. 

Grouping by the question, we can use `summarise` to reduce the dataset to one row per question, adding the variable `perc_problem` for the percentage of responses that thought something was a challenge often or most of the time. This way, we can make one graph with data for all the questions and easily compare them. 

```
{r eval=FALSE, include=FALSE, echo=TRUE}
perc_problem_work_challenge <- WorkChallenges %>%
  mutate(response = if_else(response %in% c("Most of the time", "Often"), 1, 0)) %>%
  group_by(question) %>%
  summarise(perc_problem = mean(response)) 
```

```
{r perc_problem_work_challenge_graph, fig.width = 8, fig.height = 5}
ggplot(perc_problem_work_challenge, aes(x = question, y = perc_problem)) + 
  geom_point() +
  coord_flip()
```

This is better, but it's hard to read because the points are scattered all over the place. Although you can spot the highest one, you then have to track it back to the correct variable. And it's also hard to tell the order of the ones in the middle. 

We can use `forcats:fct_reorder` to change the x-axis to be ordered by another variable, in this case the y-axis. While we're at it, we can use `scale_y_continuous` and`scales::percent` to update our axis to display in percent and `labs` to change our axis labels. 

```
{r perc_problem_work_challenge_graph2, fig.width = 8, fig.height = 5}
ggplot(perc_problem_work_challenge, 
       aes(x = perc_problem, 
           y = fct_reorder(question, perc_problem))) + 
  geom_point() +
  scale_x_continuous(labels = scales::percent) + 
  labs(y = "Work Challenge", 
       x = "Percentage of people encountering challenge frequently")
```

Much better! You can now easily tell which work challenges are encountered most frequently. 

### Conclusion

I'm a big advocate of using and teaching the tidyverse for data analysis and visualization in R (it runs [in the family](http://varianceexplained.org/r/teach-tidyverse/)). In addition to doing these talks, I've released a DataCamp course on [Categorical Data in the Tidyverse](https://www.datacamp.com/courses/categorical-data-in-the-tidyverse). I walk through some of the functions in this course and more from forcats. It's part of the new [Tidyverse Fundamentals skill track](https://www.datacamp.com/tracks/tidyverse-fundamentals), which is suitable for people are new to R or those looking to switch to the tidyverse. Check it out and let us know what you think. 

Some other good resources for learning the tidyverse are Hadley Wickham and Garrett Grolemund's free [R for Data Science book](http://r4ds.had.co.nz/) and [RStudio's cheat sheets](https://www.rstudio.com/resources/cheatsheets/). If you have questions, I recommend using the [tidyverse section of RStudio community](https://community.rstudio.com/c/tidyverse) and/or the #rstats hashtag on Twitter. If you do, make sure you include a reproducible example (see best practices [here](https://reprex.tidyverse.org/articles/reprex-dos-and-donts.html)) with the [reprex package](https://reprex.tidyverse.org/articles/articles/magic-reprex.html)!


<!--chapter:end:the-lesser-known-stars-of-the-tidyverse.Rmd-->

---
title: "tidycells"
output: html_notebook
---

# tidyxl

https://github.com/nacnudus/tidyxl




# tidycells

```{r eval=FALSE, include=FALSE, echo=TRUE}
# remotes::install_github("nacnudus/tidyxl")
# remotes::install_github("r-rudra/tidycells")
# install.packages("tidycells")
# install.packages("tidyxl")
library(tidycells)
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
system.file("extdata", "marks.xlsx", package = "tidycells", mustWork = TRUE)
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
# you should have tidyxl installed
system.file("extdata", "marks.xlsx", package = "tidycells", mustWork = TRUE) %>% 
  read_cells()
```




<!--chapter:end:tidycells.Rmd-->

---
title: "Tufte Handout"
subtitle: "An implementation in R Markdown"
author: "JJ Allaire and Yihui Xie"
date: "`{r #  Sys.Date()`"
output:
  tufte::tufte_html: default
  tufte::tufte_handout:
    citation_package: natbib
    latex_engine: xelatex
  tufte::tufte_book:
    citation_package: natbib
    latex_engine: xelatex
bibliography: skeleton.bib
link-citations: yes
---

```{r eval=FALSE, include=FALSE, echo=TRUE}
library(tufte)
# invalidate cache when the tufte version changes
knitr::opts_chunk$set(tidy = FALSE, cache.extra = packageVersion('tufte'))
options(htmltools.dir.version = FALSE)
```

# Introduction

The Tufte handout style is a style that Edward Tufte uses in his books and handouts. Tufte's style is known for its extensive use of sidenotes, tight integration of graphics with text, and well-set typography. This style has been implemented in LaTeX and HTML/CSS^[See Github repositories [tufte-latex](https://github.com/tufte-latex/tufte-latex) and [tufte-css](https://github.com/edwardtufte/tufte-css)], respectively. We have ported both implementations into the [**tufte** package](https://github.com/rstudio/tufte). If you want LaTeX/PDF output, you may use the `tufte_handout` format for handouts, and `tufte_book` for books. For HTML output, use `tufte_html`. These formats can be either specified in the YAML metadata at the beginning of an R Markdown document (see an example below), or passed to the `{r # markdown::render()` function. See @R-rmarkdown for more information about **rmarkdown**.

```yaml
---
title: "An Example Using the Tufte Style"
author: "John Smith"
output:
  tufte::tufte_handout: default
  tufte::tufte_html: default
---
```

There are two goals of this package:

1. To produce both PDF and HTML output with similar styles from the same R Markdown document;
1. To provide simple syntax to write elements of the Tufte style such as side notes and margin figures, e.g. when you want a margin figure, all you need to do is the chunk option `fig.margin = TRUE`, and we will take care of the details for you, so you never need to think about `\begin{marginfigure} \end{marginfigure}` or `<span class="marginfigure"> </span>`; the LaTeX and HTML code under the hood may be complicated, but you never need to learn or write such code.

If you have any feature requests or find bugs in **tufte**, please do not hesitate to file them to https://github.com/rstudio/tufte/issues. For general questions, you may ask them on StackOverflow: http://stackoverflow.com/tags/rmarkdown.

# Headings

This style provides first and second-level headings (that is, `#` and `##`), demonstrated in the next section. You may get unexpected output if you try to use `###` and smaller headings.

`{r #  newthought('In his later books')`^[[Beautiful Evidence](http://www.edwardtufte.com/tufte/books_be)], Tufte starts each section with a bit of vertical space, a non-indented paragraph, and sets the first few words of the sentence in small caps. To accomplish this using this style, call the `newthought()` function in **tufte** in an _inline R expression_ `` `{r #  ` `` as demonstrated at the beginning of this paragraph.^[Note you should not assume **tufte** has been attached to your R session. You should either `library(tufte)` in your R Markdown document before you call `newthought()`, or use `tufte::newthought()`.]

# Figures

## Margin Figures

Images and graphics play an integral role in Tufte's work. To place figures in the margin you can use the **knitr** chunk option `fig.margin = TRUE`. For example:

```{r fig-margin, fig.margin = TRUE, fig.cap = "MPG vs horsepower, colored by transmission.", fig.width=3.5, fig.height=3.5, cache=TRUE, message=FALSE}
library(ggplot2)
mtcars2 <- mtcars
mtcars2$am <- factor(
  mtcars$am, labels = c('automatic', 'manual')
)
ggplot(mtcars2, aes(hp, mpg, color = am)) +
  geom_point() + geom_smooth() +
  theme(legend.position = 'bottom')
```

Note the use of the `fig.cap` chunk option to provide a figure caption. You can adjust the proportions of figures using the `fig.width` and `fig.height` chunk options. These are specified in inches, and will be automatically scaled down to fit within the handout margin.

## Arbitrary Margin Content

In fact, you can include anything in the margin using the **knitr** engine named `marginfigure`. Unlike R code chunks ```` ```{r eval=FALSE, include=FALSE, echo=TRUE} ````, you write a chunk starting with ```` ```{marginfigure} ```` instead, then put the content in the chunk. See an example on the right about the first fundamental theorem of calculus.

```{marginfigure}
We know from _the first fundamental theorem of calculus_ that for $x$ in $[a, b]$:
$$\frac{d}{dx}\left( \int_{a}^{x} f(u)\,du\right)=f(x).$$
```

For the sake of portability between LaTeX and HTML, you should keep the margin content as simple as possible (syntax-wise) in the `marginefigure` blocks. You may use simple Markdown syntax like `**bold**` and `_italic_` text, but please refrain from using footnotes, citations, or block-level elements (e.g. blockquotes and lists) there.

Note: if you set `echo = TRUE` in your global chunk options, you will have to add `echo = TRUE` to the chunk to display a margin figure, for example ```` ```{marginfigure, echo = TRUE} ````.

## Full Width Figures

You can arrange for figures to span across the entire page by using the chunk option `fig.fullwidth = TRUE`.

```{r fig-fullwidth, fig.width = 10, fig.height = 2, fig.fullwidth = TRUE, fig.cap = "A full width figure.", warning=FALSE, message=FALSE, cache=TRUE}
ggplot(diamonds, aes(carat, price)) + geom_smooth() +
  facet_grid(~ cut)
```

 Other chunk options related to figures can still be used, such as `fig.width`, `fig.cap`, `out.width`, and so on. For full width figures, usually `fig.width` is large and `fig.height` is small. In the above example, the plot size is $10 \times 2$.

## Main Column Figures

Besides margin and full width figures, you can of course also include figures constrained to the main column. This is the default type of figures in the LaTeX/HTML output.

```{r fig-main, fig.cap = "A figure in the main column.", cache=TRUE}
ggplot(diamonds, aes(cut, price)) + geom_boxplot()
```

# Sidenotes

One of the most prominent and distinctive features of this style is the extensive use of sidenotes. There is a wide margin to provide ample room for sidenotes and small figures. Any use of a footnote will automatically be converted to a sidenote. ^[This is a sidenote that was entered using a footnote.] 

If you'd like to place ancillary information in the margin without the sidenote mark (the superscript number), you can use the `margin_note()` function from **tufte** in an inline R expression. `{r #  margin_note("This is a margin note.  Notice that there is no number preceding the note.")` This function does not process the text with Pandoc, so Markdown syntax will not work here. If you need to write anything in Markdown syntax, please use the `marginfigure` block described previously.

# References

References can be displayed as margin notes for HTML output. For example, we can cite R here [@R-base]. To enable this feature, you must set `link-citations: yes` in the YAML metadata, and the version of `pandoc-citeproc` should be at least 0.7.2. You can always install your own version of Pandoc from http://pandoc.org/installing.html if the version is not sufficient. To check the version of `pandoc-citeproc` in your system, you may run this in R:

```{r eval=FALSE, include=FALSE, echo=TRUE}
system2('pandoc-citeproc', '--version')
```

If your version of `pandoc-citeproc` is too low, or you did not set `link-citations: yes` in YAML, references in the HTML output will be placed at the end of the output document.

# Tables

You can use the `kable()` function from the **knitr** package to format tables that integrate well with the rest of the Tufte handout style. The table captions are placed in the margin like figures in the HTML output.

```{r eval=FALSE, include=FALSE, echo=TRUE}
knitr::kable(
  mtcars[1:6, 1:6], caption = 'A subset of mtcars.'
)
```

# Block Quotes

We know from the Markdown syntax that paragraphs that start with `>` are converted to block quotes. If you want to add a right-aligned footer for the quote, you may use the function `quote_footer()` from **tufte** in an inline R expression. Here is an example:

> "If it weren't for my lawyer, I'd still be in prison. It went a lot faster with two people digging."
>
> `{r #  tufte::quote_footer('--- Joe Martin')`

Without using `quote_footer()`, it looks like this (the second line is just a normal paragraph):

> "Great people talk about ideas, average people talk about things, and small people talk about wine."
>
> --- Fran Lebowitz

# Responsiveness

The HTML page is responsive in the sense that when the page width is smaller than 760px, sidenotes and margin notes will be hidden by default. For sidenotes, you can click their numbers (the superscripts) to toggle their visibility. For margin notes, you may click the circled plus signs to toggle visibility.

# More Examples

The rest of this document consists of a few test cases to make sure everything still works well in slightly more complicated scenarios. First we generate two plots in one figure environment with the chunk option `fig.show = 'hold'`:

```{r fig-two-together, fig.cap="Two plots in one figure environment.", fig.show='hold', cache=TRUE, message=FALSE}
p <- ggplot(mtcars2, aes(hp, mpg, color = am)) +
  geom_point()
p
p + geom_smooth()
```

Then two plots in separate figure environments (the code is identical to the previous code chunk, but the chunk option is the default `fig.show = 'asis'` now):

```{r fig-two-separate, ref.label='fig-two-together', fig.cap=sprintf("Two plots in separate figure environments (the %s plot).", c("first", "second")), cache=TRUE, message=FALSE}
```

You may have noticed that the two figures have different captions, and that is because we used a character vector of length 2 for the chunk option `fig.cap` (something like `fig.cap = c('first plot', 'second plot')`).

Next we show multiple plots in margin figures. Similarly, two plots in the same figure environment in the margin:

```{r fig-margin-together, fig.margin=TRUE, fig.show='hold', fig.cap="Two plots in one figure environment in the margin.", fig.width=3.5, fig.height=2.5, cache=TRUE}
p
p + geom_smooth(method = 'lm')
```

Then two plots from the same code chunk placed in different figure environments:

```{r fig-margin-separate, fig.margin=TRUE, fig.cap=sprintf("Two plots in separate figure environments in the margin (the %s plot).", c("first", "second")), fig.width=3.5, fig.height=2.5, cache=TRUE}
knitr::kable(head(iris, 15))
p
knitr::kable(head(iris, 12))
p + geom_smooth(method = 'lm')
knitr::kable(head(iris, 5))
```

We blended some tables in the above code chunk only as _placeholders_ to make sure there is enough vertical space among the margin figures, otherwise they will be stacked tightly together. For a practical document, you should not insert too many margin figures consecutively and make the margin crowded. 

You do not have to assign captions to figures. We show three figures with no captions below in the margin, in the main column, and in full width, respectively.

```{r fig-nocap-margin, fig.margin=TRUE, fig.width=3.5, fig.height=2, cache=TRUE}
# a boxplot of weight vs transmission; this figure
# will be placed in the margin
ggplot(mtcars2, aes(am, wt)) + geom_boxplot() +
  coord_flip()
```
```{r fig-nocap-main, cache=TRUE}
# a figure in the main column
p <- ggplot(mtcars, aes(wt, hp)) + geom_point()
p
```
```{r fig-nocap-fullwidth, fig.fullwidth=TRUE, fig.width=10, fig.height=3, cache=TRUE}
# a fullwidth figure
p + geom_smooth(method = 'lm') + facet_grid(~ gear)
```

# Some Notes on Tufte CSS

There are a few other things in Tufte CSS that we have not mentioned so far. If you prefer `{r #  sans_serif('sans-serif fonts')`, use the function `sans_serif()` in **tufte**. For epigraphs, you may use a pair of underscores to make the paragraph italic in a block quote, e.g.

> _I can win an argument on any topic, against any opponent. People know this, and steer clear of me at parties. Often, as a sign of their great respect, they don't even invite me._
>
> `{r #  quote_footer('--- Dave Barry')`

We hope you will enjoy the simplicity of R Markdown and this R package, and we sincerely thank the authors of the Tufte-CSS and Tufte-LaTeX projects for developing the beautiful CSS and LaTeX classes. Our **tufte** package would not have been possible without their heavy lifting.

You can turn on/off some features of the Tufte style in HTML output. The default features enabled are:

```yaml
output:
  tufte::tufte_html:
    tufte_features: ["fonts", "background", "italics"]
```

If you do not want the page background to be lightyellow, you can remove `background` from `tufte_features`. You can also customize the style of the HTML page via a CSS file. For example, if you do not want the subtitle to be italic, you can define

```css
h3.subtitle em {
  font-style: normal;
}
```

in, say, a CSS file `my_style.css` (under the same directory of your Rmd document), and apply it to your HTML output via the `css` option, e.g.,

```yaml
output:
  tufte::tufte_html:
    tufte_features: ["fonts", "background"]
    css: "my_style.css"
```

There is also a variant of the Tufte style in HTML/CSS named "[Envisoned CSS](http://nogginfuel.com/envisioned-css/)". This style can be used by specifying the argument `tufte_variant = 'envisioned'` in `tufte_html()`^[The actual Envisioned CSS was not used in the **tufte** package. We only changed the fonts, background color, and text color based on the default Tufte style.], e.g.

```yaml
output:
  tufte::tufte_html:
    tufte_variant: "envisioned"
```

To see the R Markdown source of this example document, you may follow [this link to Github](https://github.com/rstudio/tufte/raw/master/inst/rmarkdown/templates/tufte_html/skeleton/skeleton.Rmd), use the wizard in RStudio IDE (`File -> New File -> R Markdown -> From Template`), or open the Rmd file in the package:

```{r eval=FALSE, include=FALSE, echo=TRUE}
file.edit(
  tufte:::template_resources(
    'tufte_html', '..', 'skeleton', 'skeleton.Rmd'
  )
)
```

This document is also available in [Chinese](http://rstudio.github.io/tufte/cn/), and its `envisioned` style can be found [here](http://rstudio.github.io/tufte/envisioned/).

```{r bib, include=FALSE}
# create a bib file for the R packages used in this document
knitr::write_bib(c('base', 'rmarkdown'), file = 'skeleton.bib')
```

<!--chapter:end:tuftedoc.Rmd-->

---
title: "Tutorials"
output: html_notebook
---


- Exploratory Data Analysis & Data Preparation with 'funModeling'

https://blog.datascienceheroes.com/exploratory-data-analysis-data-preparation-with-funmodeling/


- Content for "Wrangling data in the Tidyverse" - a tutorial given at useR! 2018

https://github.com/drsimonj/tidyverse_tutorial-useR2018


- Teaching R to New Users - From tapply to the Tidyverse

https://simplystatistics.org/2018/07/12/use-r-keynote-2018/


- jstor

An R Package for Analysing Scientific Articles 

https://speakerdeck.com/tklebel/jstor-an-r-package-for-analysing-scientific-articles?slide=25

- workshop in survival analysis using R

https://github.com/dave-harrington/survival_workshop

https://github.com/dave-harrington/eventtimedata

- Teaching R to New Users - From tapply to the Tidyverse

https://simplystatistics.org/2018/07/12/use-r-keynote-2018/

- Data Driven Decision Making (D3M)

http://www.vishalsingh.org/teaching/#content

- Tidymodeling Titanic Tragedy

https://cdn.rawgit.com/ClaytonJY/tidymodels-talk/145e6574/slides.html#1

- Disease risk modelling and visualization using R

https://paula-moraga.github.io/teaching/

- Creating a Geodemographic Classification Using K-means Clustering in R 

https://data.cdrc.ac.uk/tutorial/creating-a-geodemographic-classification-using-k-means-clustering-in-r


- Advanced R

http://adv-r.had.co.nz/


- How to use R for matching samples (propensity score)

https://datascienceplus.com/how-to-use-r-for-matching-samples-propensity-score/






<!--chapter:end:Tutorials.Rmd-->

---
title: "tweetbook1"
---


https://rtweet.info/



```
{r include=FALSE, eval=FALSE, echo = TRUE}
library(tidyverse)
library(rtweet)
```

# get data

```
{r include=FALSE, eval=FALSE, echo = TRUE}
gipath <- rtweet::search_tweets(q = "#gipath",
                                n = 18000,
                                include_rts = FALSE,
                                # retryonratelimit = TRUE
                                )

# gipath %>% 
#   select(user_id,status_id, contains("url")) %>%
#   filter(!is.na(ext_media_url)) %>% 
# View()

```


# filter tweet

```
{r filter tweet}

for (i in 1:dim(gipath)[1]) {
  nam <- paste0("gitweetid", i)
  assign(nam, gipath$status_id[i])
  nam2 <- paste0("gitweet", i)
  assign(nam2, gipath[gipath$status_id==gitweetid1, ])
}

```


# tweet owner

```
{r eval=FALSE, include=FALSE, echo=TRUE}
gitweet1$screen_name
```

# tweet time


```
{r eval=FALSE, include=FALSE, echo=TRUE}
gitweet1$created_at
```


# tweet text

```
{r eval=FALSE, include=FALSE, echo=TRUE}
gitweet1$text
```


# tweet media


```
{r tweet media}
unlist(gitweet2$ext_media_url)
```


```
{r tweet media length}
length(unlist(gitweet2$ext_media_url))
```

```
{r eval=FALSE, include=FALSE, echo=TRUE}

```


# download image, read image

```
{r download image read image, error=FALSE, comment=FALSE}
for (i in 1:length(unlist(gitweet2$ext_media_url))) {
    urls <- unlist(gitweet2$ext_media_url)[i]
    dest <- paste0("twfigure/jpeg",i,".jpg", collapse = "")
    download.file(url = urls, destfile = dest, mode = 'wb')
}

images <- capture.output(
    cat(
        for (i in 1:length(unlist(gitweet2$ext_media_url))) {
            cat(
                paste0("![](twfigure/jpeg",
                       i,
                       ".jpg){width=30%}", 
                       collapse = ""
                ),
                "\n"
            )
        }
        , 
        sep = "\n")
)
```

---
    
# Auto Output

Tweet by: **`{r #  gitweet2$screen_name`**

Tweet Time: **`{r #  gitweet2$created_at`**

*`{r #  gitweet2$text`*

`{r #  unlist(gitweet2$hashtags)`

`{r #  unlist(gitweet2$symbols)`

`{r #  unlist(gitweet2$urls_expanded_url)`

`{r #  images`

---
    
    
```
{r eval=FALSE, include=FALSE, echo=TRUE}
tweetbody1 <- "gitweet1$screen_name; gitweet1$created_at; gitweet1$text; unlist(gitweet1$hashtags); unlist(gitweet1$symbols); unlist(gitweet1$urls_expanded_url)"

evaluate::replay(evaluate::evaluate(tweetbody1))


```

```
{r eval=FALSE, include=FALSE, echo=TRUE}
library(evaluate)
s <-  paste(capture.output(replay(evaluate::evaluate(tweetbody1))), collapse="\n")
cat(s)
```


```
{r eval=FALSE, include=FALSE, echo=TRUE}
dont_print_source = function(x){
    if (class(x)!="source"){
        cat(x)
    }
}

L <-  evaluate::evaluate(tweetbody1)

# library(R.utils)
# s3 <-  paste(captureOutput(
# for(i in 1:length(L)) dont_print_source(L[[i]])  
# ), collapse="\n")

s3 <- gsub("\\[1]|\"", "", paste(capture.output(
for(i in 1:length(L)) dont_print_source(L[[i]])
), collapse="\n"))

```




`{r #  s3`

---

https://community.rstudio.com/t/how-to-use-an-object-rather-than-a-file-as-source-for-knitting-resolved/3057/6

---

```
{r eval=FALSE, include=FALSE, echo=TRUE}
tweetAutoOutput <- function(x) {
  
  
  
}
```


Tweet by: **`{r #  gitweet2$screen_name`**

Tweet Time: **`{r #  gitweet2$created_at`**

*`{r #  gitweet2$text`*

`{r #  unlist(gitweet2$hashtags)`

`{r #  unlist(gitweet2$symbols)`

`{r #  unlist(gitweet2$urls_expanded_url)`

`{r #  images`





---

---

---

---

---





# olders


rtweet::get_collections("serdarbalci")






---

```
{r eval=FALSE, include=FALSE, echo=TRUE}
gitweet1 <- gipath %>%
  filter(user_id == 3011337389)
```


```
{r eval=FALSE, include=FALSE, echo=TRUE}
gitweet1$user_id
```

```
{r eval=FALSE, include=FALSE, echo=TRUE}
gitweet1$status_id
```

```
{r eval=FALSE, include=FALSE, echo=TRUE}
gitweet1$created_at
```


```
{r eval=FALSE, include=FALSE, echo=TRUE}
gitweet1$screen_name
```

```
{r eval=FALSE, include=FALSE, echo=TRUE}
gitweet1$ext_media_url
```

```
{r eval=FALSE, include=FALSE, echo=TRUE}
length(gitweet1$ext_media_url)
```

---

```
{r eval=FALSE, include=FALSE, echo=TRUE}
imageurl1 <- gitweet1$ext_media_url[[1]][1]
```


```
{r eval=FALSE, include=FALSE, echo=TRUE}
imageurl1
```

```
{r eval=FALSE, include=FALSE, echo=TRUE}
imageurl2 <- gitweet1$ext_media_url[[1]][2]
```


```
{r eval=FALSE, include=FALSE, echo=TRUE}
imageurl2
```



---


<!-- ![](http://pbs.twimg.com/media/DzjHIuEXgAEqNL3.jpg) -->


<!-- ![](http://pbs.twimg.com/media/DzjHIuFXgAER5Op.jpg) -->

---


```
{r eval=FALSE, include=FALSE, echo=TRUE}
gitweet1$text
```

```
{r eval=FALSE, include=FALSE, echo=TRUE}
gitweet1$source
```



```
{r eval=FALSE, include=FALSE, echo=TRUE}
gitweet1$display_text_width
```

```
{r eval=FALSE, include=FALSE, echo=TRUE}
gitweet1$reply_to_status_id
```

```
{r eval=FALSE, include=FALSE, echo=TRUE}
gitweet1$reply_to_status_id
```

---

```
{r eval=FALSE, include=FALSE, echo=TRUE}
gitweet1$reply_to_user_id
```

```
{r eval=FALSE, include=FALSE, echo=TRUE}
gitweet1$reply_to_screen_name
```

---

```
{r eval=FALSE, include=FALSE, echo=TRUE}
gitweet1$is_quote
```

```
{r eval=FALSE, include=FALSE, echo=TRUE}
gitweet1$is_retweet
```

---

```
{r eval=FALSE, include=FALSE, echo=TRUE}
gitweet1$favorite_count
```


```
{r eval=FALSE, include=FALSE, echo=TRUE}
gitweet1$retweet_count
```


```
{r eval=FALSE, include=FALSE, echo=TRUE}
gitweet1$hashtags
```


```
{r eval=FALSE, include=FALSE, echo=TRUE}
gitweet1$symbols
```

---

```
{r eval=FALSE, include=FALSE, echo=TRUE}
gitweet1$urls_url
```

```
{r eval=FALSE, include=FALSE, echo=TRUE}
gitweet1$urls_t.co
```


```
{r eval=FALSE, include=FALSE, echo=TRUE}
gitweet1$urls_expanded_url
```



```
{r eval=FALSE, include=FALSE, echo=TRUE}
gitweet1$media_url
```


```
{r eval=FALSE, include=FALSE, echo=TRUE}
gitweet1$media_t.co
```


```
{r eval=FALSE, include=FALSE, echo=TRUE}
gitweet1$media_expanded_url
```

---

```{r eval=FALSE, include=FALSE, echo=TRUE}
gitweet1$media_type
```



```{r eval=FALSE, include=FALSE, echo=TRUE}
gitweet1$ext_media_url
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
gitweet1$ext_media_t.co
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
gitweet1$ext_media_expanded_url
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
gitweet1$ext_media_type
```

---

```{r eval=FALSE, include=FALSE, echo=TRUE}
gitweet1$mentions_user_id
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
gitweet1$mentions_screen_name
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
gitweet1$lang
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
gitweet1$quoted_status_id
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
gitweet1$quoted_text
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
gitweet1$quoted_created_at
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
gitweet1$quoted_source
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
gitweet1$quoted_favorite_count
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
gitweet1$quoted_retweet_count
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
gitweet1$quoted_user_id
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
gitweet1$quoted_screen_name
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
gitweet1$quoted_name
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
gitweet1$quoted_followers_count
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
gitweet1$quoted_friends_count
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
gitweet1$quoted_statuses_count
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
gitweet1$quoted_location
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
gitweet1$quoted_description
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
gitweet1$quoted_verified
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
gitweet1$retweet_status_id
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
gitweet1$retweet_text
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
gitweet1$retweet_created_at
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
gitweet1$retweet_source
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
gitweet1$retweet_favorite_count
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
gitweet1$retweet_retweet_count
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
gitweet1$retweet_user_id
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
gitweet1$retweet_screen_name
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
gitweet1$retweet_name
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
gitweet1$retweet_followers_count
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
gitweet1$retweet_friends_count
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
gitweet1$retweet_statuses_count
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
gitweet1$retweet_location
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
gitweet1$retweet_description
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
gitweet1$retweet_verified
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
gitweet1$place_url
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
gitweet1$place_name
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
gitweet1$place_full_name
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
gitweet1$place_type
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
gitweet1$country
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
gitweet1$country_code
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
gitweet1$geo_coords
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
gitweet1$coords_coords
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
gitweet1$bbox_coords
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
gitweet1$status_url
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
gitweet1$name
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
gitweet1$location
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
gitweet1$description
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
gitweet1$url
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
gitweet1$protected
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
gitweet1$followers_count
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
gitweet1$friends_count
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
gitweet1$listed_count
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
gitweet1$statuses_count
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
gitweet1$favourites_count
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
gitweet1$account_created_at
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
gitweet1$verified
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
gitweet1$profile_url
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
gitweet1$profile_expanded_url
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
gitweet1$account_lang
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
gitweet1$profile_banner_url
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
gitweet1$profile_background_url
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
gitweet1$profile_image_url
```

---

```{r eval=FALSE, include=FALSE, echo=TRUE}
# gipath %>%
#   select(user_id,status_id, contains("url")) %>%
#   filter(!is.na(ext_media_url)) %>%
#   select(status_id, contains("media_url")) %>%
# View()
```

---

```{r eval=FALSE, include=FALSE, echo=TRUE}
gitweet2 <- gipath %>% 
  filter(status_id ==1096465400848699392)
```

```{r eval=FALSE, include=FALSE, echo=TRUE}
gitweet2$ext_media_url
```

---

```{r eval=FALSE, include=FALSE, echo=TRUE}
unlist(gitweet2$ext_media_url)
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
length(unlist(gitweet2$ext_media_url))
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
# jpeg1 <- paste0("\"", unlist(gitweet2$ext_media_url)[1], "\"")

# jpeg1 <- unlist(gitweet2$ext_media_url)[1]
# jpeg2 <- unlist(gitweet2$ext_media_url)[2]
# jpeg3 <- unlist(gitweet2$ext_media_url)[3]
# jpeg4 <- unlist(gitweet2$ext_media_url)[4]

# <center><img src="`{r #  jpeg1`"><img src="`{r #  jpeg2`"></center>
```



---

```{r eval=FALSE, include=FALSE, echo=TRUE}
for (i in 1:length(unlist(gitweet2$ext_media_url))) {
  nam <- paste0("jpeg", i)
  assign(nam, unlist(gitweet2$ext_media_url)[i])
}
```

```{r eval=FALSE, include=FALSE, echo=TRUE}
download.file(url = jpeg1, destfile = 'twfigure/jpeg1.jpg', mode = 'wb')
```





dene1


---

dene2

<!-- ![](twfigure/jpeg1.jpg) -->





---

image 0

![](`{r #  jpeg1`)  ![](`{r #  jpeg2`)  ![](`{r #  jpeg3`) 



---

image 1


<img src="`{r #  jpeg1`">



---

image 2


<img src="`{r #  jpeg1`"><img src="`{r #  jpeg2`"><img src="`{r #  jpeg3`">


---

image 3

<center><img src="`{r #  jpeg1`"><img src="`{r #  jpeg2`"><img src="`{r #  jpeg3`"></center>

---




<!--chapter:end:tweetbook1.Rmd-->

---
title: "tweetrmd"
---

https://github.com/gadenbuie/tweetrmd


# tweetrmd


Easily embed Tweets anywhere R Markdown turns plain text into HTML.

## Installation

You can install the released version of **tweetrmd** from GitHub:

``` r
# install.packages("devtools")
devtools::install_github("gadenbuie/tweetrmd")
```

## Embed a Tweet

```
{r eval=FALSE, include=FALSE, echo=TRUE}
library(tweetrmd)

tweet_embed("https://twitter.com/alexpghayes/status/1211748406730706944")
```

Or if you would rather use the screen name and status id.

```
{r eval=FALSE, include=FALSE, echo=TRUE}
tweet_embed(tweet_url("alexpghayes", "1211748406730706944"))
```

## Take a screenshot of a tweet

Screenshots are automatically embedded in R Markdown documents,
or you can save the screenshot as a `.png` or `.pdf` file.
Uses the [rstudio/webshot2](https://github.com/rstudio/webshot2) package.

```
{r screenshot, out.width="400px"}
tweet_screenshot(tweet_url("alexpghayes", "1211748406730706944"))
```

## Customize tweet appearance

Twitter's [oembed API](https://developer.twitter.com/en/docs/tweets/post-and-engage/api-reference/get-statuses-oembed)
provides a number of options, 
all of which are made available for customization in `tweet_embed()` and `tweet_screenshot()`.

```
{r screenshot-customized, out.width="300px"}
tweet_screenshot(
  tweet_url("alexpghayes", "1211748406730706944"),
  maxwidth = 300,
  hide_media = TRUE,
  theme = "dark"
)
```

---

Note: When using `tweet_embed()`,
you may need to add the following line to your YAML header
for strict markdown output formats.


yaml
always_allow_html: true


<!--chapter:end:tweetrmd.Rmd-->

---
title: "R Notebook"
output: html_notebook
---


### API connection

http://rtweet.info/
https://apps.twitter.com/


```{r eval=FALSE, include=FALSE, echo=TRUE}
# install.packages("rtweet")
library(rtweet)
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
# web browser method: create token and save it as an environment variable
create_token(
    app = "",
     consumer_key = "",
     consumer_secret = "")


# ## authenticate via access token
# token <- create_token(
#   app = "",
#   consumer_key = "",
#   acess_token = "",
#   access_secret = "")

# 
# 
# ## create token and save it as an environment variable
# twitter_token <- create_token(
#   app = appname,
#   consumer_key = key,
#   consumer_secret = secret,
#   access_token = access_token,
#   access_secret = access_secret
# )
# 
# 
# ## check to see if the token is loaded
# identical(twitter_token, get_token())


```


### Search Tweets

```{r eval=FALSE, include=FALSE, echo=TRUE}
## search for 18000 tweets using the hashtag
rt <- search_tweets(
    "#GIpath", n = 18000, include_rts = FALSE
)
```

```{r eval=FALSE, include=FALSE, echo=TRUE}
## preview tweets data
head(rt)
dplyr::glimpse(rt)
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
## preview users data
users_data(rt)
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
## plot time series (if ggplot2 is installed)
ts_plot(rt)
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
## plot time series of tweets
ts_plot(rt, "3 hours") +
    ggplot2::theme_minimal() +
    ggplot2::theme(plot.title = ggplot2::element_text(face = "bold")) +
    ggplot2::labs(
        x = NULL, y = NULL,
        title = "Frequency of #pathologists Twitter statuses from past 9 days",
        subtitle = "Twitter status (tweet) counts aggregated using three-hour intervals",
        caption = "\nSource: Data collected from Twitter's REST API via rtweet"
    )
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
## search for 250,000 tweets containing the word data
rt1 <- search_tweets(
    "USCAP", n = 250000, retryonratelimit = TRUE
)
```

```{r eval=FALSE, include=FALSE, echo=TRUE}
## search for 10,000 tweets sent from the US
rt1 <- search_tweets(
    "lang:en", geocode = lookup_coords("usa"), n = 10000
)
```


```{r eval=FALSE, include=FALSE, echo=TRUE}

## create lat/lng variables using all available tweet and profile geo-location data
rt1 <- lat_lng(rt1)

## plot state boundaries
par(mar = c(0, 0, 0, 0))
maps::map("state", lwd = .25)

## plot lat and lng points onto state map
with(rt1, points(lng, lat, pch = 20, cex = .75, col = rgb(0, .3, .7, .75)))
```



### Stream Tweets

```{r eval=FALSE, include=FALSE, echo=TRUE}
## random sample for 30 seconds (default)
rt3 <- stream_tweets("")
```

```{r eval=FALSE, include=FALSE, echo=TRUE}
rt3
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
## stream tweets from london for 60 seconds
rt <- stream_tweets(lookup_coords("london, uk"), timeout = 60)


```


```{r eval=FALSE, include=FALSE, echo=TRUE}
## stream london tweets for a week (60 secs x 60 mins * 24 hours *  7 days)
# stream_tweets(
#     "realdonaldtrump,trump",
#     timeout = 60 * 60 * 24 * 7,
#     file_name = "tweetsabouttrump.json",
#     parse = FALSE
# )


stream_tweets(
    "realdonaldtrump,trump",
    timeout = 60 * 60 * 1 * 1,
    file_name = "tweetsabouttrump.json",
    parse = FALSE
)


```

```{r eval=FALSE, include=FALSE, echo=TRUE}
## read in the data as a tidy tbl data frame
djt <- parse_stream("tweetsabouttrump.json")
```

```{r eval=FALSE, include=FALSE, echo=TRUE}
djt
```




### Twitter connections and user information


```{r eval=FALSE, include=FALSE, echo=TRUE}
## get user IDs of accounts followed by CNN
# cnn_fds <- get_friends("cnn")

sb_fds <- get_friends("serdarbalci")

sb_fds
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
## lookup data on those accounts
# cnn_fds_data <- lookup_users(cnn_fds$user_id)

sb_fds_data <- lookup_users(sb_fds$user_id)
sb_fds_data
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
sb_follows <- sb_fds_data$screen_name
sb_follows[1:50]
```

```{r eval=FALSE, include=FALSE, echo=TRUE}
## get user IDs of accounts following CNN
# cnn_flw <- get_followers("cnn", n = 75000)

sb_flw <- get_followers("serdarbalci")
sb_flw

```

```{r eval=FALSE, include=FALSE, echo=TRUE}
## lookup data on those accounts
# cnn_flw_data <- lookup_users(cnn_flw$user_id)

sb_flw_data <- lookup_users(sb_flw$user_id[1:10])

sb_flw_data

```


```{r eval=FALSE, include=FALSE, echo=TRUE}
# ## how many total follows does cnn have?
# cnn <- lookup_users("cnn")
# 
# ## get them all (this would take a little over 5 days)
# cnn_flw <- get_followers(
#     "cnn", n = cnn$followers_count, retryonratelimit = TRUE
# )
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
## get user IDs of accounts followed by CNN
tmls <- get_timelines(c("cnn", "BBCWorld", "foxnews"), n = 3200)
```

```{r eval=FALSE, include=FALSE, echo=TRUE}
## plot the frequency of tweets for each user over time
tmls %>%
    dplyr::filter(created_at > "2017-10-29") %>%
    dplyr::group_by(screen_name) %>%
    ts_plot("days", trim = 1L) +
    ggplot2::geom_point() +
    ggplot2::theme_minimal() +
    ggplot2::theme(
        legend.title = ggplot2::element_blank(),
        legend.position = "bottom",
        plot.title = ggplot2::element_text(face = "bold")) +
    ggplot2::labs(
        x = NULL, y = NULL,
        title = "Frequency of Twitter statuses posted by news organization",
        subtitle = "Twitter status (tweet) counts aggregated by day from October/November 2017",
        caption = "\nSource: Data collected from Twitter's REST API via rtweet"
    )


```






```{r eval=FALSE, include=FALSE, echo=TRUE}
# jkr <- get_favorites("jk_rowling", n = 3000)

sbf <- get_favorites("serdarbalci", n = 3000)
sbf

```


```{r eval=FALSE, include=FALSE, echo=TRUE}
## search for users with #rstats in their profiles
# usrs <- search_users("#rstats", n = 1000)

path_usrs <- search_users("pathology", n = 1000)

```


```{r eval=FALSE, include=FALSE, echo=TRUE}
head(path_usrs)

```

### Twitter Trends


```{r eval=FALSE, include=FALSE, echo=TRUE}
sf <- get_trends("san francisco")
```

```{r eval=FALSE, include=FALSE, echo=TRUE}
sf
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
## lookup users by screen_name or user_id
users <- c("KimKardashian", "justinbieber", "taylorswift13",
           "espn", "JoelEmbiid", "cstonehoops", "KUHoops",
           "upshotnyt", "fivethirtyeight", "hadleywickham",
           "cnn", "foxnews", "msnbc", "maddow", "seanhannity",
           "potus", "epa", "hillaryclinton", "realdonaldtrump",
           "natesilver538", "ezraklein", "annecoulter")
famous_tweeters <- lookup_users(users)
```

```{r eval=FALSE, include=FALSE, echo=TRUE}
## preview users data
famous_tweeters

```

```{r eval=FALSE, include=FALSE, echo=TRUE}
# extract most recent tweets data from the famous tweeters
tweets_data(famous_tweeters)
```


### Post tweet via R



```{r eval=FALSE, include=FALSE, echo=TRUE}
# post_tweet("my first rtweet #rstats, let us see if it works :) http://rtweet.info/")
```

### Follow via R


```{r eval=FALSE, include=FALSE, echo=TRUE}

## ty for the follow ;)
post_follow("kearneymw")
```

```{r eval=FALSE, include=FALSE, echo=TRUE}
## quick overview of rtweet functions
vignette("auth", package = "rtweet")

## quick overview of rtweet functions
vignette("intro", package = "rtweet")


## working with the stream
vignette("stream", package = "rtweet")


## working with the stream
vignette("FAQ", package = "rtweet")

```



### Election Analysis on Twitter


```{r eval=FALSE, include=FALSE, echo=TRUE}
## Stream keywords used to filter tweets
q <- "hillaryclinton,imwithher,realdonaldtrump,maga,electionday"

## Stream time in seconds so for one minute set timeout = 60
## For larger chunks of time, I recommend multiplying 60 by the number
## of desired minutes. This method scales up to hours as well
## (x * 60 = x mins, x * 60 * 60 = x hours)
## Stream for 30 minutes
# streamtime <- 30 * 60
streamtime <- 3 * 60


## Filename to save json data (backup)
filename <- "rtelect.json"
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
## Stream election tweets
rt_election <- stream_tweets(q = q, timeout = streamtime, file_name = filename)
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
## No upfront-parse save as json file instead method
stream_tweets(
  q = q,
  parse = FALSE,
  timeout = streamtime,
  file_name = filename
)
```

```{r eval=FALSE, include=FALSE, echo=TRUE}
## Parse from json file
rt_election <- parse_stream(filename)
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
## stream_tweets2 method
# twoweeks <- 60L * 60L * 24L * 7L * 2L
# congress <- "congress,senate,house of representatives,representatives,senators,legislative"
# stream_tweets2(
#   q = congress,
#   parse = FALSE,
#   timeout = twoweeks,
#   dir = "congress-stream"
# )
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
## Parse from json file
rt <- parse_stream("congress-stream.json")
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
## Preview tweets data
rt

```

```{r eval=FALSE, include=FALSE, echo=TRUE}
## Preview users data
users_data(rt)
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
## Plot time series of all tweets aggregated by second
ts_plot(rt, by = "secs")
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
## plot multiple time series by first grouping the data by screen name
rt %>%
  dplyr::group_by(screen_name) %>%
  ts_plot() +
  ggplot2::labs(
    title = "Tweets during election day for the 2016 U.S. election",
    subtitle = "Tweets collected, parsed, and plotted using `{r # tweet`"
  )
```







### Graphing Tweets

http://graphtweets.john-coene.com/index.html

```{r eval=FALSE, include=FALSE, echo=TRUE}
install.packages("graphTweets") # CRAN release v0.4
library(graphTweets)
library(igraph) # for plot
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
# tweets <- rtweet::search_tweets("rstats")

tweets <- rtweet::search_tweets("#pathologists")

```


```{r eval=FALSE, include=FALSE, echo=TRUE}
tweets %>% 
    gt_edges(text, screen_name, status_id) %>% 
    gt_graph() %>% 
    plot()

```


```{r eval=FALSE, include=FALSE, echo=TRUE}
tweets %>% 
  gt_edges(text, screen_name, status_id) %>% 
  gt_graph() -> graph

class(graph)

```

```{r eval=FALSE, include=FALSE, echo=TRUE}
graph
```

```{r eval=FALSE, include=FALSE, echo=TRUE}
tweets %>% 
  gt_edges(text, screen_name, status_id) %>% 
  gt_collect() -> edges

names(edges)

```

```{r eval=FALSE, include=FALSE, echo=TRUE}
edges
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
tweets %>% 
  gt_edges(text, screen_name, status_id) %>% 
  gt_nodes() %>% 
  gt_collect() -> graph

graph
```



```{r eval=FALSE, include=FALSE, echo=TRUE}
lapply(graph, nrow) # number of edges and nodes
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
lapply(graph, names) # names of data.frames returned
```

```{r eval=FALSE, include=FALSE, echo=TRUE}
tweets %>% 
  gt_edges(text, screen_name, status_id) %>% 
  gt_nodes(meta = TRUE) %>% 
  gt_collect() -> graph

graph

# lapply(graph, names) # names of data.frames returned
```

```{r eval=FALSE, include=FALSE, echo=TRUE}
tweets %>% 
  gt_edges(text, screen_name, status_id, datetime = "created_at") %>% 
  gt_nodes(meta = TRUE) %>% 
  gt_collect() -> graph

graph

```


```{r eval=FALSE, include=FALSE, echo=TRUE}
# install.packages("sigmajs")
library(dplyr)
library(sigmajs) # for plots

tweets %>% 
  gt_edges(text, screen_name, status_id, datetime = "created_at") %>% 
  gt_nodes(meta = TRUE) %>% 
  gt_collect() -> gt

nodes <- gt$nodes %>% 
  mutate(
    id = nodes,
    label = ifelse(is.na(name), nodes, name),
    size = n_edges,
    color = "#1967be"
  ) 

edges <- gt$edges %>% 
  mutate(
    id = 1:n()
  )

sigmajs() %>% 
  sg_force_start() %>% 
  sg_nodes(nodes, id, label, size, color) %>% 
  sg_edges(edges, id, source, target) %>% 
  sg_force_stop(10000)
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
library(igraph)

tweets %>% 
  gt_edges(text, screen_name, status_id) %>% 
  gt_graph() -> g

# communities
wc <- walktrap.community(g)
V(g)$color <- membership(wc)

# plot
# tons of arrguments because defaults are awful
plot(g, 
     layout = igraph::layout.fruchterman.reingold(g), 
     vertex.color = V(g)$color,
     vertex.label.family = "sans",
     vertex.label.color = hsv(h = 0, s = 0, v = 0, alpha = 0.0),
     vertex.size = igraph::degree(g), 
     edge.arrow.size = 0.2, 
     edge.arrow.width = 0.3, edge.width = 1,
     edge.color = hsv(h = 1, s = .59, v = .91, alpha = 0.7),
     vertex.frame.color="#fcfcfc")
```



```{r eval=FALSE, include=FALSE, echo=TRUE}
tweets %>% 
  gt_edges(text, screen_name, status_id, "created_at") %>% 
  gt_nodes() %>% 
  gt_dyn() %>% 
  gt_collect() -> net
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
knitr::kable(head(net$edges))

```

```{r eval=FALSE, include=FALSE, echo=TRUE}
knitr::kable(head(net$nodes))

```

```{r eval=FALSE, include=FALSE, echo=TRUE}
library(sigmajs)

# convert to numeric & rescale
edges <- net$edges %>% 
  dplyr::mutate( 
    id = 1:n(),
    created_at = as.numeric(created_at),
    created_at = (created_at - min(created_at)) / (max(created_at) - min(created_at)),
    created_at = created_at * 5000
  )

nodes <- net$nodes %>% 
  dplyr::mutate(
    id = source,
    size = n_edges
  )

mx <- max(edges$created_at) + 500

sigmajs() %>% 
  sg_force_start() %>% 
  sg_nodes(nodes, id, size) %>% 
  sg_add_edges(edges, created_at, id, source, target, 
               cumsum = FALSE, refresh = FALSE) %>% 
  sg_force_stop(delay = mx) %>% 
  sg_settings(defaultNodeColor = "#1967be")
```




# A Shiny App with rtweet

https://aj17.shinyapps.io/twitteranalytics/


```{r eval=FALSE, include=FALSE, echo=TRUE}
library(blogdown)
blogdown::shortcode('tweet', '1014492923981803521')
```


# Conference tweets

https://twitter.com/APo_ORV/status/1016412207867973632

https://twitter.com/grrrck/status/959137137118646272

https://gadenbuie.shinyapps.io/rsconf_tweets/

https://github.com/gadenbuie/rsconf_tweets

https://behindbars.shinyapps.io/user2018/

https://github.com/oliviergimenez/isec2018_tweet_analysis




# Twitter Article Mentions and Citations

https://github.com/dsquintana/ajp


# twitterreport

https://github.com/gvegayon/twitterreport



# streamR

https://cran.r-project.org/web/packages/streamR/


---

- Retweet count for specific tweet

https://stackoverflow.com/questions/10427147/retweet-count-for-specific-tweet


---

# statquotes


```{r eval=FALSE, include=FALSE, echo=TRUE}
install.packages("statquotes")
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
statquotes::statquote()
```



# rtweet-workshop

https://github.com/mkearney/rtweet-workshop

https://rtweet-workshop.mikewk.com/



<!--chapter:end:Twitter.Rmd-->

---
title: "twitter dashboard"
output: html_notebook
---

# Making a twitter dashboard with R

https://jsta.rbind.io/blog/making-a-twitter-dashboard-with-r/

https://jsta.rbind.io/tweets


http://rtweet.info/articles/auth.html



```{r eval=FALSE, include=FALSE, echo=TRUE}
library(rtweet)
library(magrittr)
library(dplyr)
library(DT)
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
user_name <- "serdarbalci"
my_likes  <- get_favorites(user_name, n = 100) %>% 
  select("created_at", "screen_name", "text", "urls_expanded_url") %>%
  arrange(desc(created_at))
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
my_likes$created_at <- strptime(as.POSIXct(my_likes$created_at), 
                                format = "%Y-%m-%d")
my_likes$created_at <- format(my_likes$created_at, "%Y-%m-%d")
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
createLink <- function(x) {
  if(is.na(x)){
    return("")
  }else{
    sprintf(paste0('<a href="', URLdecode(x),'" target="_blank">', 
                   substr(x, 1, 25) ,'</a>'))
  }
}

my_likes$urls_expanded_url <- lapply(my_likes$urls_expanded_url, 
                                     function(x) sapply(x, createLink))
```



```{r eval=FALSE, include=FALSE, echo=TRUE}
my_table <- datatable(my_likes, 
                      options = list(scrollX = TRUE, autoWidth = TRUE,
                                     columnDefs = list(list(
                                           width = '70%', 
                                           targets = c(2)))), 
                      rownames = FALSE,
                      fillContainer = TRUE,
                      width = "100%", 
                      height = "100%",  
                      colnames = c("Date", "Handle", "Text", "URL"))

my_table <- formatStyle(my_table, columns = 1:4, fontSize = '70%')
my_table <- formatStyle(my_table, columns = 3, width = '500px')

```


```{r eval=FALSE, include=FALSE, echo=TRUE}
my_table
```



```{r eval=FALSE, include=FALSE, echo=TRUE}
library(widgetframe)

frameWidget(my_table, width = "100%", height = 800,
            options = frameOptions(allowfullscreen = TRUE))
```




```{r eval=FALSE, include=FALSE, echo=TRUE}
frameableWidget(my_table)
```














<!--chapter:end:TwitterDashboard.Rmd-->

---
title: "Visualisation: Graphs & Plots"
output: html_notebook
---

- Visualization of Biomedical Data
 
 https://www.annualreviews.org/doi/10.1146/annurev-biodatasci-080917-013424

- Regional population structures at a glance

https://ikashnitsky.github.io/2018/the-lancet-paper/

- Global Migration, animated with R

http://blog.revolutionanalytics.com/2018/06/global-migration-animated-with-r.html

- Creating a Geodemographic Classification Using K-means Clustering in R

https://data.cdrc.ac.uk/tutorial/creating-a-geodemographic-classification-using-k-means-clustering-in-r


- Tidy Eval Meets ggplot2

http://www.onceupondata.com/2018/07/06/ggplot-tidyeval/

- 
http://serialmentor.com/dataviz/

- You Can Design a Good Chart with R

https://towardsdatascience.com/you-can-design-a-good-chart-with-r-5d00ed7dd18e

- Data to Viz

https://www.data-to-viz.com/

[![](https://www.data-to-viz.com/img/poster/poster_big.png)](https://www.data-to-viz.com/)

-  ggpubr
ggpubr: ‘ggplot2’ Based Publication Ready Plots
http://www.sthda.com/english/rpkgs/ggpubr/

- R Graph Gallery

https://www.r-graph-gallery.com/

- The Data Visualisation Catalogue

https://datavizcatalogue.com/


- Dataviz Project

http://datavizproject.com/

-  Python Graph Gallery

https://python-graph-gallery.com/

- Financial Times Visual Vocabulary

https://github.com/ft-interactive/chart-doctor/tree/master/visual-vocabulary

- Xenographics Weird but (sometimes) useful charts

https://xeno.graphics/

-

https://vis.design/


- Show Me Shiny Gallery of R Web Apps

https://www.showmeshiny.com/

- Where Work Pays: Occupations & Earnings across the United States

http://www.hamiltonproject.org/charts/where_work_pays_interactive

- You Can Design a Good Chart with R But do R users invest in design?

https://towardsdatascience.com/you-can-design-a-good-chart-with-r-5d00ed7dd18e

- Machine Learning Results in R: one plot to rule them all!

https://datascienceplus.com/machine-learning-results-one-plot-to-rule-them-all/

- data visualisation applications, tools and libraries

http://www.visualisingdata.com/resources/

<!--chapter:end:VisualisationGraphsPlots.Rmd-->

---
title: "R Notebook"
---

- The ultimate online collection toolbox: Combining RSelenium and Rvest - Part 1


https://www.youtube.com/watch?v=OxbvFiYxEzI

https://gist.github.com/HanjoStudy/aeb331b7a277be9639f3cfb3bf875ba2

https://hanjostudy.github.io/Presentations/UseR2018/RSelenium/rselenium.html#1

https://hanjostudy.github.io/Presentations/UseR2018/Rvest/rvest.html#1



- Automated Text Feature Engineering using textfeatures in R

https://www.r-bloggers.com/automated-text-feature-engineering-using-textfeatures-in-r/


# Selectorgadget

https://cran.r-project.org/web/packages/rvest/vignettes/selectorgadget.html


# polite

https://github.com/dmi3kno/polite






<!--chapter:end:WebScrapping.Rmd-->

---
title: "Where to learn R"
---

# Introduction to Data Science

https://rafalab.github.io/dsbook/

# RStudio Education  

https://education.rstudio.com/  

# Statistics in Action with R  

http://sia.webpopix.org/index.html

# swirlstats  

https://swirlstats.com/


# Data Skills for Reproducible Science  


https://gupsych.github.io/data_skills/index.html


# rstats-ed


https://github.com/rstudio-education/rstats-ed


# new online courses psu

https://newonlinecourses.science.psu.edu/statprogram/


# Think Stats

https://greenteapress.com/wp/think-stats-2e/


# statsthinking21

http://statsthinking21.org/index.html


# RStudio  

https://www.rstudio.com/online-learning/


# r-statistics

http://r-statistics.co/R-Tutorial.html


# Advanced Data Science

https://jhu-advdatasci.github.io/2018/

# How Do I? …

https://smach.github.io/R4JournalismBook/HowDoI.html

# Data management and manipulation using R 

https://ozanj.github.io/rclass/resources/

# Teaching Reproducible Data Analysis in R

https://gupsych.github.io/trdair_workshop/

# Academic Websites

https://gupsych.github.io/acadweb/

# Data Skills for Reproducible Science

https://gupsych.github.io/data_skills/

# Methodology & Metascience 

https://gupsych.github.io/mms/

# Causal Inference Book

https://www.hsph.harvard.edu/miguel-hernan/causal-inference-book/

# IntroToR

https://github.com/sbalci/IntroToR.git


# YouTube Collections

---

<iframe width="560" height="315" src="https://www.youtube.com/embed/videoseries?list=PLxRBOaoEoP4KUSgmcNjSGBFu7PYipGVrX" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>


---

- Jamovi  
https://www.youtube.com/playlist?list=PLkk92zzyru5OAtc_ItUubaSSq6S_TGfRn

- Do More with R  

https://www.youtube.com/playlist?list=PL7D2RMSmRO9JOvPC1gbA8Mc3azvSfm8Vv

https://www.youtube.com/playlist?list=PLYaGSokOr0MPz1tgwTW4JKcelhdJyUIrb

- Neat proofs/perspectives

https://www.youtube.com/playlist?list=PLZHQObOWTQDPSKntUcMArGheySM4gL7wS

- Learning medical statistics with python and Jupyter notebooks

https://www.youtube.com/playlist?list=PLsu0TcgLDUiIueDMfTX3322AZhdGb0_zm

- Data Visualization and R

https://www.youtube.com/playlist?list=PLCj1LhGni3hPGy6Kj1AFxHYkKklxenO9D


# What They Forgot to Teach You About R  
  
https://whattheyforgot.org/




---

# Keeping up to date with R news  
https://masalmon.eu/2019/01/25/uptodate/



# Advanced R Markdown Workshop  
https://arm.rbind.io/days/day1/learnr/


# CRAN Task Views

https://cran.r-project.org/web/views/

# WikiBook R Programming

https://en.wikibooks.org/wiki/R_Programming


---

# Happy Git and GitHub for the useR

https://happygitwithr.com


---

https://r-bootcamp.netlify.com/


# How I Use R

https://howiuser.com








<!--chapter:end:WhereToLearnR.Rmd-->

---
title: "R Pirate YaRrr"
---


```{r eval=FALSE, include=FALSE, echo=TRUE}
install.packages("yarrr")
library("yarrr")
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
pirateplot(formula = weight ~ Time,
data = ChickWeight,
pal = "xmen")
```


```{r eval=FALSE, include=FALSE, echo=TRUE}
yarrr::pirateplot(formula = weight ~ Diet,
                  data = ChickWeight)
```






















<!--chapter:end:YaRrr.Rmd-->

