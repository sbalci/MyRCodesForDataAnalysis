---
title: "My R Codes For Data Analysis"
subtitle: "In this repository I am going to collect `R codes` for data analysis. Codes are from various resources and I try to give original link as much as possible."
author: "[Serdar Balcı, MD, Pathologist](https://www.serdarbalci.com/)"
date: '`r format(Sys.Date())`'
output: 
  html_notebook: 
    fig_caption: yes
    highlight: tango
    number_sections: yes
    theme: paper
    toc: yes
    toc_depth: 5
    toc_float: yes
  html_document: 
    code_folding: hide
    df_print: kable
    keep_md: yes
    number_sections: yes
    theme: cerulean
    toc: yes
    toc_float: yes
    highlight: kate
---


```{r setup, cache = F, warning = FALSE}
knitr::opts_chunk$set(
	error = TRUE,
	message = FALSE,
	warning = FALSE
)
```


**Load required packages**


`Load required packages`


- Load required packages


> Gerekli paketleri yükle 



```{r load required packages 1, message=FALSE, warning=FALSE}
library(tidyverse)
```



# Getting Data into R / Veriyi R'a yükleme

## Import Data

### Import using RStudio

### Import CSV File

```{r}
scabies <- read.csv(file = "http://datacompass.lshtm.ac.uk/607/2/S1-Dataset_CSV.csv", header = TRUE, sep = ",")
scabies
```

### Import TXT File

```{r}
ebola <- read.csv(file = "http://datacompass.lshtm.ac.uk/608/1/mmc1.txt", header = TRUE, sep = ",")
ebola
```


### Import Excel File

#### Sheets

<!-- # Sheets -->

<!-- library(tidyverse) -->
<!-- library(readxl) -->


<!-- path <- "ICPN_RAW.xls" -->
<!-- sheetlist <- path %>%  -->
<!--     excel_sheets() %>%  -->
<!--     set_names() %>%  -->
<!--     map(read_excel, skip = 1, path = path) -->

<!-- sheetlist <- sheetlist[-29] -->


<!-- sheetlist <- sheetlist %>% -->
<!--     reduce(left_join, by = "Respondant", .id = "id", suffix = c("1","2")) -->

<!-- names(sheetlist) -->

<!-- columnNames <- c( -->
<!--     "ID", -->
<!--     "DX", -->
<!--     "y/n", -->
<!--     "~plastic", -->
<!--     "WHO- 2010 classification", -->
<!--     "GRADE", -->
<!--     "INVASIVE CA", -->
<!--     "CELL LINEAGE", -->
<!--     "IPMN Lineage (PRE/MIN)", -->
<!--     "experience with this invasion", -->
<!--     "COMMENTS" -->
<!-- ) -->

<!-- columnNumbers <- rep(c(1:28), each = 11) -->

<!-- columnNames2 <- paste(columnNames, columnNumbers, sep = "") -->

<!-- columnNames2 <- c("Respondant", columnNames2) -->

<!-- names(sheetlist) <- columnNames2 -->



<!-- # "Respondant"	"ID"	"y/n"	"WHO- 2010 classification"	"GRADE"	"INVASIVE CA"	"CELL LINEAGE"	"IPMN Lineage (PRE/MIN)" -->

<!-- deneme <- sheetlist %>%  -->
<!--     select(starts_with("Respondant"), starts_with("GRADE")) -->







<!-- # deneme[deneme=="HIGH"] <- 3 -->
<!-- # deneme[deneme=="HIGH (focal)"] <- 3 -->
<!-- # deneme[deneme=="MODERATE"] <- 2 -->
<!-- # deneme[deneme=="LOW"] <- 1 -->
<!-- #  -->
<!-- # deneme <- as.data.frame(deneme) -->
<!-- #  -->
<!-- # row.names(deneme) <- deneme$Respondant -->
<!-- #  -->
<!-- # deneme <- deneme[-1] -->
<!-- #  -->
<!-- # deneme[is.na(deneme)] <- 1 -->
<!-- #  -->
<!-- # deneme <- map_df(deneme, as.matrix) -->
<!-- #  -->
<!-- # glimpse(deneme) -->
<!-- #  -->
<!-- # heatmap(deneme) -->

<!-- write_csv(deneme, "deneme.csv") -->






### Import SPSS File

## Export Data

### Export to SPSS, while keeping labels

R'da `factor` olan label verdiğiniz değişkenleri `SPSS` ya da diğer istatistik programlarına aktardığınızda bu tanımlamaları korumak işimize yarar. Bunun için `foreign` paketi ile bir `txt` dosyası ve bir `sps` dosyası oluşturuyoruz. SPSS'te `sps` dosyasını açıp kodu çalıştırarak tekrar atanan değerler geri yükleniyor.


```{r}
library(foreign)
write.foreign(mydata, "mydata.txt", "mydata.sps",   package = "SPSS")
```




## Prepare Data for Analysis / Veriyi Analiz için hazırlamak

### Keep SPSS labels

```{r}
library(foreign) # foreign paketi yükleniyor
```

read.spss komutu ile değer etiketlerini almasını ve bunu liste olarak değil de data.frame olarak kaydetmesini istiyoruz

```{r}
mydata <- read.spss("mydata.sav", use.value.labels = TRUE, to.data.frame = TRUE)
```

aktardığımız data.frame'in özellikleri (attr) içinde değişkenlerin etiketleri var, bunları dışarı çıkartıyoruz

```{r}
VariableLabels <- as.data.frame(attr(mydata, "variable.labels"))
```


elde ettiğimiz data.frame'deki satır isimleri değişkenlerin isimleri oluyor, karşılarında da değişken etiketleri var
satır isimlerini de dışarı çıkartıyoruz

```{r}
VariableLabels$original <- rownames(VariableLabels)
```

Değişken etiketi olanları etiketleri ile diğerlerini olduğu gibi saklıyoruz  

```{r}
VariableLabels$label[VariableLabels$label ==""] <- NA 
VariableLabels$colname <- VariableLabels$original
VariableLabels$colname[!is.na(VariableLabels$label)] <- as.vector(VariableLabels$label[!is.na(VariableLabels$label)])
```

son olarak da data.frame'deki sütun isimlerini değiştiriyoruz

```{r}
names(mydata) <- VariableLabels$colname
```


### Make both computer and human readible variable names

turkce karakter donusumu

```{r turkce karakter donusumu}
# https://suatatan.wordpress.com/2017/10/07/bulk-replacing-turkish-characters-in-r/
to.plain <- function(s) {
        # 1 character substitutions
    old1 <- "çğşıüöÇĞŞİÖÜ"
    new1 <- "cgsiuocgsiou"
    s1 <- chartr(old1, new1, s)
    # 2 character substitutions
    old2 <- c("œ", "ß", "æ", "ø")
    new2 <- c("oe", "ss", "ae", "oe")
    s2 <- s1
    for(i in seq_along(old2)) s2 <- gsub(old2[i], new2[i], s2, fixed = TRUE)
    s2
}


df$source=as.vector(sapply(df$source,to.plain))

makeNames(tolower(names(df)))
to.plain(names(df))

purrr::map(df, to.plain)

```

### Anonimisation

### Add subject ID to the data / veriye ID ekleme

```{r}
df <- tibble::rowid_to_column(df, "subject")
```




<!-- # gerekli paketi yükleme -->
<!-- library(tidyverse) -->
<!-- library(haven) -->
<!-- # dosyayı yükleme -->
<!-- EDT256 <- read_sav("EDT256.sav") -->



<!-- # gerekli sütunları seçme -->
<!-- EDT256 <- EDT256 %>% -->
<!--     select(subject, tanı, starts_with("beck")) -->

<!-- # değişken değerlerini yeniden atama -->
<!-- EDT256$tanı <- as_factor(EDT256$tanı, labels = "values") -->

<!-- # yatay veriyi uzun hale getirme -->
<!-- EDT256_new <- EDT256 %>% -->
<!--     gather(- c("subject","tanı"), key = zaman, value = beck_int_score) -->


<!-- # değişkenleri rakamsal hale getirme -->
<!-- EDT256_new$zaman[which(EDT256_new$zaman == "beck_int_P")] <- 0 -->
<!-- EDT256_new$zaman[which(EDT256_new$zaman == "beck_int_A1")] <- 1 -->
<!-- EDT256_new$zaman[which(EDT256_new$zaman == "beck_int_A2")] <- 2 -->

<!-- # değişken değerlerini atama  -->
<!-- EDT256_new$zaman <- factor(EDT256_new$zaman, levels = c(0,1,2), -->
<!--                            labels = c("Tedavi Öncesi", "T. Sonrası 1. Hafta", -->
<!--                                       "T. Sonrası 2. Hafta")) -->


<!-- # nparLD analizi -->

<!-- # gerekli paketi yükleme -->
<!-- library(nparLD) -->

<!-- # model oluşturma -->
<!-- modelEDT256 <- nparLD(beck_int_score ~ tanı * zaman, data = EDT256_new, -->
<!--                       subject = "subject", -->
<!--                       plot.CI = TRUE, show.covariance = TRUE) -->

<!-- # model sonucunu yazdır -->
<!-- out <- capture.output(modelEDT256) -->
<!-- cat("EDT256 Model", out, file = "EDT256.txt", sep = "\n", append=TRUE) -->

<!-- # model grafiği -->
<!-- plot(modelEDT256) -->

<!-- # plot sonucunu yazdır -->
<!-- out2 <- capture.output(plot(modelEDT256)) -->
<!-- cat("\n","EDT256 Plot", out2, file = "EDT256.txt", sep = "\n", append = TRUE) -->

<!-- # grafiği yazdır -->
<!-- jpeg('modelEDT256.jpeg') -->
<!-- plot(modelEDT256) -->
<!-- dev.off() -->

<!-- # model özeti -->
<!-- summary(modelEDT256) -->

<!-- # model özeti sonucunu yazdır -->
<!-- out3 <- capture.output(modelEDT256) -->
<!-- cat("\n","EDT256 summary", out3, file = "EDT256.txt", sep = "\n", append=TRUE) -->


<!-- # ek olarak boxplot oluşturma -->
<!-- boxplot(beck_int_score ~ tanı * zaman, data = EDT256_new, -->
<!--         names = FALSE,col = c("grey",2),lwd = 2) -->
<!-- axis(1,at = 1.5,labels = "Tedavi Öncesi",font = 2,cex = 3) -->
<!-- axis(1,at = 3.5,labels = "T. Sonrası 1. Hafta",font = 2,cex = 3) -->
<!-- axis(1,at = 5.5,labels = "T. Sonrası 2. Hafta",font = 2,cex = 3) -->
<!-- legend(5,35,c("Tanı 1","Tanı 2"), -->
<!--        lwd = c(2,2), -->
<!--        col = c("grey",2),cex = 0.7) -->

<!-- # boxplot yazdır -->
<!-- jpeg('boxplotEDT256.jpeg') -->
<!-- boxplot(beck_int_score ~ tanı * zaman, data = EDT256_new, -->
<!--         names = FALSE,col = c("grey",2),lwd = 2) -->
<!-- axis(1,at = 1.5,labels = "Tedavi Öncesi",font = 2,cex = 3) -->
<!-- axis(1,at = 3.5,labels = "T. Sonrası 1. Hafta",font = 2,cex = 3) -->
<!-- axis(1,at = 5.5,labels = "T. Sonrası 2. Hafta",font = 2,cex = 3) -->
<!-- legend(5,35,c("Tanı 1","Tanı 2"), -->
<!--        lwd = c(2,2), -->
<!--        col = c("grey",2),cex = 0.7) -->
<!-- dev.off() -->










<!-- # Analysis -->

<!-- ## Survival Analysis -->

<!-- https://github.com/datacamp/tutorial -->

<!-- Based on https://www.datacamp.com/community/tutorials/survival-analysis-R -->


<!-- ```{r} -->
<!-- library(tutorial) -->
<!-- library(survival) -->
<!-- library(survminer) -->
<!-- library(tidyverse) -->
<!-- library(jmv) -->
<!-- ``` -->


<!-- ```{r} -->
<!-- data(ovarian) -->
<!-- glimpse(ovarian) -->
<!-- help(ovarian) -->
<!-- ``` -->


<!-- ```{r} -->
<!-- summary(ovarian) -->
<!-- ``` -->


<!-- ```{r} -->
<!-- varOvarian <- names(ovarian) -->

<!-- ovarian2 <- ovarian %>%  -->
<!--     map_df(as.factor) -->

<!-- jmv::descriptives(ovarian, varOvarian, freq = TRUE) -->

<!-- jmv::descriptives(ovarian2, varOvarian, freq = TRUE) -->

<!-- ``` -->


<!-- ```{r} -->
<!-- # Dichotomize age and change data labels -->
<!-- ovarian$rx <- factor(ovarian$rx,  -->
<!--                      levels = c("1", "2"),  -->
<!--                      labels = c("A", "B")) -->
<!-- ovarian$resid.ds <- factor(ovarian$resid.ds,  -->
<!--                            levels = c("1", "2"),  -->
<!--                            labels = c("no", "yes")) -->
<!-- ovarian$ecog.ps <- factor(ovarian$ecog.ps,  -->
<!--                           levels = c("1", "2"),  -->
<!--                           labels = c("good", "bad")) -->

<!-- jmv::descriptives(ovarian, varOvarian, freq = TRUE) -->

<!-- ``` -->


<!-- ```{r} -->
<!-- # Data seems to be bimodal -->
<!-- hist(ovarian$age) -->
<!-- ``` -->


<!-- ```{r} -->
<!-- ovarian <- ovarian %>% mutate(age_group = ifelse(age >= 50, "old", "young")) -->
<!-- ovarian$age_group <- factor(ovarian$age_group) -->

<!-- varOvarian <- names(ovarian) -->

<!-- jmv::descriptives(ovarian, varOvarian, freq = TRUE) -->
<!-- ``` -->


<!-- ```{r} -->
<!-- # Fit survival data using the Kaplan-Meier method -->
<!-- surv_object <- Surv(time = ovarian$futime, event = ovarian$fustat) -->
<!-- surv_object -->
<!-- ``` -->

<!-- ```{r} -->
<!-- fit1 <- survfit(surv_object ~ rx, data = ovarian) -->
<!-- summary(fit1) -->
<!-- ``` -->


<!-- ```{r} -->
<!-- ggsurvplot(fit1, data = ovarian, pval = TRUE) -->
<!-- ``` -->


<!-- ```{r} -->
<!-- # Examine prdictive value of residual disease status -->
<!-- fit2 <- survfit(surv_object ~ resid.ds, data = ovarian) -->
<!-- ggsurvplot(fit2, data = ovarian, pval = TRUE) -->
<!-- ``` -->

<!-- ```{r} -->
<!-- # Fit a Cox proportional hazards model -->
<!-- fit.coxph <- coxph(surv_object ~ rx + resid.ds + age_group + ecog.ps,  -->
<!--                    data = ovarian) -->
<!-- ggforest(fit.coxph, data = ovarian) + -->
<!--     geom_errorbar(na.rm = TRUE) -->
<!-- ``` -->



## Data reorganization

```{r}
scabies <- read.csv(file = "http://datacompass.lshtm.ac.uk/607/2/S1-Dataset_CSV.csv", header = TRUE, sep = ",")

scabies$gender == "male"

scabies$age[scabies$gender == "male"]

mean(scabies$age[scabies$gender == "male"])


scabies %>% 
    filter(gender == "male") %>% 
    summarise_at("age" ,mean) 


```



```{r}

scabies$agegroups <- as.factor(cut(scabies$age, c(0,10,20,Inf), labels = c("0-10","11-20","21+"), include.lowest = TRUE)) 

scabies$house_cat <- as.factor(cut(scabies$house_inhabitants, c(0,5,10,Inf), labels = c("0-5","6-10","10+"), include.lowest = TRUE))


table(scabies$house_cat, scabies$house_inhabitants)

```

```{r}
ebola$status <- as.numeric(ebola$status) 

```


```{r}
ebola$transmission <- recode(ebola$transmission, syringe = "needle")

```

```{r}
scabies$house_cat <- relevel(scabies$house_cat, ref = "0-5")
#Make 0-5 household size the baseline group
```





```{r}
df <- data.frame(month=rep(1:3,2),
                 student=rep(c("Amy", "Bob"), each=3),
                 A=c(9, 7, 6, 8, 6, 9),
                 B=c(6, 7, 8, 5, 6, 7))
#Here we have a where each student is on a different row for each month
#The students took two tests A and B. 
#For each student/month combination we have a value for A and a value for B

df2 <- gather(df,test,score,A:B)
#Make a new datatable df2
#Have a column called "test". 
#This will have the value either A or B as these are the names of the columns we specified.
#Have a column called "score". 
#This will have the value previously in column A or B respectively for each row

df2
```


```{r}
#Now we have a single row for each combination of month/student/test 
#Their score is in the score column

df3 <- spread(df2,test,score)
#Make a new datatable df3
#Make a column for each unique value in the test variable.
#Name each of these columns based on that unique value
#Under each column put the corresponding value that was in the score column

df3
```

```{r}
dt3 <- expandRows(dt, 2)
#Expand the original datatable. Replicate each row by the value in column 2. 
dt3
```



## File organization best practices

This page summarises how to organize files and analysis before everything gets jumbled up:
[Setting up a reproducible data analysis workflow in R](https://andrewbtran.github.io/NICAR/2018/workflow/docs/01-workflow_intro.html)

Basically they suggest:
- using a project and project folder in RStudio for each analysis
- using `packrat` as much as possible

`setwd()` and `getwd()` is not necesary when you use projects.





# Analysis

## Descriptive Statistics


```{r}
Epi::stat.table(gender,mean(age), data = scabies)

```

```{r}
table <- Epi::stat.table(gender,mean(age), data = scabies)

pander::pander(table)


```

```{r}
#Tabulate, by gender, the mean age from the scabies dataset

Epi::stat.table(gender,list(mean(age),median(age)), data = scabies)
```

```{r}
summary_data <- arsenal::tableby(gender~age+scabies_infestation,data=scabies)
summary(summary_data)

```






## Hypothesis Testing

### Compare Means

```{r}
t.test(scabies$age[scabies$gender=="male"],scabies$age[scabies$gender=="female"])

```

```{r}
test <- t.test(scabies$age[scabies$gender=="male"],scabies$age[scabies$gender=="female"])
psycho::analyze(test)
```



```{r}
prop.test(numerator,denominator)
```


```{r}
table(impetigo = scabies$impetigo_active, scabies = scabies$scabies_infestation)
# dependent ~ independent
```

```{r}
#See that because 'no' is the 'base' level the table is laid out
#               No Disease      Has Disease

# Not-exposed

# Exposed

#This is dependent on how your data is coded so you need to check this before using epi.2by2
#If the table is laid out correctly then you can input straight into epi.2by2, otherwise you #need to recode or re-order the variables so that the table will be laid out correctly

#epi.2by2 wants the data with the exposed/disease group in top right corner
#So we just tell R to order the variables differently when we draw the table

epiR::epi.2by2(table(relevel(scabies$scabies_infestation,"yes"), relevel(scabies$impetigo_active,"yes")))
```











---

<!-- Stratified MH Odds Ratios for Confounding -->
<!-- We can also stratify our odds ratio by a third variable to look for confounding -->

<!-- epi.2by2(table(datadepvar,dataindep_var, data$stratificationvariable)) #As above make sure the levels of dep_var and indep_var are set such that the table is ordered in the way that epi.2by2 wants the data -->

<!-- The output gives us the crude odds ratio (identical to the one we got without the stratification variable) and then an adjusted odds ratio. We can look at these two for evidence of confounding. -->

<!-- We can get (if we desire) the stratum specific odds ratios out -->

<!-- stratum <- epi.2by2(table(relevel(scabies$scabies_infestation,"yes"), relevel(scabies$impetigo_active,"yes"),scabies$gender)) -->


<!--  #Note this time I'm saving the 2*2 table -->
<!-- stratum -->
<!-- ##              Outcome +    Outcome -      Total        Inc risk * -->
<!-- ## Exposed +          179          187        366              48.9 -->
<!-- ## Exposed -          444         1098       1542              28.8 -->
<!-- ## Total              623         1285       1908              32.7 -->
<!-- ##                  Odds -->
<!-- ## Exposed +       0.957 -->
<!-- ## Exposed -       0.404 -->
<!-- ## Total           0.485 -->
<!-- ##  -->
<!-- ##  -->
<!-- ## Point estimates and 95 % CIs: -->
<!-- ## ------------------------------------------------------------------- -->
<!-- ## Inc risk ratio (crude)                       1.70 (1.49, 1.94) -->
<!-- ## Inc risk ratio (M-H)                         1.58 (1.39, 1.79) -->
<!-- ## Inc risk ratio (crude:M-H)                   1.08 -->
<!-- ## Odds ratio (crude)                           2.37 (1.88, 2.99) -->
<!-- ## Odds ratio (M-H)                             2.18 (1.72, 2.76) -->
<!-- ## Odds ratio (crude:M-H)                       1.09 -->
<!-- ## Attrib risk (crude) *                        20.11 (14.52, 25.71) -->
<!-- ## Attrib risk (M-H) *                          17.75 (5.52, 29.99) -->
<!-- ## Attrib risk (crude:M-H)                      1.13 -->
<!-- ## ------------------------------------------------------------------- -->
<!-- ##  Test of homogeneity of IRR: X2 test statistic: 8.011 p-value: 0.005 -->
<!-- ##  Test of homogeneity of  OR: X2 test statistic: 0.629 p-value: 0.428 -->
<!-- ##  Wald confidence limits -->
<!-- ##  M-H: Mantel-Haenszel -->
<!-- ##  * Outcomes per 100 population units -->


<!-- summary(stratum)$OR.strata.wald -->


<!-- ##        est    lower   upper -->
<!-- ## 1 2.417620 1.716620 3.40488 -->
<!-- ## 2 1.996753 1.439806 2.76914 -->



<!-- # Plots -->


<!-- ## Histogram -->

<!-- ```{r} -->
<!-- hist(scabies$age) -->

<!-- ``` -->




<!-- Scatterplot -->
<!-- We could do a basic scatter plot of house size vs number of people in a room -->

<!-- plot(scabies$house_inhabitants,scabies$room_cohabitation) -->


<!-- Colour Scatter -->
<!-- We might make this look nicer by adding some colours representing key variables -->

<!-- scabies$scatter_colour[scabies$scabies_infestation == "yes"] <- "red"  -->
<!-- #If scabies_infestation is 0 then the colour is red -->

<!-- scabies$scatter_colour[scabies$scabies_infestation == "no"] <- "blue" -->
<!-- #If scabies_infestation is 0 then the colour is blue -->

<!-- plot(scabies$house_inhabitants,scabies$room_cohabitation, col = scabies$scatter_colour) -->


<!-- #For each value look in the variable scatter_colours and work out what colour to make each person -->
<!-- Lots of the points overlap so we can always make jitter them -->

<!-- plot(jitter(scabies$house_inhabitants),scabies$room_cohabitation, col = scabies$scatter_colour) -->


<!-- Later we will come back to GGPLOT2 which makes much nicer graphs! -->

<!-- Logistic Regression -->
<!-- Sample Dataset 3 -->
<!-- The examples here use the “scabies dataset” again. For ease I quickly repeat below the code needed to implement the data-cleaning steps demonstrated above on the dataset -->

<!-- scabies <- read.csv(file = "S1-Dataset_CSV.csv", header = TRUE, sep = ",") -->
<!-- #Load dataset -->
<!-- scabies$agegroups <- as.factor(cut(scabies$age, c(0,10,20,Inf), labels = c("0-10","11-20","21+"), include.lowest = TRUE))  -->
<!-- scabies$agegroups <-relevel(scabies$agegroups, ref = "21+") -->
<!-- #Categorise age and set a baseline -->

<!-- scabies$house_cat <- as.factor(cut(scabies$house_inhabitants, c(0,5,10,Inf), labels = c("0-5","6-10","10+"), include.lowest = TRUE)) -->
<!-- scabies$house_cat <- relevel(scabies$house_cat, ref = "0-5") -->
<!-- #Categorise housesize and set a baseline -->
<!-- We can perform logistic regression in a straightforward fashion in. The formula for any logistic regression is -->

<!-- logistic_model <- glm(dependent_variable~indepdent_variable+independent_variable, data = dataframe, family = binomial) -->

<!-- If we want to include a variable as a Factor then we state that by saying > factor(independent_variable) -->

<!-- logistic_model <- glm(dependent_variable~factor(indepdent_variable), data = dataframe, family = binomial) -->

<!-- Unlike in STATA this does not automatically generate an output. As a clinical epidemiologist we want a table with the odds ratio, co-efficients, standard errors, confidence interval and the Wald-test P-Value. -->

<!-- We get the Odds ratios and Confidence intervals by taking the exponential of the output from the logistic regression model -->

<!-- logistic_model_OR <- cbind(OR = exp(coef(logistic_model)), exp(confint(logistic_model))) -->

<!-- Then we need to combine the Odds ratios and CI values with the raw coefficients and p-values into a single table -->

<!-- logistic_model_summary <- summary(logistic_model) logistic_model_summary <- cbind(logistic_model_OR, logistic_model_summary) -->

<!-- We can display then display the complte table of the output -->

<!-- logistic_model_summary -->

<!-- An example of logistic regression -->
<!-- Here we will see if the size of the household is associated with risk of scabies (variable = house_cat), after controlling for age (categorised) and gender. -->

<!-- scabiesrisk <- glm(scabies_infestation~factor(agegroups)+factor(gender)+factor(house_cat),data=scabies,family=binomial()) -->
<!-- scabiesrisk_OR <- exp(cbind(OR= coef(scabiesrisk), confint(scabiesrisk))) -->
<!-- ## Waiting for profiling to be done... -->
<!-- scabiesrisk_summary <- summary(scabiesrisk) -->
<!-- scabiesrisk_summary <- cbind(scabiesrisk_OR, scabiesrisk_summary$coefficients) -->
<!-- scabiesrisk_summary -->
<!-- ##                                OR      2.5 %    97.5 %   Estimate -->
<!-- ## (Intercept)            0.09357141 0.06925691 0.1243857 -2.3690303 -->
<!-- ## factor(agegroups)0-10  2.20016940 1.61679635 3.0237474  0.7885344 -->
<!-- ## factor(agegroups)11-20 2.53291768 1.80434783 3.5769136  0.9293719 -->
<!-- ## factor(gender)male     1.44749159 1.13940858 1.8399368  0.3698321 -->
<!-- ## factor(house_cat)6-10  1.30521927 1.02664372 1.6624397  0.2663710 -->
<!-- ## factor(house_cat)10+   1.17003712 0.65654905 1.9900581  0.1570355 -->
<!-- ##                        Std. Error     z value     Pr(>|z|) -->
<!-- ## (Intercept)             0.1492092 -15.8772359 9.110557e-57 -->
<!-- ## factor(agegroups)0-10   0.1594864   4.9442116 7.645264e-07 -->
<!-- ## factor(agegroups)11-20  0.1743214   5.3313714 9.747386e-08 -->
<!-- ## factor(gender)male      0.1221866   3.0267824 2.471718e-03 -->
<!-- ## factor(house_cat)6-10   0.1228792   2.1677478 3.017788e-02 -->
<!-- ## factor(house_cat)10+    0.2813713   0.5581076 5.767709e-01 -->
<!-- The output gives us the OR associated with each variable, the 95% CI of that OR, and a Wald-Test P-Value for each variable -->

<!-- Trying a logistic regression for yourself -->
<!-- Investigate using logistic regression whether having impetigo (impetigo_odds) is associated with scabies (scabies_infestation) after controlling for age (agegroups) and sex (gender). -->

<!-- ## Waiting for profiling to be done... -->
<!-- ##                               OR     2.5 %    97.5 %   Estimate Std. Error -->
<!-- ## (Intercept)            0.1379817 0.1064948 0.1768205 -1.9806341  0.1292414 -->
<!-- ## scabies_infestationyes 1.9380091 1.5180396 2.4740877  0.6616612  0.1245435 -->
<!-- ## agegroups0-10          3.7264517 2.8550721 4.8992194  1.3154565  0.1376324 -->
<!-- ## agegroups11-20         2.7487745 2.0310065 3.7353394  1.0111552  0.1553069 -->
<!-- ## gendermale             1.7509457 1.4225953 2.1557118  0.5601561  0.1060015 -->
<!-- ## house_cat6-10          0.9548564 0.7741093 1.1776160 -0.0461943  0.1069907 -->
<!-- ## house_cat10+           0.8941528 0.5449317 1.4384328 -0.1118786  0.2468512 -->
<!-- ##                            z value     Pr(>|z|) -->
<!-- ## (Intercept)            -15.3250751 5.199446e-53 -->
<!-- ## scabies_infestationyes   5.3126904 1.080185e-07 -->
<!-- ## agegroups0-10            9.5577530 1.203411e-21 -->
<!-- ## agegroups11-20           6.5106924 7.480520e-11 -->
<!-- ## gendermale               5.2844143 1.261076e-07 -->
<!-- ## house_cat6-10           -0.4317601 6.659158e-01 -->
<!-- ## house_cat10+            -0.4532228 6.503883e-01 -->
<!-- You should get a table showing an Odds Ration of 2 - i.e the odds of impetigo are twice as high in people with scabies. The p-value suggests this is a highly significant finding. -->

<!-- Comparing two models via a Likelihood Ratio Test -->
<!-- We might want to compare two models to see if overall the factor variable house_cat (size of the house) is significant after controlling for other variables (as opposed to the individual Wald tests for specific values of house_cat). -->

<!-- As in STATA we simply run the model without the Variable we are interested in and do a Likelihood Ratio test -->

<!-- scabiesrisk2 <- glm(scabies_infestation~factor(agegroups)+factor(gender),data=scabies,family=binomial()) -->
<!-- #We could have got out all the results of this new model just like we did for the initial Logistic Regression but for the purpose of this demonstration we don't need to -->
<!-- lrtest(scabiesrisk,scabiesrisk2) -->
<!-- ## Likelihood ratio test -->
<!-- ##  -->
<!-- ## Model 1: scabies_infestation ~ factor(agegroups) + factor(gender) + factor(house_cat) -->
<!-- ## Model 2: scabies_infestation ~ factor(agegroups) + factor(gender) -->
<!-- ##   #Df  LogLik Df  Chisq Pr(>Chisq)   -->
<!-- ## 1   6 -900.44                        -->
<!-- ## 2   4 -902.80 -2 4.7328    0.09382 . -->
<!-- ## --- -->
<!-- ## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 -->
<!-- We se that overall there is borderline significance between household size and scabies risk. This makes sense if we look at the Wald-Test for each category as one house size category was associated and one was not. -->

<!-- Fiting an interaction term -->
<!-- We can easily fit an interaction term to our model. The general syntax is -->

<!-- interaction_model <- glm(dependent_variable~indepdent_variable*independent_variable, data = dataframe family = binomial) -->

<!-- scabiesrisk3 <- glm(scabies_infestation~factor(agegroups)*factor(gender),data=scabies,family=binomial()) -->
<!-- scabiesrisk3_OR <- exp(cbind(OR= coef(scabiesrisk3), confint(scabiesrisk3))) -->
<!-- ## Waiting for profiling to be done... -->
<!-- scabiesrisk3_summary <- summary(scabiesrisk3) -->
<!-- scabiesrisk3_summary <- cbind(scabiesrisk3_OR, scabiesrisk3_summary$coefficients) -->
<!-- scabiesrisk3_summary -->
<!-- ##                                                  OR      2.5 %    97.5 % -->
<!-- ## (Intercept)                               0.1018100 0.07391201 0.1366901 -->
<!-- ## factor(agegroups)0-10                     2.6697985 1.81088534 3.9801955 -->
<!-- ## factor(agegroups)11-20                    2.3655612 1.51556214 3.6962701 -->
<!-- ## factor(gender)male                        1.6634407 0.93937388 2.8651848 -->
<!-- ## factor(agegroups)0-10:factor(gender)male  0.7017255 0.37220313 1.3517237 -->
<!-- ## factor(agegroups)11-20:factor(gender)male 1.1363006 0.56352873 2.3357191 -->
<!-- ##                                             Estimate Std. Error -->
<!-- ## (Intercept)                               -2.2846473  0.1564481 -->
<!-- ## factor(agegroups)0-10                      0.9820030  0.2004429 -->
<!-- ## factor(agegroups)11-20                     0.8610153  0.2268275 -->
<!-- ## factor(gender)male                         0.5088882  0.2831246 -->
<!-- ## factor(agegroups)0-10:factor(gender)male  -0.3542130  0.3280584 -->
<!-- ## factor(agegroups)11-20:factor(gender)male  0.1277779  0.3619259 -->
<!-- ##                                               z value     Pr(>|z|) -->
<!-- ## (Intercept)                               -14.6032328 2.678375e-48 -->
<!-- ## factor(agegroups)0-10                       4.8991654 9.624460e-07 -->
<!-- ## factor(agegroups)11-20                      3.7959034 1.471068e-04 -->
<!-- ## factor(gender)male                          1.7974000 7.227214e-02 -->
<!-- ## factor(agegroups)0-10:factor(gender)male   -1.0797255 2.802644e-01 -->
<!-- ## factor(agegroups)11-20:factor(gender)male   0.3530499 7.240510e-01 -->
<!-- The output givs us 1) The Odds Ratio of each factor in the interaction in the baseline group of the other factor: i.e odds ratio for scabies, amongst people aged 0-10 compared to those aged 21+ (the reference group for age) in the baseline group of gender (female) 2) The interaction terms -->

<!-- As in STATA we can then compare the model with and without the interaction term using a likelihood ratio test to see if the interaction is statistically significant. -->

<!-- #Compare our model containing age & gender (scabiesrisk2) without an interaction term to our third model (scabiesrisk3) which does have an interaction term between age and gender -->
<!-- lrtest(scabiesrisk2,scabiesrisk3) -->
<!-- ## Likelihood ratio test -->
<!-- ##  -->
<!-- ## Model 1: scabies_infestation ~ factor(agegroups) + factor(gender) -->
<!-- ## Model 2: scabies_infestation ~ factor(agegroups) * factor(gender) -->
<!-- ##   #Df  LogLik Df  Chisq Pr(>Chisq) -->
<!-- ## 1   4 -902.80                      -->
<!-- ## 2   6 -901.15  2 3.3131     0.1908 -->
<!-- Remember the null-hypothesis = No interaction. So here the P-value suggests no evidence against the null; so we should not fit this interaction term. -->

<!-- Matched Case-Control Data -->
<!-- Conditional Logistic Regression -->
<!-- For individually matched case control studies we use Conditional Logistic Regression not normal logistic regression -->

<!-- The syntax for this is: -->

<!-- conditional_model <- clogit(variable_denoting)or_control~indepdent_variable+independent_variable+strata(matching_var), data = dataframe) -->

<!-- Where ‘matching_var’ is a variable saying which Case is matched to which Control -->

<!-- We get the ORs, 95% CIs and everything else exactly the same as for logistic regression. -->

<!-- > conditional <- clogit(case~independent_variable+strata(matching_var), data = dataframe) -->
<!-- > conditional_OR <- exp(cbind(OR= coef(conditional), confint(conditional))) -->
<!-- > conditional_summary <- summary(conditional) -->
<!-- > conditional_summary <- cbind(conditional_OR, conditional_summary$coefficients) -->
<!-- > conditional_summary -->
<!-- Equally everything else is just like normal logistic regression i.e we can fit multiple models, run Likelihood-Ratio Tests etc -->

<!-- Poisson and Cox Regression -->
<!-- Sample Dataset 4 -->
<!-- For the purpose of Poisson and cox regression we will initially go back to a clean version of our “Ebola dataset”. For ease I quickly repeat below the code needed to implement the data-cleaning steps demonstrated above on the dataset -->

<!-- ##This is the ebola dataset again -->
<!-- ebola <- read.csv(file = "mmc1.txt", header = TRUE, sep = ",") -->

<!-- #Lets set 'person_to_person' route of transmission as baseline -->
<!-- ebola$transmission <- relevel(ebola$transmission, "person_to_person") -->

<!-- #Replace blank values of sex as missing -->
<!-- ebola$sex[ebola$sex ==""] <- is.null(ebola$sex)  -->
<!-- ## Warning in `[<-.factor`(`*tmp*`, ebola$sex == "", value = structure(c(3L, : -->
<!-- ## invalid factor level, NA generated -->
<!-- Setting up our time variables -->
<!-- The package we are using for the Poisson and Cox Regression needs the dates in a particular format to work. -->

<!-- Specifically it needs them in a ‘decimal year’ i.e 1982.145 If our data isn’t already like this then we set it up using the ‘cal.yr’ command. The cal.yr command wants a date as an input - so remember if your dates are stored as strings and not dates in the raw data then you need to convert them to a date and then to a decimal date. You can do this in one step. -->

<!-- class(ebola$disease_onset) -->
<!-- ## [1] "factor" -->
<!-- #We see that the timein variable is currently a factor (i.e just recognisd as a bunch of characters), so we need to convert factor>data>decimal date.  -->
<!-- ebola$disease_onset <- cal.yr(as.Date(ebola$disease_onset, format = "%Y-%m-%d")) -->
<!-- ebola$disease_ended <- cal.yr(as.Date(ebola$disease_ended, format = "%Y-%m-%d")) -->
<!-- Declaring our periods of follow-up: Making a lexis model -->
<!-- We need to declare the way time is modelled when doing this kind of analysis. This is like STSET in STATA. -->

<!-- That is we need to specify 1) Entry point 2) Exit point -->

<!-- We may have multiple time scales - for example both Calendar Time and Chronological age. The general syntax is: -->

<!-- lexis_object <- Lexis(  -->
<!--   entry = list ( name_first_time_scale = variable_setting_that, -->
<!--                  name_second_time_scale = variable_setting_that), -->
<!--   exit = list (name_of_time_scale_we calculate_exit_against = variable_for exit), -->
<!--   exit.status = (where_we_define_the_outcome), -->
<!--   data = data_frame) -->
<!-- For example if we might be interested in time in follow-up and chronological age as time scales. So we might do: -->

<!-- ebola_lexis_model <- Lexis(entry = list("calendar_time"=disease_onset), exit = list("calendar_time" = disease_ended), exit.status = status =="died", data = ebola) -->
<!-- ## NOTE: entry.status has been set to FALSE for all. -->
<!-- ## Warning in Lexis(entry = list(calendar_time = disease_onset), exit = list(calendar_time = disease_ended), : Dropping NA rows with duration of follow up < tol -->
<!-- #We define the beginning of calendar time in the study as the date in the 'disease_onset' variable -->
<!-- #We call time on this scale "calendar_time" -->
<!-- #Time out is on the same timescale as time in - i.e calendar_time -->
<!-- #If people are missing either a time in or time-out so they get dropped -->
<!-- R is also happy if we just have a duration of follow-up as opposed to a Date of Entry and a Date of Exit. In that case we can just say: -->

<!-- lexis_model <- Lexis(exit = list("calendar_time" = duration), exit.status = died, data = dataset) -->
<!-- ##Here we specify the exit time as being the duration of follow-up -->
<!-- ##We haven't stated an entry time so R assumes it to be 0 - i.e people were followed up from time 0 for the duration of time specified as the exit time -->
<!-- Key variables in the Lexis model -->
<!-- When we make this model it creates several key variable we need to reference when running our Cox and Poisson models. -->

<!-- ‘lex.dur’ = duration of follow-up ‘lex.Xst’ = Status of the individual at the end of each time block. For example in the above lexis model this is whether or not the event CHD occured in the any given time period. -->

<!-- We will reference these variables when we run our poisson and cox regression models. -->

<!-- Making a rate table -->
<!-- Once we have a lexis model it is easy to see crude rates and rates stratified by a variable -->

<!-- The general syntax is: > rate_table <- rate(lexis_model, obs = lex.Xst, pyrs = lex.dur, print = stratifying_var) -->

<!-- If we want we can give per 100 or 1,000 years by simply adjusting ‘pyrs’ > rate_table <- rate(lexis_model, obs = lex.Xst, pyrs = lex.dur/1000, print = stratifying_var) -->

<!-- average_rate <- rate(ebola_lexis_model, obs = lex.Xst, pyrs = lex.dur) -->
<!-- #Show the overall rate of cardiovascular outcomes per 1 person years (because people die fast of ebola and follow-up is short) -->
<!-- rate_by_transmission <- rate(ebola_lexis_model, obs = lex.Xst, pyrs = lex.dur, print = transmission) -->
<!-- #Show the rates of cardiovascular outcome by sex per 1 person years (because people die fast of ebola and follow-up is short) -->
<!-- average_rate -->
<!-- ##  -->
<!-- ## Crude rates and 95% confidence intervals: -->
<!-- ##  -->
<!-- ##    lex.Xst  lex.dur     rate  SE.rate  rate.lo  rate.hi -->
<!-- ## 1:     245 5.303217 46.19837 2.951506 40.76096 52.36112 -->
<!-- rate_by_transmission -->
<!-- ##  -->
<!-- ## Crude rates and 95% confidence intervals: -->
<!-- ##  -->
<!-- ##        transmission lex.Xst   lex.dur     rate   SE.rate  rate.lo -->
<!-- ## 1: person_to_person     129 2.8993840 44.49221  3.917321 37.44023 -->
<!-- ## 2:             both       8 0.2491444 32.10989 11.352561 16.05788 -->
<!-- ## 3:            other      10 0.1341547 74.54082 23.571876 40.10658 -->
<!-- ## 4:          syringe      86 1.7686516 48.62461  5.243327 39.36108 -->
<!-- ## 5:          unknown      12 0.2518823 47.64130 13.752860 27.05568 -->
<!-- ##      rate.hi -->
<!-- ## 1:  52.87245 -->
<!-- ## 2:  64.20806 -->
<!-- ## 3: 138.53919 -->
<!-- ## 4:  60.06829 -->
<!-- ## 5:  83.88975 -->
<!-- Running a simple Poisson Regression -->
<!-- We run a poisson regression essentially the same as a logistic regression. We use the same GLM command as before. -->

<!-- Instead of setting ‘family = binomial’ we need to set ‘family = poisson’ -->

<!-- We include an additional variable called offset. This represents the fact that follow-up time is different for each person in the study (i.e it gives you a rate against person-time rather than a number of events / people). This variable is the log of the follow-up time in the lexis-model: ‘log(lex.dur)’ -->

<!-- We set the outcome as whether lex.Xst occured (i.e did the event occur) -->

<!-- The generic formula is > poisson_model <- glm(lex.Xst~offset(log(lex.dur))+indepdent_variable+independent_variable, data = lexis_dataframe, family = poisson) -->

<!-- #Fit a model for HR of death from CHD against grade -->
<!-- ebola_model <- glm(lex.Xst~factor(transmission)+sex+offset(log(lex.dur)), data = ebola_lexis_model, family = poisson) -->
<!-- #The outcome is lex.Xst - whatever we set as the outcome when we made our lexis dataset  -->
<!-- #The offset is the log(lex.dur) - the log of duration of follow-up -->
<!-- #We fit a poisson model  -->

<!-- ebola_ratios <- cbind(HR = exp(coef(ebola_model)), exp(confint(ebola_model))) -->
<!-- ## Waiting for profiling to be done... -->
<!-- #Just like in the Logistic model we take exponentials of the coefficients to get our Hazard Rations -->
<!-- ebola_model_out <- summary(ebola_model)  -->
<!-- ebola_model_out <-cbind (ebola_ratios, ebola_model_out$coefficients) -->
<!-- #We combine this all in to a nice table -->
<!-- ebola_model_out -->
<!-- ##                                     HR      2.5 %    97.5 %     Estimate -->
<!-- ## (Intercept)                 44.5963777 36.2674133 54.231421  3.797652638 -->
<!-- ## factor(transmission)both     0.7230170  0.3237964  1.389407 -0.324322562 -->
<!-- ## factor(transmission)other    1.6052597  0.7574323  2.976279  0.473285546 -->
<!-- ## factor(transmission)syringe  1.0928077  0.8294145  1.432693  0.088750226 -->
<!-- ## factor(transmission)unknown  1.0722117  0.5607743  1.861057  0.069723525 -->
<!-- ## sexmale                      0.9941797  0.7663835  1.284689 -0.005837258 -->
<!-- ##                             Std. Error     z value      Pr(>|z|) -->
<!-- ## (Intercept)                  0.1025784 37.02195634 5.077767e-300 -->
<!-- ## factor(transmission)both     0.3666753 -0.88449514  3.764290e-01 -->
<!-- ## factor(transmission)other    0.3450006  1.37183997  1.701133e-01 -->
<!-- ## factor(transmission)syringe  0.1392193  0.63748492  5.238090e-01 -->
<!-- ## factor(transmission)unknown  0.3033065  0.22987807  8.181865e-01 -->
<!-- ## sexmale                      0.1316124 -0.04435188  9.646239e-01 -->
<!-- Lexis Expansions and time as a confounder. -->
<!-- Time (for example age) is a key confounder so we need to be able to control for this. In STATA we would use STSPLIT to do this. We can do the same in R. -->

<!-- We do this using: > expanded_lexis_model <- splitLexis(lexis_model, “time_scale_to_split_on”, breaks = c(break1,break2,break3)) -->

<!-- Sample Dataset 5 -->
<!-- We need a different dataset for this example. Again the dataset is freely available on the LSHTM Data Repository: “Alcohol outcomes” This is a simulated dataset (a fully-anoynmised, tweaked dataset based on a real-one) looking at the association between age, alcohol and death. -->

<!-- #Load in our new dataset -->
<!-- alcohol <- read.csv(file = "alcohol_outcomes.csv", header = TRUE, sep=",") -->

<!-- #Check how dates are saved -->
<!-- class(alcohol$timein) -->
<!-- ## [1] "factor" -->
<!-- #time is saved as a factor so we need to convert it -->
<!-- alcohol$timein <- cal.yr(as.Date(alcohol$timein, format = "%d/%m/%Y")) -->
<!-- alcohol$timeout <- cal.yr(as.Date(alcohol$timeout, format = "%d/%m/%Y")) -->
<!-- alcohol$timebth <- cal.yr(as.Date(alcohol$timebth, format = "%d/%m/%Y")) -->
<!-- #Setup our lexis model -->
<!-- alcohol_lexis <- Lexis(entry = list("calendar_time"=timein, "age" = timein - timebth), exit = list("calendar_time" = timeout), exit.status = death ==1, data = alcohol) -->
<!-- ## NOTE: entry.status has been set to FALSE for all. -->
<!-- #We define the beginning of calendar time in the study as the date in the 'timein' variable -->
<!-- #We call time on this scale "calendar_time" -->
<!-- #We define the beginning of the age-time scale as the difference between when you joined the study and your date of birth. We call this time scale "age" -->
<!-- #Time out is on the same timescale as time in - i.e calendar_time -->

<!-- #Perform a lexis expansion -->
<!-- split_alcohol_lexis <- splitLexis(alcohol_lexis, breaks = c(50,60,70,80), time.scale = "age") -->
<!-- #We say we want to split our lexis_model -->
<!-- #We want to do this on our defined "age" time scale -->
<!-- #We want the breaks to be at the listed points -->
<!-- Now we need to tell R we want to use those Time-Blocks as a variable in our regression. NB Compare to STATA - In stata we cut the time and make it a new variable in one go with -->

<!-- stplit newvariable, at(timepoint,timepoint,timepoint) -->

<!-- But each time we want to split on a new timescale in STATA we need to stset the data again and do a new ‘stplit’ command. -->

<!-- In R we declare all the timescales initially (in the lexis command), then we split each timeperiod and then make a new variable reflecting each split. The generic syntax is -->

<!-- expanded_model$block_name <- timeBand(expanded_model, “name_of_split”, type = “factor”) -->

<!-- split_alcohol_lexis$age_blocks <- timeBand(split_alcohol_lexis, "age", type = "factor") -->
<!-- #We make a new variable called age_blocks. -->
<!-- #We used the splits we defined on the "age" time-scale for this -->
<!-- #Make each segment of this time-scale an element of a categorical variable -->
<!-- #We then remove any blank 'time-blocks' i.e segments with no observations -->
<!-- #This is done by re-factoring the variable -->
<!-- split_alcohol_lexis$age_blocks <- factor(split_alcohol_lexis$age_blocks) -->
<!-- Running a poisson regression including a time-varying variable -->
<!-- Now we can run the poisson regression with age (in blocks) as a variable just like in STATA. -->

<!-- age_alcohol_model <- glm(lex.Xst~factor(alcohol)+factor(age_blocks)+offset(log(lex.dur)), data = split_alcohol_lexis, family = poisson) -->
<!-- age_alcohol_ratios <- cbind(HR = exp(coef(age_alcohol_model)), exp(confint(age_alcohol_model))) -->
<!-- ## Waiting for profiling to be done... -->
<!-- age_alcohol_out <- summary(age_alcohol_model)  -->
<!-- age_alcohol_out <-cbind (age_alcohol_ratios, age_alcohol_out$coefficients) -->
<!-- age_alcohol_out -->
<!-- ##                                      HR        2.5 %       97.5 % -->
<!-- ## (Intercept)                 0.001758427  0.000698216 3.568313e-03 -->
<!-- ## factor(alcohol)2            1.583354946  1.287840186 1.943563e+00 -->
<!-- ## factor(age_blocks)(50,60]   2.788198316  1.304756320 7.235037e+00 -->
<!-- ## factor(age_blocks)(60,70]   8.358271714  4.051253875 2.125521e+01 -->
<!-- ## factor(age_blocks)(70,80]  18.434422669  8.856542929 4.712319e+01 -->
<!-- ## factor(age_blocks)(80,Inf] 49.721819767 18.829170021 1.449072e+02 -->
<!-- ##                             Estimate Std. Error    z value     Pr(>|z|) -->
<!-- ## (Intercept)                -6.343336  0.4088553 -15.514867 2.752023e-54 -->
<!-- ## factor(alcohol)2            0.459546  0.1049087   4.380437 1.184415e-05 -->
<!-- ## factor(age_blocks)(50,60]   1.025396  0.4292502   2.388806 1.690321e-02 -->
<!-- ## factor(age_blocks)(60,70]   2.123252  0.4154268   5.111013 3.204365e-07 -->
<!-- ## factor(age_blocks)(70,80]   2.914220  0.4189527   6.955964 3.501584e-12 -->
<!-- ## factor(age_blocks)(80,Inf]  3.906444  0.5098479   7.661979 1.830903e-14 -->
<!-- #Now fit a model adjusting for the effect of changing age through follow-up time -->
<!-- Comparing Models -->
<!-- Just like in logistic regression we can compare our two poisson models using ‘lrtest’. Remember the two models need to be fittd on the same number of observations. This means both models need to be fitted AFTER the lexis expansion - so in the current example we would compare: Lexis Expanded Dataset - controlling for Age & Grade Lexis Expanded Dataset - controlling for Age Only The LRTEST would be whether Grade was associated having controlled for Age -->

<!-- age_model <- glm(lex.Xst~age_blocks+offset(log(lex.dur)), data = split_alcohol_lexis, family = poisson) -->
<!-- #Fit a model on the expanded dataset where only Age is an independent variable -->
<!-- age_model_ratios <- cbind(HR = exp(coef(age_model)), exp(confint(age_model))) -->
<!-- ## Waiting for profiling to be done... -->
<!-- age_model_out <- summary(age_model)  -->
<!-- age_model_out <-cbind (age_model_ratios, age_model_out$coefficients) -->
<!-- age_model_out -->
<!-- ##                              HR        2.5 %       97.5 %  Estimate -->
<!-- ## (Intercept)         0.001908232 7.583858e-04   0.00386644 -6.261578 -->
<!-- ## age_blocks(50,60]   2.864554178 1.340670e+00   7.43252992  1.052413 -->
<!-- ## age_blocks(60,70]   9.040064110 4.386843e+00  22.97293591  2.201666 -->
<!-- ## age_blocks(70,80]  21.271550431 1.025650e+01  54.25717095  3.057371 -->
<!-- ## age_blocks(80,Inf] 61.056794442 2.323188e+01 177.24621063  4.111804 -->
<!-- ##                    Std. Error    z value     Pr(>|z|) -->
<!-- ## (Intercept)         0.4082441 -15.337829 4.272431e-53 -->
<!-- ## age_blocks(50,60]   0.4291935   2.452070 1.420369e-02 -->
<!-- ## age_blocks(60,70]   0.4149555   5.305789 1.121869e-07 -->
<!-- ## age_blocks(70,80]   0.4174887   7.323242 2.420515e-13 -->
<!-- ## age_blocks(80,Inf]  0.5075158   8.101825 5.414068e-16 -->
<!-- lrtest(age_model,age_alcohol_model) -->
<!-- ## Likelihood ratio test -->
<!-- ##  -->
<!-- ## Model 1: lex.Xst ~ age_blocks + offset(log(lex.dur)) -->
<!-- ## Model 2: lex.Xst ~ factor(alcohol) + factor(age_blocks) + offset(log(lex.dur)) -->
<!-- ##   #Df  LogLik Df Chisq Pr(>Chisq)     -->
<!-- ## 1   5 -1487.5                         -->
<!-- ## 2   6 -1478.1  1 18.72  1.514e-05 *** -->
<!-- ## --- -->
<!-- ## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 -->
<!-- #Perform a likelihood test of the model with Age&Alcohol compared to the model with only Age -->
<!-- If we wanted to test if age was associated we would compare: Lexis Expanded Dataset - controlling for Age & Alcohol Lexis Expanded Dataset - controlling for Alcohol Only Note even though strictly we don’t need to expand the dataset for the Alcohol model (as its not a time varying factor) we need the two models to have the same number of observations so it does need to be run against the Lexis Expanded dataset -->

<!-- Cox Regression -->
<!-- Cox Regression is done in a very similar fashion to all other models. We declare the outcome and follow-up time slightly differently than from in the poisson but otherwise things look very similar! -->

<!-- The generic formula is: -->

<!-- cox_model <- coxph(Surv(lex.dur,lex.Xst)+indepdent_variable+independent_variable, data = lexis_dataframe) -->

<!-- cox_lexis_model <- coxph(Surv(lex.dur,lex.Xst)~factor(alcohol),data = split_alcohol_lexis) -->
<!-- ##Fit a cox model.  -->
<!-- ##The initial statement Surv(lex.dur,lex.Xst) says that the duration of follow-up is in lex.dur and the outcome variable in lex.Xst (as outlined above) -->

<!-- cox_lexis_ratios <- cbind(HR = exp(coef(cox_lexis_model)), exp(confint(cox_lexis_model))) -->
<!-- #As every other time we exponentiat the coefficients to get hazard ratios -->
<!-- cox_lexis_out <- summary(cox_lexis_model) -->
<!-- cox_lexis_out <- cbind(cox_lexis_ratios,cox_lexis_out$coefficients) -->
<!-- #And we put everything in a nice table -->
<!-- cox_lexis_out -->
<!-- ##                        HR    2.5 %   97.5 %      coef exp(coef)  se(coef) -->
<!-- ## factor(alcohol)2 2.237758 1.830073 2.736263 0.8054745  2.237758 0.1026134 -->
<!-- ##                         z     Pr(>|z|) -->
<!-- ## factor(alcohol)2 7.849601 4.218847e-15 -->
<!-- Note that the poisson and the cox give very similar estimates of the impact of alcohol on the outcome. -->

<!-- As in Poisson Regression we can compare two cox-models using ‘lrtest’ if we need to. Remember as always that both models must be performed on a dataset with the same number of observations - i.e after any lexis-expansion has been performed. -->

<!-- ##Kaplan Meier Plots -->

<!-- We can plot very nice KM plots. As before we have declared out data as a Lexis Model. We input the dataset in a similar fashion to our cox-regression but use the survfit command rather than coxph > km <- survfit(Surv(lex.dur, lex.Xst) ~ var_to_stratify_on, data = dataset) -->

<!-- Then we plot the KM Plot > ggsurvplot(km, data = dataset, risk.table = TRUE / FALSE) -->

<!-- km <- survfit(Surv(lex.dur,lex.Xst)~factor(alcohol),data = alcohol_lexis) -->
<!-- ##Fit the data -->
<!-- ##The initial statement Surv(lex.dur,lex.Xst) says that the duration of follow-up is in lex.dur and the outcome variable in lex.Xst (as outlined above).  -->
<!-- #We want to stratify by alcohol -->

<!-- ggsurvplot(km,data=alcohol_lexis) -->


<!-- #Plot the Kaplan Meier Plot -->
<!-- Correlated Outcomes -->
<!-- We often have data that is correlated. For example 1) Multiple episods of flu in a single individual 2) Clustered-survey data - i.e individuals within households -->

<!-- Commonly we use a random-effects model to adjust for this correlation. -->

<!-- We do this using a very similar model to all previous ones. The general syntax for a clustered Logistic Regression is: -->

<!-- correlated_logistic <- glmer(DEPVAR~INDEPVAR + (1|CLUSTER_VAR),family = “binomial”, data = data) #For example if we have individuals within households then CLUSTER_VAR would be the variable specifying which house they were in -->

<!-- The general syntax for a clustered Poisson Regression (after Lexis setup as before is): -->

<!-- correlated_poisson <- glmer(lex.Xst~INDEPVAR + offset(log(lex.dur))+ (1|CLUSTER_VAR),family = “poisson”, data = data) #For example if we have individuals followed over time with multiple episodes of infection, with one line in our dataframe per period of follow-up, then CLUSTER_VAR would be the variable identifying that person across each block of follow-up -->

<!-- Specifying the Clustering and getting the data out -->
<!-- In both cases the key point in setting up the formula is the addition of: > +(1|CLUSTER_VAR) -->

<!-- If we have nested levels of clustering we can specify this easily: -->

<!-- +(1|first_clustering_level/nested_clustering_level) #For example +(1|province/village) where villages are nested within provinces -->

<!-- When we want to get out our odds ratios and confidence intervbals we have to specify that slightly diffrently than in a non-random-effects model -->

<!-- OR <- cbind(OR = exp(fixef(re_model)),exp(confint(re_model,parm=“beta_”))) ##Rather than the co-efficients we want the Fixed-Effects for our Odds Ratios ##And we want the confidence intervals specifcally for those rather than all the other variables a RE-Model makes -->

<!-- Otherwise everything is the same as all our other models -->

<!-- Saying how many integration points to use -->
<!-- R-Defaults to 1 integration point for RE-Models. We can if needed change this (more accurate vs more time) > model_name <- glmer(DEPVAR~INDEPVAR +(1|CLUSTERVAR), family = “binomial”, nAGQ = X, data = DATA) #Currently you can only set integration points >1 for models with a single level of clustering -->

<!-- Random Effects Logistic Regression -->
<!-- Sample Dataset 6 -->
<!-- To look at a random effects logistic regression we are using a (simulated) dataset about trachoma. This is a clustered dataset with many individuals per household. Again the dataset is freely available on the LSHTM Data Repository: “Trachoma Household Survey” We want to find out if a latrine is protective from getting trachoma. -->

<!-- hh_tf <- read.csv (file = "hh_tf.csv", header = TRUE, sep = ",") -->
<!-- #Read in data -->

<!-- re_hh_tf <- glmer(trachoma~factor(latrine)+ (1 | household),family = "binomial", data = hh_tf) -->
<!-- summary(re_hh_tf) -->
<!-- ## Generalized linear mixed model fit by maximum likelihood (Laplace -->
<!-- ##   Approximation) [glmerMod] -->
<!-- ##  Family: binomial  ( logit ) -->
<!-- ## Formula: trachoma ~ factor(latrine) + (1 | household) -->
<!-- ##    Data: hh_tf -->
<!-- ##  -->
<!-- ##      AIC      BIC   logLik deviance df.resid  -->
<!-- ##   1922.3   1940.9   -958.1   1916.3     3676  -->
<!-- ##  -->
<!-- ## Scaled residuals:  -->
<!-- ##      Min       1Q   Median       3Q      Max  -->
<!-- ## -2.27997 -0.01801 -0.01775 -0.01742  3.06478  -->
<!-- ##  -->
<!-- ## Random effects: -->
<!-- ##  Groups    Name        Variance Std.Dev. -->
<!-- ##  household (Intercept) 74.66    8.641    -->
<!-- ## Number of obs: 3679, groups:  household, 1761 -->
<!-- ##  -->
<!-- ## Fixed effects: -->
<!-- ##                  Estimate Std. Error z value Pr(>|z|)     -->
<!-- ## (Intercept)       -8.0091     0.3434 -23.321   <2e-16 *** -->
<!-- ## factor(latrine)1  -1.2450     0.5981  -2.082   0.0374 *   -->
<!-- ## --- -->
<!-- ## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 -->
<!-- ##  -->
<!-- ## Correlation of Fixed Effects: -->
<!-- ##             (Intr) -->
<!-- ## fctr(ltrn)1 0.017 -->
<!-- #Run a Random Effects model -->
<!-- #Individuals share a household which is designated by the 'household' variable -->
<!-- #This therefore is the unit of clustering -->

<!-- re_hh_tf_OR <- cbind(OR = exp(fixef(re_hh_tf)),exp(confint(re_hh_tf,parm="beta_"))) -->
<!-- ## Computing profile confidence intervals ... -->
<!-- re_hh_tf_out <- summary(re_hh_tf) -->
<!-- #We still want our P-Values and raw data just like before -->

<!-- re_hh_tf_out <- cbind (re_hh_tf_OR, re_hh_tf_out$coefficients) -->
<!-- #Combine it all together -->
<!-- re_hh_tf_out -->
<!-- ##                            OR        2.5 %       97.5 %  Estimate -->
<!-- ## (Intercept)      0.0003324303 0.0001656782 0.0006394189 -8.009080 -->
<!-- ## factor(latrine)1 0.2879269306 0.0828465249 0.8755127240 -1.245049 -->
<!-- ##                  Std. Error    z value      Pr(>|z|) -->
<!-- ## (Intercept)       0.3434317 -23.320735 2.731632e-120 -->
<!-- ## factor(latrine)1  0.5981090  -2.081642  3.737522e-02 -->
<!-- Random Effects Poisson Regression -->
<!-- Sample Dataset 7 -->
<!-- We are using a dataset about trachoma and PCR results - this is taken from a series of surveys in the Gambia. -->

<!-- Again the dataset is freely available on the LSHTM Data Repository: “Trachoma Cohort Study” -->

<!-- For your information: PCR is coded 2 for positive and 1 for negative. Clinical evidence of active trachoma is recorded as RE.Grade == TF -->

<!-- tf_pcr <- read.csv (file = "TF_PCR.csv", header = TRUE, sep = ",") -->
<!-- #Read in data -->

<!-- tf_pcr$timein <- cal.yr(as.Date(tf_pcr$timein, format = "%d-%b-%y")) -->
<!-- tf_pcr$timeout <- cal.yr(as.Date(tf_pcr$timeout, format = "%d-%b-%y")) -->
<!-- #Convert dates in to decimal dates -->

<!-- tf_pcr$RE.Grade <- relevel(tf_pcr$RE.Grade,"N") -->
<!-- #Make a normal clinical examination of the eye the baseline level -->

<!-- tf_pcr_lexis <- Lexis(entry = list("calendar" = timein), exit = list("calendar" = timeout), exit.status = PCR==2, data = tf_pcr) -->
<!-- ## NOTE: entry.status has been set to FALSE for all. -->
<!-- #Setup a lexis model -->

<!-- re_tf_pcr <- glmer(lex.Xst~factor(RE.Grade)+offset(log(lex.dur))+(1|ID), family = "poisson", data = tf_pcr_lexis) -->
<!-- #Multiple readings from the same invdividual -->
<!-- #The ID for the individuals is in the 'ID' variable -->
<!-- #This therefore is the unit of clustering -->

<!-- re_tf_OR <- cbind(HR = exp(fixef(re_tf_pcr)),exp(confint(re_tf_pcr,parm="beta_"))) -->
<!-- ## Computing profile confidence intervals ... -->
<!-- ##Rather than the co-efficients we want the Fixed-Effects for our Odds Ratios -->
<!-- ##And we want the confidence intervals specifcally for those rather than all the other variabls a RE-Model makes -->

<!-- re_tf_pcr_out <- summary(re_tf_pcr) -->
<!-- #We still want our P-Values and raw data just like before -->

<!-- re_tf_pcr_out <- cbind (re_tf_OR, re_tf_pcr_out$coefficients) -->
<!-- #Combine it all together -->
<!-- re_tf_pcr_out -->
<!-- ##                           HR     2.5 %   97.5 %   Estimate Std. Error -->
<!-- ## (Intercept)        1.5609469 1.1207591 2.080427  0.4452926  0.1563783 -->
<!-- ## factor(RE.Grade)TF 2.0846403 1.4417488 2.975819  0.7345963  0.1845991 -->
<!-- ## factor(RE.Grade)TS 0.5650381 0.1418873 1.824576 -0.5708621  0.6397415 -->
<!-- ##                       z value     Pr(>|z|) -->
<!-- ## (Intercept)         2.8475347 4.405929e-03 -->
<!-- ## factor(RE.Grade)TF  3.9794144 6.908522e-05 -->
<!-- ## factor(RE.Grade)TS -0.8923325 3.722148e-01 -->
<!-- #Individuals with clinical evidence of trachoma (TF) are twice as likely to have a positive PCR result. -->
<!-- Confirming there is clustering -->
<!-- When we run a Random-Effects model we also want to see if whether there is or is not evidence of clustering. -->

<!-- STATA does this by looking to see if rho (logistic regression, measure of within cluster variation) or alpha (poisson, measure of within cluster variation) = zero. -->

<!-- It does this STATA runs a likelihood ratio test comparing the model with and without random-effects (where ro/alpha = 0). -->

<!-- We can do the same by fitting a model without clustering and running an LRT comparing it to our RE-model. We can also get the equivalent estimate of rho. -->

<!-- no_re_hh_tf <- glm(trachoma~factor(latrine),family = "binomial", data = hh_tf) -->

<!-- re_hh_tf <- glmer(trachoma~factor(latrine)+ (1 | household),family = "binomial", data = hh_tf) -->

<!-- icc(re_hh_tf) -->
<!-- ##  -->
<!-- ## Generalized linear mixed model -->
<!-- ##  Family: binomial (logit) -->
<!-- ## Formula: trachoma ~ factor(latrine) + (1 | household) -->
<!-- ##  -->
<!-- ##   ICC (household): 0.957795 -->
<!-- #Give estimate of the intra-cluster correlation == rho -->

<!-- lrtest(re_hh_tf,no_re_hh_tf) -->
<!-- ## Warning in modelUpdate(objects[[i - 1]], objects[[i]]): original model was -->
<!-- ## of class "glmerMod", updated model is of class "glm" -->
<!-- ## Likelihood ratio test -->
<!-- ##  -->
<!-- ## Model 1: trachoma ~ factor(latrine) + (1 | household) -->
<!-- ## Model 2: trachoma ~ factor(latrine) -->
<!-- ##   #Df   LogLik Df  Chisq Pr(>Chisq)     -->
<!-- ## 1   3  -958.15                          -->
<!-- ## 2   2 -1188.67 -1 461.05  < 2.2e-16 *** -->
<!-- ## --- -->
<!-- ## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 -->
<!-- #Test the null hypothesis that rho = 0 and that there is therefore no evidence of clustering -->
<!-- The difference between the two models is not Zero and the p-value is highly suggestive that there is evidence of clustering -->

<!-- Introduction to Sample Size Calculations -->
<!-- Suggested by “Chrissy h Roberts” -->

<!-- Below the basics of a few types of sample size calculations - for example comparing proportions, means or to detect an effect size of a certain magnitude are shown. -->

<!-- Comparing proportions -->
<!-- The general syntax is -->

<!-- epi.propsize(treat = X, control = Y, power = Z, n = Z) #Specify either n - to compute power OR power to compute sample size #Specify the other as NA -->

<!-- epi.propsize(treat = 0.1, control = 0.2, power = 0.8, n = NA) -->
<!-- ## $n.total -->
<!-- ## [1] 398 -->
<!-- ##  -->
<!-- ## $n.treat -->
<!-- ## [1] 199 -->
<!-- ##  -->
<!-- ## $n.control -->
<!-- ## [1] 199 -->
<!-- ##  -->
<!-- ## $power -->
<!-- ## [1] 0.8 -->
<!-- ##  -->
<!-- ## $lambda -->
<!-- ## [1] 0.5 -->
<!-- #Calculate sample size needed to have 80% power to show difference of 10% assuming group 1 has proportion of 10% -->

<!-- epi.propsize(treat = 0.1, control = 0.2, power = 0.8, n = NA, sided.test = 1) -->
<!-- ## $n.total -->
<!-- ## [1] 314 -->
<!-- ##  -->
<!-- ## $n.treat -->
<!-- ## [1] 157 -->
<!-- ##  -->
<!-- ## $n.control -->
<!-- ## [1] 157 -->
<!-- ##  -->
<!-- ## $power -->
<!-- ## [1] 0.8 -->
<!-- ##  -->
<!-- ## $lambda -->
<!-- ## [1] 0.5 -->
<!-- #As above but a 1-sided test -->

<!-- epi.propsize(treat = 0.1, control = 0.2, power = NA, n = 300) -->
<!-- ## $n.total -->
<!-- ## [1] 300 -->
<!-- ##  -->
<!-- ## $n.treat -->
<!-- ## [1] 150 -->
<!-- ##  -->
<!-- ## $n.control -->
<!-- ## [1] 150 -->
<!-- ##  -->
<!-- ## $power -->
<!-- ## [1] 0.6808308 -->
<!-- ##  -->
<!-- ## $lambda -->
<!-- ## [1] 0.5 -->
<!-- #Calculate the power to detect a difference between arms given a sample size of 300 -->
<!-- Sample size to detect a difference in proportions -->
<!-- We need to know the 1) Proportion in control group 2) Odds ratio we wish to detect -->

<!-- The general syntax when giving an Odds Ratio to detect is -->

<!-- epi.ccsize(OR = X, p0 = Y, n = Z, power = Z) #Specify either n - to compute power OR power to compute sample size #Specify the other as NA epi.ccsize(OR = X, p0 = Y, n = Z, power = Z, r = ) #Where R is the ratio of controls to cases -->

<!-- epi.ccsize(OR = 2, p0 = 0.2, n = NA, power = 0.8) -->
<!-- ## $n.total -->
<!-- ## [1] 344 -->
<!-- ##  -->
<!-- ## $n.case -->
<!-- ## [1] 172 -->
<!-- ##  -->
<!-- ## $n.control -->
<!-- ## [1] 172 -->
<!-- ##  -->
<!-- ## $power -->
<!-- ## [1] 0.8 -->
<!-- ##  -->
<!-- ## $OR -->
<!-- ## [1] 2 -->
<!-- help("epi.ccsize") -->
<!-- #Calculate sample size required to detect an Odds Ratio of 2 given an expected prevalence of 20% in the control group -->

<!-- epi.ccsize(OR = 4, p0 = 0.3, n = NA, power = 0.8, r =4) -->
<!-- ## $n.total -->
<!-- ## [1] 110 -->
<!-- ##  -->
<!-- ## $n.case -->
<!-- ## [1] 22 -->
<!-- ##  -->
<!-- ## $n.control -->
<!-- ## [1] 88 -->
<!-- ##  -->
<!-- ## $power -->
<!-- ## [1] 0.8 -->
<!-- ##  -->
<!-- ## $OR -->
<!-- ## [1] 4 -->
<!-- #Calculate sample size required to detect an Odds Ratio of 4 given an expected prevalence of 30% in the control group and 4 controls for every case -->
<!-- Sample size for detecting a specific prevalence -->
<!-- The general syntax is > epi.simplesize(Py = prevalence_to_find, epsilon.R = variation_around_prevalence, method = “proportion”) #epsilon.R is specified as a proportion of Py #So if we want to find 10% +/- 2% we would set epsion.R as 0.2 -->

<!-- epi.simplesize(Py = 0.2, epsilon.r = 0.3, method = "proportion") -->
<!-- ## [1] 171 -->
<!-- #Calculate sample size required to detect a true prevalence of 10% +/- 3% -->

<!-- #We can specify a fixed population calculation by adding in 'n ='  -->
<!-- Sample size for comparing means -->
<!-- The general syntax is -->

<!-- power.t.test(delta = DIFF_BETWEEN_MEANS, power = Z, type =“SAMPLE_TYPE”) #Type may be “two.sample”, “one.sample”, “paired” #We can additionally specify the anticiapted standard deviation for example power.t.test(delta = DIFF_BETWEEN_MEANS, power = Z, type =“SAMPLE_TYPE”, sd = 1) -->

<!-- OR alternatively (using same package as other sample size calculations) >epi.meansize(treat = X, control = Y, n = Z, power = Z, sigma = A) #Specify either n - to compute power OR power to compute sample size #Specify the other as NA #Sigma is the anticipated Standard Deviation -->

<!-- power.t.test(delta = 2, power = 0.8, type ="two.sample", sd = 1) -->
<!-- ##  -->
<!-- ##      Two-sample t test power calculation  -->
<!-- ##  -->
<!-- ##               n = 5.090008 -->
<!-- ##           delta = 2 -->
<!-- ##              sd = 1 -->
<!-- ##       sig.level = 0.05 -->
<!-- ##           power = 0.8 -->
<!-- ##     alternative = two.sided -->
<!-- ##  -->
<!-- ## NOTE: n is number in *each* group -->
<!-- #Calculate sample size needed to have 80% power to show difference of 2 in the mean value between two groups -->

<!-- power.t.test(delta = 2, power = 0.8, type ="paired", sd = 1) -->
<!-- ##  -->
<!-- ##      Paired t test power calculation  -->
<!-- ##  -->
<!-- ##               n = 4.220731 -->
<!-- ##           delta = 2 -->
<!-- ##              sd = 1 -->
<!-- ##       sig.level = 0.05 -->
<!-- ##           power = 0.8 -->
<!-- ##     alternative = two.sided -->
<!-- ##  -->
<!-- ## NOTE: n is number of *pairs*, sd is std.dev. of *differences* within pairs -->
<!-- #Calculate sample size needed to have 80% power to show difference of 0.5 in the mean of two paired values - assuming a standard deviation of 1 -->

<!-- epi.meansize(treat = 14 , control = 12, n = NA, power = 0.8, sigma = 1) -->
<!-- ## $n.total -->
<!-- ## [1] 8 -->
<!-- ##  -->
<!-- ## $n.treat -->
<!-- ## [1] 4 -->
<!-- ##  -->
<!-- ## $n.control -->
<!-- ## [1] 4 -->
<!-- ##  -->
<!-- ## $power -->
<!-- ## [1] 0.8 -->
<!-- ##  -->
<!-- ## $delta -->
<!-- ## [1] 2 -->
<!-- #Calculate sample size needed to have 80% power to show difference of 2 between mean assuming a standard deviation of 1 -->
<!-- #Capture Recapture Population Estimates We can calculate the predicted true population based on results of a capture-recapture survey The general syntax is > epi.popsize(people_seen_firs_time,people_seen_second_time,people_seen_both_times) -->

<!-- epi.popsize(1104,1112,920) -->
<!-- ##  -->
<!-- ##  Estimated population size: 1334 (95% CI 1319 - 1350) -->
<!-- ##  Estimated number of untested subjects: 38 (95% CI 23 - 54) -->
<!-- #We saw a 1104 people in our first survey -->
<!-- #We saw 1112 people in our second survey -->
<!-- #920 people were seen both times -->
<!-- #Calculate estimated true population and proportion of individuals missed both times -->
<!-- Multiple Imputation -->
<!-- Suggested by “Tom Yates” -->

<!-- I presume as always that you understand the caveats of multiple imputation. The MICE package uses Multivariate Imputation via Chained Equations which is a common approach to multiple imputation. -->

<!-- Sample Dataset 8 -->
<!-- I deleted some data from our earlier scabies dataset so we can look at how multiple imputation works. -->

<!-- Again the dataset with missing data is available on the LSHTM Data-Repository: “Scabies Missing Data” -->

<!-- scabies_missing <- read.csv(file = "scabies_results_missing.csv", header = TRUE, sep = ",") -->
<!-- Looking at what data is missing -->
<!-- We can see what data is missing using the ‘md.pattern’ command. The general syntax is: -->

<!-- md.pattern(dataset) -->

<!-- md.pattern(scabies_missing) -->
<!-- ##      village impetigo_inactive other_lesions room_cohabitation age -->
<!-- ## 1439       1                 1             1                 1   1 -->
<!-- ##   63       1                 1             1                 1   0 -->
<!-- ##   81       1                 1             1                 1   1 -->
<!-- ##   52       1                 1             1                 1   1 -->
<!-- ##   54       1                 1             1                 0   1 -->
<!-- ##   70       1                 1             1                 1   1 -->
<!-- ##  103       1                 1             1                 1   1 -->
<!-- ##    4       1                 1             1                 1   0 -->
<!-- ##    7       1                 1             1                 1   1 -->
<!-- ##    2       1                 1             1                 0   0 -->
<!-- ##    2       1                 1             1                 0   1 -->
<!-- ##    4       1                 1             1                 0   1 -->
<!-- ##    1       1                 1             1                 1   0 -->
<!-- ##    3       1                 1             1                 1   1 -->
<!-- ##    2       1                 1             1                 1   1 -->
<!-- ##    3       1                 1             1                 0   1 -->
<!-- ##    3       1                 1             1                 1   0 -->
<!-- ##    2       1                 1             1                 1   1 -->
<!-- ##    4       1                 1             1                 1   1 -->
<!-- ##    4       1                 1             1                 0   1 -->
<!-- ##    3       1                 1             1                 1   1 -->
<!-- ##    1       1                 1             1                 0   0 -->
<!-- ##    1       1                 1             1                 0   1 -->
<!-- ##            0                 0             0                71  74 -->
<!-- ##      house_inhabitants scabies_infestation gender impetigo_active     -->
<!-- ## 1439                 1                   1      1               1   0 -->
<!-- ##   63                 1                   1      1               1   1 -->
<!-- ##   81                 1                   1      0               1   1 -->
<!-- ##   52                 0                   1      1               1   1 -->
<!-- ##   54                 1                   1      1               1   1 -->
<!-- ##   70                 1                   0      1               1   1 -->
<!-- ##  103                 1                   1      1               0   1 -->
<!-- ##    4                 0                   1      1               1   2 -->
<!-- ##    7                 0                   1      0               1   2 -->
<!-- ##    2                 1                   1      1               1   2 -->
<!-- ##    2                 1                   1      0               1   2 -->
<!-- ##    4                 0                   1      1               1   2 -->
<!-- ##    1                 1                   0      1               1   2 -->
<!-- ##    3                 1                   0      0               1   2 -->
<!-- ##    2                 0                   0      1               1   2 -->
<!-- ##    3                 1                   0      1               1   2 -->
<!-- ##    3                 1                   1      1               0   2 -->
<!-- ##    2                 1                   1      0               0   2 -->
<!-- ##    4                 0                   1      1               0   2 -->
<!-- ##    4                 1                   1      1               0   2 -->
<!-- ##    3                 1                   0      1               0   2 -->
<!-- ##    1                 1                   1      0               1   3 -->
<!-- ##    1                 0                   0      1               1   3 -->
<!-- ##                     74                  83     96             119 517 -->
<!-- Imputing data using Multivariate Imputation via Chained Equations -->
<!-- We can impute our data using the ‘mice’ command. The general syntax is: -->

<!-- imputed_data <- mice(data = datasets, m = number_imputet_sets_to_make, defaultMethod = “method_for_imputing_values”, maxit = number_iterations_to_impute_values, seed = 500) -->

<!-- This generates ‘m’ datasets each with slightly different imputed data. -->

<!-- NB Seed is just a number which kicks off the random-number generator. So if you alter the seed number the imputed dataset is going to change. -->

<!-- Specifying which imputation method to use -->
<!-- We don’t actually need to specify defaultMethod unless we want to change how R imputes values. -->

<!-- By default ‘mice’ will use the following methods: * Numeric values: pmm, predictive mean matching * Binary data: logreg, logistic regression imputation * Unordered categorical data with >2 levels: polyreg, polytomous regression * Ordered categorical data with >2 levels: polr, proportional odds model -->

<!-- We can specify alternative methods * norm = Bayesian linear regression -->

<!-- For example we could use Bayesian linear regression for Numeric imputation by stating -->

<!-- defaultMethod = c(“norm”,“logreg”, “polyreg”, “polr”), -->

<!-- imputed_scabies_data <- mice(data = scabies_missing, m = 5, maxit = 10, seed = 500) -->
<!-- By default MICE uses all values to impute from and imputes all missing values. These settings can be changed if needed -->

<!-- Checking our imputation -->
<!-- MICE includes a specific graphics package which is set up to work well with imputed data. We can use this to see how reasonable our imputations are. -->

<!-- We can show box and whisker plots to see if our nurmerical imputed data looks reasonable with -->

<!-- bwplot(imputed_dataset, variable) #R automatically colour codes the raw and imputed datasets differently -->

<!-- bwplot(imputed_scabies_data, age) -->


<!-- We can see distributions also with a density plot -->

<!-- densityplot(imputed_dataset) #R automatically colour codes the raw and imputed datasets differently -->

<!-- We can see if association still look reasonable -->

<!-- xyplot(imputer_dataset,variable_a~variable_b) #R automatically colour codes the raw and imputed datasets differently -->

<!-- Seeing an imputed dataset -->
<!-- We can see an individual imputed dataset with -->

<!-- imputed_set_1 <- complete(imputed_data,1) -->

<!-- We can merge the datasets together - for example into a long dataset -->

<!-- imputed_long <- complete(imputed_data, “long”) -->

<!-- If we want to include the original dataset we can do that too -->

<!-- imputed_long <- complete(imputed_data, “long”, include = TRUE) -->

<!-- Manipulating an imputed dataset -->
<!-- If we want to apply a data-manipulation such as converting a continuous to a categorical variable or making a new variable we can do this across our datasets. -->

<!-- First of all we convert the imputed data into a standard-dataset that we can work with: -->

<!-- imputed_long <- mice::complete(imputed_data, “long”, include = TRUE) #We have to specify mice::complete as ‘tidyr’ also has function called complete, so this lets R know which version to use. #Its important we include the Raw data as this lets us put the data back in to an ‘imputed’ dataset for the purposes of analysis #Now we can perform whatever manipulations we want #Then we convert the manipulated dataset back to an ‘imputed’ dataset using “as.mids” -->

<!-- coded_imputed_data <- as.mids(imputed_long) -->

<!-- complete_scabies_data <- mice::complete(imputed_scabies_data,"long", include = TRUE) -->
<!-- #Take out imputed data and make a long data-table. Include the original dataset with missing values. -->


<!-- complete_scabies_data$agegroups <- as.factor(cut(complete_scabies_data$age, c(0,10,20,Inf), include.lowest = TRUE, labels = FALSE))  -->
<!-- #Categorise age into groups across all our imputed datasets -->
<!-- #Labels == False asigns a numeric value to each category (i.e 1,2,3) -->

<!-- complete_scabies_data$agegroups <-relevel(complete_scabies_data$agegroups, ref = 3) -->
<!-- #Set the baseline value -->

<!-- coded_imputed_scabies <- as.mids(complete_scabies_data) -->
<!-- #Convert back to "Imputed" format -->
<!-- Fitting a model across all the imputed datasets -->
<!-- If we want to fit a model across all the different imputed datasets then we can do that! The general syntax is very similar to a normal logistic regression but with a few changes as we are working with multiple imputed datasets and then pooling the results. -->

<!-- imputed_model <- with(imputed_data,glm(attack~ smokes+female+hsgrad, family = binomial())) #We specify the ‘with’ command to say we are modelling across each of our imputed datasets -->

<!-- imputed_model_summary <- (summary(pool(imputed_model))) #We specify the ’pool’command to now pool the estimates across each of our imputed datasets -->

<!-- imputed_model_OR <- exp(cbind(imputed_model_summary[,“est”],imputed_model_summary[,“lo 95”],imputed_model_summary[,“hi 95”])) #exponentiate the coefficient and the lower and upper 95% CI -->

<!-- imputed_model_summary <- (cbind(imputed_model_OR,imputed_model_summary)) #Combine into a single table -->

<!-- imputed_model_summary Show the summary -->

<!-- #Imputed Data -->
<!-- imputed_scabies_model <- with(coded_imputed_scabies,glm(impetigo_active~ factor(scabies_infestation)+factor(gender)+factor(agegroups), family = binomial())) -->
<!-- imputed_scabies_model_summary <- (summary(pool(imputed_scabies_model))) -->
<!-- imputed_scabies_model_OR <- exp(cbind(imputed_scabies_model_summary[,"est"],imputed_scabies_model_summary[,"lo 95"],imputed_scabies_model_summary[,"hi 95"])) -->
<!-- imputed_scabies_model_summary <- (cbind(imputed_scabies_model_OR,imputed_scabies_model_summary)) -->

<!-- #Let us also fit this same model to the full-original dataset -->
<!-- impetigorisk <- glm(impetigo_active~factor(scabies_infestation)+factor(gender)+factor(agegroups),data=scabies,family=binomial()) -->
<!-- impetigorisk_OR <- exp(cbind(OR= coef(impetigorisk), confint(impetigorisk))) -->
<!-- ## Waiting for profiling to be done... -->
<!-- impetigorisk_summary <- summary(impetigorisk) -->
<!-- impetigorisk_summary <- cbind(impetigorisk_OR, impetigorisk_summary$coefficients) -->
<!-- You can see we get very similar estimates of risk factors for impetigo from both our imputed dataset and our original dataset -->

<!-- imputed_scabies_model_summary -->
<!-- ##                                                                    est -->
<!-- ## (Intercept)                  0.1276546 0.09976431 0.1633419 -2.0584271 -->
<!-- ## factor(scabies_infestation)1 1.8930284 1.45590022 2.4614026  0.6381779 -->
<!-- ## factor(gender)male           1.7632437 1.40651386 2.2104499  0.5671551 -->
<!-- ## factor(agegroups)1           3.9041378 2.96900602 5.1338031  1.3620370 -->
<!-- ## factor(agegroups)2           2.9657803 2.16356394 4.0654461  1.0871402 -->
<!-- ##                                     se          t        df     Pr(>|t|) -->
<!-- ## (Intercept)                  0.1255421 -16.396309  649.1702 0.000000e+00 -->
<!-- ## factor(scabies_infestation)1 0.1333104   4.787156  250.2391 2.896359e-06 -->
<!-- ## factor(gender)male           0.1145406   4.951566  176.9970 1.708353e-06 -->
<!-- ## factor(agegroups)1           0.1396040   9.756431 1736.0393 0.000000e+00 -->
<!-- ## factor(agegroups)2           0.1606484   6.767200  736.6786 2.679634e-11 -->
<!-- ##                                   lo 95      hi 95 nmis        fmi -->
<!-- ## (Intercept)                  -2.3049448 -1.8119095   NA 0.06546130 -->
<!-- ## factor(scabies_infestation)1  0.3756244  0.9007314   NA 0.12360737 -->
<!-- ## factor(gender)male            0.3411142  0.7931961   NA 0.15147441 -->
<!-- ## factor(agegroups)1            1.0882272  1.6358467   NA 0.01424917 -->
<!-- ## factor(agegroups)2            0.7717568  1.4025235   NA 0.05911479 -->
<!-- ##                                  lambda -->
<!-- ## (Intercept)                  0.06258655 -->
<!-- ## factor(scabies_infestation)1 0.11663081 -->
<!-- ## factor(gender)male           0.14194025 -->
<!-- ## factor(agegroups)1           0.01311419 -->
<!-- ## factor(agegroups)2           0.05656385 -->
<!-- #Show the result of logistic regression on the imputed dataset -->
<!-- impetigorisk_summary -->
<!-- ##                                       OR     2.5 %    97.5 %   Estimate -->
<!-- ## (Intercept)                    0.1345342 0.1057763 0.1691508 -2.0059369 -->
<!-- ## factor(scabies_infestation)yes 1.9325497 1.5143192 2.4661511  0.6588402 -->
<!-- ## factor(gender)male             1.7577258 1.4287526 2.1630912  0.5640208 -->
<!-- ## factor(agegroups)0-10          3.7025777 2.8401712 4.8620068  1.3090292 -->
<!-- ## factor(agegroups)11-20         2.7347978 2.0226139 3.7127619  1.0060575 -->
<!-- ##                                Std. Error    z value     Pr(>|z|) -->
<!-- ## (Intercept)                     0.1196454 -16.765690 4.349338e-63 -->
<!-- ## factor(scabies_infestation)yes  0.1243502   5.298267 1.169072e-07 -->
<!-- ## factor(gender)male              0.1057719   5.332426 9.690930e-08 -->
<!-- ## factor(agegroups)0-10           0.1370214   9.553466 1.254279e-21 -->
<!-- ## factor(agegroups)11-20          0.1548162   6.498398 8.117993e-11 -->
<!-- #Show the result of logistic regression on the original dataset -->
<!-- Comparing two imputed datasets -->
<!-- When we want to compate two nested models derived from imputed datasets we can do so using ‘pool.compare’ -->

<!-- pool.compare(model_1, model_2, data = NULL, method = “Wald”) #This is similar to performing an LRT on two normal models - but in the circumstance of pooled data this approach should be used. -->

<!-- Intra-class-correlation Co-efficients and Design Effects -->
<!-- Suggested and with help from “Peter MacPherson” and “Daniel Parker” -->

<!-- Statisitcally there are lots of different ways to calculate an Intra-Class-Correlation coefficient. -->

<!-- Below I provide an outline of a few otions. Which one is appropriate may depend on why you need an ICC. -->

<!-- Survey Data and Design Effects -->
<!-- If we have used a complex survey design then we can declare the survey design and then calculate the design effect. -->

<!-- We declare the survey design using > survey_dataset <- svydesign(id=~cluster_var,data=dataset) -->

<!-- We can then calculate the frequency of our outcome (say presence of Trachoma) adjusting for clustering and then get out the design effect of our data compared to SRS with replacement. >svymean(~outcome,survey_dataset,proportion = TRUE, deff = “replace”) -->

<!-- #Lets look at our scabies survey again -->
<!-- scabies <- read.csv(file = "S1-Dataset_CSV.csv", header = TRUE, sep = ",") -->

<!-- #Declare the survey design -->
<!-- survey_scabies <- svydesign(id=~Village, data = scabies) -->
<!-- ## Warning in svydesign.default(id = ~Village, data = scabies): No weights or -->
<!-- ## probabilities supplied, assuming equal probability -->
<!-- svymean(~scabies_infestation,survey_scabies,proportion = TRUE, deff = "replace") -->
<!-- ##                            mean       SE   DEff -->
<!-- ## scabies_infestationno  0.808176 0.010242 1.2903 -->
<!-- ## scabies_infestationyes 0.191824 0.010242 1.2903 -->
<!-- #Work out the DF of scabies adjusting for clustering at the village level -->
<!-- Survey Data and ICC -->
<!-- If we have used a complex survey design then we can also get out an ICC and then given the cluster size calculate the design effect. -->

<!-- The general syntax is > ICCest(group_var,outcome_var,data=dataset) -->

<!-- It expects outcome_var to be coded numerically -->

<!-- scabies$hasscabies <- 0 -->
<!-- scabies$hasscabies[scabies$scabies_infestation == "yes"] <- 1 -->
<!-- #Convert our yes/no variable to 0 / 1 variable as expected by ICCest -->

<!-- ICCest(Village,hasscabies, data = scabies) -->
<!-- ## Warning in ICCest(Village, hasscabies, data = scabies): 'x' has been -->
<!-- ## coerced to a factor -->
<!-- ## $ICC -->
<!-- ## [1] 0.004087404 -->
<!-- ##  -->
<!-- ## $LowerCI -->
<!-- ## [1] -0.0009097985 -->
<!-- ##  -->
<!-- ## $UpperCI -->
<!-- ## [1] 0.02560099 -->
<!-- ##  -->
<!-- ## $N -->
<!-- ## [1] 10 -->
<!-- ##  -->
<!-- ## $k -->
<!-- ## [1] 185.7484 -->
<!-- ##  -->
<!-- ## $varw -->
<!-- ## [1] 0.1545527 -->
<!-- ##  -->
<!-- ## $vara -->
<!-- ## [1] 0.0006343122 -->
<!-- #Gives the ICC for scabies and the mean village size. -->
<!-- #ICC of 0.004087404 -->
<!-- #K (average cluster size) 185 -->
<!-- #So DF = 1 + (185-1)*0.004087404 -->
<!-- 1 + (185-1)*0.004087404 -->
<!-- ## [1] 1.752082 -->
<!-- #Which is broadly similar to the output of the previous estimate -->
<!-- Getting an ICC from a Random-Effects Model -->
<!-- Alternatively in some situations you might want to fit a random-effects model and then calculate the ICC from the model. -->

<!-- The command for this is just: > icc (re_model) -->

<!-- re_village_scabies <- glmer(scabies_infestation~ + (1 | Village),family = "binomial", data = scabies, nAGQ = 10) -->
<!-- #Run a model clustered at household level. Ten intgration points. -->
<!-- icc(re_village_scabies) -->
<!-- ##  -->
<!-- ## Generalized linear mixed model -->
<!-- ##  Family: binomial (logit) -->
<!-- ## Formula: scabies_infestation ~ +(1 | Village) -->
<!-- ##  -->
<!-- ##   ICC (Village): 0.003350 -->
<!-- #Get out our ICC -->
<!-- #You can then use the ICC to calculate the design effect using standard formulas -->
<!-- Meta-analysis -->
<!-- ##Meta-analysis of proportions -->

<!-- The general syntax is > metaprop(Number_Events, Number_Observations) -->

<!-- metaprop(c(267,175,219,117),c(406,329,223,137),method="GLMM") -->
<!-- ##   proportion           95%-CI %W(fixed) %W(random) -->
<!-- ## 1     0.6576 [0.6092; 0.7037]        --         -- -->
<!-- ## 2     0.5319 [0.4764; 0.5869]        --         -- -->
<!-- ## 3     0.9821 [0.9547; 0.9951]        --         -- -->
<!-- ## 4     0.8540 [0.7836; 0.9085]        --         -- -->
<!-- ##  -->
<!-- ## Number of studies combined: k = 4 -->
<!-- ##  -->
<!-- ##                      proportion           95%-CI  z p-value -->
<!-- ## Fixed effect model       0.7105 [0.6829; 0.7366] --      -- -->
<!-- ## Random effects model     0.8330 [0.5400; 0.9550] --      -- -->
<!-- ##  -->
<!-- ## Quantifying heterogeneity: -->
<!-- ## tau^2 = 2.1010; H = 9.04; I^2 = 98.8% -->
<!-- ##  -->
<!-- ## Test of heterogeneity: -->
<!-- ##       Q d.f.  p-value             Test -->
<!-- ##   87.24    3 < 0.0001        Wald-type -->
<!-- ##  187.20    3 < 0.0001 Likelihood-Ratio -->
<!-- ##  -->
<!-- ## Details on meta-analytical method: -->
<!-- ## - Random intercept logistic regression model -->
<!-- ## - Maximum-likelihood estimator for tau^2 -->
<!-- ## - Logit transformation -->
<!-- ## - Clopper-Pearson confidence interval for individual studies -->
<!-- #Calculate the weighted proportion of events across four studies -->
<!-- #Method = GLMM specifies that a random-intercept should be used -->
<!-- meta_data <- metaprop(c(267,175,219,117),c(406,329,223,137),method="GLMM", comb.fixed = FALSE) -->
<!-- #As above but don't give a fixed-effec estimate and store the results so we can use them in our plot -->

<!-- meta_forest <- forest(meta_data) -->


<!-- #Calculate the weighted proportion of events across four studies -->
<!-- #Method = GLMM specifies that a random-intercept should be used -->
<!-- GGPLOT2 -->
<!-- Basic elements of a GGPLOT -->
<!-- The GGPLOT package makes much better graphs than the basic R package. This is a very brief introduction to the syntax of GGPLOT2. -->

<!-- Everything in ggplot combines a similar group of elements to make its graphs. In any plot we specify 1) Dataset 2) A Co-ordinate system 3) How to show the data points (ggplot calls these ‘geoms’) -->

<!-- We add elements on top of one another to build up the plot. The first two elements are given together via the syntax > plot <- ggplot2 (dataset, aes(co-ordinate system)) #Make something called plot. Take data from ‘dataset’ and define the way to show the data as statd by the co-ordinate system. -->

<!-- There are a variety of co-ordinate systems. When our plot involves a single variable (i.e age as a histograme) then we need to specify X = -->

<!-- plot <- ggplot2 (scabies, aes(x = age)) #Look in the scabies dataframe - we are plotting age along the X-Axis -->

<!-- If we want to stratify our plots by a factor we can do so by telling GGPLOT with ‘fill’ -->

<!-- plot <- ggplot2 (scabies, aes(x = age, fill = gender)) #Stratify by Gender - i.e draw two histograms with different colours based on the variable gender -->

<!-- When our plot involves two variables (i.e a scatter plot or a box-plot) then we specify X= and y = -->

<!-- plot <- ggplot2 (scabies, aes(x = house_inhabitants, y = room_cohabitation)) #Look in the scabies dataframe - we are plotting number people in the house along the X-Axis and number of people sharing a room on the Y-Axis -->

<!-- Specifcying the type of graph - ‘geoms’ -->
<!-- We then tell GGPLOT how to render the data - this is the geom. -->

<!-- Some common geoms are: geom_point = scatterplot geom_histogram = histogram geom_boxplot -->

<!-- We can specify specific values for the data-points (“geom”) like colour: > + geom_point(aes(colour = factor(gender))) #Add to our plot by drawing each value as a point i.e make a scatter-plot #Set the colour of each datapoint based on the factor variable gender -->

<!-- Labels, Legends etc -->
<!-- We can add labels using: > + labs (title = “Graph Title”, x = “X-Axis Label”, y = “Y-Axis Label”) #Add to our plot by adding labels for axis etc -->

<!-- Splitting GGPLOT code across multiple lines -->
<!-- Sometimes to make the code easier to follow we split it across several lines. For example -->

<!-- plot <- ggplot2 (scabies, aes(x = house_inhabitants, y = room_cohabitation))+ geom_point(aes(colour = factor(gender))) + labs (title = “House size and Room Cohabitation”, x = “House size”, y = “Room size”) #Note we put a PLUS sign at the end of each line to tell R that the code is continuing -->

<!-- Example Plots with GGPLOT -->
<!-- Here are some examples similar to the graphs we drew with basic R earlier. The aim is just to give a feel for how the plots are constructed. -->

<!-- GGPLOT Scatterplot -->
<!-- gg_scatter <- ggplot (scabies, aes(x = house_inhabitants, y = room_cohabitation)) + -->
<!-- #Two variable House size and room cohabitation -->

<!-- #NB I'm saving the plot under the name gg_scatter by saying gg_scatter <- X,Y,Z -->
<!--   geom_point(aes(colour = factor(gender))) + -->

<!-- #I'm drawing a scatter plot and colouring dots based on gender   -->
<!--   labs(title = "House size vs Number people sharing a room", x ="House Size", y = "People sharing a room") -->

<!-- #I'm giving some labels to the graph -->

<!-- gg_scatter -->


<!-- #Show the gg_scatter graph -->
<!-- GGPLOT Histogram -->
<!-- gg_histo <- ggplot (scabies, aes(x = age)) + -->

<!-- #I have a single variable age -->
<!-- geom_histogram() + -->

<!-- #With which I will draw a scatterplot   -->
<!-- labs(title = "Distribution of age in study", y = "No. participants", x = "Age (yrs)") -->

<!-- #I will give some axis and graph labels -->
<!-- gg_histo -->
<!-- ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. -->


<!-- GGPLOT Boxplot -->
<!-- gg_boxplot <- ggplot (scabies, aes(x = gender, y = age)) + -->

<!-- #I have two variable Gender and Age -->
<!--   geom_boxplot(aes(colour = factor(gender))) + -->

<!-- #I am drawing a box plot and colouring them in by age -->
<!--   labs(title = "Distribution of age by gender", y = "Age", x = "Gender") -->

<!-- #I have some axis labels -->
<!-- gg_boxplot -->


<!-- Examples of powerful plotting with GGPLOT -->
<!-- We can do really fancy things with GGPLOT like make multiple graphs in a single panel. We declare these using one of the “facet” options. For example we could draw age distribution histograms by village. -->

<!-- library(ggplot2) -->
<!-- gg_histo_by_village <- ggplot (scabies, aes(x = age, fill = gender)) + -->

<!-- #I have only one co-ordinate x = age -->
<!-- #Stratifying on = gender; i.e fill in  colour based on  value in gender -->
<!--   geom_histogram(breaks = seq(0,80, by = 2)) + -->

<!-- #I'm plotting a histogram. I've divided the data between 0 to 80 it to blocks 5 years wide. -->
<!--   labs(title = "Distribution of age in study", y = "No. participants", x = "Age (yrs)")+ -->

<!-- #I've given some labels -->
<!--   facet_wrap("Village") -->

<!-- #I've broken the data down in to several facets based on the village -->
<!-- gg_histo_by_village -->







































---

## Survival Analysis in R

https://rviews.rstudio.com/2017/09/25/survival-analysis-with-r/

<!-- library(survival) -->
<!-- library(ranger) -->
<!-- library(ggplot2) -->
<!-- library(dplyr) -->
<!-- library(ggfortify) -->

<!-- data(veteran) -->
<!-- head(veteran) -->

<!-- km <- with(veteran, Surv(time, status)) -->

<!-- head(km,80) -->


<!-- km_fit <- survfit(Surv(time, status) ~ 1, data = veteran) -->

<!-- km_fit -->

<!-- summary(km_fit, times = c(1,30,60,90*(1:10))) -->

<!-- plot(km_fit, xlab="Days", main = 'Kaplan Meyer Plot') #base graphics is always ready -->
<!-- autoplot(km_fit) -->

<!-- km_trt_fit <- survfit(Surv(time, status) ~ trt, data = veteran) -->
<!-- km_trt_fit -->

<!-- autoplot(km_trt_fit) -->


<!-- vet <- mutate(veteran, AG = ifelse((age < 60), "LT60", "OV60"), -->
<!--               AG = factor(AG), -->
<!--               trt = factor(trt,labels=c("standard","test")), -->
<!--               prior = factor(prior,labels=c("N0","Yes"))) -->

<!-- km_AG_fit <- survfit(Surv(time, status) ~ AG, data = vet) -->
<!-- km_AG_fit -->

<!-- autoplot(km_AG_fit) -->


<!-- Fit Cox Model -->
<!-- cox <- coxph(Surv(time, status) ~ trt + celltype + karno + diagtime + age + prior , data = vet) -->
<!-- cox -->
<!-- summary(cox) -->

<!-- cox_fit <- survfit(cox) -->
<!-- cox_fit -->
<!-- plot(cox_fit, main = "cph model", xlab="Days") -->
<!-- autoplot(cox_fit) -->

<!-- aa_fit <-aareg(Surv(time, status) ~ trt + celltype + -->
<!--                    karno + diagtime + age + prior ,  -->
<!--                data = vet) -->
<!-- aa_fit -->


<!-- #summary(aa_fit)  # provides a more complete summary of results -->
<!-- autoplot(aa_fit) -->


<!-- # ranger model -->
<!-- r_fit <- ranger(Surv(time, status) ~ trt + celltype +  -->
<!--                     karno + diagtime + age + prior, -->
<!--                 data = vet, -->
<!--                 mtry = 4, -->
<!--                 importance = "permutation", -->
<!--                 splitrule = "extratrees", -->
<!--                 verbose = TRUE) -->


<!-- # Average the survival models -->
<!-- death_times <- r_fit$unique.death.times  -->
<!-- surv_prob <- data.frame(r_fit$survival) -->
<!-- avg_prob <- sapply(surv_prob,mean) -->

<!-- # Plot the survival models for each patient -->
<!-- plot(r_fit$unique.death.times,r_fit$survival[1,],  -->
<!--      type = "l",  -->
<!--      ylim = c(0,1), -->
<!--      col = "red", -->
<!--      xlab = "Days", -->
<!--      ylab = "survival", -->
<!--      main = "Patient Survival Curves") -->

<!-- # -->
<!-- cols <- colors() -->
<!-- for (n in sample(c(2:dim(vet)[1]), 20)){ -->
<!--     lines(r_fit$unique.death.times, r_fit$survival[n,], type = "l", col = cols[n]) -->
<!-- } -->
<!-- lines(death_times, avg_prob, lwd = 2) -->
<!-- legend(500, 0.7, legend = c('Average = black')) -->


<!-- vi <- data.frame(sort(round(r_fit$variable.importance, 4), decreasing = TRUE)) -->
<!-- names(vi) <- "importance" -->
<!-- head(vi) -->


<!-- cat("Prediction Error = 1 - Harrell's c-index = ", r_fit$prediction.error) -->


<!-- # Set up for ggplot -->
<!-- kmi <- rep("KM",length(km_fit$time)) -->
<!-- km_df <- data.frame(km_fit$time,km_fit$surv,kmi) -->
<!-- names(km_df) <- c("Time","Surv","Model") -->

<!-- coxi <- rep("Cox",length(cox_fit$time)) -->
<!-- cox_df <- data.frame(cox_fit$time,cox_fit$surv,coxi) -->
<!-- names(cox_df) <- c("Time","Surv","Model") -->

<!-- rfi <- rep("RF",length(r_fit$unique.death.times)) -->
<!-- rf_df <- data.frame(r_fit$unique.death.times,avg_prob,rfi) -->
<!-- names(rf_df) <- c("Time","Surv","Model") -->

<!-- plot_df <- rbind(km_df,cox_df,rf_df) -->

<!-- p <- ggplot(plot_df, aes(x = Time, y = Surv, color = Model)) -->
<!-- p + geom_line() -->




<!-- ############ -->





<!-- install.packages("OIsurv") -->
<!-- library(OIsurv) -->

<!-- library(help=KMsurv) -->

<!-- data(aids) -->
<!-- glimpse(aids) -->


<!-- data(tongue) -->

<!-- glimpse(tongue) -->

<!-- my.surv.object <- Surv(tongue$time[tongue$type==1], tongue$delta[tongue$type==1]) -->

<!-- my.surv.object -->

<!-- data(psych) -->

<!-- my.surv.object2 <- Surv(psych$age, psych$age+psych$time, psych$death) -->

<!-- my.surv.object2 -->



<!-- ```{r} -->
<!-- library("survival") -->
<!-- fit <- survfit(Surv(time,status) -->
<!-- ~ sex, data = lung) -->
<!-- class(fit) -->
<!-- ## [1] "survfit" -->
<!-- library("survminer") -->
<!-- ggsurvplot(fit, data = lung) -->
<!-- ``` -->


<!-- ```{r} -->
<!-- ggsurvplot(fit, data = lung, fun = "event") -->

<!-- ``` -->


<!-- ```{r} -->
<!-- ggsurvplot(fit, data = lung, fun = "cumhaz") -->
<!-- ``` -->

<!-- ```{r} -->
<!-- ggsurvplot(fit, data = lung, -->
<!-- conf.int = TRUE, -->
<!-- pval = TRUE, -->
<!-- fun = "pct", -->
<!-- risk.table = TRUE, -->
<!-- tables.height = 0.3, -->
<!-- size = 0.8, -->
<!-- linetype = "strata", -->
<!-- palette = c("#E7B800", -->
<!-- "#2E9FDF"), -->
<!-- legend = "top", -->
<!-- legend.title = "Sex", -->
<!-- legend.labs = c("Male", -->
<!-- "Female"), -->
<!-- cex = 0.2 -->
<!-- ) -->
<!-- ``` -->

<!-- ```{r} -->
<!-- library("survival") -->
<!-- fit <- coxph(Surv(time, status) ~ sex + age, data = lung) -->
<!-- ftest <- cox.zph(fit) -->
<!-- ftest -->
<!-- ``` -->

<!-- ```{r} -->
<!-- library("survminer") -->
<!-- ggcoxzph(ftest) -->
<!-- ``` -->


<!-- ```{r} -->
<!-- library("survival") -->
<!-- library("survminer") -->
<!-- fit <- coxph(Surv(time, status) ~ -->
<!-- sex + age, data = lung) -->

<!-- ``` -->




<!-- ```{r} -->
<!-- ggcoxdiagnostics(fit, -->
<!-- type = "deviance", -->
<!-- ox.scale = "linear.predictions") -->

<!-- ``` -->



<!-- ```{r} -->
<!-- ggcoxdiagnostics(fit, -->
<!-- type = "schoenfeld", -->
<!-- ox.scale = "time") -->
<!-- ``` -->




















# Reporting

## Use R Markdown
https://andrewbtran.github.io/NICAR/2018/workflow/docs/02-rmarkdown.html

## Tables


```{r}
library(xtable)
xtable(BMT)
```


```{r}
library(kableExtra)
kable(BMT)
```


```{r display results as table}

require(rhandsontable)
rhandsontable(BMT)

```

## tttable

https://github.com/leeper/tttable







### - renderers

####  - xtable

https://rdrr.io/cran/xtable/man/xtable.html










####  - flextable

https://cran.r-project.org/web/packages/flextable/vignettes/overview.html

```{r}
library(flextable)
library(officer)
```

```{r}
data <- iris[c(1:3, 51:53, 101:104),]

data

```


```{r}
flextable::regulartable(data)

```

```{r}
myft <- regulartable(
  head(mtcars), 
  col_keys = c("am", "carb", "gear", "mpg", "drat" ))
myft
```

```{r}
myft <- theme_vanilla(myft)
myft
```

```{r}
myft <- merge_v(myft, j = c("am", "carb") )
myft
```

```{r}
myft <- set_header_labels( myft, carb = "# carb." )
myft <- width(myft, width = .75) # set width of all columns to .75 in
myft
```

```{r}
myft <- autofit(myft)
myft
```

```{r}
myft <- italic(myft, j = 1)
myft <- bg(myft, bg = "#C90000", part = "header")
myft <- color(myft, color = "white", part = "header")
myft
```


```{r}
myft <- color(myft, ~ drat > 3.5, ~ drat, color = "red")
myft <- bold(myft, ~ drat > 3.5, ~ drat, bold = TRUE)
myft <- autofit(myft)

myft
```

```{r}
library(officer)


ft <- regulartable(head(mtcars))
ft <- theme_booktabs(ft)
ft <- autofit(ft)

ppt <- read_pptx()
ppt <- add_slide(ppt, layout = "Title and Content", master = "Office Theme")
ppt <- ph_with_flextable(ppt, value = ft, type = "body") 

print(ppt, target = "output/example.pptx")

# https://view.officeapps.live.com/op/view.aspx?src=https://davidgohel.github.io/flextable/articles/assets/pptx/example.pptx

```





```{r}
doc <- read_docx()
doc <- body_add_flextable(doc, value = ft)
print(doc, target = "output/example.docx")
```























####  - knitr::kable()


####  - formatttable

```{r}
library(formattable)

p <- percent(c(0.1, 0.02, 0.03, 0.12))
p
p + 0.05
max(p)

```

```{r}
balance <- accounting(c(1000, 500, 200, -150, 0, 1200))
balance

balance + 1000

```


```{r}
p <- data.frame(
  id = c(1, 2, 3, 4, 5), 
  name = c("A1", "A2", "B1", "B2", "C1"),
  balance = accounting(c(52500, 36150, 25000, 18300, 7600), format = "d"),
  growth = percent(c(0.3, 0.3, 0.1, 0.15, 0.15), format = "d"),
  ready = formattable(c(TRUE, TRUE, FALSE, FALSE, TRUE), "yes", "no"))
p
print.AsIs(p)
```

```{r}
df <- data.frame(
  id = 1:10,
  name = c("Bob", "Ashley", "James", "David", "Jenny", 
    "Hans", "Leo", "John", "Emily", "Lee"), 
  age = c(28, 27, 30, 28, 29, 29, 27, 27, 31, 30),
  grade = c("C", "A", "A", "C", "B", "B", "B", "A", "C", "C"),
  test1_score = c(8.9, 9.5, 9.6, 8.9, 9.1, 9.3, 9.3, 9.9, 8.5, 8.6),
  test2_score = c(9.1, 9.1, 9.2, 9.1, 8.9, 8.5, 9.2, 9.3, 9.1, 8.8),
  final_score = c(9, 9.3, 9.4, 9, 9, 8.9, 9.25, 9.6, 8.8, 8.7),
  registered = c(TRUE, FALSE, TRUE, FALSE, TRUE, TRUE, TRUE, FALSE, FALSE, FALSE),
  stringsAsFactors = FALSE)
```









```{r}


library(formattable)

formattable(df, list(
  age = color_tile("white", "orange"),
  grade = formatter("span", style = x ~ ifelse(x == "A", 
    style(color = "green", font.weight = "bold"), NA)),
  area(col = c(test1_score, test2_score)) ~ normalize_bar("pink", 0.2),
  final_score = formatter("span",
    style = x ~ style(color = ifelse(rank(-x) <= 3, "green", "gray")),
    x ~ sprintf("%.2f (rank: %02d)", x, rank(-x))),
  registered = formatter("span",
    style = x ~ style(color = ifelse(x, "green", "red")),
    x ~ icontext(ifelse(x, "ok", "remove"), ifelse(x, "Yes", "No")))
))
```



####  - knitLatex



####  - htmlTable


####  - psytabs


####  - SortableHTMLTables


####  - tablaxlsx


####  - table1xls


####  - tableHTML


####  - TableMonster


####  - texreg


####  - ztable


####  - apaStyle


####  - apaTables


####  - apsrtable


####  - DT

### - higher-level functionality

#### - finalfit




https://github.com/ewenharrison/finalfit

http://www.datasurg.net/2018/05/16/elegant-regression-results-tables-and-plots-the-finalfit-package/


```{r}
# devtools::install_github("ewenharrison/finalfit")
# install.packages("rstan")
# install.packages("boot")

library(tidyverse)
library(finalfit)
library(rstan)
library(boot)

```




```{r}
# Load example dataset, modified version of survival::colon
data(colon_s)



colon_s2$age <- as.numeric(colon_s$age)
colon_s2$age.factor <- as_factor(colon_s$age.factor)
colon_s2$sex.factor <- as_factor(colon_s$sex.factor)
colon_s2$obstruct.factor <- as_factor(colon_s$obstruct.factor)
colon_s2$perfor.factor <- as_factor(colon_s$perfor.factor)
colon_s2$mort_5yr <- as_factor(colon_s$mort_5yr)
colon_s2$hospital <- as_factor(colon_s$hospital)
colon_s2$status <- as.numeric(colon_s$status)
colon_s2$time <- as.numeric(colon_s$time)
colon_s2


```


```{r}
# Table 1 - Patient demographics by variable of interest ----
explanatory = c("age", "age.factor", 
  "sex.factor", "obstruct.factor")
dependent = "perfor.factor" # Bowel perforation
table <- colon_s2 %>%
  summary_factorlist(dependent, explanatory,
  p=TRUE, add_dependent_label=TRUE, total_col = TRUE)

print.AsIs(table)



```




```{r}
# Table 2 - 5 yr mortality ----
explanatory = c("age.factor", 
  "sex.factor",
  "obstruct.factor")
dependent = 'mort_5yr'
table <- colon_s %>%
  summary_factorlist(dependent, explanatory, 
  p=TRUE, add_dependent_label=TRUE)

print.AsIs(table)

```




```{r results='asis'}
knitr::kable(table, row.names=FALSE, 
    align=c("l", "l", "r", "r", "r", "r"))
```



```{r}
explanatory = c("age.factor", "sex.factor", 
  "obstruct.factor", "perfor.factor")
dependent = 'mort_5yr'
table_surv <- colon_s2 %>%
  finalfit(dependent, explanatory)
```




```{r results='asis'}
knitr::kable(table_surv, row.names = FALSE, 
    align = c("l", "l", "r", "r", "r", "r"))
```


```{r}
explanatory = c("age.factor", "sex.factor", 
  "obstruct.factor", "perfor.factor")
explanatory_multi = c("age.factor", 
  "obstruct.factor")
dependent = 'mort_5yr'
colon_s2 %>%
  finalfit(dependent, explanatory, 
  explanatory_multi)
```




```{r}
explanatory = c("age.factor", "sex.factor", 
  "obstruct.factor", "perfor.factor")
explanatory_multi = c("age.factor", "obstruct.factor")
random_effect = "hospital"
dependent = 'mort_5yr'
colon_s2 %>%
  finalfit(dependent, explanatory, 
  explanatory_multi, random_effect)
```




```{r}

explanatory = c("age.factor", "sex.factor", 
"obstruct.factor", "perfor.factor")
dependent = "Surv(time, status)"
colon_s2 %>%
  finalfit(dependent, explanatory)
```





```{r}
explanatory = c("age.factor", "sex.factor", 
  "obstruct.factor", "perfor.factor")
dependent = 'mort_5yr'
table7 <- colon_s2 %>%
  finalfit(dependent, explanatory, 
  metrics=TRUE)
```





```{r, echo=FALSE, results="asis"}
knitr::kable(table7[[1]], row.names=FALSE, align=c("l", "l", "r", "r", "r"))
knitr::kable(table7[[2]], row.names=FALSE)
```





```{r}
explanatory = c("age.factor", "sex.factor", 
  "obstruct.factor", "perfor.factor")
explanatory_multi = c("age.factor", "obstruct.factor")
random_effect = "hospital"
dependent = 'mort_5yr'

# Separate tables
colon_s2 %>%
  summary_factorlist(dependent, 
  explanatory, fit_id=TRUE) -> example.summary

colon_s2 %>%
  glmuni(dependent, explanatory) %>%
  fit2df(estimate_suffix=" (univariable)") -> example.univariable

colon_s2 %>%
  glmmulti(dependent, explanatory) %>%
  fit2df(estimate_suffix=" (multivariable)") -> example.multivariable

colon_s2 %>%
  glmmixed(dependent, explanatory, random_effect) %>%
  fit2df(estimate_suffix=" (multilevel)") -> example.multilevel

# Pipe together
example.summary %>%
  finalfit_merge(example.univariable) %>%
  finalfit_merge(example.multivariable) %>%
  finalfit_merge(example.multilevel) %>%
  select(-c(fit_id, index)) %>% # remove unnecessary columns
  dependent_label(colon_s2, dependent, prefix="") # place dependent variable label
```







```{r fig.height=5, fig.width=6}
# OR plot
explanatory = c("age.factor", "sex.factor", 
  "obstruct.factor", "perfor.factor")
dependent = 'mort_5yr'
colon_s2 %>%
  or_plot(dependent, explanatory)
# Previously fitted models (`glmmulti()` or 
# `glmmixed()`) can be provided directly to `glmfit`
```


```{r fig.height=5, fig.width=6}
# HR plot
explanatory = c("age.factor", "sex.factor", 
  "obstruct.factor", "perfor.factor")
dependent = "Surv(time, status)"
colon_s2 %>%
  hr_plot(dependent, explanatory, dependent_label = "Survival")
# Previously fitted models (`coxphmulti`) can be provided directly using `coxfit`
```


```{r}
# KM plot
explanatory = c("perfor.factor")
dependent = "Surv(time, status)"

plotKM <- colon_s2 %>%
  surv_plot(dependent, explanatory, 
  xlab = "Time (days)", pval = TRUE, legend = "none")

plotKM

```




















####   - janitor


####   - huxtable


####   - tables


####   - stargazer


####   - pixiedust


####   - reporttools


####   - rtable


####   - summarytools


####   - tab


####   - tableone


####   - carpenter


####   - dtables


####   - etable

####   - tangram
tangram (grammar of tables)

https://github.com/spgarbet/tangram
http://htmlpreview.github.io/?https://github.com/spgarbet/tg/blob/master/vignettes/example.html


<!-- library(tangram) -->
<!-- library(Hmisc) -->
<!-- getHdata(pbc) -->
<!-- View(pbc) -->
<!-- table <- tangram(drug ~ bili + albumin + stage + protime + sex + age + spiders, data = pbc) -->

<!-- table -->
<!-- html5(table) -->
<!-- latex(table) -->
<!-- index(table) -->

<!-- write( -->
<!-- html5(tangram("drug ~ bili[2] + albumin + stage::Categorical + protime + sex + age + spiders", pbc, msd=TRUE, quant=seq(0, 1, 0.25)), -->
<!--       fragment=TRUE, inline="hmisc.css", caption = "HTML5 Table Hmisc Style", id="tbl2"), -->
<!-- "tangram1.html") -->

<!-- write( -->
<!-- html5(tangram("drug ~ bili[2] + albumin + stage::Categorical + protime + sex + age + spiders", pbc), -->
<!--       fragment=TRUE, inline="nejm.css", caption = "HTML5 Table NEJM Style", id="tbl3"), -->
<!-- "tangram_nejm.html") -->


<!-- tbl <- tangram("drug ~ bili[2] + albumin + stage::Categorical[1] + protime + sex[1] + age + spiders[1]",  -->
<!--                data=pbc, -->
<!--                pformat = 5) -->
<!-- write(html5(tbl, -->
<!--       fragment=TRUE, -->
<!--       inline="lancet.css", -->
<!--       caption = "HTML5 Table Lancet Style", id="tbl4" -->
<!-- ), -->
<!-- "tangram_lancet.html") -->

<!-- index(tangram("drug ~ bili + albumin + stage::Categorical + protime + sex + age + spiders", pbc))[1:20,] -->


<!-- library(readxl) -->
<!-- MDL307_Data <- read_excel("MDL307 - Data.xlsx") -->

<!-- MDL307_Data <- as.data.frame(MDL307_Data) -->

<!-- names(MDL307_Data) -->

<!-- View(MDL307_Data) -->

<!-- MDL307_Data$biyokimyasalrekurrens <- as.factor(MDL307_Data$biyokimyasalrekurrens) -->
<!-- levels(MDL307_Data$biyokimyasalrekurrens)[1] <- "yok" -->
<!-- levels(MDL307_Data$biyokimyasalrekurrens)[2] <- "var" -->

<!-- collist <- c("gleasonskor", -->
<!--                  "tersiyer", -->
<!--                  "kribriform", -->
<!--                  "cerrahisinir", -->
<!--                  "ekstaprostatik", -->
<!--                  "lenfnodu", -->
<!--                  "seminalvezikul" -->
<!--                  ) -->


<!-- MDL307_Data[collist] <- lapply(MDL307_Data[collist], as.factor) -->


<!-- table <- tangram(biyokimyasalrekurrens ~ yas + -->
<!--                  gleasonskor + -->
<!--                  tersiyer + -->
<!--                  kribriform + -->
<!--                  kribriformyuzde + -->
<!--                  cerrahisinir + -->
<!--                  ekstaprostatik + -->
<!--                  lenfnodu + -->
<!--                  seminalvezikul + -->
<!--                  biyokimyasalrekurrens, -->
<!--                  data = MDL307_Data) -->
<!-- table -->


<!-- ## SemiCompRisks -->

<!-- install.packages("SemiCompRisks") -->
<!-- library(SemiCompRisks) -->
<!-- data(BMT) -->

<!-- ## smcure -->

<!-- install.packages("smcure") -->
<!-- library(smcure) -->
<!-- data(e1684) -->




### - pivots


####   - rpivotttable

### - other


####   - gtable


#### - sjtlm
https://strengejacke.github.io/sjPlot/articles/sjtlm.html


#### - arsenal::tableby

https://cran.r-project.org/web/packages/arsenal/vignettes/tableby.html




## Describe results of analysis

Copy/paste t-tests Directly to Manuscripts: https://neuropsychology.github.io/psycho.R//2018/06/19/analyze_ttest.html

https://github.com/neuropsychology/psycho.R

    
```{r}
# Load packages
library(tidyverse)

# devtools::install_github("neuropsychology/psycho.R")  # Install the latest psycho version
library(psycho)

```

```{r}
df <- psycho::affective  # Load the data

df
```

```{r}

results <- t.test(df$Age ~ df$Sex)  # Perform a simple t-test
results
```


```{r}
psycho::analyze(results)

```


```{r}
t.test(df$Adjusting ~ df$Sex,
       var.equal=TRUE, 
       conf.level = .90) %>% 
  psycho::analyze()
```

```{r}

t.test(df$Adjusting, 
       mu = 0,
       conf.level = .90) %>% 
      psycho::analyze()

```

```{r}

t.test(df$Adjusting ~ df$Sex) %>% 
  psycho::analyze() %>% 
  summary()

```




# Backup Analysis and Data

## GitHub

https://andrewbtran.github.io/NICAR/2018/workflow/docs/03-integrating_github.html

https://aberdeenstudygroup.github.io/studyGroup/lessons/SG-T1-GitHubVersionControl/VersionControl/

http://r-bio.github.io/intro-git-rstudio/

https://stackoverflow.com/questions/41688164/using-rstudio-to-make-pull-requests-in-git

https://bookdown.org/rdpeng/RProgDA/version-control-and-github.html

https://www.r-bloggers.com/rstudio-and-github/

http://happygitwithr.com/fork.html



# Text Analysis Sentiment Analysis

## Twitter Analysis With R

### API connection

http://rtweet.info/
https://apps.twitter.com/


```{r}
# install.packages("rtweet")
library(rtweet)
```


```{r}
# web browser method: create token and save it as an environment variable
create_token(
    app = "",
     consumer_key = "",
     consumer_secret = "")


# ## authenticate via access token
# token <- create_token(
#   app = "",
#   consumer_key = "",
#   acess_token = "",
#   access_secret = "")

# 
# 
# ## create token and save it as an environment variable
# twitter_token <- create_token(
#   app = appname,
#   consumer_key = key,
#   consumer_secret = secret,
#   access_token = access_token,
#   access_secret = access_secret
# )
# 
# 
# ## check to see if the token is loaded
# identical(twitter_token, get_token())


```


### Search Tweets

```{r}
## search for 18000 tweets using the hashtag
rt <- search_tweets(
    "#GIpath", n = 18000, include_rts = FALSE
)
```

```{r}
## preview tweets data
head(rt)
dplyr::glimpse(rt)
```


```{r}
## preview users data
users_data(rt)
```


```{r}
## plot time series (if ggplot2 is installed)
ts_plot(rt)
```


```{r}
## plot time series of tweets
ts_plot(rt, "3 hours") +
    ggplot2::theme_minimal() +
    ggplot2::theme(plot.title = ggplot2::element_text(face = "bold")) +
    ggplot2::labs(
        x = NULL, y = NULL,
        title = "Frequency of #pathologists Twitter statuses from past 9 days",
        subtitle = "Twitter status (tweet) counts aggregated using three-hour intervals",
        caption = "\nSource: Data collected from Twitter's REST API via rtweet"
    )
```


```{r}
## search for 250,000 tweets containing the word data
rt1 <- search_tweets(
    "USCAP", n = 250000, retryonratelimit = TRUE
)
```

```{r}
## search for 10,000 tweets sent from the US
rt1 <- search_tweets(
    "lang:en", geocode = lookup_coords("usa"), n = 10000
)
```


```{r}

## create lat/lng variables using all available tweet and profile geo-location data
rt1 <- lat_lng(rt1)

## plot state boundaries
par(mar = c(0, 0, 0, 0))
maps::map("state", lwd = .25)

## plot lat and lng points onto state map
with(rt1, points(lng, lat, pch = 20, cex = .75, col = rgb(0, .3, .7, .75)))
```



### Stream Tweets

```{r}
## random sample for 30 seconds (default)
rt3 <- stream_tweets("")
```

```{r}
rt3
```


```{r}
## stream tweets from london for 60 seconds
rt <- stream_tweets(lookup_coords("london, uk"), timeout = 60)


```


```{r}
## stream london tweets for a week (60 secs x 60 mins * 24 hours *  7 days)
# stream_tweets(
#     "realdonaldtrump,trump",
#     timeout = 60 * 60 * 24 * 7,
#     file_name = "tweetsabouttrump.json",
#     parse = FALSE
# )


stream_tweets(
    "realdonaldtrump,trump",
    timeout = 60 * 60 * 1 * 1,
    file_name = "tweetsabouttrump.json",
    parse = FALSE
)


```

```{r}
## read in the data as a tidy tbl data frame
djt <- parse_stream("tweetsabouttrump.json")
```

```{r}
djt
```




### Twitter connections and user information


```{r}
## get user IDs of accounts followed by CNN
# cnn_fds <- get_friends("cnn")

sb_fds <- get_friends("serdarbalci")

sb_fds
```


```{r}
## lookup data on those accounts
# cnn_fds_data <- lookup_users(cnn_fds$user_id)

sb_fds_data <- lookup_users(sb_fds$user_id)
sb_fds_data
```


```{r}
sb_follows <- sb_fds_data$screen_name
sb_follows[1:50]
```

```{r}
## get user IDs of accounts following CNN
# cnn_flw <- get_followers("cnn", n = 75000)

sb_flw <- get_followers("serdarbalci")
sb_flw

```

```{r}
## lookup data on those accounts
# cnn_flw_data <- lookup_users(cnn_flw$user_id)

sb_flw_data <- lookup_users(sb_flw$user_id[1:10])

sb_flw_data

```


```{r}
# ## how many total follows does cnn have?
# cnn <- lookup_users("cnn")
# 
# ## get them all (this would take a little over 5 days)
# cnn_flw <- get_followers(
#     "cnn", n = cnn$followers_count, retryonratelimit = TRUE
# )
```


```{r}
## get user IDs of accounts followed by CNN
tmls <- get_timelines(c("cnn", "BBCWorld", "foxnews"), n = 3200)
```

```{r}
## plot the frequency of tweets for each user over time
tmls %>%
    dplyr::filter(created_at > "2017-10-29") %>%
    dplyr::group_by(screen_name) %>%
    ts_plot("days", trim = 1L) +
    ggplot2::geom_point() +
    ggplot2::theme_minimal() +
    ggplot2::theme(
        legend.title = ggplot2::element_blank(),
        legend.position = "bottom",
        plot.title = ggplot2::element_text(face = "bold")) +
    ggplot2::labs(
        x = NULL, y = NULL,
        title = "Frequency of Twitter statuses posted by news organization",
        subtitle = "Twitter status (tweet) counts aggregated by day from October/November 2017",
        caption = "\nSource: Data collected from Twitter's REST API via rtweet"
    )


```






```{r}
# jkr <- get_favorites("jk_rowling", n = 3000)

sbf <- get_favorites("serdarbalci", n = 3000)
sbf

```


```{r}
## search for users with #rstats in their profiles
# usrs <- search_users("#rstats", n = 1000)

path_usrs <- search_users("pathology", n = 1000)

```


```{r}
head(path_usrs)

```

### Twitter Trends


```{r}
sf <- get_trends("san francisco")
```

```{r}
sf
```


```{r}
## lookup users by screen_name or user_id
users <- c("KimKardashian", "justinbieber", "taylorswift13",
           "espn", "JoelEmbiid", "cstonehoops", "KUHoops",
           "upshotnyt", "fivethirtyeight", "hadleywickham",
           "cnn", "foxnews", "msnbc", "maddow", "seanhannity",
           "potus", "epa", "hillaryclinton", "realdonaldtrump",
           "natesilver538", "ezraklein", "annecoulter")
famous_tweeters <- lookup_users(users)
```

```{r}
## preview users data
famous_tweeters

```

```{r}
# extract most recent tweets data from the famous tweeters
tweets_data(famous_tweeters)
```


### Post tweet via R



```{r}
# post_tweet("my first rtweet #rstats, let us see if it works :) http://rtweet.info/")
```

### Follow via R


```{r}

## ty for the follow ;)
post_follow("kearneymw")
```

```{r}
## quick overview of rtweet functions
vignette("auth", package = "rtweet")

## quick overview of rtweet functions
vignette("intro", package = "rtweet")


## working with the stream
vignette("stream", package = "rtweet")


## working with the stream
vignette("FAQ", package = "rtweet")

```



### Election Analysis on Twitter


```{r}
## Stream keywords used to filter tweets
q <- "hillaryclinton,imwithher,realdonaldtrump,maga,electionday"

## Stream time in seconds so for one minute set timeout = 60
## For larger chunks of time, I recommend multiplying 60 by the number
## of desired minutes. This method scales up to hours as well
## (x * 60 = x mins, x * 60 * 60 = x hours)
## Stream for 30 minutes
# streamtime <- 30 * 60
streamtime <- 3 * 60


## Filename to save json data (backup)
filename <- "rtelect.json"
```


```{r}
## Stream election tweets
rt_election <- stream_tweets(q = q, timeout = streamtime, file_name = filename)
```


```{r}
## No upfront-parse save as json file instead method
stream_tweets(
  q = q,
  parse = FALSE,
  timeout = streamtime,
  file_name = filename
)
```

```{r}
## Parse from json file
rt_election <- parse_stream(filename)
```


```{r}
## stream_tweets2 method
# twoweeks <- 60L * 60L * 24L * 7L * 2L
# congress <- "congress,senate,house of representatives,representatives,senators,legislative"
# stream_tweets2(
#   q = congress,
#   parse = FALSE,
#   timeout = twoweeks,
#   dir = "congress-stream"
# )
```


```{r}
## Parse from json file
rt <- parse_stream("congress-stream.json")
```


```{r}
## Preview tweets data
rt

```

```{r}
## Preview users data
users_data(rt)
```


```{r}
## Plot time series of all tweets aggregated by second
ts_plot(rt, by = "secs")
```


```{r}
## plot multiple time series by first grouping the data by screen name
rt %>%
  dplyr::group_by(screen_name) %>%
  ts_plot() +
  ggplot2::labs(
    title = "Tweets during election day for the 2016 U.S. election",
    subtitle = "Tweets collected, parsed, and plotted using `rtweet`"
  )
```







### Graphing Tweets

http://graphtweets.john-coene.com/index.html

```{r}
install.packages("graphTweets") # CRAN release v0.4
library(graphTweets)
library(igraph) # for plot
```


```{r}
# tweets <- rtweet::search_tweets("rstats")

tweets <- rtweet::search_tweets("#pathologists")

```


```{r}
tweets %>% 
    gt_edges(text, screen_name, status_id) %>% 
    gt_graph() %>% 
    plot()

```


```{r}
tweets %>% 
  gt_edges(text, screen_name, status_id) %>% 
  gt_graph() -> graph

class(graph)

```

```{r}
graph
```

```{r}
tweets %>% 
  gt_edges(text, screen_name, status_id) %>% 
  gt_collect() -> edges

names(edges)

```

```{r}
edges
```


```{r}
tweets %>% 
  gt_edges(text, screen_name, status_id) %>% 
  gt_nodes() %>% 
  gt_collect() -> graph

graph
```



```{r}
lapply(graph, nrow) # number of edges and nodes
```


```{r}
lapply(graph, names) # names of data.frames returned
```

```{r}
tweets %>% 
  gt_edges(text, screen_name, status_id) %>% 
  gt_nodes(meta = TRUE) %>% 
  gt_collect() -> graph

graph

# lapply(graph, names) # names of data.frames returned
```

```{r}
tweets %>% 
  gt_edges(text, screen_name, status_id, datetime = "created_at") %>% 
  gt_nodes(meta = TRUE) %>% 
  gt_collect() -> graph

graph

```


```{r}
# install.packages("sigmajs")
library(dplyr)
library(sigmajs) # for plots

tweets %>% 
  gt_edges(text, screen_name, status_id, datetime = "created_at") %>% 
  gt_nodes(meta = TRUE) %>% 
  gt_collect() -> gt

nodes <- gt$nodes %>% 
  mutate(
    id = nodes,
    label = ifelse(is.na(name), nodes, name),
    size = n_edges,
    color = "#1967be"
  ) 

edges <- gt$edges %>% 
  mutate(
    id = 1:n()
  )

sigmajs() %>% 
  sg_force_start() %>% 
  sg_nodes(nodes, id, label, size, color) %>% 
  sg_edges(edges, id, source, target) %>% 
  sg_force_stop(10000)
```


```{r}
library(igraph)

tweets %>% 
  gt_edges(text, screen_name, status_id) %>% 
  gt_graph() -> g

# communities
wc <- walktrap.community(g)
V(g)$color <- membership(wc)

# plot
# tons of arrguments because defaults are awful
plot(g, 
     layout = igraph::layout.fruchterman.reingold(g), 
     vertex.color = V(g)$color,
     vertex.label.family = "sans",
     vertex.label.color = hsv(h = 0, s = 0, v = 0, alpha = 0.0),
     vertex.size = igraph::degree(g), 
     edge.arrow.size = 0.2, 
     edge.arrow.width = 0.3, edge.width = 1,
     edge.color = hsv(h = 1, s = .59, v = .91, alpha = 0.7),
     vertex.frame.color="#fcfcfc")
```



```{r}
tweets %>% 
  gt_edges(text, screen_name, status_id, "created_at") %>% 
  gt_nodes() %>% 
  gt_dyn() %>% 
  gt_collect() -> net
```


```{r}
knitr::kable(head(net$edges))

```

```{r}
knitr::kable(head(net$nodes))

```

```{r}
library(sigmajs)

# convert to numeric & rescale
edges <- net$edges %>% 
  dplyr::mutate( 
    id = 1:n(),
    created_at = as.numeric(created_at),
    created_at = (created_at - min(created_at)) / (max(created_at) - min(created_at)),
    created_at = created_at * 5000
  )

nodes <- net$nodes %>% 
  dplyr::mutate(
    id = source,
    size = n_edges
  )

mx <- max(edges$created_at) + 500

sigmajs() %>% 
  sg_force_start() %>% 
  sg_nodes(nodes, id, size) %>% 
  sg_add_edges(edges, created_at, id, source, target, 
               cumsum = FALSE, refresh = FALSE) %>% 
  sg_force_stop(delay = mx) %>% 
  sg_settings(defaultNodeColor = "#1967be")
```

---

# Bibliography

## Google Scholar

### scholar
Analyse citation data from Google Scholar: https://github.com/jkeirstead/scholar/


### coauthornetwork
Exploring Google Scholar coauthorship: https://cimentadaj.github.io/blog/2018-06-19-exploring-google-scholar-coauthorship/exploring-google-scholar-coauthorship/

```{r}
# devtools::install_github("cimentadaj/coauthornetwork")
library(coauthornetwork)
```


```{r}
network <- grab_network("citations?user=q40DcqYAAAAJ&hl=en")
network
```



```{r}
plot_coauthors(grab_network("citations?user=q40DcqYAAAAJ&hl=en", n_coauthors = 15), size_labels = 2)

```


```{r}
plot_coauthors(grab_network("citations?user=RJNKLHgAAAAJ&hl=en", n_coauthors = 15), size_labels = 2)

```


```{r fig.height=5, fig.width=6}
plot_coauthors(grab_network("citations?user=VYE2H0wAAAAJ&hl=en", n_coauthors = 15), size_labels = 3)

```


```{r}
plot_coauthors(grab_network("citations?user=joN_UxsAAAAJ&hl=en", n_coauthors = 15), size_labels = 1)

```



---

# Miscellaneous

## Formulas
https://www.datacamp.com/community/tutorials/r-formula-tutorial

typeof()

## Flipping Coin

```{r}
rbinom(n = 1, size = 1, prob = 0.5)
```

```{r}
rbinom(n = 10, size = 1, prob = 0.5)
```

```{r}
rbinom(n = 1, size = 10, prob = 0.5)
```


```{r}
rbinom(n = 100, size = 100, prob = 0.5)
```

```{r}
rbinom(n = 10, size = 10, prob = 0.3)
```




## Decision Trees

https://www.datacamp.com/community/tutorials/decision-trees-R


---

# Packages to be tested




https://twitter.com/mitchoharawild/status/1007297976711110659?s=12

https://twitter.com/mf_viz/status/1004954297962917891?s=12

https://github.com/cmap/morpheus.R

https://github.com/talgalili/heatmaply/

https://github.com/rstudio/d3heatmap

http://worksmarter.pl/en/post/webscraping-case-study-1/

http://worksmarter.pl/en/post/job-automation-r-1/

http://www.brodrigues.co/blog/2018-06-10-scraping_pdfs/





---

# General Resources

- One Page R

https://togaware.com/onepager/

- htmlwidgets for R

http://www.htmlwidgets.org/

http://gallery.htmlwidgets.org/

- Learning R for Clinical Epidemiologists

http://rpubs.com/michaelmarks/R-Clin-Epi

---
# Package List

See package list here: 

---

# Data List

- Learning Clinical Epidemiology with R
http://datacompass.lshtm.ac.uk/599/

- 

---

# Feedback

- This document will be continiously updated and the last update was on `r Sys.Date()`.

- [Serdar Balcı, MD, Pathologist](https://github.com/sbalci) would like to hear your feedback: [feedback form](https://goo.gl/forms/YjGZ5DHgtPlR1RnB3)

- See [https://sbalci.github.io/](https://sbalci.github.io/) for other analysis.





<!-- ```{r} -->
<!-- library(tidyverse) -->
<!-- library(readxl) -->
<!-- ``` -->


<!-- ```{r} -->
<!-- path <- "ICPN_RAW.xls" -->
<!-- icpn <- path %>%  -->
<!--     excel_sheets() %>%  -->
<!--     set_names() %>% -->
<!--     map(read_excel, skip = 1, path = path) -->

<!-- icpn <- icpn[-29] -->

<!-- icpn <- icpn %>%  -->
<!--     reduce(left_join, by = "Respondant") -->

<!-- # names(icpn) -->

<!-- namesColumn <- make.names(names(read_excel(path, skip = 1))) -->

<!-- namesColumn <- namesColumn[-1] -->

<!-- # length(names(icpn)) -->
<!-- # length(namesColumn) -->

<!-- names(icpn) <- c("Respondant", paste(namesColumn, rep(1:28, each = 11), sep = "")) -->


<!-- ``` -->







<!-- ```{r} -->
<!-- # glimpse(icpn) -->

<!-- icpn$country <- icpn$ID1 -->

<!-- icpn <- icpn %>%  -->
<!--     select(-starts_with("X.plastic")) %>%  -->
<!--     select(-starts_with("COMMENTS")) %>%  -->
<!--     select(-starts_with("ID")) -->

<!-- # glimpse(icpn) -->

<!-- ``` -->

<!-- <!-- -->
<!-- ```{r} -->
<!-- jmv::descriptives(vars = names(select(icpn, starts_with("DX")) -->
<!--                                ), data = icpn, freq = TRUE, n = FALSE, missing = FALSE, mean = FALSE, median = FALSE, min = FALSE, max = FALSE) -->

<!-- ``` -->
<!-- -->


<!-- ```{r} -->
<!-- icpn %>%  -->
<!--     select(starts_with("y.n")) %>%  -->
<!--     gather() %>%  -->
<!--     select(value) %>%  -->
<!--     unique() -->


<!-- ``` -->

<!-- ```{r} -->

<!-- # recode invasion N = 0, Y = 1 -->
<!-- namesYN <- icpn %>%  -->
<!--     select(starts_with("y.n")) %>%  -->
<!--     names() -->



<!-- icpn <- icpn %>%  -->
<!--      mutate_at(namesYN, funs(recode(., `N` = 0, `Y` = 1, .default = 999))) %>%  -->
<!--      mutate_at(.vars = namesYN,  -->
<!--             .funs = funs(ifelse(. == 999, NA, .))) -->

<!-- icpn <- icpn %>%  -->
<!--      mutate_at(namesYN, funs(as.character)) %>%  -->
<!--     mutate_at(namesYN, funs(factor(., -->
<!--                                     levels = c(0,1),  -->
<!--                                     labels = c("no", "invasion") -->
<!--                                     ))) -->

<!-- ``` -->


<!-- WHOclass <- icpn %>%  -->
<!--     select(starts_with("WHO..2010.classification")) %>%  -->
<!--     gather() %>%  -->
<!--     select(value) %>%  -->
<!--     unique() -->



<!-- write(lineage$value, "lineage.txt", sep = "\n") -->

<!-- write(paste("'", readLines("lineage.txt"), "'", " =   ,", sep = ""), "lineage2.txt") -->


<!-- icpn <- icpn %>% -->
<!--     select(-starts_with("DX")) -->


<!-- file.choose() -->






