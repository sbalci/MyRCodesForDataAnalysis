---
title: "My R Codes For Data Analysis"
subtitle: "In this repository I am going to collect `R codes` for data analysis. Codes are from various resources and I try to give original link as much as possible."
author: "[Serdar Balcı, MD, Pathologist](https://www.serdarbalci.com/)"
date: '`r format(Sys.Date())`'
output: 
  html_notebook: 
    fig_caption: yes
    highlight: tango
    number_sections: yes
    theme: paper
    toc: yes
    toc_float: yes
  html_document: 
    code_folding: hide
    df_print: kable
    keep_md: yes
    number_sections: yes
    theme: cerulean
    toc: yes
    toc_float: yes
    highlight: kate
---

- Load required packages
- Gerekli paketleri yükle 
```{r load required packages 1, message=FALSE, warning=FALSE}
library(tidyverse)
```



# Getting Data into R / Veriyi R'a yükleme

## Import Data

### Import using RStudio


### Import Excel File

#### Sheets

<!-- # Sheets -->

<!-- library(tidyverse) -->
<!-- library(readxl) -->


<!-- path <- "ICPN_RAW.xls" -->
<!-- sheetlist <- path %>%  -->
<!--     excel_sheets() %>%  -->
<!--     set_names() %>%  -->
<!--     map(read_excel, skip = 1, path = path) -->

<!-- sheetlist <- sheetlist[-29] -->


<!-- sheetlist <- sheetlist %>% -->
<!--     reduce(left_join, by = "Respondant", .id = "id", suffix = c("1","2")) -->

<!-- names(sheetlist) -->

<!-- columnNames <- c( -->
<!--     "ID", -->
<!--     "DX", -->
<!--     "y/n", -->
<!--     "~plastic", -->
<!--     "WHO- 2010 classification", -->
<!--     "GRADE", -->
<!--     "INVASIVE CA", -->
<!--     "CELL LINEAGE", -->
<!--     "IPMN Lineage (PRE/MIN)", -->
<!--     "experience with this invasion", -->
<!--     "COMMENTS" -->
<!-- ) -->

<!-- columnNumbers <- rep(c(1:28), each = 11) -->

<!-- columnNames2 <- paste(columnNames, columnNumbers, sep = "") -->

<!-- columnNames2 <- c("Respondant", columnNames2) -->

<!-- names(sheetlist) <- columnNames2 -->



<!-- # "Respondant"	"ID"	"y/n"	"WHO- 2010 classification"	"GRADE"	"INVASIVE CA"	"CELL LINEAGE"	"IPMN Lineage (PRE/MIN)" -->

<!-- deneme <- sheetlist %>%  -->
<!--     select(starts_with("Respondant"), starts_with("GRADE")) -->







<!-- # deneme[deneme=="HIGH"] <- 3 -->
<!-- # deneme[deneme=="HIGH (focal)"] <- 3 -->
<!-- # deneme[deneme=="MODERATE"] <- 2 -->
<!-- # deneme[deneme=="LOW"] <- 1 -->
<!-- #  -->
<!-- # deneme <- as.data.frame(deneme) -->
<!-- #  -->
<!-- # row.names(deneme) <- deneme$Respondant -->
<!-- #  -->
<!-- # deneme <- deneme[-1] -->
<!-- #  -->
<!-- # deneme[is.na(deneme)] <- 1 -->
<!-- #  -->
<!-- # deneme <- map_df(deneme, as.matrix) -->
<!-- #  -->
<!-- # glimpse(deneme) -->
<!-- #  -->
<!-- # heatmap(deneme) -->

<!-- write_csv(deneme, "deneme.csv") -->






### Import SPSS File

## Export Data

### Export to SPSS, while keeping labels

R'da `factor` olan label verdiğiniz değişkenleri `SPSS` ya da diğer istatistik programlarına aktardığınızda bu tanımlamaları korumak işimize yarar. Bunun için `foreign` paketi ile bir `txt` dosyası ve bir `sps` dosyası oluşturuyoruz. SPSS'te `sps` dosyasını açıp kodu çalıştırarak tekrar atanan değerler geri yükleniyor.


```{r}
library(foreign)
write.foreign(mydata, "mydata.txt", "mydata.sps",   package = "SPSS")
```




## Prepare Data for Analysis / Veriyi Analiz için hazırlamak

### Keep SPSS labels

```{r}
library(foreign) # foreign paketi yükleniyor
```

read.spss komutu ile değer etiketlerini almasını ve bunu liste olarak değil de data.frame olarak kaydetmesini istiyoruz

```{r}
mydata <- read.spss("mydata.sav", use.value.labels = TRUE, to.data.frame = TRUE)
```

aktardığımız data.frame'in özellikleri (attr) içinde değişkenlerin etiketleri var, bunları dışarı çıkartıyoruz

```{r}
VariableLabels <- as.data.frame(attr(mydata, "variable.labels"))
```


elde ettiğimiz data.frame'deki satır isimleri değişkenlerin isimleri oluyor, karşılarında da değişken etiketleri var
satır isimlerini de dışarı çıkartıyoruz

```{r}
VariableLabels$original <- rownames(VariableLabels)
```

Değişken etiketi olanları etiketleri ile diğerlerini olduğu gibi saklıyoruz  

```{r}
VariableLabels$label[VariableLabels$label ==""] <- NA 
VariableLabels$colname <- VariableLabels$original
VariableLabels$colname[!is.na(VariableLabels$label)] <- as.vector(VariableLabels$label[!is.na(VariableLabels$label)])
```

son olarak da data.frame'deki sütun isimlerini değiştiriyoruz

```{r}
names(mydata) <- VariableLabels$colname
```


### Make both computer and human readible variable names

turkce karakter donusumu

```{r turkce karakter donusumu}
# https://suatatan.wordpress.com/2017/10/07/bulk-replacing-turkish-characters-in-r/
to.plain <- function(s) {
        # 1 character substitutions
    old1 <- "çğşıüöÇĞŞİÖÜ"
    new1 <- "cgsiuocgsiou"
    s1 <- chartr(old1, new1, s)
    # 2 character substitutions
    old2 <- c("œ", "ß", "æ", "ø")
    new2 <- c("oe", "ss", "ae", "oe")
    s2 <- s1
    for(i in seq_along(old2)) s2 <- gsub(old2[i], new2[i], s2, fixed = TRUE)
    s2
}


df$source=as.vector(sapply(df$source,to.plain))

makeNames(tolower(names(df)))
to.plain(names(df))

purrr::map(df, to.plain)

```

### Anonimisation

### Add subject ID to the data / veriye ID ekleme

```{r}
df <- tibble::rowid_to_column(df, "subject")
```




<!-- # gerekli paketi yükleme -->
<!-- library(tidyverse) -->
<!-- library(haven) -->
<!-- # dosyayı yükleme -->
<!-- EDT256 <- read_sav("EDT256.sav") -->



<!-- # gerekli sütunları seçme -->
<!-- EDT256 <- EDT256 %>% -->
<!--     select(subject, tanı, starts_with("beck")) -->

<!-- # değişken değerlerini yeniden atama -->
<!-- EDT256$tanı <- as_factor(EDT256$tanı, labels = "values") -->

<!-- # yatay veriyi uzun hale getirme -->
<!-- EDT256_new <- EDT256 %>% -->
<!--     gather(- c("subject","tanı"), key = zaman, value = beck_int_score) -->


<!-- # değişkenleri rakamsal hale getirme -->
<!-- EDT256_new$zaman[which(EDT256_new$zaman == "beck_int_P")] <- 0 -->
<!-- EDT256_new$zaman[which(EDT256_new$zaman == "beck_int_A1")] <- 1 -->
<!-- EDT256_new$zaman[which(EDT256_new$zaman == "beck_int_A2")] <- 2 -->

<!-- # değişken değerlerini atama  -->
<!-- EDT256_new$zaman <- factor(EDT256_new$zaman, levels = c(0,1,2), -->
<!--                            labels = c("Tedavi Öncesi", "T. Sonrası 1. Hafta", -->
<!--                                       "T. Sonrası 2. Hafta")) -->


<!-- # nparLD analizi -->

<!-- # gerekli paketi yükleme -->
<!-- library(nparLD) -->

<!-- # model oluşturma -->
<!-- modelEDT256 <- nparLD(beck_int_score ~ tanı * zaman, data = EDT256_new, -->
<!--                       subject = "subject", -->
<!--                       plot.CI = TRUE, show.covariance = TRUE) -->

<!-- # model sonucunu yazdır -->
<!-- out <- capture.output(modelEDT256) -->
<!-- cat("EDT256 Model", out, file = "EDT256.txt", sep = "\n", append=TRUE) -->

<!-- # model grafiği -->
<!-- plot(modelEDT256) -->

<!-- # plot sonucunu yazdır -->
<!-- out2 <- capture.output(plot(modelEDT256)) -->
<!-- cat("\n","EDT256 Plot", out2, file = "EDT256.txt", sep = "\n", append = TRUE) -->

<!-- # grafiği yazdır -->
<!-- jpeg('modelEDT256.jpeg') -->
<!-- plot(modelEDT256) -->
<!-- dev.off() -->

<!-- # model özeti -->
<!-- summary(modelEDT256) -->

<!-- # model özeti sonucunu yazdır -->
<!-- out3 <- capture.output(modelEDT256) -->
<!-- cat("\n","EDT256 summary", out3, file = "EDT256.txt", sep = "\n", append=TRUE) -->


<!-- # ek olarak boxplot oluşturma -->
<!-- boxplot(beck_int_score ~ tanı * zaman, data = EDT256_new, -->
<!--         names = FALSE,col = c("grey",2),lwd = 2) -->
<!-- axis(1,at = 1.5,labels = "Tedavi Öncesi",font = 2,cex = 3) -->
<!-- axis(1,at = 3.5,labels = "T. Sonrası 1. Hafta",font = 2,cex = 3) -->
<!-- axis(1,at = 5.5,labels = "T. Sonrası 2. Hafta",font = 2,cex = 3) -->
<!-- legend(5,35,c("Tanı 1","Tanı 2"), -->
<!--        lwd = c(2,2), -->
<!--        col = c("grey",2),cex = 0.7) -->

<!-- # boxplot yazdır -->
<!-- jpeg('boxplotEDT256.jpeg') -->
<!-- boxplot(beck_int_score ~ tanı * zaman, data = EDT256_new, -->
<!--         names = FALSE,col = c("grey",2),lwd = 2) -->
<!-- axis(1,at = 1.5,labels = "Tedavi Öncesi",font = 2,cex = 3) -->
<!-- axis(1,at = 3.5,labels = "T. Sonrası 1. Hafta",font = 2,cex = 3) -->
<!-- axis(1,at = 5.5,labels = "T. Sonrası 2. Hafta",font = 2,cex = 3) -->
<!-- legend(5,35,c("Tanı 1","Tanı 2"), -->
<!--        lwd = c(2,2), -->
<!--        col = c("grey",2),cex = 0.7) -->
<!-- dev.off() -->










<!-- # Analysis -->

<!-- ## Survival Analysis -->

<!-- https://github.com/datacamp/tutorial -->

<!-- Based on https://www.datacamp.com/community/tutorials/survival-analysis-R -->


<!-- ```{r} -->
<!-- library(tutorial) -->
<!-- library(survival) -->
<!-- library(survminer) -->
<!-- library(tidyverse) -->
<!-- library(jmv) -->
<!-- ``` -->


<!-- ```{r} -->
<!-- data(ovarian) -->
<!-- glimpse(ovarian) -->
<!-- help(ovarian) -->
<!-- ``` -->


<!-- ```{r} -->
<!-- summary(ovarian) -->
<!-- ``` -->


<!-- ```{r} -->
<!-- varOvarian <- names(ovarian) -->

<!-- ovarian2 <- ovarian %>%  -->
<!--     map_df(as.factor) -->

<!-- jmv::descriptives(ovarian, varOvarian, freq = TRUE) -->

<!-- jmv::descriptives(ovarian2, varOvarian, freq = TRUE) -->

<!-- ``` -->


<!-- ```{r} -->
<!-- # Dichotomize age and change data labels -->
<!-- ovarian$rx <- factor(ovarian$rx,  -->
<!--                      levels = c("1", "2"),  -->
<!--                      labels = c("A", "B")) -->
<!-- ovarian$resid.ds <- factor(ovarian$resid.ds,  -->
<!--                            levels = c("1", "2"),  -->
<!--                            labels = c("no", "yes")) -->
<!-- ovarian$ecog.ps <- factor(ovarian$ecog.ps,  -->
<!--                           levels = c("1", "2"),  -->
<!--                           labels = c("good", "bad")) -->

<!-- jmv::descriptives(ovarian, varOvarian, freq = TRUE) -->

<!-- ``` -->


<!-- ```{r} -->
<!-- # Data seems to be bimodal -->
<!-- hist(ovarian$age) -->
<!-- ``` -->


<!-- ```{r} -->
<!-- ovarian <- ovarian %>% mutate(age_group = ifelse(age >= 50, "old", "young")) -->
<!-- ovarian$age_group <- factor(ovarian$age_group) -->

<!-- varOvarian <- names(ovarian) -->

<!-- jmv::descriptives(ovarian, varOvarian, freq = TRUE) -->
<!-- ``` -->


<!-- ```{r} -->
<!-- # Fit survival data using the Kaplan-Meier method -->
<!-- surv_object <- Surv(time = ovarian$futime, event = ovarian$fustat) -->
<!-- surv_object -->
<!-- ``` -->

<!-- ```{r} -->
<!-- fit1 <- survfit(surv_object ~ rx, data = ovarian) -->
<!-- summary(fit1) -->
<!-- ``` -->


<!-- ```{r} -->
<!-- ggsurvplot(fit1, data = ovarian, pval = TRUE) -->
<!-- ``` -->


<!-- ```{r} -->
<!-- # Examine prdictive value of residual disease status -->
<!-- fit2 <- survfit(surv_object ~ resid.ds, data = ovarian) -->
<!-- ggsurvplot(fit2, data = ovarian, pval = TRUE) -->
<!-- ``` -->

<!-- ```{r} -->
<!-- # Fit a Cox proportional hazards model -->
<!-- fit.coxph <- coxph(surv_object ~ rx + resid.ds + age_group + ecog.ps,  -->
<!--                    data = ovarian) -->
<!-- ggforest(fit.coxph, data = ovarian) + -->
<!--     geom_errorbar(na.rm = TRUE) -->
<!-- ``` -->


## File organization best practices

This page summarises how to organize files and analysis before everything gets jumbled up:
[Setting up a reproducible data analysis workflow in R](https://andrewbtran.github.io/NICAR/2018/workflow/docs/01-workflow_intro.html)

Basically they suggest:
- using a project and project folder in RStudio for each analysis
- using `packrat` as much as possible



# Analysis

## Descriptive Statistics


## Inferential Statistics

### Compare Means


## Survival Analysis in R

https://rviews.rstudio.com/2017/09/25/survival-analysis-with-r/

<!-- library(survival) -->
<!-- library(ranger) -->
<!-- library(ggplot2) -->
<!-- library(dplyr) -->
<!-- library(ggfortify) -->

<!-- data(veteran) -->
<!-- head(veteran) -->

<!-- km <- with(veteran, Surv(time, status)) -->

<!-- head(km,80) -->


<!-- km_fit <- survfit(Surv(time, status) ~ 1, data = veteran) -->

<!-- km_fit -->

<!-- summary(km_fit, times = c(1,30,60,90*(1:10))) -->

<!-- plot(km_fit, xlab="Days", main = 'Kaplan Meyer Plot') #base graphics is always ready -->
<!-- autoplot(km_fit) -->

<!-- km_trt_fit <- survfit(Surv(time, status) ~ trt, data = veteran) -->
<!-- km_trt_fit -->

<!-- autoplot(km_trt_fit) -->


<!-- vet <- mutate(veteran, AG = ifelse((age < 60), "LT60", "OV60"), -->
<!--               AG = factor(AG), -->
<!--               trt = factor(trt,labels=c("standard","test")), -->
<!--               prior = factor(prior,labels=c("N0","Yes"))) -->

<!-- km_AG_fit <- survfit(Surv(time, status) ~ AG, data = vet) -->
<!-- km_AG_fit -->

<!-- autoplot(km_AG_fit) -->


<!-- Fit Cox Model -->
<!-- cox <- coxph(Surv(time, status) ~ trt + celltype + karno + diagtime + age + prior , data = vet) -->
<!-- cox -->
<!-- summary(cox) -->

<!-- cox_fit <- survfit(cox) -->
<!-- cox_fit -->
<!-- plot(cox_fit, main = "cph model", xlab="Days") -->
<!-- autoplot(cox_fit) -->

<!-- aa_fit <-aareg(Surv(time, status) ~ trt + celltype + -->
<!--                    karno + diagtime + age + prior ,  -->
<!--                data = vet) -->
<!-- aa_fit -->


<!-- #summary(aa_fit)  # provides a more complete summary of results -->
<!-- autoplot(aa_fit) -->


<!-- # ranger model -->
<!-- r_fit <- ranger(Surv(time, status) ~ trt + celltype +  -->
<!--                     karno + diagtime + age + prior, -->
<!--                 data = vet, -->
<!--                 mtry = 4, -->
<!--                 importance = "permutation", -->
<!--                 splitrule = "extratrees", -->
<!--                 verbose = TRUE) -->


<!-- # Average the survival models -->
<!-- death_times <- r_fit$unique.death.times  -->
<!-- surv_prob <- data.frame(r_fit$survival) -->
<!-- avg_prob <- sapply(surv_prob,mean) -->

<!-- # Plot the survival models for each patient -->
<!-- plot(r_fit$unique.death.times,r_fit$survival[1,],  -->
<!--      type = "l",  -->
<!--      ylim = c(0,1), -->
<!--      col = "red", -->
<!--      xlab = "Days", -->
<!--      ylab = "survival", -->
<!--      main = "Patient Survival Curves") -->

<!-- # -->
<!-- cols <- colors() -->
<!-- for (n in sample(c(2:dim(vet)[1]), 20)){ -->
<!--     lines(r_fit$unique.death.times, r_fit$survival[n,], type = "l", col = cols[n]) -->
<!-- } -->
<!-- lines(death_times, avg_prob, lwd = 2) -->
<!-- legend(500, 0.7, legend = c('Average = black')) -->


<!-- vi <- data.frame(sort(round(r_fit$variable.importance, 4), decreasing = TRUE)) -->
<!-- names(vi) <- "importance" -->
<!-- head(vi) -->


<!-- cat("Prediction Error = 1 - Harrell's c-index = ", r_fit$prediction.error) -->


<!-- # Set up for ggplot -->
<!-- kmi <- rep("KM",length(km_fit$time)) -->
<!-- km_df <- data.frame(km_fit$time,km_fit$surv,kmi) -->
<!-- names(km_df) <- c("Time","Surv","Model") -->

<!-- coxi <- rep("Cox",length(cox_fit$time)) -->
<!-- cox_df <- data.frame(cox_fit$time,cox_fit$surv,coxi) -->
<!-- names(cox_df) <- c("Time","Surv","Model") -->

<!-- rfi <- rep("RF",length(r_fit$unique.death.times)) -->
<!-- rf_df <- data.frame(r_fit$unique.death.times,avg_prob,rfi) -->
<!-- names(rf_df) <- c("Time","Surv","Model") -->

<!-- plot_df <- rbind(km_df,cox_df,rf_df) -->

<!-- p <- ggplot(plot_df, aes(x = Time, y = Surv, color = Model)) -->
<!-- p + geom_line() -->




<!-- ############ -->





<!-- install.packages("OIsurv") -->
<!-- library(OIsurv) -->

<!-- library(help=KMsurv) -->

<!-- data(aids) -->
<!-- glimpse(aids) -->


<!-- data(tongue) -->

<!-- glimpse(tongue) -->

<!-- my.surv.object <- Surv(tongue$time[tongue$type==1], tongue$delta[tongue$type==1]) -->

<!-- my.surv.object -->

<!-- data(psych) -->

<!-- my.surv.object2 <- Surv(psych$age, psych$age+psych$time, psych$death) -->

<!-- my.surv.object2 -->



<!-- ```{r} -->
<!-- library("survival") -->
<!-- fit <- survfit(Surv(time,status) -->
<!-- ~ sex, data = lung) -->
<!-- class(fit) -->
<!-- ## [1] "survfit" -->
<!-- library("survminer") -->
<!-- ggsurvplot(fit, data = lung) -->
<!-- ``` -->


<!-- ```{r} -->
<!-- ggsurvplot(fit, data = lung, fun = "event") -->

<!-- ``` -->


<!-- ```{r} -->
<!-- ggsurvplot(fit, data = lung, fun = "cumhaz") -->
<!-- ``` -->

<!-- ```{r} -->
<!-- ggsurvplot(fit, data = lung, -->
<!-- conf.int = TRUE, -->
<!-- pval = TRUE, -->
<!-- fun = "pct", -->
<!-- risk.table = TRUE, -->
<!-- tables.height = 0.3, -->
<!-- size = 0.8, -->
<!-- linetype = "strata", -->
<!-- palette = c("#E7B800", -->
<!-- "#2E9FDF"), -->
<!-- legend = "top", -->
<!-- legend.title = "Sex", -->
<!-- legend.labs = c("Male", -->
<!-- "Female"), -->
<!-- cex = 0.2 -->
<!-- ) -->
<!-- ``` -->

<!-- ```{r} -->
<!-- library("survival") -->
<!-- fit <- coxph(Surv(time, status) ~ sex + age, data = lung) -->
<!-- ftest <- cox.zph(fit) -->
<!-- ftest -->
<!-- ``` -->

<!-- ```{r} -->
<!-- library("survminer") -->
<!-- ggcoxzph(ftest) -->
<!-- ``` -->


<!-- ```{r} -->
<!-- library("survival") -->
<!-- library("survminer") -->
<!-- fit <- coxph(Surv(time, status) ~ -->
<!-- sex + age, data = lung) -->

<!-- ``` -->




<!-- ```{r} -->
<!-- ggcoxdiagnostics(fit, -->
<!-- type = "deviance", -->
<!-- ox.scale = "linear.predictions") -->

<!-- ``` -->



<!-- ```{r} -->
<!-- ggcoxdiagnostics(fit, -->
<!-- type = "schoenfeld", -->
<!-- ox.scale = "time") -->
<!-- ``` -->




















# Reporting

## Use R Markdown
https://andrewbtran.github.io/NICAR/2018/workflow/docs/02-rmarkdown.html

## Tables


```{r}
library(xtable)
xtable(BMT)
```


```{r}
library(kableExtra)
kable(BMT)
```


```{r display results as table}

require(rhandsontable)
rhandsontable(BMT)

```

## tttable

https://github.com/leeper/tttable







### - renderers

####  - xtable
####  - flextable

https://cran.r-project.org/web/packages/flextable/vignettes/overview.html

```{r}
library(flextable)
```


```{r}
data <- iris[c(1:3, 51:53, 101:104),]

data

```


```{r}
flextable::regulartable(data)

```





####  - rtf
####  - knitr::kable()
####  - formatttable
####  - knitLatex
####  - htmlTable
####  - psytabs
####  - SortableHTMLTables
####  - tablaxlsx
####  - table1xls
####  - tableHTML
####  - TableMonster
####  - texreg
####  - ztable
####  - apaStyle
####  - apaTables
####  - apsrtable
####  - DT

### - higher-level functionality
####   - janitor
####   - huxtable
####   - tables
####   - stargazer
####   - pixiedust
####   - reporttools
####   - rtable
####   - summarytools
####   - tab
####   - tableone
####   - carpenter
####   - dtables
####   - etable

####   - tangram
tangram (grammar of tables)

https://github.com/spgarbet/tangram
http://htmlpreview.github.io/?https://github.com/spgarbet/tg/blob/master/vignettes/example.html


<!-- library(tangram) -->
<!-- library(Hmisc) -->
<!-- getHdata(pbc) -->
<!-- View(pbc) -->
<!-- table <- tangram(drug ~ bili + albumin + stage + protime + sex + age + spiders, data = pbc) -->

<!-- table -->
<!-- html5(table) -->
<!-- latex(table) -->
<!-- index(table) -->

<!-- write( -->
<!-- html5(tangram("drug ~ bili[2] + albumin + stage::Categorical + protime + sex + age + spiders", pbc, msd=TRUE, quant=seq(0, 1, 0.25)), -->
<!--       fragment=TRUE, inline="hmisc.css", caption = "HTML5 Table Hmisc Style", id="tbl2"), -->
<!-- "tangram1.html") -->

<!-- write( -->
<!-- html5(tangram("drug ~ bili[2] + albumin + stage::Categorical + protime + sex + age + spiders", pbc), -->
<!--       fragment=TRUE, inline="nejm.css", caption = "HTML5 Table NEJM Style", id="tbl3"), -->
<!-- "tangram_nejm.html") -->


<!-- tbl <- tangram("drug ~ bili[2] + albumin + stage::Categorical[1] + protime + sex[1] + age + spiders[1]",  -->
<!--                data=pbc, -->
<!--                pformat = 5) -->
<!-- write(html5(tbl, -->
<!--       fragment=TRUE, -->
<!--       inline="lancet.css", -->
<!--       caption = "HTML5 Table Lancet Style", id="tbl4" -->
<!-- ), -->
<!-- "tangram_lancet.html") -->

<!-- index(tangram("drug ~ bili + albumin + stage::Categorical + protime + sex + age + spiders", pbc))[1:20,] -->


<!-- library(readxl) -->
<!-- MDL307_Data <- read_excel("MDL307 - Data.xlsx") -->

<!-- MDL307_Data <- as.data.frame(MDL307_Data) -->

<!-- names(MDL307_Data) -->

<!-- View(MDL307_Data) -->

<!-- MDL307_Data$biyokimyasalrekurrens <- as.factor(MDL307_Data$biyokimyasalrekurrens) -->
<!-- levels(MDL307_Data$biyokimyasalrekurrens)[1] <- "yok" -->
<!-- levels(MDL307_Data$biyokimyasalrekurrens)[2] <- "var" -->

<!-- collist <- c("gleasonskor", -->
<!--                  "tersiyer", -->
<!--                  "kribriform", -->
<!--                  "cerrahisinir", -->
<!--                  "ekstaprostatik", -->
<!--                  "lenfnodu", -->
<!--                  "seminalvezikul" -->
<!--                  ) -->


<!-- MDL307_Data[collist] <- lapply(MDL307_Data[collist], as.factor) -->


<!-- table <- tangram(biyokimyasalrekurrens ~ yas + -->
<!--                  gleasonskor + -->
<!--                  tersiyer + -->
<!--                  kribriform + -->
<!--                  kribriformyuzde + -->
<!--                  cerrahisinir + -->
<!--                  ekstaprostatik + -->
<!--                  lenfnodu + -->
<!--                  seminalvezikul + -->
<!--                  biyokimyasalrekurrens, -->
<!--                  data = MDL307_Data) -->
<!-- table -->


<!-- ## SemiCompRisks -->

<!-- install.packages("SemiCompRisks") -->
<!-- library(SemiCompRisks) -->
<!-- data(BMT) -->

<!-- ## smcure -->

<!-- install.packages("smcure") -->
<!-- library(smcure) -->
<!-- data(e1684) -->




### - pivots
####   - rpivotttable

### - other
####   - gtable
#### sjtlm
https://strengejacke.github.io/sjPlot/articles/sjtlm.html






## Describe results of analysis

Copy/paste t-tests Directly to Manuscripts: https://neuropsychology.github.io/psycho.R//2018/06/19/analyze_ttest.html

https://github.com/neuropsychology/psycho.R

    
```{r}
# Load packages
library(tidyverse)

# devtools::install_github("neuropsychology/psycho.R")  # Install the latest psycho version
library(psycho)

```

```{r}
df <- psycho::affective  # Load the data

df
```

```{r}

results <- t.test(df$Age ~ df$Sex)  # Perform a simple t-test
results
```


```{r}
psycho::analyze(results)

```


```{r}
t.test(df$Adjusting ~ df$Sex,
       var.equal=TRUE, 
       conf.level = .90) %>% 
  psycho::analyze()
```

```{r}

result <- t.test(df$Adjusting, 
       mu = 0,
       conf.level = .90) %>% 
      psycho::analyze(result)

```

```{r}

t.test(df$Adjusting ~ df$Sex) %>% 
  psycho::analyze() %>% 
  summary()

```




# Backup Analysis and Data

https://andrewbtran.github.io/NICAR/2018/workflow/docs/03-integrating_github.html





# Text Analysis Sentiment Analysis

## Twitter Analysis With R

### API connection

http://rtweet.info/
https://apps.twitter.com/


```{r}
# install.packages("rtweet")
library(rtweet)
```


```{r}
# web browser method: create token and save it as an environment variable
create_token(
    app = "",
     consumer_key = "",
     consumer_secret = "")


# ## authenticate via access token
# token <- create_token(
#   app = "",
#   consumer_key = "",
#   acess_token = "",
#   access_secret = "")

# 
# 
# ## create token and save it as an environment variable
# twitter_token <- create_token(
#   app = appname,
#   consumer_key = key,
#   consumer_secret = secret,
#   access_token = access_token,
#   access_secret = access_secret
# )
# 
# 
# ## check to see if the token is loaded
# identical(twitter_token, get_token())


```


### Search Tweets

```{r}
## search for 18000 tweets using the hashtag
rt <- search_tweets(
    "#GIpath", n = 18000, include_rts = FALSE
)
```

```{r}
## preview tweets data
head(rt)
dplyr::glimpse(rt)
```


```{r}
## preview users data
users_data(rt)
```


```{r}
## plot time series (if ggplot2 is installed)
ts_plot(rt)
```


```{r}
## plot time series of tweets
ts_plot(rt, "3 hours") +
    ggplot2::theme_minimal() +
    ggplot2::theme(plot.title = ggplot2::element_text(face = "bold")) +
    ggplot2::labs(
        x = NULL, y = NULL,
        title = "Frequency of #pathologists Twitter statuses from past 9 days",
        subtitle = "Twitter status (tweet) counts aggregated using three-hour intervals",
        caption = "\nSource: Data collected from Twitter's REST API via rtweet"
    )
```


```{r}
## search for 250,000 tweets containing the word data
rt1 <- search_tweets(
    "USCAP", n = 250000, retryonratelimit = TRUE
)
```

```{r}
## search for 10,000 tweets sent from the US
rt1 <- search_tweets(
    "lang:en", geocode = lookup_coords("usa"), n = 10000
)
```


```{r}

## create lat/lng variables using all available tweet and profile geo-location data
rt1 <- lat_lng(rt1)

## plot state boundaries
par(mar = c(0, 0, 0, 0))
maps::map("state", lwd = .25)

## plot lat and lng points onto state map
with(rt1, points(lng, lat, pch = 20, cex = .75, col = rgb(0, .3, .7, .75)))
```



### Stream Tweets

```{r}
## random sample for 30 seconds (default)
rt3 <- stream_tweets("")
```

```{r}
rt3
```


```{r}
## stream tweets from london for 60 seconds
rt <- stream_tweets(lookup_coords("london, uk"), timeout = 60)


```


```{r}
## stream london tweets for a week (60 secs x 60 mins * 24 hours *  7 days)
# stream_tweets(
#     "realdonaldtrump,trump",
#     timeout = 60 * 60 * 24 * 7,
#     file_name = "tweetsabouttrump.json",
#     parse = FALSE
# )


stream_tweets(
    "realdonaldtrump,trump",
    timeout = 60 * 60 * 1 * 1,
    file_name = "tweetsabouttrump.json",
    parse = FALSE
)


```

```{r}
## read in the data as a tidy tbl data frame
djt <- parse_stream("tweetsabouttrump.json")
```

```{r}
djt
```




### Twitter connections and user information


```{r}
## get user IDs of accounts followed by CNN
# cnn_fds <- get_friends("cnn")

sb_fds <- get_friends("serdarbalci")

sb_fds
```


```{r}
## lookup data on those accounts
# cnn_fds_data <- lookup_users(cnn_fds$user_id)

sb_fds_data <- lookup_users(sb_fds$user_id)
sb_fds_data
```


```{r}
sb_follows <- sb_fds_data$screen_name
sb_follows[1:50]
```

```{r}
## get user IDs of accounts following CNN
# cnn_flw <- get_followers("cnn", n = 75000)

sb_flw <- get_followers("serdarbalci")
sb_flw

```

```{r}
## lookup data on those accounts
# cnn_flw_data <- lookup_users(cnn_flw$user_id)

sb_flw_data <- lookup_users(sb_flw$user_id[1:10])

sb_flw_data

```


```{r}
# ## how many total follows does cnn have?
# cnn <- lookup_users("cnn")
# 
# ## get them all (this would take a little over 5 days)
# cnn_flw <- get_followers(
#     "cnn", n = cnn$followers_count, retryonratelimit = TRUE
# )
```


```{r}
## get user IDs of accounts followed by CNN
tmls <- get_timelines(c("cnn", "BBCWorld", "foxnews"), n = 3200)
```

```{r}
## plot the frequency of tweets for each user over time
tmls %>%
    dplyr::filter(created_at > "2017-10-29") %>%
    dplyr::group_by(screen_name) %>%
    ts_plot("days", trim = 1L) +
    ggplot2::geom_point() +
    ggplot2::theme_minimal() +
    ggplot2::theme(
        legend.title = ggplot2::element_blank(),
        legend.position = "bottom",
        plot.title = ggplot2::element_text(face = "bold")) +
    ggplot2::labs(
        x = NULL, y = NULL,
        title = "Frequency of Twitter statuses posted by news organization",
        subtitle = "Twitter status (tweet) counts aggregated by day from October/November 2017",
        caption = "\nSource: Data collected from Twitter's REST API via rtweet"
    )


```






```{r}
# jkr <- get_favorites("jk_rowling", n = 3000)

sbf <- get_favorites("serdarbalci", n = 3000)
sbf

```


```{r}
## search for users with #rstats in their profiles
# usrs <- search_users("#rstats", n = 1000)

path_usrs <- search_users("pathology", n = 1000)

```


```{r}
head(path_usrs)

```

### Twitter Trends


```{r}
sf <- get_trends("san francisco")
```

```{r}
sf
```


```{r}
## lookup users by screen_name or user_id
users <- c("KimKardashian", "justinbieber", "taylorswift13",
           "espn", "JoelEmbiid", "cstonehoops", "KUHoops",
           "upshotnyt", "fivethirtyeight", "hadleywickham",
           "cnn", "foxnews", "msnbc", "maddow", "seanhannity",
           "potus", "epa", "hillaryclinton", "realdonaldtrump",
           "natesilver538", "ezraklein", "annecoulter")
famous_tweeters <- lookup_users(users)
```

```{r}
## preview users data
famous_tweeters

```

```{r}
# extract most recent tweets data from the famous tweeters
tweets_data(famous_tweeters)
```


### Post tweet via R



```{r}
post_tweet("my first rtweet #rstats, let us see if it works :) http://rtweet.info/")
```

### Follow via R


```{r}

## ty for the follow ;)
post_follow("kearneymw")
```

```{r}
## quick overview of rtweet functions
vignette("auth", package = "rtweet")

## quick overview of rtweet functions
vignette("intro", package = "rtweet")


## working with the stream
vignette("stream", package = "rtweet")


## working with the stream
vignette("FAQ", package = "rtweet")

```



### Election Analysis on Twitter


```{r}
## Stream keywords used to filter tweets
q <- "hillaryclinton,imwithher,realdonaldtrump,maga,electionday"

## Stream time in seconds so for one minute set timeout = 60
## For larger chunks of time, I recommend multiplying 60 by the number
## of desired minutes. This method scales up to hours as well
## (x * 60 = x mins, x * 60 * 60 = x hours)
## Stream for 30 minutes
# streamtime <- 30 * 60
streamtime <- 3 * 60


## Filename to save json data (backup)
filename <- "rtelect.json"
```


```{r}
## Stream election tweets
rt_election <- stream_tweets(q = q, timeout = streamtime, file_name = filename)
```


```{r}
## No upfront-parse save as json file instead method
stream_tweets(
  q = q,
  parse = FALSE,
  timeout = streamtime,
  file_name = filename
)
```

```{r}
## Parse from json file
rt_election <- parse_stream(filename)
```


```{r}
## stream_tweets2 method
twoweeks <- 60L * 60L * 24L * 7L * 2L
congress <- "congress,senate,house of representatives,representatives,senators,legislative"
stream_tweets2(
  q = congress,
  parse = FALSE,
  timeout = twoweeks,
  dir = "congress-stream"
)
```


```{r}
## Parse from json file
rt <- parse_stream("congress-stream.json")
```


```{r}
## Preview tweets data
rt

```

```{r}
## Preview users data
users_data(rt)
```


```{r}
## Plot time series of all tweets aggregated by second
ts_plot(rt, by = "secs")
```


```{r}
## plot multiple time series by first grouping the data by screen name
rt %>%
  dplyr::group_by(screen_name) %>%
  ts_plot() +
  ggplot2::labs(
    title = "Tweets during election day for the 2016 U.S. election",
    subtitle = "Tweets collected, parsed, and plotted using `rtweet`"
  )
```







### Graphing Tweets

http://graphtweets.john-coene.com/index.html

```{r}
install.packages("graphTweets") # CRAN release v0.4
library(graphTweets)
library(igraph) # for plot
```


```{r}
# tweets <- rtweet::search_tweets("rstats")

tweets <- rtweet::search_tweets("#pathologists")

```


```{r}
tweets %>% 
    gt_edges(text, screen_name, status_id) %>% 
    gt_graph() %>% 
    plot()

```


```{r}
tweets %>% 
  gt_edges(text, screen_name, status_id) %>% 
  gt_graph() -> graph

class(graph)

```

```{r}
graph
```

```{r}
tweets %>% 
  gt_edges(text, screen_name, status_id) %>% 
  gt_collect() -> edges

names(edges)

```

```{r}
edges
```


```{r}
tweets %>% 
  gt_edges(text, screen_name, status_id) %>% 
  gt_nodes() %>% 
  gt_collect() -> graph

graph
```



```{r}
lapply(graph, nrow) # number of edges and nodes
```


```{r}
lapply(graph, names) # names of data.frames returned
```

```{r}
tweets %>% 
  gt_edges(text, screen_name, status_id) %>% 
  gt_nodes(meta = TRUE) %>% 
  gt_collect() -> graph

graph

# lapply(graph, names) # names of data.frames returned
```

```{r}
tweets %>% 
  gt_edges(text, screen_name, status_id, datetime = "created_at") %>% 
  gt_nodes(meta = TRUE) %>% 
  gt_collect() -> graph

graph

```


```{r}
# install.packages("sigmajs")
library(dplyr)
library(sigmajs) # for plots

tweets %>% 
  gt_edges(text, screen_name, status_id, datetime = "created_at") %>% 
  gt_nodes(meta = TRUE) %>% 
  gt_collect() -> gt

nodes <- gt$nodes %>% 
  mutate(
    id = nodes,
    label = ifelse(is.na(name), nodes, name),
    size = n_edges,
    color = "#1967be"
  ) 

edges <- gt$edges %>% 
  mutate(
    id = 1:n()
  )

sigmajs() %>% 
  sg_force_start() %>% 
  sg_nodes(nodes, id, label, size, color) %>% 
  sg_edges(edges, id, source, target) %>% 
  sg_force_stop(10000)
```


```{r}
library(igraph)

tweets %>% 
  gt_edges(text, screen_name, status_id) %>% 
  gt_graph() -> g

# communities
wc <- walktrap.community(g)
V(g)$color <- membership(wc)

# plot
# tons of arrguments because defaults are awful
plot(g, 
     layout = igraph::layout.fruchterman.reingold(g), 
     vertex.color = V(g)$color,
     vertex.label.family = "sans",
     vertex.label.color = hsv(h = 0, s = 0, v = 0, alpha = 0.0),
     vertex.size = igraph::degree(g), 
     edge.arrow.size = 0.2, 
     edge.arrow.width = 0.3, edge.width = 1,
     edge.color = hsv(h = 1, s = .59, v = .91, alpha = 0.7),
     vertex.frame.color="#fcfcfc")
```



```{r}
tweets %>% 
  gt_edges(text, screen_name, status_id, "created_at") %>% 
  gt_nodes() %>% 
  gt_dyn() %>% 
  gt_collect() -> net
```


```{r}
knitr::kable(head(net$edges))

```

```{r}
knitr::kable(head(net$nodes))

```

```{r}
library(sigmajs)

# convert to numeric & rescale
edges <- net$edges %>% 
  dplyr::mutate( 
    id = 1:n(),
    created_at = as.numeric(created_at),
    created_at = (created_at - min(created_at)) / (max(created_at) - min(created_at)),
    created_at = created_at * 5000
  )

nodes <- net$nodes %>% 
  dplyr::mutate(
    id = source,
    size = n_edges
  )

mx <- max(edges$created_at) + 500

sigmajs() %>% 
  sg_force_start() %>% 
  sg_nodes(nodes, id, size) %>% 
  sg_add_edges(edges, created_at, id, source, target, 
               cumsum = FALSE, refresh = FALSE) %>% 
  sg_force_stop(delay = mx) %>% 
  sg_settings(defaultNodeColor = "#1967be")
```

---

# Bibliography

## Google Scholar

### scholar
Analyse citation data from Google Scholar: https://github.com/jkeirstead/scholar/


### coauthornetwork
Exploring Google Scholar coauthorship: https://cimentadaj.github.io/blog/2018-06-19-exploring-google-scholar-coauthorship/exploring-google-scholar-coauthorship/

```{r}
# devtools::install_github("cimentadaj/coauthornetwork")
library(coauthornetwork)
```


```{r}
network <- grab_network("citations?user=q40DcqYAAAAJ&hl=en")
network
```



```{r}
plot_coauthors(grab_network("citations?user=q40DcqYAAAAJ&hl=en", n_coauthors = 15), size_labels = 2)

```


```{r}
plot_coauthors(grab_network("citations?user=RJNKLHgAAAAJ&hl=en", n_coauthors = 15), size_labels = 2)

```


```{r}
plot_coauthors(grab_network("citations?user=VYE2H0wAAAAJ&hl=en", n_coauthors = 15), size_labels = 2)

```


```{r}
plot_coauthors(grab_network("citations?user=joN_UxsAAAAJ&hl=en", n_coauthors = 15), size_labels = 1)

```



---

# Miscellaneous

## Formulas
https://www.datacamp.com/community/tutorials/r-formula-tutorial

typeof()

## Flipping Coin

```{r}
rbinom(n = 1, size = 1, prob = 0.5)
```

```{r}
rbinom(n = 10, size = 1, prob = 0.5)
```

```{r}
rbinom(n = 1, size = 10, prob = 0.5)
```


```{r}
rbinom(n = 100, size = 100, prob = 0.5)
```

```{r}
rbinom(n = 10, size = 10, prob = 0.3)
```




## Decision Trees

https://www.datacamp.com/community/tutorials/decision-trees-R


---

# Packages

- htmlwidgets for R

http://www.htmlwidgets.org/

http://gallery.htmlwidgets.org/

- 



https://togaware.com/onepager/

https://twitter.com/thosjleeper/status/1003227915981553664?s=12

https://twitter.com/serdarbalci/status/996805588192518144?s=12

https://twitter.com/dataandme/status/1006720064526213121?s=12

https://twitter.com/kirkdborne/status/1005981319669665792?s=12

https://twitter.com/mitchoharawild/status/1007297976711110659?s=12

https://twitter.com/mf_viz/status/1004954297962917891?s=12

https://github.com/cmap/morpheus.R

https://github.com/talgalili/heatmaply/

https://github.com/rstudio/d3heatmap





---

# Feedback

- This document will be continiously updated and the last update was on `r Sys.Date()`.

- [Serdar Balcı, MD, Pathologist](https://github.com/sbalci) would like to hear your feedback: [feedback form](https://goo.gl/forms/YjGZ5DHgtPlR1RnB3)

- See [https://sbalci.github.io/](https://sbalci.github.io/) for other analysis.





<!-- ```{r} -->
<!-- library(tidyverse) -->
<!-- library(readxl) -->
<!-- ``` -->


<!-- ```{r} -->
<!-- path <- "ICPN_RAW.xls" -->
<!-- icpn <- path %>%  -->
<!--     excel_sheets() %>%  -->
<!--     set_names() %>% -->
<!--     map(read_excel, skip = 1, path = path) -->

<!-- icpn <- icpn[-29] -->

<!-- icpn <- icpn %>%  -->
<!--     reduce(left_join, by = "Respondant") -->

<!-- # names(icpn) -->

<!-- namesColumn <- make.names(names(read_excel(path, skip = 1))) -->

<!-- namesColumn <- namesColumn[-1] -->

<!-- # length(names(icpn)) -->
<!-- # length(namesColumn) -->

<!-- names(icpn) <- c("Respondant", paste(namesColumn, rep(1:28, each = 11), sep = "")) -->


<!-- ``` -->







<!-- ```{r} -->
<!-- # glimpse(icpn) -->

<!-- icpn$country <- icpn$ID1 -->

<!-- icpn <- icpn %>%  -->
<!--     select(-starts_with("X.plastic")) %>%  -->
<!--     select(-starts_with("COMMENTS")) %>%  -->
<!--     select(-starts_with("ID")) -->

<!-- # glimpse(icpn) -->

<!-- ``` -->

<!-- <!-- -->
<!-- ```{r} -->
<!-- jmv::descriptives(vars = names(select(icpn, starts_with("DX")) -->
<!--                                ), data = icpn, freq = TRUE, n = FALSE, missing = FALSE, mean = FALSE, median = FALSE, min = FALSE, max = FALSE) -->

<!-- ``` -->
<!-- -->


<!-- ```{r} -->
<!-- icpn %>%  -->
<!--     select(starts_with("y.n")) %>%  -->
<!--     gather() %>%  -->
<!--     select(value) %>%  -->
<!--     unique() -->


<!-- ``` -->

<!-- ```{r} -->

<!-- # recode invasion N = 0, Y = 1 -->
<!-- namesYN <- icpn %>%  -->
<!--     select(starts_with("y.n")) %>%  -->
<!--     names() -->



<!-- icpn <- icpn %>%  -->
<!--      mutate_at(namesYN, funs(recode(., `N` = 0, `Y` = 1, .default = 999))) %>%  -->
<!--      mutate_at(.vars = namesYN,  -->
<!--             .funs = funs(ifelse(. == 999, NA, .))) -->

<!-- icpn <- icpn %>%  -->
<!--      mutate_at(namesYN, funs(as.character)) %>%  -->
<!--     mutate_at(namesYN, funs(factor(., -->
<!--                                     levels = c(0,1),  -->
<!--                                     labels = c("no", "invasion") -->
<!--                                     ))) -->

<!-- ``` -->


<!-- WHOclass <- icpn %>%  -->
<!--     select(starts_with("WHO..2010.classification")) %>%  -->
<!--     gather() %>%  -->
<!--     select(value) %>%  -->
<!--     unique() -->



<!-- write(lineage$value, "lineage.txt", sep = "\n") -->

<!-- write(paste("'", readLines("lineage.txt"), "'", " =   ,", sep = ""), "lineage2.txt") -->


<!-- icpn <- icpn %>% -->
<!--     select(-starts_with("DX")) -->


<!-- file.choose() -->






