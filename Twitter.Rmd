---
title: "R Notebook"
output: html_notebook
---


### API connection

http://rtweet.info/
https://apps.twitter.com/


```{r}
# install.packages("rtweet")
library(rtweet)
```


```{r}
# web browser method: create token and save it as an environment variable
create_token(
    app = "",
     consumer_key = "",
     consumer_secret = "")


# ## authenticate via access token
# token <- create_token(
#   app = "",
#   consumer_key = "",
#   acess_token = "",
#   access_secret = "")

# 
# 
# ## create token and save it as an environment variable
# twitter_token <- create_token(
#   app = appname,
#   consumer_key = key,
#   consumer_secret = secret,
#   access_token = access_token,
#   access_secret = access_secret
# )
# 
# 
# ## check to see if the token is loaded
# identical(twitter_token, get_token())


```


### Search Tweets

```{r}
## search for 18000 tweets using the hashtag
rt <- search_tweets(
    "#GIpath", n = 18000, include_rts = FALSE
)
```

```{r}
## preview tweets data
head(rt)
dplyr::glimpse(rt)
```


```{r}
## preview users data
users_data(rt)
```


```{r}
## plot time series (if ggplot2 is installed)
ts_plot(rt)
```


```{r}
## plot time series of tweets
ts_plot(rt, "3 hours") +
    ggplot2::theme_minimal() +
    ggplot2::theme(plot.title = ggplot2::element_text(face = "bold")) +
    ggplot2::labs(
        x = NULL, y = NULL,
        title = "Frequency of #pathologists Twitter statuses from past 9 days",
        subtitle = "Twitter status (tweet) counts aggregated using three-hour intervals",
        caption = "\nSource: Data collected from Twitter's REST API via rtweet"
    )
```


```{r}
## search for 250,000 tweets containing the word data
rt1 <- search_tweets(
    "USCAP", n = 250000, retryonratelimit = TRUE
)
```

```{r}
## search for 10,000 tweets sent from the US
rt1 <- search_tweets(
    "lang:en", geocode = lookup_coords("usa"), n = 10000
)
```


```{r}

## create lat/lng variables using all available tweet and profile geo-location data
rt1 <- lat_lng(rt1)

## plot state boundaries
par(mar = c(0, 0, 0, 0))
maps::map("state", lwd = .25)

## plot lat and lng points onto state map
with(rt1, points(lng, lat, pch = 20, cex = .75, col = rgb(0, .3, .7, .75)))
```



### Stream Tweets

```{r}
## random sample for 30 seconds (default)
rt3 <- stream_tweets("")
```

```{r}
rt3
```


```{r}
## stream tweets from london for 60 seconds
rt <- stream_tweets(lookup_coords("london, uk"), timeout = 60)


```


```{r}
## stream london tweets for a week (60 secs x 60 mins * 24 hours *  7 days)
# stream_tweets(
#     "realdonaldtrump,trump",
#     timeout = 60 * 60 * 24 * 7,
#     file_name = "tweetsabouttrump.json",
#     parse = FALSE
# )


stream_tweets(
    "realdonaldtrump,trump",
    timeout = 60 * 60 * 1 * 1,
    file_name = "tweetsabouttrump.json",
    parse = FALSE
)


```

```{r}
## read in the data as a tidy tbl data frame
djt <- parse_stream("tweetsabouttrump.json")
```

```{r}
djt
```




### Twitter connections and user information


```{r}
## get user IDs of accounts followed by CNN
# cnn_fds <- get_friends("cnn")

sb_fds <- get_friends("serdarbalci")

sb_fds
```


```{r}
## lookup data on those accounts
# cnn_fds_data <- lookup_users(cnn_fds$user_id)

sb_fds_data <- lookup_users(sb_fds$user_id)
sb_fds_data
```


```{r}
sb_follows <- sb_fds_data$screen_name
sb_follows[1:50]
```

```{r}
## get user IDs of accounts following CNN
# cnn_flw <- get_followers("cnn", n = 75000)

sb_flw <- get_followers("serdarbalci")
sb_flw

```

```{r}
## lookup data on those accounts
# cnn_flw_data <- lookup_users(cnn_flw$user_id)

sb_flw_data <- lookup_users(sb_flw$user_id[1:10])

sb_flw_data

```


```{r}
# ## how many total follows does cnn have?
# cnn <- lookup_users("cnn")
# 
# ## get them all (this would take a little over 5 days)
# cnn_flw <- get_followers(
#     "cnn", n = cnn$followers_count, retryonratelimit = TRUE
# )
```


```{r}
## get user IDs of accounts followed by CNN
tmls <- get_timelines(c("cnn", "BBCWorld", "foxnews"), n = 3200)
```

```{r}
## plot the frequency of tweets for each user over time
tmls %>%
    dplyr::filter(created_at > "2017-10-29") %>%
    dplyr::group_by(screen_name) %>%
    ts_plot("days", trim = 1L) +
    ggplot2::geom_point() +
    ggplot2::theme_minimal() +
    ggplot2::theme(
        legend.title = ggplot2::element_blank(),
        legend.position = "bottom",
        plot.title = ggplot2::element_text(face = "bold")) +
    ggplot2::labs(
        x = NULL, y = NULL,
        title = "Frequency of Twitter statuses posted by news organization",
        subtitle = "Twitter status (tweet) counts aggregated by day from October/November 2017",
        caption = "\nSource: Data collected from Twitter's REST API via rtweet"
    )


```






```{r}
# jkr <- get_favorites("jk_rowling", n = 3000)

sbf <- get_favorites("serdarbalci", n = 3000)
sbf

```


```{r}
## search for users with #rstats in their profiles
# usrs <- search_users("#rstats", n = 1000)

path_usrs <- search_users("pathology", n = 1000)

```


```{r}
head(path_usrs)

```

### Twitter Trends


```{r}
sf <- get_trends("san francisco")
```

```{r}
sf
```


```{r}
## lookup users by screen_name or user_id
users <- c("KimKardashian", "justinbieber", "taylorswift13",
           "espn", "JoelEmbiid", "cstonehoops", "KUHoops",
           "upshotnyt", "fivethirtyeight", "hadleywickham",
           "cnn", "foxnews", "msnbc", "maddow", "seanhannity",
           "potus", "epa", "hillaryclinton", "realdonaldtrump",
           "natesilver538", "ezraklein", "annecoulter")
famous_tweeters <- lookup_users(users)
```

```{r}
## preview users data
famous_tweeters

```

```{r}
# extract most recent tweets data from the famous tweeters
tweets_data(famous_tweeters)
```


### Post tweet via R



```{r}
# post_tweet("my first rtweet #rstats, let us see if it works :) http://rtweet.info/")
```

### Follow via R


```{r}

## ty for the follow ;)
post_follow("kearneymw")
```

```{r}
## quick overview of rtweet functions
vignette("auth", package = "rtweet")

## quick overview of rtweet functions
vignette("intro", package = "rtweet")


## working with the stream
vignette("stream", package = "rtweet")


## working with the stream
vignette("FAQ", package = "rtweet")

```



### Election Analysis on Twitter


```{r}
## Stream keywords used to filter tweets
q <- "hillaryclinton,imwithher,realdonaldtrump,maga,electionday"

## Stream time in seconds so for one minute set timeout = 60
## For larger chunks of time, I recommend multiplying 60 by the number
## of desired minutes. This method scales up to hours as well
## (x * 60 = x mins, x * 60 * 60 = x hours)
## Stream for 30 minutes
# streamtime <- 30 * 60
streamtime <- 3 * 60


## Filename to save json data (backup)
filename <- "rtelect.json"
```


```{r}
## Stream election tweets
rt_election <- stream_tweets(q = q, timeout = streamtime, file_name = filename)
```


```{r}
## No upfront-parse save as json file instead method
stream_tweets(
  q = q,
  parse = FALSE,
  timeout = streamtime,
  file_name = filename
)
```

```{r}
## Parse from json file
rt_election <- parse_stream(filename)
```


```{r}
## stream_tweets2 method
# twoweeks <- 60L * 60L * 24L * 7L * 2L
# congress <- "congress,senate,house of representatives,representatives,senators,legislative"
# stream_tweets2(
#   q = congress,
#   parse = FALSE,
#   timeout = twoweeks,
#   dir = "congress-stream"
# )
```


```{r}
## Parse from json file
rt <- parse_stream("congress-stream.json")
```


```{r}
## Preview tweets data
rt

```

```{r}
## Preview users data
users_data(rt)
```


```{r}
## Plot time series of all tweets aggregated by second
ts_plot(rt, by = "secs")
```


```{r}
## plot multiple time series by first grouping the data by screen name
rt %>%
  dplyr::group_by(screen_name) %>%
  ts_plot() +
  ggplot2::labs(
    title = "Tweets during election day for the 2016 U.S. election",
    subtitle = "Tweets collected, parsed, and plotted using `rtweet`"
  )
```







### Graphing Tweets

http://graphtweets.john-coene.com/index.html

```{r}
install.packages("graphTweets") # CRAN release v0.4
library(graphTweets)
library(igraph) # for plot
```


```{r}
# tweets <- rtweet::search_tweets("rstats")

tweets <- rtweet::search_tweets("#pathologists")

```


```{r}
tweets %>% 
    gt_edges(text, screen_name, status_id) %>% 
    gt_graph() %>% 
    plot()

```


```{r}
tweets %>% 
  gt_edges(text, screen_name, status_id) %>% 
  gt_graph() -> graph

class(graph)

```

```{r}
graph
```

```{r}
tweets %>% 
  gt_edges(text, screen_name, status_id) %>% 
  gt_collect() -> edges

names(edges)

```

```{r}
edges
```


```{r}
tweets %>% 
  gt_edges(text, screen_name, status_id) %>% 
  gt_nodes() %>% 
  gt_collect() -> graph

graph
```



```{r}
lapply(graph, nrow) # number of edges and nodes
```


```{r}
lapply(graph, names) # names of data.frames returned
```

```{r}
tweets %>% 
  gt_edges(text, screen_name, status_id) %>% 
  gt_nodes(meta = TRUE) %>% 
  gt_collect() -> graph

graph

# lapply(graph, names) # names of data.frames returned
```

```{r}
tweets %>% 
  gt_edges(text, screen_name, status_id, datetime = "created_at") %>% 
  gt_nodes(meta = TRUE) %>% 
  gt_collect() -> graph

graph

```


```{r}
# install.packages("sigmajs")
library(dplyr)
library(sigmajs) # for plots

tweets %>% 
  gt_edges(text, screen_name, status_id, datetime = "created_at") %>% 
  gt_nodes(meta = TRUE) %>% 
  gt_collect() -> gt

nodes <- gt$nodes %>% 
  mutate(
    id = nodes,
    label = ifelse(is.na(name), nodes, name),
    size = n_edges,
    color = "#1967be"
  ) 

edges <- gt$edges %>% 
  mutate(
    id = 1:n()
  )

sigmajs() %>% 
  sg_force_start() %>% 
  sg_nodes(nodes, id, label, size, color) %>% 
  sg_edges(edges, id, source, target) %>% 
  sg_force_stop(10000)
```


```{r}
library(igraph)

tweets %>% 
  gt_edges(text, screen_name, status_id) %>% 
  gt_graph() -> g

# communities
wc <- walktrap.community(g)
V(g)$color <- membership(wc)

# plot
# tons of arrguments because defaults are awful
plot(g, 
     layout = igraph::layout.fruchterman.reingold(g), 
     vertex.color = V(g)$color,
     vertex.label.family = "sans",
     vertex.label.color = hsv(h = 0, s = 0, v = 0, alpha = 0.0),
     vertex.size = igraph::degree(g), 
     edge.arrow.size = 0.2, 
     edge.arrow.width = 0.3, edge.width = 1,
     edge.color = hsv(h = 1, s = .59, v = .91, alpha = 0.7),
     vertex.frame.color="#fcfcfc")
```



```{r}
tweets %>% 
  gt_edges(text, screen_name, status_id, "created_at") %>% 
  gt_nodes() %>% 
  gt_dyn() %>% 
  gt_collect() -> net
```


```{r}
knitr::kable(head(net$edges))

```

```{r}
knitr::kable(head(net$nodes))

```

```{r}
library(sigmajs)

# convert to numeric & rescale
edges <- net$edges %>% 
  dplyr::mutate( 
    id = 1:n(),
    created_at = as.numeric(created_at),
    created_at = (created_at - min(created_at)) / (max(created_at) - min(created_at)),
    created_at = created_at * 5000
  )

nodes <- net$nodes %>% 
  dplyr::mutate(
    id = source,
    size = n_edges
  )

mx <- max(edges$created_at) + 500

sigmajs() %>% 
  sg_force_start() %>% 
  sg_nodes(nodes, id, size) %>% 
  sg_add_edges(edges, created_at, id, source, target, 
               cumsum = FALSE, refresh = FALSE) %>% 
  sg_force_stop(delay = mx) %>% 
  sg_settings(defaultNodeColor = "#1967be")
```




# A Shiny App with rtweet

https://aj17.shinyapps.io/twitteranalytics/


```{r}
library(blogdown)
blogdown::shortcode('tweet', '1014492923981803521')
```


# Conference tweets

https://twitter.com/APo_ORV/status/1016412207867973632

https://twitter.com/grrrck/status/959137137118646272

https://gadenbuie.shinyapps.io/rsconf_tweets/

https://github.com/gadenbuie/rsconf_tweets

https://behindbars.shinyapps.io/user2018/

https://github.com/oliviergimenez/isec2018_tweet_analysis




# Twitter Article Mentions and Citations

https://github.com/dsquintana/ajp


# twitterreport

https://github.com/gvegayon/twitterreport



# streamR

https://cran.r-project.org/web/packages/streamR/


---

- Retweet count for specific tweet

https://stackoverflow.com/questions/10427147/retweet-count-for-specific-tweet

