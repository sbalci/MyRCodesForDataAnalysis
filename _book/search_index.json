[["index.html", "My R Codes for Data Analysis Chapter 1 Preface", " My R Codes for Data Analysis Serdar Balcƒ± {r Sys.Date() Chapter 1 Preface \\n{r echo=TRUE} # install.packages( bookdown ) # or the development version # devtools::install_github( rstudio/bookdown ) \\n{r echo=TRUE} # automatically create a bib database for R packages knitr::write_bib(c( .packages(), 'bookdown', 'knitr', 'rmarkdown' ), 'bib/packages.bib') UNDER CONSTRUCTION üõ†Ô∏èÔ∏èüî© This repository is a draft version of many different codes. Organizing them will take some time. That is why I have started a template repository on GitHub. https://github.com/sbalci/histopathology-template/ https://sbalci.github.io/histopathology-template/ These templates will allow me to make histopathology research data analysis easier and more standard. ‚Äì Bir sonraki R-project sunumuna ≈üu linkten belirtilen g√ºn ve saatte eri≈üebilirsiniz. Bir sonraki sunum: R, RStudio ve RMarkdown ile Tekrarlanabilir Rapor Join Zoom Meeting https://us04web.zoom.us/j/808337924 Meeting ID: 808 337 924 Sunum linkleri: https://sbalci.github.io/MyRCodesForDataAnalysis/R-Markdown.nb.html https://sbalci.github.io/MyRCodesForDataAnalysis/R-Markdown.html 25 Eyl√ºl 2019 https://youtu.be/GZ85WE9f2R0 https://sbalci.github.io/MyRCodesForDataAnalysis/R-Giris.html https://sbalci.github.io/MyRCodesForDataAnalysis/R-Giris.nb.html Anonim Geri Bildirim: https://goo.gl/forms/YjGZ5DHgtPlR1RnB3 "],["lecture-notes.html", "Chapter 2 Lecture Notes 2.1 Introduction 2.2 Use R Markdown", " Chapter 2 Lecture Notes 2.1 Introduction R-Giris R-Giris Sunum R-Arayuzler Where To Learn R 2.2 Use R Markdown R-Markdown R-Markdown Sunum ‚Äì "],["my-r-codes-for-data-analysis.html", "Chapter 3 My R Codes For Data Analysis", " Chapter 3 My R Codes For Data Analysis In this repository I am going to collect R codes for data analysis. The title says ‚ÄúMy R Codes‚Äù but I am only the collector. I will try to refer the original sources as far as I can. Serdar Balci, MD, Pathologist My aim is to collect all the codes one needs, where one starts with an excel or spss file and then end with the most common analysis used in histopathology papers : example table There are plenty of ways to do an analysis in R, which is great but also confusing for the newbies. I will collect the codes here so that I can refer later and then update them as I learn more. See the links for the Codes below: ‚Äì WorldBankCountryAnalysis.R 1 cards.R 2 .R 3 GABAHip.R 4 googleCite.R 5 makenames.R 6 R-Program-Env-1-2.R 8 SurvivalAnalysis.R 9 SurvivalAnalysisR.R 10 tangram.R 11 TurkPathScholar.R deneme.R geom_bartext.R gilbert-dahl.R GitHubUpdateV2.R join-animations-with-gganimate.R plumber.R plumberRun.R power_multiplot.R quRan-data-raw-clean_data.R Retrieve_pubmed_citation_data.R sf_transitions.R silge.R ‚Äì 12 ArticlesPerJournalsPerCountry.Rmd 13 CountryBasedComparison.Rmd 14 JournalWatchPBPath.Rmd 15 MeSH_Terms_Pathology_Articles_From_Turkey.Rmd 16 6-tables.Rmd arsenal.Rmd AutomatedDashboardDeviation.Rmd Autoreport.Rmd bbplot.Rmd Bibliography.Rmd bioconductor.Rmd Biyoinformatik.Rmd CancerInSilico.Rmd CancerPackages.Rmd CloudForResearch.Rmd codes.Rmd CompareMeans.Rmd CompareProportions.Rmd ContingencyTables.Rmd Correlations.Rmd DataList.Rmd DataScienceLiveBook.Rmd datatable.Rmd DataTools.Rmd DecisionTreeKararAgaci.Rmd DescriptiveStatistics.Rmd drive.Rmd edirect-addin.Rmd eurostat.Rmd EvidenceSynthesisProjects.Rmd ExplatoryDataAnalysisSummaryStatistics.Rmd FileOrganization.Rmd finalfit.Rmd finalfit2.Rmd FlippingCoin.Rmd formattable.Rmd Formulas.Rmd GeneralLinearModels.Rmd GeneralResources.Rmd GettingDataVeriYukleme.Rmd GitHub.Rmd githubdocument.Rmd googledrive-trial.Rmd GoogleScholar.Rmd Graphs.Rmd h2o.Rmd HierarchicalClustering.Rmd HistopathologyResearchTemplate.Rmd How-To-Use-R-With-Excel.Rmd htmlclean.Rmd htmldocco.Rmd huxtable.Rmd HypothesisTesting.Rmd keras.Rmd KMeansClustering.Rmd lessR.Rmd LinearRegression.Rmd MachineLearning.Rmd material.Rmd MultiplePages.Rmd mxnet.Rmd news.Rmd Ninja.Rmd OpenCPU.Rmd papeR.Rmd papeR2.Rmd Power_Analysis.Rmd power.Rmd PowerAnalysis.Rmd PrepareData.Rmd PythonPandas.Rmd R-Arayuzler.Rmd R-Giris.Rmd R-Tipps.Rmd radix.Rmd rchess.Rmd readthedown.Rmd Regression.Rmd reprex.Rmd ReproducibleResearch.Rmd RISmed.Rmd RinPathologyResearch.Rmd rmarkdown_websites_tutorial.Rmd rmarkdown_websites_tutorial2.Rmd ROC.Rmd rorcid.Rmd RPackagesUsed.Rmd SankeyDiagrams.Rmd SensitivitySpecificity.Rmd shiny.Rmd ShinyCodes.Rmd snahelper.Rmd summarytools_introduction.Rmd summarytools_markdown.Rmd survival_analysis_in_r_tutorial.Rmd survival_analysis_in_r_tutorial2.Rmd SurvivalAnalysis.Rmd SyncingGitHubFork.Rmd Table.Rmd tensorflow.Rmd TextMining.Rmd the-lesser-known-stars-of-the-tidyverse.Rmd tuftedoc.Rmd Tutorials.Rmd tweetbook1.Rmd Twitter.Rmd TwitterDashboard.Rmd Untitled1.Rmd Untitled22.Rmd VisualisationGraphsPlots.Rmd WebScrapping.Rmd WhereToLearnR.Rmd ‚Äì "],["getting-data-into-r-veriyi-ra-y√ºkleme.html", "Chapter 4 Getting Data into R / Veriyi R‚Äôa y√ºkleme", " Chapter 4 Getting Data into R / Veriyi R‚Äôa y√ºkleme https://sbalci.github.io/MyRCodesForDataAnalysis/GettingDataVeriYukleme.nb.html Import Data Import using RStudio Import CSV File Import TXT File Import Excel File Import Sheets Import SPSS File Export Data Export to SPSS, while keeping labels "],["prepare-data-for-analysis-veriyi-analiz-i√ßin-hazƒ±rlamak.html", "Chapter 5 Prepare Data for Analysis / Veriyi Analiz i√ßin hazƒ±rlamak 5.1 data.table", " Chapter 5 Prepare Data for Analysis / Veriyi Analiz i√ßin hazƒ±rlamak https://sbalci.github.io/MyRCodesForDataAnalysis/PrepareData.nb.html 5.1 data.table https://sbalci.github.io/MyRCodesForDataAnalysis/datatable.nb.html "],["file-organization-best-practices.html", "Chapter 6 File organization best practices", " Chapter 6 File organization best practices https://sbalci.github.io/MyRCodesForDataAnalysis/FileOrganization.nb.html "],["analysis.html", "Chapter 7 Analysis 7.1 Descriptive Statistics, Exploratory Data Analysis, Summary Statistics 7.2 Hypothesis Testing 7.3 Survival Analysis in R 7.4 Contingency Tables 7.5 Other Analysis", " Chapter 7 Analysis 7.1 Descriptive Statistics, Exploratory Data Analysis, Summary Statistics https://sbalci.github.io/MyRCodesForDataAnalysis/DescriptiveStatistics.nb.html https://sbalci.github.io/MyRCodesForDataAnalysis/ExplatoryDataAnalysisSummaryStatistics.nb.html https://sbalci.github.io/MyRCodesForDataAnalysis/freq-tables.html the-lesser-known-stars-of-the-tidyverse.nb.html 7.2 Hypothesis Testing https://sbalci.github.io/MyRCodesForDataAnalysis/HypothesisTesting.nb.html 7.2.1 Compare Means https://sbalci.github.io/MyRCodesForDataAnalysis/CompareMeans.nb.html 7.2.2 Compare Proportions https://sbalci.github.io/MyRCodesForDataAnalysis/CompareProportions.nb.html 7.3 Survival Analysis in R https://sbalci.github.io/MyRCodesForDataAnalysis/SurvivalAnalysis.nb.html https://www.emilyzabor.com/tutorials/survival_analysis_in_r_tutorial.html 7.4 Contingency Tables https://sbalci.github.io/MyRCodesForDataAnalysis/ContingencyTables.nb.html 7.5 Other Analysis 7.5.1 Regression https://sbalci.github.io/MyRCodesForDataAnalysis/Regression.nb.html 7.5.2 LinearRegression.nb.html https://sbalci.github.io/MyRCodesForDataAnalysis/LinearRegression.nb.html 7.5.3 General Linear Models https://sbalci.github.io/MyRCodesForDataAnalysis/GeneralLinearModels.nb.html 7.5.4 Decision Trees https://sbalci.github.io/MyRCodesForDataAnalysis/DecisionTreeKararAgaci.nb.html 7.5.5 Clustering 7.5.6 K Means Clustering https://sbalci.github.io/MyRCodesForDataAnalysis/KMeansClustering.nb.html 7.5.7 Hierarchical Clustering https://sbalci.github.io/MyRCodesForDataAnalysis/HierarchicalClustering.nb.html "],["graphs-plots.html", "Chapter 8 Graphs Plots 8.1 Sankey Diagrams", " Chapter 8 Graphs Plots https://sbalci.github.io/MyRCodesForDataAnalysis/VisualisationGraphsPlots.nb.html https://sbalci.github.io/MyRCodesForDataAnalysis/Graphs.nb.html 8.1 Sankey Diagrams https://sbalci.github.io/MyRCodesForDataAnalysis/SankeyDiagrams.nb.html "],["reporting.html", "Chapter 9 Reporting 9.1 Reproducible Research 9.2 Tables 9.3 Autoreport 9.4 shiny 9.5 Creating websites in R", " Chapter 9 Reporting 9.1 Reproducible Research https://sbalci.github.io/MyRCodesForDataAnalysis/ReproducibleResearch.nb.html 9.2 Tables https://sbalci.github.io/MyRCodesForDataAnalysis/Table.nb.html https://sbalci.github.io/MyRCodesForDataAnalysis/finalfit.nb.html https://sbalci.github.io/MyRCodesForDataAnalysis/formattable.nb.html 9.3 Autoreport https://sbalci.github.io/MyRCodesForDataAnalysis/Autoreport.nb.html 9.4 shiny https://sbalci.github.io/MyRCodesForDataAnalysis/shiny.nb.html 9.5 Creating websites in R https://www.emilyzabor.com/tutorials/rmarkdown_websites_tutorial.html "],["bioinformatics.html", "Chapter 10 Bioinformatics 10.1 bioconductor", " Chapter 10 Bioinformatics 10.1 bioconductor https://sbalci.github.io/MyRCodesForDataAnalysis/bioconductor.nb.html https://sbalci.github.io/MyRCodesForDataAnalysis/CancerInSilico.nb.html https://sbalci.github.io/MyRCodesForDataAnalysis/CancerPackages.nb.html "],["backup-analysis-and-data.html", "Chapter 11 Backup Analysis and Data 11.1 GitHub", " Chapter 11 Backup Analysis and Data 11.1 GitHub https://sbalci.github.io/MyRCodesForDataAnalysis/GitHub.nb.html SyncingGitHubFork.nb.html "],["text-analysis-sentiment-analysis.html", "Chapter 12 Text Analysis Sentiment Analysis 12.1 Twitter Analysis With R 12.2 News 12.3 web scrapping", " Chapter 12 Text Analysis Sentiment Analysis https://sbalci.github.io/MyRCodesForDataAnalysis/TextMining.nb.html 12.1 Twitter Analysis With R https://sbalci.github.io/MyRCodesForDataAnalysis/Twitter.nb.html 12.2 News https://sbalci.github.io/MyRCodesForDataAnalysis/news.nb.html 12.3 web scrapping https://sbalci.github.io/MyRCodesForDataAnalysis/WebScrapping.nb.html "],["bibliography.html", "Chapter 13 Bibliography 13.1 PubMed 13.2 ORCID 13.3 Google Scholar 13.4 Power Analysis 13.5 Formulas 13.6 Flipping Coin", " Chapter 13 Bibliography Other Bibliographic Studies: https://sbalci.github.io/ResearchOnBibliography/ https://sbalci.github.io/MyRCodesForDataAnalysis/Bibliography.nb.html 13.1 PubMed 13.1.1 RISmed https://sbalci.github.io/MyRCodesForDataAnalysis/RISmed.nb.html 13.2 ORCID 13.2.1 rorcid https://sbalci.github.io/MyRCodesForDataAnalysis/rorcid.nb.html 13.3 Google Scholar https://sbalci.github.io/MyRCodesForDataAnalysis/GoogleScholar.nb.html 13.3.1 Scholar 13.3.2 Coauthor 13.4 Power Analysis https://sbalci.github.io/MyRCodesForDataAnalysis/PowerAnalysis.nb.html https://sbalci.github.io/MyRCodesForDataAnalysis/Power_Analysis.nb.html https://sbalci.github.io/MyRCodesForDataAnalysis/PowerAnalysis.nb.html 13.5 Formulas https://sbalci.github.io/MyRCodesForDataAnalysis/Formulas.nb.html 13.6 Flipping Coin https://sbalci.github.io/MyRCodesForDataAnalysis/FlippingCoin.nb.html "],["general-resources.html", "Chapter 14 General Resources", " Chapter 14 General Resources https://sbalci.github.io/MyRCodesForDataAnalysis/GeneralResources.nb.html https://sbalci.github.io/MyRCodesForDataAnalysis/DataScienceLiveBook.nb.html "],["package-list.html", "Chapter 15 Package List", " Chapter 15 Package List https://sbalci.github.io/MyRCodesForDataAnalysis/RPackagesUsed.nb.html "],["data-list.html", "Chapter 16 Data List", " Chapter 16 Data List https://sbalci.github.io/MyRCodesForDataAnalysis/DataList.nb.html https://sbalci.github.io/MyRCodesForDataAnalysis/eurostat.nb.html "],["data-tools.html", "Chapter 17 Data Tools", " Chapter 17 Data Tools https://sbalci.github.io/MyRCodesForDataAnalysis/DataTools.nb.html "],["miscellaneous.html", "Chapter 18 Miscellaneous", " Chapter 18 Miscellaneous https://sbalci.github.io/MyRCodesForDataAnalysis/codes.nb.html https://sbalci.github.io/MyRCodesForDataAnalysis/OpenCPU.nb.html https://sbalci.github.io/MyRCodesForDataAnalysis/papeR.nb.html https://sbalci.github.io/MyRCodesForDataAnalysis/Tutorials.nb.html https://sbalci.github.io/MyRCodesForDataAnalysis/PythonPandas.nb.html https://sbalci.github.io/MyRCodesForDataAnalysis/lessR.nb.html https://sbalci.github.io/MyRCodesForDataAnalysis/arsenal.nb.html https://sbalci.github.io/MyRCodesForDataAnalysis/rchess.nb.html https://sbalci.github.io/MyRCodesForDataAnalysis/EvidenceSynthesisProjects.nb.html https://sbalci.github.io/MyRCodesForDataAnalysis/MachineLearning.nb.html https://sbalci.github.io/MyRCodesForDataAnalysis/Correlations.nb.html https://xgboost.readthedocs.io/en/latest/index.html https://sbalci.github.io/MyRCodesForDataAnalysis/R-Tipps.nb.html "],["feedback.html", "Chapter 19 Feedback", " Chapter 19 Feedback Yours truly would like to hear your feedback: feedback form See https://sbalci.github.io/ for other analysis. You may also contact with me with the comment field below. ‚Äì {% if page.comments %} Please enable JavaScript to view the comments powered by Disqus. {% endif %} ‚Äì "],["getting-data-into-r.html", "Chapter 20 Getting Data into R", " Chapter 20 Getting Data into R "],["intro.html", "Chapter 21 Introduction", " Chapter 21 Introduction You can label chapter and section titles using {#label} after them, e.g., we can reference Chapter 21. If you do not manually label them, there will be automatic labels anyway, e.g., Chapter 440.150. Figures and tables with captions will be placed in figure and table environments, respectively. {r , fig.cap=&#39;Here is a nice figure!&#39;, out.width=&#39;80%&#39;, fig.asp=.75, fig.align=&#39;center&#39;} par(mar = c(4, 4, .1, .1)) plot(pressure, type = &#39;b&#39;, pch = 19) Reference a figure by its code chunk label with the fig: prefix, e.g., see Figure ??. Similarly, you can reference tables generated from knitr::kable(), e.g., see Table ??. {r , tidy=FALSE} knitr::kable( head(iris, 20), caption = &#39;Here is a nice table!&#39;, booktabs = TRUE ) You can write citations, too. For example, we are using the bookdown package (Xie 2020) in this sample book, which was built on top of R Markdown and knitr (Xie 2015). References "],["bibliographic-studies.html", "Chapter 22 Bibliographic Studies 22.1 Articles per journals per country", " Chapter 22 Bibliographic Studies 22.1 Articles per journals per country If you want to see the code used in the analysis please click the code button on the right upper corner or throughout the page. 22.1.1 Analysis {r eval=FALSE, include=FALSE, echo=TRUE} knitr::opts_chunk$set( eval = FALSE, message = FALSE, warning = FALSE, include = FALSE, tidy = TRUE ) 22.1.1.1 Articles per journals per country Aim: In the previous analysis we have observed that Japanese researchers have much more articles than German and Turkish researchers. Here we will look at the distribution of articles per journals per country. Methods: {r , eval=FALSE, include=FALSE, echo=TRUE} # load required packages library(tidyverse) library(RISmed) Pathology Journal ISSN List was retrieved from In Cites Clarivate, and Journal Data Filtered as follows: JCR Year: 2016 Selected Editions: SCIE,SSCI Selected Categories: 'PATHOLOGY' Selected Category Scheme: WoS {r Get ISSN List from data downloaded from WoS, eval=FALSE, include=FALSE, echo=TRUE} # Get ISSN List from data downloaded from WoS ISSNList &lt;- JournalHomeGrid &lt;- read_csv( data/JournalHomeGrid.csv , skip = 1) %&gt;% select(ISSN) %&gt;% filter(!is.na(ISSN)) %&gt;% t() %&gt;% paste( OR , collapse = ) # add OR between ISSN List ISSNList &lt;- gsub( OR $ , ,ISSNList) # to remove last OR Data is retrieved from PubMed via RISmed package. PubMed collection from National Library of Medicine (https://www.ncbi.nlm.nih.gov/pubmed/), has the most comprehensive information about peer reviewed articles in medicine. The API (https://dataguide.nlm.nih.gov/), and R packages are available for getting and fetching data from the server. The search formula for PubMed is generated as ISSN List AND Country[Affiliation] like done in advanced search of PubMed. {r Generate Search Formula For Pathology Journals AND Countries, eval=FALSE, include=FALSE, echo=TRUE} # Generate Search Formula For Pathology Journals AND Countries searchformulaTR &lt;- paste( &#39; ,ISSNList, &#39; , AND , Turkey[Affiliation] ) searchformulaDE &lt;- paste( &#39; ,ISSNList, &#39; , AND , Germany[Affiliation] ) searchformulaJP &lt;- paste( &#39; ,ISSNList, &#39; , AND , Japan[Affiliation] ) Articles from Japan, German and Turkey are retrieved limiting the search with pathology journals, affiliation and last 10 years. {r Search PubMed, eval=FALSE, include=FALSE, echo=TRUE} # Search PubMed, Get and Fetch TurkeyArticles &lt;- EUtilsSummary(searchformulaTR, type = &#39;esearch&#39;, db = &#39;pubmed&#39;, mindate = 2007, maxdate = 2017, retmax = 10000) fetchTurkey &lt;- EUtilsGet(TurkeyArticles) GermanyArticles &lt;- EUtilsSummary(searchformulaDE, type = &#39;esearch&#39;, db = &#39;pubmed&#39;, mindate = 2007, maxdate = 2017, retmax = 10000) fetchGermany &lt;- EUtilsGet(GermanyArticles) JapanArticles &lt;- EUtilsSummary(searchformulaJP, type = &#39;esearch&#39;, db = &#39;pubmed&#39;, mindate = 2007, maxdate = 2017, retmax = 10000) fetchJapan &lt;- EUtilsGet(JapanArticles) The retrieved information was compiled in a table. {r eval=FALSE, include=FALSE, echo=TRUE} ISSNTR &lt;- table(ISSN(fetchTurkey)) %&gt;% as_tibble() %&gt;% rename(Turkey = n, Journal = Var1) ISSNDE &lt;- table(ISSN(fetchGermany)) %&gt;% as_tibble() %&gt;% rename(Germany = n, Journal = Var1) ISSNJP &lt;- table(ISSN(fetchJapan)) %&gt;% as_tibble() %&gt;% rename(Japan = n, Journal = Var1) articles_per_journal &lt;- list( ISSNTR, ISSNDE, ISSNJP ) %&gt;% reduce(left_join, by = Journal , .id = id ) %&gt;% gather(Country, n, 2:4) articles_per_journal$Country &lt;- factor(articles_per_journal$Country, levels =c( Japan , Germany , Turkey )) Result: In this graph x-axis is the list of journals with decreasing impact factor, and y-axis is the number of articles published in that journal. The colors and shapes are showing the country of affiliation. We see that in one journal articles from Japan is more than 800. {r eval=FALSE, include=FALSE, echo=TRUE} ggplot(data = articles_per_journal, aes(x = Journal, y = n, group = Country, colour = Country, shape = Country, levels = Country )) + geom_point() + labs(x = Journals with decreasing impact factor , y = Number of Articles ) + ggtitle( Pathology Articles Per Journal ) + theme(plot.title = element_text(hjust = 0.5), axis.text.x=element_blank()) Comment: It is seen that one of the journals ISSN: 1440-1827 has more than 800 articles from Japan. This journal is also from Japan. Here we wonder if there is an editorial preference for articles from their home country. We sometimes observe this situation if there is a conference in that country, and the conference abstracts are indexed. This may also be a clue that if a country has a journal listed in indexes, than it is more easy for the researchers in that country to publish their results. Future Work: Whether this observation is a unique situation, or there is a tendency in the journals to publish article from their country of origin, merits further investigation. "],["country-based-comparison.html", "Chapter 23 Country Based Comparison 23.1 Analysis", " Chapter 23 Country Based Comparison 23.1 Analysis 23.1.1 PubMed Indexed Peer Reviewed Articles in Pathology Journals: A country based comparison Aim: Here, we are going to compare 3 countries (German, Japan and Turkey), in terms of number of articles in pathology journals during the last decade. Methods: If you want to see the code used in the analysis please click the code button on the right upper corner or throughout the page. {r , eval=FALSE, include=FALSE, echo=TRUE} # load required packages library(tidyverse) library(RISmed) Pathology Journal ISSN List was retrieved from In Cites Clarivate, and Journal Data Filtered as follows: JCR Year: 2016 Selected Editions: SCIE,SSCI Selected Categories: 'PATHOLOGY' Selected Category Scheme: WoS {r Get ISSN List from data downloaded from WoS 2, eval=FALSE, include=FALSE, echo=TRUE} # Get ISSN List from data downloaded from WoS ISSNList &lt;- JournalHomeGrid &lt;- read_csv( data/JournalHomeGrid.csv , skip = 1) %&gt;% select(ISSN) %&gt;% filter(!is.na(ISSN)) %&gt;% t() %&gt;% paste( OR , collapse = ) # add OR between ISSN List ISSNList &lt;- gsub( OR $ , ,ISSNList) # to remove last OR Data is retrieved from PubMed via RISmed package. PubMed collection from National Library of Medicine (https://www.ncbi.nlm.nih.gov/pubmed/), has the most comprehensive information about peer reviewed articles in medicine. The API (https://dataguide.nlm.nih.gov/), and R packages are available for getting and fetching data from the server. The search formula for PubMed is generated as ISSN List AND Country[Affiliation] like done in advanced search of PubMed. {r Generate Search Formula For Pathology Journals AND Countries 2, eval=FALSE, include=FALSE, echo=TRUE} # Generate Search Formula For Pathology Journals AND Countries searchformulaTR &lt;- paste( &#39; ,ISSNList, &#39; , AND , Turkey[Affiliation] ) searchformulaDE &lt;- paste( &#39; ,ISSNList, &#39; , AND , Germany[Affiliation] ) searchformulaJP &lt;- paste( &#39; ,ISSNList, &#39; , AND , Japan[Affiliation] ) {r Search PubMed 2, eval=FALSE, include=FALSE, echo=TRUE} # Search PubMed, Get and Fetch TurkeyArticles &lt;- EUtilsSummary(searchformulaTR, type = &#39;esearch&#39;, db = &#39;pubmed&#39;, mindate = 2007, maxdate = 2017, retmax = 10000) fetchTurkey &lt;- EUtilsGet(TurkeyArticles) GermanyArticles &lt;- EUtilsSummary(searchformulaDE, type = &#39;esearch&#39;, db = &#39;pubmed&#39;, mindate = 2007, maxdate = 2017, retmax = 10000) fetchGermany &lt;- EUtilsGet(GermanyArticles) JapanArticles &lt;- EUtilsSummary(searchformulaJP, type = &#39;esearch&#39;, db = &#39;pubmed&#39;, mindate = 2007, maxdate = 2017, retmax = 10000) fetchJapan &lt;- EUtilsGet(JapanArticles) From the fetched data the year of articles are grouped and counted by country. {r Articles per countries per year 2, eval=FALSE, include=FALSE, echo=TRUE} # Articles per countries per year tableTR &lt;- table(YearPubmed(fetchTurkey)) %&gt;% as_tibble() %&gt;% rename(Turkey = n, Year = Var1) tableDE &lt;- table(YearPubmed(fetchGermany)) %&gt;% as_tibble() %&gt;% rename(Germany = n, Year = Var1) tableJP &lt;- table(YearPubmed(fetchJapan)) %&gt;% as_tibble() %&gt;% rename(Japan = n, Year = Var1) # Join Tables articles_per_year_table &lt;- list( tableTR, tableDE, tableJP ) %&gt;% reduce(left_join, by = Year , .id = id ) {r Prepare table for output 2, eval=FALSE, include=FALSE, echo=TRUE} # Prepare table for output articles_per_year &lt;- articles_per_year_table %&gt;% gather(Country, n, 2:4) articles_per_year$Country &lt;- factor(articles_per_year$Country, levels =c( Japan , Germany , Turkey )) Result: In the below table we see the number of articles per country in the last decade. {r Print the Table of Articles per year per country 2, eval=FALSE, include=FALSE, echo=TRUE} # Print the Table of Articles per year, per country knitr::kable(articles_per_year_table, caption = Table of Articles per year, per country ) And the figure below shows this data in a line graph. {r Graph of Table of Articles per year per country 2, eval=FALSE, fig.align= center , include=FALSE} ggplot(data = articles_per_year, aes(x = Year, y = n, group = Country, colour = Country, shape = Country, levels = Country )) + geom_line() + geom_point() + labs(x = Year , y = Number of Articles ) + ggtitle( Pathology Articles Per Year ) + theme(plot.title = element_text(hjust = 0.4), text = element_text(size = 9)) Comment: We see that Japan has much more articles than German and Turkey. Turkey has a small increase in number of articles. Future Work: Indentify why Japan has too much articles. Compare Japan with other countries. Compare Turkey with neighbours, EU, OECD &amp; Middle East countries. Analyse multinational studies. Analyse adding journal impact as a factor. "],["pbpath-journal-watch.html", "Chapter 24 PBPath Journal Watch 24.1 Recent Articles from PubMed", " Chapter 24 PBPath Journal Watch 24.1 Recent Articles from PubMed 24.1.1 Analysis of Recent Pancreas Related Articles Pancreas Journals https://www.ncbi.nlm.nih.gov/nlmcatalog/?term=pancreas Pathology Journals Member List DOI Link PubMed Link Journal Link Altmetric API Dimensions API USCAP abstracts vs publication Member list vs worldmap {r , eval=FALSE, include=FALSE, echo=TRUE} # load required packages library(tidyverse) library(knitr) library(rstudioapi) {r eval=FALSE, include=FALSE, echo=TRUE} knitr::opts_chunk$set( eval = FALSE, message = FALSE, warning = FALSE, include = FALSE, tidy = TRUE ) {r eval=FALSE, include=FALSE, echo=TRUE} myTerm &lt;- rstudioapi::terminalCreate(show = FALSE) rstudioapi::terminalSend(myTerm, esearch -db pubmed -query &#39;pancreas[Title/Abstract]) AND pathology&#39; -datetype EDAT -min 2018/05/01 -max 3000 | \\ efetch -format xml | \\ xtract -pattern PubmedArticle -element MedlineCitation/PMID \\ -block ArticleId -if ArticleId@IdType -equals doi -element ArticleId &amp;&gt; myquery.txt ) Sys.sleep(1) repeat{ Sys.sleep(0.1) if(rstudioapi::terminalBusy(myTerm) == FALSE){ print( Code Executed ) break } } {r eval=FALSE, include=FALSE, echo=TRUE} readLines( myquery.txt ) Pathology Journal ISSN List was retrieved from In Cites Clarivate, and Journal Data Filtered as follows: JCR Year: 2016 Selected Editions: SCIE,SSCI Selected Categories: 'PATHOLOGY' Selected Category Scheme: WoS {r Get ISSN List from data downloaded from WoS 1, eval=FALSE, include=FALSE, echo=TRUE} # Get ISSN List from data downloaded from WoS ISSNList &lt;- JournalHomeGrid &lt;- read_csv( data/JournalHomeGrid.csv , skip = 1) %&gt;% select(ISSN) %&gt;% filter(!is.na(ISSN)) %&gt;% t() %&gt;% paste( OR , collapse = ) # add OR between ISSN List ISSNList &lt;- gsub( OR $ , ,ISSNList) # to remove last OR Data is retrieved from PubMed via E-direct. PubMed collection from National Library of Medicine (https://www.ncbi.nlm.nih.gov/pubmed/), has the most comprehensive information about peer reviewed articles in medicine. The API (https://dataguide.nlm.nih.gov/) is available for getting and fetching data from the server. The query for PubMed is generated as ISSN List AND keywords like done in advanced search of PubMed. {r Generate Search Formula For Pathology Journals AND Countries 1, eval=FALSE, include=FALSE, echo=TRUE} # Generate Search Formula For Pathology Journals AND Countries searchformulaTR &lt;- paste( &#39; ,ISSNList, &#39; , AND , Turkey[Affiliation] ) searchformulaDE &lt;- paste( &#39; ,ISSNList, &#39; , AND , Germany[Affiliation] ) searchformulaJP &lt;- paste( &#39; ,ISSNList, &#39; , AND , Japan[Affiliation] ) From the fetched data articles are grouped by country and keywords. {r Articles per countries per year 1, eval=FALSE, include=FALSE, echo=TRUE} # Articles per countries per year tableTR &lt;- table(YearPubmed(fetchTurkey)) %&gt;% as_tibble() %&gt;% rename(Turkey = n, Year = Var1) tableDE &lt;- table(YearPubmed(fetchGermany)) %&gt;% as_tibble() %&gt;% rename(Germany = n, Year = Var1) tableJP &lt;- table(YearPubmed(fetchJapan)) %&gt;% as_tibble() %&gt;% rename(Japan = n, Year = Var1) # Join Tables articles_per_year_table &lt;- list( tableTR, tableDE, tableJP ) %&gt;% reduce(left_join, by = Year , .id = id ) {r Prepare table for output 1, eval=FALSE, include=FALSE, echo=TRUE} # Prepare table for output articles_per_year &lt;- articles_per_year_table %&gt;% gather(Country, n, 2:4) articles_per_year$Country &lt;- factor(articles_per_year$Country, levels =c( Japan , Germany , Turkey )) Result: {r Print the Table of Articles per year per country 1, eval=FALSE, include=FALSE, echo=TRUE} # Print the Table of Articles per year, per country knitr::kable(articles_per_year_table, caption = Table of Articles per year, per country ) mapgraph And the figure below shows this data in a line graph. "],["bibliographic-studies-1.html", "Chapter 25 Bibliographic Studies", " Chapter 25 Bibliographic Studies output: html_notebook: code_folding: hide fig_caption: yes highlight: kate number_sections: yes theme: cerulean toc: yes toc_float: yes html_document: code_folding: hide df_print: kable keep_md: yes number_sections: yes theme: cerulean toc: yes toc_float: yes highlight: kate If you want to see the code used in the analysis please click the code button on the right upper corner or throughout the page. "],["analysis-3.html", "Chapter 26 Analysis 26.1 MeSH Terms In Pathology Articles From Turkey", " Chapter 26 Analysis {r eval=FALSE, include=FALSE, echo=TRUE} knitr::opts_chunk$set( eval = FALSE, message = FALSE, warning = FALSE, include = FALSE, tidy = TRUE ) 26.1 MeSH Terms In Pathology Articles From Turkey Background PubMed collection from National Library of Medicine, has the most comprehensive information about peer reviewed articles in medicine. MeSH Terms is a controlled vocabulary that is used to label PubMed articles according to their content. It is done by experts in National Library of Medicine. Keywords are lables that are given by authors of the article. Both are included in a PubMed record of an article. Aim: In this analysis we aimed to identify the common research topics Turkish pathologists are interested. We extracted most common MeSH terms and keywords from PubMed articles using EDirect: MeSH Terms Pathology Articles From Turkey Methods: Packages used for analysis. Tidyverse is used for data manipulation, and rstudioapi to run e-utilities commands from RStudio. {r load -if not present install- required packages 3, eval=FALSE, include=FALSE, echo=TRUE} usePackage &lt;- function(p) { if (!is.element(p, installed.packages()[,1])) install.packages(p, dep = TRUE) require(p, character.only = TRUE) } usePackage( tidyverse ) usePackage( rstudioapi ) Pathology Journal ISSN List was retrieved from In Cites Clarivate, and Journal Data Filtered as follows: JCR Year: 2016 Selected Editions: SCIE,SSCI Selected Categories: &#39;PATHOLOGY&#39; Selected Category Scheme: WoS {r Get ISSN List from data downloaded from WoS 3, eval=FALSE, include=FALSE, echo=TRUE} # Get ISSN List from data downloaded from WoS ISSNList &lt;- JournalHomeGrid &lt;- read_csv( data/JournalHomeGrid.csv , skip = 1) %&gt;% select(ISSN) %&gt;% filter(!is.na(ISSN)) %&gt;% t() %&gt;% paste( OR , collapse = ) # add OR between ISSN List ISSNList &lt;- gsub( OR $ , ,ISSNList) # to remove last OR Data is retrieved from PubMed via e-Utilities. The search formula for PubMed is generated as ISSN List AND Country[Affiliation] like done in advanced search of PubMed. {r Generate Search Formula For Pathology Journals AND Countries 3, eval=FALSE, include=FALSE, echo=TRUE} # Generate Search Formula For Pathology Journals AND Countries searchformula &lt;- paste( &#39; ,ISSNList, &#39; , AND , Turkey[Affiliation] ) write(searchformula, data/searchformula.txt ) Articles are downloaded as xml. {r Search PubMed 3, eval=FALSE, include=FALSE, echo=TRUE} myTerm &lt;- rstudioapi::terminalCreate(show = FALSE) rstudioapi::terminalSend(myTerm, esearch -db pubmed -query \\ $(cat data/searchformula.txt)\\ -datetype PDAT -mindate 1900 -maxdate 3000 | efetch -format xml &gt; data/PathologyTurkey.xml \\n ) Sys.sleep(1) repeat{ Sys.sleep(0.1) if(rstudioapi::terminalBusy(myTerm) == FALSE){ print( Code Executed ) break } } MeSH terms are extracted from xml. Common terms are excluded and major topics are selected. {r extract major MeSH topics -excluding common tags- from xml 3, eval=FALSE, include=FALSE, echo=TRUE} myTerm &lt;- rstudioapi::terminalCreate(show = FALSE) rstudioapi::terminalSend(myTerm, xtract -input data/PathologyTurkey.xml -pattern MeshHeading -if DescriptorName@MajorTopicYN -equals Y -or QualifierName@MajorTopicYN -equals Y -element DescriptorName| grep -vxf data/checktags.txt | sort-uniq-count-rank &gt; data/PathologyTurkeyMeSH.txt \\n ) Sys.sleep(1) repeat{ Sys.sleep(0.1) if(rstudioapi::terminalBusy(myTerm) == FALSE){ print( Code Executed ) break } } Keywords are extracted from xml. {r extract author keywords from xml 3, eval=FALSE, include=FALSE, echo=TRUE} myTerm &lt;- rstudioapi::terminalCreate(show = FALSE) rstudioapi::terminalSend(myTerm, xtract -input data/PathologyTurkey.xml -pattern Keyword -element Keyword | sort-uniq-count-rank &gt; authorkeywords.txt \\n ) Sys.sleep(1) repeat{ Sys.sleep(0.1) if(rstudioapi::terminalBusy(myTerm) == FALSE){ print( Code Executed ) break } } Result: The retrieved information was compiled in a table. {r display results as table 3, eval=FALSE, include=FALSE, echo=TRUE} my_tbl &lt;- tibble::tribble( ~Col_1, ~Col_2, ~Col_3, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA ) require(rhandsontable) rhandsontable(my_tbl, rowHeaders = NULL, digits = 3, useTypes = FALSE, search = FALSE, width = NULL, height = NULL) Comment: Future Work: "],["feedback-1.html", "Chapter 27 Feedback", " Chapter 27 Feedback Serdar Balcƒ±, MD, Pathologist would like to hear your feedback: https://goo.gl/forms/YjGZ5DHgtPlR1RnB3 This document will be continiously updated and the last update was on . "],["back-to-main-menu.html", "Chapter 28 Back to Main Menu", " Chapter 28 Back to Main Menu Main Page for Bibliographic Analysis "],["table-options.html", "Chapter 29 Table options", " Chapter 29 Table options Several packages support making beautiful tables with R, such as xtable stargazer pander tables ascii etc. It is also very easy to make tables with knitr‚Äôs kable function: {r eval=FALSE, include=FALSE, echo=TRUE} knitr::kable(head(iris), caption = Title of the table ) {r eval=FALSE, include=FALSE, echo=TRUE} pander::pander(mtcars) {r eval=FALSE, include=FALSE, echo=TRUE} stargazer::stargazer(mtcars) {r echo = TRUE, results = &#39;asis&#39;} library(knitr) kable(mtcars[1:5, ], caption = A knitr kable. ) {r eval=FALSE, include=FALSE, echo=TRUE} library(rhandsontable) rhandsontable(mtcars) {r eval=FALSE, include=FALSE, echo=TRUE} xtable::xtable(mtcars) "],["references.html", "References", " References "],["analysing-the-hiv-pandemic.html", "Chapter 30 Analysing the HIV pandemic", " Chapter 30 Analysing the HIV pandemic https://rviews.rstudio.com/2019/04/30/analysing-hiv-pandemic-part-1/ "],["arsenal.html", "Chapter 31 arsenal 31.1 The compare function", " Chapter 31 arsenal 31.1 The compare function https://cran.r-project.org/web/packages/arsenal/vignettes/compare.html { eval=FALSE, include=FALSE, echo=TRUE} library(arsenal) { eval=FALSE, include=FALSE, echo=TRUE} df1 &lt;- data.frame(id = paste0( person , 1:3), a = c( a , b , c ), b = c(1, 3, 4), c = c( f , e , d ), row.names = paste0( rn , 1:3), stringsAsFactors = FALSE) df2 &lt;- data.frame(id = paste0( person , 3:1), a = c( c , b , a ), b = c(1, 3, 4), d = paste0( rn , 1:3), row.names = paste0( rn , c(1,3,2)), stringsAsFactors = FALSE) { eval=FALSE, include=FALSE, echo=TRUE} compare(df1, df2) { eval=FALSE, include=FALSE, echo=TRUE} summary(compare(df1, df2)) { eval=FALSE, include=FALSE, echo=TRUE} summary(compare(df1, df2, by = id )) { eval=FALSE, include=FALSE, echo=TRUE} data(mockstudy) mockstudy2 &lt;- muck_up_mockstudy() { eval=FALSE, include=FALSE, echo=TRUE} summary(compare(mockstudy, mockstudy2, by = case )) Summary of data.frames version arg ncol nrow x mockstudy 14 1499 y mockstudy2 13 1495 Variables not shared version variable position class x age 2 integer x arm 3 character x fu.time 6 integer x fu.stat 7 integer y fu_time 11 integer y fu stat 12 integer y Arm 13 character Other variables not compared var.x pos.x class.x var.y pos.y class.y race 5 character race 3 factor ast 12 integer ast 8 numeric Observations not shared version case observation x 88989 9 x 90158 8 x 99508 7 x 112263 5 Differences detected by variable var.x var.y n NAs sex sex 1495 0 ps ps 1 1 hgb hgb 266 266 bmi bmi 0 0 alk.phos alk.phos 0 0 mdquality.s mdquality.s 0 0 age.ord age.ord 0 0 First 10 differences detected per variable (1741 differences not shown) var.x var.y case values.x values.y row.x row.y sex sex 76170 Male Male 26 20 sex sex 76240 Male Male 27 21 sex sex 76431 Female Female 28 22 sex sex 76712 Male Male 29 23 sex sex 76780 Female Female 30 24 sex sex 77066 Female Female 31 25 sex sex 77316 Male Male 32 26 sex sex 77355 Male Male 33 27 sex sex 77591 Male Male 34 28 sex sex 77851 Male Male 35 29 ps ps 86205 0 NA 6 3 hgb hgb 88714 NA -9 192 186 hgb hgb 88955 NA -9 204 198 hgb hgb 89549 NA -9 229 223 hgb hgb 89563 NA -9 231 225 hgb hgb 89584 NA -9 237 231 hgb hgb 89591 NA -9 238 232 hgb hgb 89595 NA -9 239 233 hgb hgb 89647 NA -9 243 237 hgb hgb 89665 NA -9 244 238 hgb hgb 89827 NA -9 255 249 Non-identical attributes var.x var.y name sex sex label sex sex levels race race class race race label race race levels bmi bmi label Column name comparison options It is possible to change which column names are considered ‚Äúthe same variable‚Äù. Ignoring case For example, to ignore case in variable names (so that Arm and arm are considered the same), pass tol.vars = case . You can do this using comparison.control() summary(compare(mockstudy, mockstudy2, by = case , control = comparison.control(tol.vars = case ))) or pass it through the ... arguments. summary(compare(mockstudy, mockstudy2, by = case , tol.vars = case )) Summary of data.frames version arg ncol nrow x mockstudy 14 1499 y mockstudy2 13 1495 Variables not shared version variable position class x age 2 integer x fu.time 6 integer x fu.stat 7 integer y fu_time 11 integer y fu stat 12 integer Other variables not compared var.x pos.x class.x var.y pos.y class.y race 5 character race 3 factor ast 12 integer ast 8 numeric Observations not shared version case observation x 88989 9 x 90158 8 x 99508 7 x 112263 5 Differences detected by variable var.x var.y n NAs arm Arm 0 0 sex sex 1495 0 ps ps 1 1 hgb hgb 266 266 bmi bmi 0 0 alk.phos alk.phos 0 0 mdquality.s mdquality.s 0 0 age.ord age.ord 0 0 First 10 differences detected per variable (1741 differences not shown) var.x var.y case values.x values.y row.x row.y sex sex 76170 Male Male 26 20 sex sex 76240 Male Male 27 21 sex sex 76431 Female Female 28 22 sex sex 76712 Male Male 29 23 sex sex 76780 Female Female 30 24 sex sex 77066 Female Female 31 25 sex sex 77316 Male Male 32 26 sex sex 77355 Male Male 33 27 sex sex 77591 Male Male 34 28 sex sex 77851 Male Male 35 29 ps ps 86205 0 NA 6 3 hgb hgb 88714 NA -9 192 186 hgb hgb 88955 NA -9 204 198 hgb hgb 89549 NA -9 229 223 hgb hgb 89563 NA -9 231 225 hgb hgb 89584 NA -9 237 231 hgb hgb 89591 NA -9 238 232 hgb hgb 89595 NA -9 239 233 hgb hgb 89647 NA -9 243 237 hgb hgb 89665 NA -9 244 238 hgb hgb 89827 NA -9 255 249 Non-identical attributes var.x var.y name arm Arm label sex sex label sex sex levels race race class race race label race race levels bmi bmi label Treating dots and underscores the same (equivalence classes) It is possible to treat certain characters or sets of characters as the same by passing a character vector of equivalence classes to the tol.vars= argument. In short, each string in the vector is split into single characters, and the resulting set of characters is replaced by the first character in the string. For example, passing c( ._ ) would replace all underscores with dots in the column names of both datasets. Similarly, passing c( aA , BbCc ) would replace all instances of A with a and all instances of b , C , or c with B . This is one way to ignore case for certain letters. Otherwise, it‚Äôs possible to combine the equivalence classes with ignoring case, by passing (e.g.) c( ._ , case ). Passing a single character as an element this vector will replace that character with the empty string. For example, passing c( ‚Äú,‚Äù.‚Äú) would remove all spaces and dots from the column names. For mockstudy, let‚Äôs treat dots, underscores, and spaces as the same, and ignore case: summary(compare(mockstudy, mockstudy2, by = case , tol.vars = c( ._ , case ) # dots=underscores=spaces, ignore case )) Summary of data.frames version arg ncol nrow x mockstudy 14 1499 y mockstudy2 13 1495 Variables not shared version variable position class x age 2 integer Other variables not compared var.x pos.x class.x var.y pos.y class.y race 5 character race 3 factor ast 12 integer ast 8 numeric Observations not shared version case observation x 88989 9 x 90158 8 x 99508 7 x 112263 5 Differences detected by variable var.x var.y n NAs arm Arm 0 0 sex sex 1495 0 fu.time fu_time 0 0 fu.stat fu stat 0 0 ps ps 1 1 hgb hgb 266 266 bmi bmi 0 0 alk.phos alk.phos 0 0 mdquality.s mdquality.s 0 0 age.ord age.ord 0 0 First 10 differences detected per variable (1741 differences not shown) var.x var.y case values.x values.y row.x row.y sex sex 76170 Male Male 26 20 sex sex 76240 Male Male 27 21 sex sex 76431 Female Female 28 22 sex sex 76712 Male Male 29 23 sex sex 76780 Female Female 30 24 sex sex 77066 Female Female 31 25 sex sex 77316 Male Male 32 26 sex sex 77355 Male Male 33 27 sex sex 77591 Male Male 34 28 sex sex 77851 Male Male 35 29 ps ps 86205 0 NA 6 3 hgb hgb 88714 NA -9 192 186 hgb hgb 88955 NA -9 204 198 hgb hgb 89549 NA -9 229 223 hgb hgb 89563 NA -9 231 225 hgb hgb 89584 NA -9 237 231 hgb hgb 89591 NA -9 238 232 hgb hgb 89595 NA -9 239 233 hgb hgb 89647 NA -9 243 237 hgb hgb 89665 NA -9 244 238 hgb hgb 89827 NA -9 255 249 Non-identical attributes var.x var.y name arm Arm label sex sex label sex sex levels race race class race race label race race levels bmi bmi label Column comparison options Logical tolerance Use the tol.logical= argument to change how logicals are compared. By default, they‚Äôre expected to be equal to each other. Numeric tolerance To allow numeric differences of a certain tolerance, use the tol.num= and tol.num.val= options. tol.num.val= determines the maximum (unsigned) difference tolerated if tol.num= absolute (default), and determines the maximum (unsigned) percent difference tolerated if tol.num= percent . Also note the option int.as.num=, which determines whether integers and numerics should be compared despite their class difference. If TRUE, the integers are coerced to numeric. Note that mockstudy$ast is integer, while mockstudy2$ast is numeric: summary(compare(mockstudy, mockstudy2, by = case , tol.vars = c( ._ , case ), # dots=underscores=spaces, ignore case int.as.num = TRUE # compare integers and numerics )) Summary of data.frames version arg ncol nrow x mockstudy 14 1499 y mockstudy2 13 1495 Variables not shared version variable position class x age 2 integer Other variables not compared var.x pos.x class.x var.y pos.y class.y race 5 character race 3 factor Observations not shared version case observation x 88989 9 x 90158 8 x 99508 7 x 112263 5 Differences detected by variable var.x var.y n NAs arm Arm 0 0 sex sex 1495 0 fu.time fu_time 0 0 fu.stat fu stat 0 0 ps ps 1 1 hgb hgb 266 266 bmi bmi 0 0 alk.phos alk.phos 0 0 ast ast 3 0 mdquality.s mdquality.s 0 0 age.ord age.ord 0 0 First 10 differences detected per variable (1741 differences not shown) var.x var.y case values.x values.y row.x row.y sex sex 76170 Male Male 26 20 sex sex 76240 Male Male 27 21 sex sex 76431 Female Female 28 22 sex sex 76712 Male Male 29 23 sex sex 76780 Female Female 30 24 sex sex 77066 Female Female 31 25 sex sex 77316 Male Male 32 26 sex sex 77355 Male Male 33 27 sex sex 77591 Male Male 34 28 sex sex 77851 Male Male 35 29 ps ps 86205 0 NA 6 3 hgb hgb 88714 NA -9 192 186 hgb hgb 88955 NA -9 204 198 hgb hgb 89549 NA -9 229 223 hgb hgb 89563 NA -9 231 225 hgb hgb 89584 NA -9 237 231 hgb hgb 89591 NA -9 238 232 hgb hgb 89595 NA -9 239 233 hgb hgb 89647 NA -9 243 237 hgb hgb 89665 NA -9 244 238 hgb hgb 89827 NA -9 255 249 ast ast 86205 27 36 6 3 ast ast 105271 100 36 3 2 ast ast 110754 35 36 1 1 Non-identical attributes var.x var.y name arm Arm label sex sex label sex sex levels race race class race race label race race levels bmi bmi label Suppose a tolerance of up to 10 is allowed for ast: summary(compare(mockstudy, mockstudy2, by = case , tol.vars = c( ._ , case ), # dots=underscores=spaces, ignore case int.as.num = TRUE, # compare integers and numerics tol.num.val = 10 # allow absolute differences &lt;= 10 )) Summary of data.frames version arg ncol nrow x mockstudy 14 1499 y mockstudy2 13 1495 Variables not shared version variable position class x age 2 integer Other variables not compared var.x pos.x class.x var.y pos.y class.y race 5 character race 3 factor Observations not shared version case observation x 88989 9 x 90158 8 x 99508 7 x 112263 5 Differences detected by variable var.x var.y n NAs arm Arm 0 0 sex sex 1495 0 fu.time fu_time 0 0 fu.stat fu stat 0 0 ps ps 1 1 hgb hgb 266 266 bmi bmi 0 0 alk.phos alk.phos 0 0 ast ast 1 0 mdquality.s mdquality.s 0 0 age.ord age.ord 0 0 First 10 differences detected per variable (1741 differences not shown) var.x var.y case values.x values.y row.x row.y sex sex 76170 Male Male 26 20 sex sex 76240 Male Male 27 21 sex sex 76431 Female Female 28 22 sex sex 76712 Male Male 29 23 sex sex 76780 Female Female 30 24 sex sex 77066 Female Female 31 25 sex sex 77316 Male Male 32 26 sex sex 77355 Male Male 33 27 sex sex 77591 Male Male 34 28 sex sex 77851 Male Male 35 29 ps ps 86205 0 NA 6 3 hgb hgb 88714 NA -9 192 186 hgb hgb 88955 NA -9 204 198 hgb hgb 89549 NA -9 229 223 hgb hgb 89563 NA -9 231 225 hgb hgb 89584 NA -9 237 231 hgb hgb 89591 NA -9 238 232 hgb hgb 89595 NA -9 239 233 hgb hgb 89647 NA -9 243 237 hgb hgb 89665 NA -9 244 238 hgb hgb 89827 NA -9 255 249 ast ast 105271 100 36 3 2 Non-identical attributes var.x var.y name arm Arm label sex sex label sex sex levels race race class race race label race race levels bmi bmi label Factor tolerance By default, factors are compared to each other based on both the labels and the underlying numeric levels. Set tol.factor= levels to match only the numeric levels, or set tol.factor= labels to match only the labels. summary(compare(mockstudy, mockstudy2, by = case , tol.vars = c( ._ , case ), # dots=underscores=spaces, ignore case int.as.num = TRUE, # compare integers and numerics tol.num.val = 10, # allow absolute differences &lt;= 10 tol.factor = labels # match only factor labels )) Summary of data.frames version arg ncol nrow x mockstudy 14 1499 y mockstudy2 13 1495 Variables not shared version variable position class x age 2 integer Other variables not compared var.x pos.x class.x var.y pos.y class.y race 5 character race 3 factor Observations not shared version case observation x 88989 9 x 90158 8 x 99508 7 x 112263 5 Differences detected by variable var.x var.y n NAs arm Arm 0 0 sex sex 0 0 fu.time fu_time 0 0 fu.stat fu stat 0 0 ps ps 1 1 hgb hgb 266 266 bmi bmi 0 0 alk.phos alk.phos 0 0 ast ast 1 0 mdquality.s mdquality.s 0 0 age.ord age.ord 0 0 First 10 differences detected per variable (256 differences not shown) var.x var.y case values.x values.y row.x row.y ps ps 86205 0 NA 6 3 hgb hgb 88714 NA -9 192 186 hgb hgb 88955 NA -9 204 198 hgb hgb 89549 NA -9 229 223 hgb hgb 89563 NA -9 231 225 hgb hgb 89584 NA -9 237 231 hgb hgb 89591 NA -9 238 232 hgb hgb 89595 NA -9 239 233 hgb hgb 89647 NA -9 243 237 hgb hgb 89665 NA -9 244 238 hgb hgb 89827 NA -9 255 249 ast ast 105271 100 36 3 2 Non-identical attributes var.x var.y name arm Arm label sex sex label sex sex levels race race class race race label race race levels bmi bmi label Also note the option factor.as.char=, which determines whether factors and characters should be compared despite their class difference. If TRUE, the factors are coerced to characters. Note that mockstudy$race is a character, while mockstudy2$race is a factor: summary(compare(mockstudy, mockstudy2, by = case , tol.vars = c( ._ , case ), # dots=underscores=spaces, ignore case int.as.num = TRUE, # compare integers and numerics tol.num.val = 10, # allow absolute differences &lt;= 10 tol.factor = labels , # match only factor labels factor.as.char = TRUE # compare factors and characters )) Summary of data.frames version arg ncol nrow x mockstudy 14 1499 y mockstudy2 13 1495 Variables not shared version variable position class x age 2 integer Other variables not compared No other variables not compared Observations not shared version case observation x 88989 9 x 90158 8 x 99508 7 x 112263 5 Differences detected by variable var.x var.y n NAs arm Arm 0 0 sex sex 0 0 race race 1285 0 fu.time fu_time 0 0 fu.stat fu stat 0 0 ps ps 1 1 hgb hgb 266 266 bmi bmi 0 0 alk.phos alk.phos 0 0 ast ast 1 0 mdquality.s mdquality.s 0 0 age.ord age.ord 0 0 First 10 differences detected per variable (1531 differences not shown) var.x var.y case values.x values.y row.x row.y race race 76170 Caucasian caucasian 26 20 race race 76240 Caucasian caucasian 27 21 race race 76431 Caucasian caucasian 28 22 race race 76712 Caucasian caucasian 29 23 race race 76780 Caucasian caucasian 30 24 race race 77066 Caucasian caucasian 31 25 race race 77316 Caucasian caucasian 32 26 race race 77591 Caucasian caucasian 34 28 race race 77851 Caucasian caucasian 35 29 race race 77956 Caucasian caucasian 36 30 ps ps 86205 0 NA 6 3 hgb hgb 88714 NA -9 192 186 hgb hgb 88955 NA -9 204 198 hgb hgb 89549 NA -9 229 223 hgb hgb 89563 NA -9 231 225 hgb hgb 89584 NA -9 237 231 hgb hgb 89591 NA -9 238 232 hgb hgb 89595 NA -9 239 233 hgb hgb 89647 NA -9 243 237 hgb hgb 89665 NA -9 244 238 hgb hgb 89827 NA -9 255 249 ast ast 105271 100 36 3 2 Non-identical attributes var.x var.y name arm Arm label sex sex label sex sex levels race race class race race label race race levels bmi bmi label Character tolerance Use the tol.char= argument to change how character variables are compared. By default, they are compared as-is, but they can be compared after ignoring case or trimming whitespace or both. summary(compare(mockstudy, mockstudy2, by = case , tol.vars = c( ._ , case ), # dots=underscores=spaces, ignore case int.as.num = TRUE, # compare integers and numerics tol.num.val = 10, # allow absolute differences &lt;= 10 tol.factor = labels , # match only factor labels factor.as.char = TRUE, # compare factors and characters tol.char = case # ignore case in character vectors )) Summary of data.frames version arg ncol nrow x mockstudy 14 1499 y mockstudy2 13 1495 Variables not shared version variable position class x age 2 integer Other variables not compared No other variables not compared Observations not shared version case observation x 88989 9 x 90158 8 x 99508 7 x 112263 5 Differences detected by variable var.x var.y n NAs arm Arm 0 0 sex sex 0 0 race race 0 0 fu.time fu_time 0 0 fu.stat fu stat 0 0 ps ps 1 1 hgb hgb 266 266 bmi bmi 0 0 alk.phos alk.phos 0 0 ast ast 1 0 mdquality.s mdquality.s 0 0 age.ord age.ord 0 0 First 10 differences detected per variable (256 differences not shown) var.x var.y case values.x values.y row.x row.y ps ps 86205 0 NA 6 3 hgb hgb 88714 NA -9 192 186 hgb hgb 88955 NA -9 204 198 hgb hgb 89549 NA -9 229 223 hgb hgb 89563 NA -9 231 225 hgb hgb 89584 NA -9 237 231 hgb hgb 89591 NA -9 238 232 hgb hgb 89595 NA -9 239 233 hgb hgb 89647 NA -9 243 237 hgb hgb 89665 NA -9 244 238 hgb hgb 89827 NA -9 255 249 ast ast 105271 100 36 3 2 Non-identical attributes var.x var.y name arm Arm label sex sex label sex sex levels race race class race race label race race levels bmi bmi label Date tolerance Use the tol.date= argument to change how dates are compared. By default, they‚Äôre expected to be equal to each other. Other data type tolerances Use the tol.other= argument to change how other objects are compared. By default, they‚Äôre expected to be identical(). User-defined tolerance functions Details The comparison.control() function accepts functions for any of the tolerance arguments in addition to the short-hand character strings. This allows the user to create custom tolerance functions to suit his/her needs. Any custom tolerance function must accept two vectors as arguments and return a logical vector of the same length. The TRUEs in the results should correspond to elements which are deemed ‚Äúdifferent‚Äù. Note that the numeric and date tolerance functions should also include a third argument for tolerance size (even if it‚Äôs not used). CAUTION: the results should not include NAs, since the logical vector is used to subset the input data.frames. The tol.NA() function is useful for considering any NAs in the two vectors (but not both) as differences, in addition to other criteria. tol.NA function (x, y, idx) { (is.na(x) &amp; !is.na(y)) | (is.na(y) &amp; !is.na(x)) | (!is.na(x) &amp; !is.na(y) &amp; idx) } &lt;environment: namespace:arsenal&gt; The tol.NA() function is used in all default tolerance functions to help handle NAs. Example 1 Suppose we want to ignore any dates which are later in the second dataset than the first. We define a custom tolerance function. my.tol &lt;- function(x, y, tol) { tol.NA(x, y, x &gt; y) } date.df1 &lt;- data.frame(dt = as.Date(c( 2017-09-07 , 2017-08-08 , 2017-07-09 , NA))) date.df2 &lt;- data.frame(dt = as.Date(c( 2017-10-01 , 2017-08-08 , 2017-07-10 , 2017-01-01 ))) n.diffs(compare(date.df1, date.df2)) # default finds any differences [1] 3 n.diffs(compare(date.df1, date.df2, tol.date = my.tol)) # our function identifies only the NA as different... [1] 1 n.diffs(compare(date.df2, date.df1, tol.date = my.tol)) # ... until we change the argument order [1] 3 Example 2 (Continuing our mockstudy example) Suppose we‚Äôre okay with NAs getting replaced by -9. tol.minus9 &lt;- function(x, y, tol) { idx1 &lt;- is.na(x) &amp; !is.na(y) &amp; y == -9 idx2 &lt;- tol.num.absolute(x, y, tol) # find other absolute differences return(!idx1 &amp; idx2) } summary(compare(mockstudy, mockstudy2, by = case , tol.vars = c( ._ , case ), # dots=underscores=spaces, ignore case int.as.num = TRUE, # compare integers and numerics tol.num.val = 10, # allow absolute differences &lt;= 10 tol.factor = labels , # match only factor labels factor.as.char = TRUE, # compare factors and characters tol.char = case , # ignore case in character vectors tol.num = tol.minus9 # ignore NA -&gt; -9 changes )) Summary of data.frames version arg ncol nrow x mockstudy 14 1499 y mockstudy2 13 1495 Variables not shared version variable position class x age 2 integer Other variables not compared No other variables not compared Observations not shared version case observation x 88989 9 x 90158 8 x 99508 7 x 112263 5 Differences detected by variable var.x var.y n NAs arm Arm 0 0 sex sex 0 0 race race 0 0 fu.time fu_time 0 0 fu.stat fu stat 0 0 ps ps 1 1 hgb hgb 0 0 bmi bmi 0 0 alk.phos alk.phos 0 0 ast ast 1 0 mdquality.s mdquality.s 0 0 age.ord age.ord 0 0 First 10 differences detected per variable var.x var.y case values.x values.y row.x row.y ps ps 86205 0 NA 6 3 ast ast 105271 100 36 3 2 Non-identical attributes var.x var.y name arm Arm label sex sex label sex sex levels race race class race race label race race levels bmi bmi label Extract Differences Differences can be easily extracted using the diffs() function. If you only want to determine how many differences were found, use the n.diffs() function. cmp &lt;- compare(mockstudy, mockstudy2, by = case , tol.vars = c( ._ , case ), int.as.num = TRUE) n.diffs(cmp) [1] 1765 head(diffs(cmp)) var.x var.y case values.x values.y row.x row.y 1 sex sex 76170 Male Male 26 20 2 sex sex 76240 Male Male 27 21 3 sex sex 76431 Female Female 28 22 4 sex sex 76712 Male Male 29 23 5 sex sex 76780 Female Female 30 24 6 sex sex 77066 Female Female 31 25 Differences can also be summarized by variable. diffs(cmp, by.var = TRUE) var.x var.y n NAs 1 arm Arm 0 0 2 sex sex 1495 0 3 fu.time fu_time 0 0 4 fu.stat fu stat 0 0 5 ps ps 1 1 6 hgb hgb 266 266 7 bmi bmi 0 0 8 alk.phos alk.phos 0 0 9 ast ast 3 0 10 mdquality.s mdquality.s 0 0 11 age.ord age.ord 0 0 To report differences from only a few variables, one can pass a list of variable names to diffs(). diffs(cmp, vars = c( ps , ast ), by.var = TRUE) var.x var.y n NAs 5 ps ps 1 1 9 ast ast 3 0 diffs(cmp, vars = c( ps , ast )) var.x var.y case values.x values.y row.x row.y 1496 ps ps 86205 0 NA 6 3 1763 ast ast 86205 27 36 6 3 1764 ast ast 105271 100 36 3 2 1765 ast ast 110754 35 36 1 1 Appendix Stucture of the Object (This section is just as much for my use as for yours!) obj &lt;- compare(mockstudy, mockstudy2, by = case ) There are two main objects in the compare.data.frame object, each with its own print method. The frame.summary contains: the substituted-deparsed arguments information about the number of columns and rows in each dataset the by-variables for each dataset (which may not be the same) the attributes for each dataset (which get counted in the print method) a data.frame of by-variables and row numbers of observations not shared between datasets the number of shared observations print(obj$frame.summary) version arg ncol nrow by attrs unique n.shared 1 x mockstudy 14 1499 case 3 attributes 4 unique obs 1495 2 y mockstudy2 13 1495 case 3 attributes 0 unique obs 1495 The vars.summary contains: variable name, column number, and class vector (with possibly more than one element) for each x and y. These are all NA if there isn‚Äôt a match in both datasets. values, a list-column of the text string by-variable for the by-variables, NULL for columns that aren‚Äôt compared, or a data.frame containing: The by-variables for differences found The values which are different for x and y The row numbers for differences found attrs, a list-column of NULL if there are no attributes, or a data.frame containing: The name of the attributes The attributes for x and y, set to NA if non-existant The actual attributes (if show.attr=TRUE). print(obj$vars.summary) var.x pos.x class.x var.y pos.y class.y values attrs 8 case 1 integer case 1 integer by-variable 0 attributes 17 sex 4 factor sex 2 factor 1495 differences 2 attributes 16 race 5 character race 3 factor Not compared 3 attributes 15 ps 8 integer ps 4 integer 1 differences 0 attributes 13 hgb 9 numeric hgb 5 numeric 266 differences 0 attributes 7 bmi 10 numeric bmi 6 numeric 0 differences 1 attributes 4 alk.phos 11 integer alk.phos 7 integer 0 differences 0 attributes 6 ast 12 integer ast 8 numeric Not compared 0 attributes 14 mdquality.s 13 integer mdquality.s 9 integer 0 differences 0 attributes 3 age.ord 14 ordered, factor age.ord 10 ordered, factor 0 differences 0 attributes 2 age 2 integer &lt;NA&gt; NA NA Not compared 0 attributes 5 arm 3 character &lt;NA&gt; NA NA Not compared 0 attributes 11 fu.time 6 integer &lt;NA&gt; NA NA Not compared 0 attributes 10 fu.stat 7 integer &lt;NA&gt; NA NA Not compared 0 attributes 12 &lt;NA&gt; NA NA fu_time 11 integer Not compared 0 attributes 9 &lt;NA&gt; NA NA fu stat 12 integer Not compared 0 attributes 1 &lt;NA&gt; NA NA Arm 13 character Not compared 0 attributes ## The freqlist function https://cran.r-project.org/web/packages/arsenal/vignettes/freqlist.html The freqlist function Tina Gunderson and Ethan Heinzen 09 November, 2018 Overview Sample dataset The freqlist object Basic output using summary() Using a formula with freqlist Rounding percentage digits or changing variable names for printing Additional examples Including combinations with frequencies of zero Options for NA handling Frequency counts and percentages subset by factor levels Change labels on the fly Using xtable() to format and print freqlist() results Use freqlist in bookdown Appendix: Notes regarding table options in R NAs Table dimname names (dnn) Overview freqlist() is a function meant to produce output similar to SAS‚Äôs PROC FREQ procedure when using the /list option of the TABLE statement. freqlist() provides options for handling missing or sparse data and can provide cumulative counts and percentages based on subgroups. It depends on the knitr package for printing. require(arsenal) Sample dataset For our examples, we‚Äôll load the mockstudy data included with this package and use it to create a basic table. Because they have fewer levels, for brevity, we‚Äôll use the variables arm, sex, and mdquality.s to create the example table. We‚Äôll retain NAs in the table creation. See the appendix for notes regarding default NA handling and other useful information regarding tables in R. # load the data data(mockstudy) # retain NAs when creating the table using the useNA argument tab.ex &lt;- table(mockstudy[, c( arm , sex , mdquality.s )], useNA = ifany ) The freqlist object The freqlist() function returns an object of class freqlist , which has three parts: freqlist, byVar, and labels. freqlist is a single data frame containing all contingency tables with calculated frequencies, cumulative frequencies, percentages, and cumulative percentages. byVar and labels are used in the summary method for subgroups and variable names, which will be covered in later examples. Note that freqlist() is an S3 generic, with methods for tables and formulas. noby &lt;- freqlist(tab.ex) str(noby) List of 3 $ freqlist:&#39;data.frame&#39;: 18 obs. of 7 variables: ..$ arm : Factor w/ 3 levels A: IFL , F: FOLFOX ,..: 1 1 1 1 1 1 2 2 2 2 ... ..$ sex : Factor w/ 2 levels Male , Female : 1 1 1 2 2 2 1 1 1 2 ... ..$ mdquality.s: Factor w/ 2 levels 0 , 1 : 1 2 NA 1 2 NA 1 2 NA 1 ... ..$ Freq : int [1:18] 29 214 34 12 118 21 31 285 95 21 ... ..$ cumFreq : int [1:18] 29 243 277 289 407 428 459 744 839 860 ... ..$ freqPercent: num [1:18] 1.93 14.28 2.27 0.8 7.87 ... ..$ cumPercent : num [1:18] 1.93 16.21 18.48 19.28 27.15 ... $ byVar : NULL $ labels : NULL - attr(*, class )= chr freqlist # view the data frame portion of freqlist output head(noby[[ freqlist ]]) ## or use as.data.frame(noby) arm sex mdquality.s Freq cumFreq freqPercent cumPercent 1 A: IFL Male 0 29 29 1.93 1.93 2 A: IFL Male 1 214 243 14.28 16.21 3 A: IFL Male &lt;NA&gt; 34 277 2.27 18.48 4 A: IFL Female 0 12 289 0.80 19.28 5 A: IFL Female 1 118 407 7.87 27.15 6 A: IFL Female &lt;NA&gt; 21 428 1.40 28.55 Basic output using summary() The summary method for freqlist() relies on the kable() function (in the knitr package) for printing. knitr::kable() converts the output to markdown which can be printed in the console or easily rendered in Word, PDF, or HTML documents. Note that you must supply results= asis to properly format the markdown output. summary(noby) arm sex mdquality.s Freq cumFreq freqPercent cumPercent A: IFL Male 0 29 29 1.93 1.93 1 214 243 14.28 16.21 NA 34 277 2.27 18.48 Female 0 12 289 0.80 19.28 1 118 407 7.87 27.15 NA 21 428 1.40 28.55 F: FOLFOX Male 0 31 459 2.07 30.62 1 285 744 19.01 49.63 NA 95 839 6.34 55.97 Female 0 21 860 1.40 57.37 1 198 1058 13.21 70.58 NA 61 1119 4.07 74.65 G: IROX Male 0 17 1136 1.13 75.78 1 187 1323 12.47 88.26 NA 24 1347 1.60 89.86 Female 0 14 1361 0.93 90.79 1 121 1482 8.07 98.87 NA 17 1499 1.13 100.00 You can print a title for the table using the title= argument. summary(noby, title = Basic freqlist output ) Basic freqlist output arm sex mdquality.s Freq cumFreq freqPercent cumPercent A: IFL Male 0 29 29 1.93 1.93 1 214 243 14.28 16.21 NA 34 277 2.27 18.48 Female 0 12 289 0.80 19.28 1 118 407 7.87 27.15 NA 21 428 1.40 28.55 F: FOLFOX Male 0 31 459 2.07 30.62 1 285 744 19.01 49.63 NA 95 839 6.34 55.97 Female 0 21 860 1.40 57.37 1 198 1058 13.21 70.58 NA 61 1119 4.07 74.65 G: IROX Male 0 17 1136 1.13 75.78 1 187 1323 12.47 88.26 NA 24 1347 1.60 89.86 Female 0 14 1361 0.93 90.79 1 121 1482 8.07 98.87 NA 17 1499 1.13 100.00 You can also easily pull out the freqlist data frame for more complicated formatting or manipulation (e.g. with another function such as xtable() or pander()) using as.data.frame(): head(as.data.frame(noby)) arm sex mdquality.s Freq cumFreq freqPercent cumPercent 1 A: IFL Male 0 29 29 1.93 1.93 2 A: IFL Male 1 214 243 14.28 16.21 3 A: IFL Male &lt;NA&gt; 34 277 2.27 18.48 4 A: IFL Female 0 12 289 0.80 19.28 5 A: IFL Female 1 118 407 7.87 27.15 6 A: IFL Female &lt;NA&gt; 21 428 1.40 28.55 Using a formula with freqlist Instead of passing a pre-computed table to freqlist(), you can instead pass a formula, which will be in turn passed to the xtabs() function. Additional freqlist() arguments are passed through the ... to the freqlist() table method. Note that the addNA= argument was added to xtabs() in R 3.4.0. In previous versions, NAs have to be added to relevant columns using addNA(). ### this works in R &gt;= 3.4.0 summary(freqlist(~ arm + sex + mdquality.s, data = ### mockstudy, addNA = TRUE)) ### This one is backwards-compatible summary(freqlist(~arm + sex + addNA(mdquality.s), data = mockstudy)) |arm |sex |addNA.mdquality.s. | Freq| cumFreq| freqPercent| cumPercent| |:|:|:|-:|-:|--:|-:| |A: IFL |Male |0 | 29| 29| 1.93| 1.93| | | |1 | 214| 243| 14.28| 16.21| | | |NA | 34| 277| 2.27| 18.48| | |Female |0 | 12| 289| 0.80| 19.28| | | |1 | 118| 407| 7.87| 27.15| | | |NA | 21| 428| 1.40| 28.55| |F: FOLFOX |Male |0 | 31| 459| 2.07| 30.62| | | |1 | 285| 744| 19.01| 49.63| | | |NA | 95| 839| 6.34| 55.97| | |Female |0 | 21| 860| 1.40| 57.37| | | |1 | 198| 1058| 13.21| 70.58| | | |NA | 61| 1119| 4.07| 74.65| |G: IROX |Male |0 | 17| 1136| 1.13| 75.78| | | |1 | 187| 1323| 12.47| 88.26| | | |NA | 24| 1347| 1.60| 89.86| | |Female |0 | 14| 1361| 0.93| 90.79| | | |1 | 121| 1482| 8.07| 98.87| | | |NA | 17| 1499| 1.13| 100.00| One can also set NAs to an explicit value using includeNA(). summary(freqlist(~arm + sex + includeNA(mdquality.s, Missing ), data = mockstudy)) |arm |sex |includeNA.mdquality.s...Missing.. | Freq| cumFreq| freqPercent| cumPercent| |:|:|:|-:|-:|--:|-:| |A: IFL |Male |0 | 29| 29| 1.93| 1.93| | | |1 | 214| 243| 14.28| 16.21| | | |Missing | 34| 277| 2.27| 18.48| | |Female |0 | 12| 289| 0.80| 19.28| | | |1 | 118| 407| 7.87| 27.15| | | |Missing | 21| 428| 1.40| 28.55| |F: FOLFOX |Male |0 | 31| 459| 2.07| 30.62| | | |1 | 285| 744| 19.01| 49.63| | | |Missing | 95| 839| 6.34| 55.97| | |Female |0 | 21| 860| 1.40| 57.37| | | |1 | 198| 1058| 13.21| 70.58| | | |Missing | 61| 1119| 4.07| 74.65| |G: IROX |Male |0 | 17| 1136| 1.13| 75.78| | | |1 | 187| 1323| 12.47| 88.26| | | |Missing | 24| 1347| 1.60| 89.86| | |Female |0 | 14| 1361| 0.93| 90.79| | | |1 | 121| 1482| 8.07| 98.87| | | |Missing | 17| 1499| 1.13| 100.00| Rounding percentage digits or changing variable names for printing The digits= argument takes a single numeric value and controls the rounding of percentages in the output. The labelTranslations= argument is a character vector or list whose length must be equal to the number of factors used in the table. Note: this does not change the names of the data frame in the freqlist object, only those used in printing. Both options are applied in the following example. withnames &lt;- freqlist(tab.ex, labelTranslations = c( Treatment Arm , Gender , LASA QOL ), digits = 0) summary(withnames) Treatment Arm Gender LASA QOL Freq cumFreq freqPercent cumPercent A: IFL Male 0 29 29 2 2 1 214 243 14 16 NA 34 277 2 18 Female 0 12 289 1 19 1 118 407 8 27 NA 21 428 1 29 F: FOLFOX Male 0 31 459 2 31 1 285 744 19 50 NA 95 839 6 56 Female 0 21 860 1 57 1 198 1058 13 71 NA 61 1119 4 75 G: IROX Male 0 17 1136 1 76 1 187 1323 12 88 NA 24 1347 2 90 Female 0 14 1361 1 91 1 121 1482 8 99 NA 17 1499 1 100 Additional examples Including combinations with frequencies of zero The sparse= argument takes a single logical value as input. The default option is FALSE. If set to TRUE, the sparse option will include combinations with frequencies of zero in the list of results. As our initial table did not have any such levels, we create a second table to use in our example. summary(freqlist(~race + sex + arm, data = mockstudy, sparse = TRUE, digits = 1)) race sex arm Freq cumFreq freqPercent cumPercent African-Am Male A: IFL 25 25 1.7 1.7 F: FOLFOX 24 49 1.6 3.3 G: IROX 16 65 1.1 4.4 Female A: IFL 14 79 0.9 5.3 F: FOLFOX 25 104 1.7 7.0 G: IROX 11 115 0.7 7.7 Asian Male A: IFL 0 115 0.0 7.7 F: FOLFOX 10 125 0.7 8.4 G: IROX 1 126 0.1 8.4 Female A: IFL 1 127 0.1 8.5 F: FOLFOX 4 131 0.3 8.8 G: IROX 2 133 0.1 8.9 Caucasian Male A: IFL 240 373 16.1 25.0 F: FOLFOX 352 725 23.6 48.6 G: IROX 195 920 13.1 61.7 Female A: IFL 131 1051 8.8 70.4 F: FOLFOX 234 1285 15.7 86.1 G: IROX 136 1421 9.1 95.2 Hawaii/Pacific Male A: IFL 1 1422 0.1 95.3 F: FOLFOX 1 1423 0.1 95.4 G: IROX 0 1423 0.0 95.4 Female A: IFL 0 1423 0.0 95.4 F: FOLFOX 2 1425 0.1 95.5 G: IROX 1 1426 0.1 95.6 Hispanic Male A: IFL 8 1434 0.5 96.1 F: FOLFOX 17 1451 1.1 97.3 G: IROX 12 1463 0.8 98.1 Female A: IFL 4 1467 0.3 98.3 F: FOLFOX 11 1478 0.7 99.1 G: IROX 2 1480 0.1 99.2 Native-Am/Alaska Male A: IFL 1 1481 0.1 99.3 F: FOLFOX 0 1481 0.0 99.3 G: IROX 2 1483 0.1 99.4 Female A: IFL 1 1484 0.1 99.5 F: FOLFOX 1 1485 0.1 99.5 G: IROX 0 1485 0.0 99.5 Other Male A: IFL 2 1487 0.1 99.7 F: FOLFOX 2 1489 0.1 99.8 G: IROX 1 1490 0.1 99.9 Female A: IFL 0 1490 0.0 99.9 F: FOLFOX 2 1492 0.1 100.0 G: IROX 0 1492 0.0 100.0 Options for NA handling The various na.options= allow you to include or exclude data with missing values for one or more factor levels in the counts and percentages, as well as show the missing data but exclude it from the cumulative counts and percentages. The default option is to include all combinations with missing values. summary(freqlist(tab.ex, na.options = include )) arm sex mdquality.s Freq cumFreq freqPercent cumPercent A: IFL Male 0 29 29 1.93 1.93 1 214 243 14.28 16.21 NA 34 277 2.27 18.48 Female 0 12 289 0.80 19.28 1 118 407 7.87 27.15 NA 21 428 1.40 28.55 F: FOLFOX Male 0 31 459 2.07 30.62 1 285 744 19.01 49.63 NA 95 839 6.34 55.97 Female 0 21 860 1.40 57.37 1 198 1058 13.21 70.58 NA 61 1119 4.07 74.65 G: IROX Male 0 17 1136 1.13 75.78 1 187 1323 12.47 88.26 NA 24 1347 1.60 89.86 Female 0 14 1361 0.93 90.79 1 121 1482 8.07 98.87 NA 17 1499 1.13 100.00 summary(freqlist(tab.ex, na.options = showexclude )) arm sex mdquality.s Freq cumFreq freqPercent cumPercent A: IFL Male 0 29 29 2.33 2.33 1 214 243 17.16 19.49 NA 34 NA NA NA Female 0 12 255 0.96 20.45 1 118 373 9.46 29.91 NA 21 NA NA NA F: FOLFOX Male 0 31 404 2.49 32.40 1 285 689 22.85 55.25 NA 95 NA NA NA Female 0 21 710 1.68 56.94 1 198 908 15.88 72.81 NA 61 NA NA NA G: IROX Male 0 17 925 1.36 74.18 1 187 1112 15.00 89.17 NA 24 NA NA NA Female 0 14 1126 1.12 90.30 1 121 1247 9.70 100.00 NA 17 NA NA NA summary(freqlist(tab.ex, na.options = remove )) arm sex mdquality.s Freq cumFreq freqPercent cumPercent A: IFL Male 0 29 29 2.33 2.33 1 214 243 17.16 19.49 Female 0 12 255 0.96 20.45 1 118 373 9.46 29.91 F: FOLFOX Male 0 31 404 2.49 32.40 1 285 689 22.85 55.25 Female 0 21 710 1.68 56.94 1 198 908 15.88 72.81 G: IROX Male 0 17 925 1.36 74.18 1 187 1112 15.00 89.17 Female 0 14 1126 1.12 90.30 1 121 1247 9.70 100.00 Frequency counts and percentages subset by factor levels The groupBy= argument internally subsets the data by the specified factor prior to calculating cumulative counts and percentages. By default, when used each subset will print in a separate table. Using the single = TRUE option when printing will collapse the subsetted result into a single table. withby &lt;- freqlist(tab.ex, groupBy = c( arm , sex )) summary(withby) arm sex mdquality.s Freq cumFreq freqPercent cumPercent A: IFL Male 0 29 29 10.47 10.47 1 214 243 77.26 87.73 NA 34 277 12.27 100.00 arm sex mdquality.s Freq cumFreq freqPercent cumPercent A: IFL Female 0 12 12 7.95 7.95 1 118 130 78.15 86.09 NA 21 151 13.91 100.00 arm sex mdquality.s Freq cumFreq freqPercent cumPercent F: FOLFOX Male 0 31 31 7.54 7.54 1 285 316 69.34 76.89 NA 95 411 23.11 100.00 arm sex mdquality.s Freq cumFreq freqPercent cumPercent F: FOLFOX Female 0 21 21 7.50 7.50 1 198 219 70.71 78.21 NA 61 280 21.79 100.00 arm sex mdquality.s Freq cumFreq freqPercent cumPercent G: IROX Male 0 17 17 7.46 7.46 1 187 204 82.02 89.47 NA 24 228 10.53 100.00 arm sex mdquality.s Freq cumFreq freqPercent cumPercent G: IROX Female 0 14 14 9.21 9.21 1 121 135 79.61 88.82 NA 17 152 11.18 100.00 # using the single = TRUE argument will collapse results into a single table for # printing summary(withby, single = TRUE) arm sex mdquality.s Freq cumFreq freqPercent cumPercent A: IFL Male 0 29 29 10.47 10.47 1 214 243 77.26 87.73 NA 34 277 12.27 100.00 Female 0 12 12 7.95 7.95 1 118 130 78.15 86.09 NA 21 151 13.91 100.00 F: FOLFOX Male 0 31 31 7.54 7.54 1 285 316 69.34 76.89 NA 95 411 23.11 100.00 Female 0 21 21 7.50 7.50 1 198 219 70.71 78.21 NA 61 280 21.79 100.00 G: IROX Male 0 17 17 7.46 7.46 1 187 204 82.02 89.47 NA 24 228 10.53 100.00 Female 0 14 14 9.21 9.21 1 121 135 79.61 88.82 NA 17 152 11.18 100.00 Change labels on the fly At this time, the labels can be changed just for the variables (e.g. not the frequency columns). labels(noby) &lt;- c( Arm , Sex , QOL ) summary(noby) Arm Sex QOL Freq cumFreq freqPercent cumPercent A: IFL Male 0 29 29 1.93 1.93 1 214 243 14.28 16.21 NA 34 277 2.27 18.48 Female 0 12 289 0.80 19.28 1 118 407 7.87 27.15 NA 21 428 1.40 28.55 F: FOLFOX Male 0 31 459 2.07 30.62 1 285 744 19.01 49.63 NA 95 839 6.34 55.97 Female 0 21 860 1.40 57.37 1 198 1058 13.21 70.58 NA 61 1119 4.07 74.65 G: IROX Male 0 17 1136 1.13 75.78 1 187 1323 12.47 88.26 NA 24 1347 1.60 89.86 Female 0 14 1361 0.93 90.79 1 121 1482 8.07 98.87 NA 17 1499 1.13 100.00 You can also supply labelTranslations= to summary(). summary(noby, labelTranslations = c( Arm , Sex , QOL )) Arm Sex QOL Freq cumFreq freqPercent cumPercent A: IFL Male 0 29 29 1.93 1.93 1 214 243 14.28 16.21 NA 34 277 2.27 18.48 Female 0 12 289 0.80 19.28 1 118 407 7.87 27.15 NA 21 428 1.40 28.55 F: FOLFOX Male 0 31 459 2.07 30.62 1 285 744 19.01 49.63 NA 95 839 6.34 55.97 Female 0 21 860 1.40 57.37 1 198 1058 13.21 70.58 NA 61 1119 4.07 74.65 G: IROX Male 0 17 1136 1.13 75.78 1 187 1323 12.47 88.26 NA 24 1347 1.60 89.86 Female 0 14 1361 0.93 90.79 1 121 1482 8.07 98.87 NA 17 1499 1.13 100.00 Using xtable() to format and print freqlist() results Fair warning: xtable() has kind of a steep learning curve. These examples are given without explanation, for more advanced users. require(xtable) Loading required package: xtable # set up custom function for xtable text italic &lt;- function(x) { paste0( &lt;i&gt; , x, &lt;/i&gt; ) } xftbl &lt;- xtable(noby[[ freqlist ]], caption = xtable formatted output of freqlist data frame , align = |r|r|r|r|c|c|c|r| ) # change the column names names(xftbl)[1:3] &lt;- c( Arm , Gender , LASA QOL ) print(xftbl, sanitize.colnames.function = italic, include.rownames = FALSE, type = html , comment = FALSE) xtable formatted output of freqlist data frame Arm Gender LASA QOL Freq cumFreq freqPercent cumPercent A: IFL Male 0 29 29 1.93 1.93 A: IFL Male 1 214 243 14.28 16.21 A: IFL Male 34 277 2.27 18.48 A: IFL Female 0 12 289 0.80 19.28 A: IFL Female 1 118 407 7.87 27.15 A: IFL Female 21 428 1.40 28.55 F: FOLFOX Male 0 31 459 2.07 30.62 F: FOLFOX Male 1 285 744 19.01 49.63 F: FOLFOX Male 95 839 6.34 55.97 F: FOLFOX Female 0 21 860 1.40 57.37 F: FOLFOX Female 1 198 1058 13.21 70.58 F: FOLFOX Female 61 1119 4.07 74.65 G: IROX Male 0 17 1136 1.13 75.78 G: IROX Male 1 187 1323 12.47 88.26 G: IROX Male 24 1347 1.60 89.86 G: IROX Female 0 14 1361 0.93 90.79 G: IROX Female 1 121 1482 8.07 98.87 G: IROX Female 17 1499 1.13 100.00 Use freqlist in bookdown Since the backbone of freqlist() is knitr::kable(), tables still render well in bookdown. However, print.summary.freqlist() doesn‚Äôt use the caption= argument of kable(), so some tables may not have a properly numbered caption. To fix this, use the method described on the bookdown site to give the table a tag/ID. summary(freqlist(~sex + age, data = mockstudy), title = (\\\\#tab:mytableby) Caption here ) Appendix: Notes regarding table options in R NAs There are several widely used options for basic tables in R. The table() function in base R is probably the most common; by default it excludes NA values. You can change NA handling in base::table() using the useNA= or exclude= arguments. # base table default removes NAs tab.d1 &lt;- base::table(mockstudy[, c( arm , sex , mdquality.s )], useNA = ifany ) tab.d1 , , mdquality.s = 0 sex arm Male Female A: IFL 29 12 F: FOLFOX 31 21 G: IROX 17 14 , , mdquality.s = 1 sex arm Male Female A: IFL 214 118 F: FOLFOX 285 198 G: IROX 187 121 , , mdquality.s = NA sex arm Male Female A: IFL 34 21 F: FOLFOX 95 61 G: IROX 24 17 xtabs() is similar to table(), but uses a formula-based syntax. However, there is not an option for retaining NAs in the xtabs() function; instead, NAs must be added to each level of the factor where present using the addNA() function, or (in R &gt;= 3.4.0) using the argument addNA = TRUE. # without specifying addNA tab.d2 &lt;- xtabs(formula = ~arm + sex + mdquality.s, data = mockstudy) tab.d2 , , mdquality.s = 0 sex arm Male Female A: IFL 29 12 F: FOLFOX 31 21 G: IROX 17 14 , , mdquality.s = 1 sex arm Male Female A: IFL 214 118 F: FOLFOX 285 198 G: IROX 187 121 # now with addNA tab.d3 &lt;- xtabs(~arm + sex + addNA(mdquality.s), data = mockstudy) tab.d3 , , addNA(mdquality.s) = 0 sex arm Male Female A: IFL 29 12 F: FOLFOX 31 21 G: IROX 17 14 , , addNA(mdquality.s) = 1 sex arm Male Female A: IFL 214 118 F: FOLFOX 285 198 G: IROX 187 121 , , addNA(mdquality.s) = NA sex arm Male Female A: IFL 34 21 F: FOLFOX 95 61 G: IROX 24 17 Since the formula method of freqlist() uses xtabs(), NAs should be treated in the same way. includeNA() can also be helpful here for setting explicit NA values. Table dimname names (dnn) Supplying a data.frame to the table() function without giving columns individually will create a contingency table using all variables in the data.frame. However, if the columns of a data.frame or matrix are supplied separately (i.e., as vectors), column names will not be preserved. # providing variables separately (as vectors) drops column names tab.d4 &lt;- base::table(mockstudy$arm, mockstudy$sex, mockstudy$mdquality.s) tab.d4 , , = 0 Male Female A: IFL 29 12 F: FOLFOX 31 21 G: IROX 17 14 , , = 1 Male Female A: IFL 214 118 F: FOLFOX 285 198 G: IROX 187 121 If desired, you can use the dnn= argument to pass variable names. # add the column name labels back using dnn option in base::table tab.dnn &lt;- base::table(mockstudy$arm, mockstudy$sex, mockstudy$mdquality.s, dnn = c( Arm , Sex , QOL )) tab.dnn , , QOL = 0 Sex Arm Male Female A: IFL 29 12 F: FOLFOX 31 21 G: IROX 17 14 , , QOL = 1 Sex Arm Male Female A: IFL 214 118 F: FOLFOX 285 198 G: IROX 187 121 If using freqlist(), you can provide the labels directly to freqlist() or to summary() using labelTranslations=. ## A Few Notes on Labels https://cran.r-project.org/web/packages/arsenal/vignettes/labels.html A Few Notes on Labels Ethan Heinzen 09 November, 2018 Introduction Examples Set labels in the function call Modify labels after the fact Add labels to a data.frame Introduction The arsenal package relies somewhat heavily on variable labels to make output more ‚Äúpretty‚Äù. A label here is understood to be a single character string with ‚Äúpretty‚Äù text (i.e., not an ‚Äúugly‚Äù variable name). Three of the main arsenal function use labels in their summary() output. There are several ways to set these labels. We‚Äôll use the mockstudy dataset for all examples here: library(arsenal) data(mockstudy) library(magrittr) # for &#39;freqlist&#39; examples tab.ex &lt;- table(mockstudy[, c( arm , sex , mdquality.s )], useNA= ifany ) Examples Set labels in the function call The summary() method for tableby(), modelsum(), and freqlist() objects contains a labelTranslations = argument to specify labels in the function call. Note that the freqlist() function matches labels in order, whereas the other two match labels by name. The labels can be input as a list or a character vector. summary(freqlist(tab.ex), labelTranslations = c( Treatment Arm , Gender , LASA QOL )) Treatment Arm Gender LASA QOL Freq cumFreq freqPercent cumPercent A: IFL Male 0 29 29 1.93 1.93 1 214 243 14.28 16.21 NA 34 277 2.27 18.48 Female 0 12 289 0.80 19.28 1 118 407 7.87 27.15 NA 21 428 1.40 28.55 F: FOLFOX Male 0 31 459 2.07 30.62 1 285 744 19.01 49.63 NA 95 839 6.34 55.97 Female 0 21 860 1.40 57.37 1 198 1058 13.21 70.58 NA 61 1119 4.07 74.65 G: IROX Male 0 17 1136 1.13 75.78 1 187 1323 12.47 88.26 NA 24 1347 1.60 89.86 Female 0 14 1361 0.93 90.79 1 121 1482 8.07 98.87 NA 17 1499 1.13 100.00 summary(tableby(arm ~ sex + age, data = mockstudy), labelTranslations = c(sex = SEX , age = Age, yrs )) A: IFL (N=428) F: FOLFOX (N=691) G: IROX (N=380) Total (N=1499) p value SEX 0.190 Male 277 (64.7%) 411 (59.5%) 228 (60.0%) 916 (61.1%) Female 151 (35.3%) 280 (40.5%) 152 (40.0%) 583 (38.9%) Age, yrs 0.614 Mean (SD) 59.673 (11.365) 60.301 (11.632) 59.763 (11.499) 59.985 (11.519) Range 27.000 - 88.000 19.000 - 88.000 26.000 - 85.000 19.000 - 88.000 summary(modelsum(bmi ~ age, adjust = ~sex, data = mockstudy), labelTranslations = list(sexFemale = Female , age = Age, yrs )) estimate std.error p.value adj.r.squared (Intercept) 26.793 0.766 &lt; 0.001 0.004 Age, yrs 0.012 0.012 0.348 Female -0.718 0.291 0.014 Modify labels after the fact Another option is to add labels after you have created the object. To do this, you can use the form labels(x) &lt;- value or use the pipe-able version, set_labels(). # the non-pipe version; somewhat clunky tmp &lt;- freqlist(tab.ex) labels(tmp) &lt;- c( Treatment Arm , Gender , LASA QOL ) summary(tmp) Treatment Arm Gender LASA QOL Freq cumFreq freqPercent cumPercent A: IFL Male 0 29 29 1.93 1.93 1 214 243 14.28 16.21 NA 34 277 2.27 18.48 Female 0 12 289 0.80 19.28 1 118 407 7.87 27.15 NA 21 428 1.40 28.55 F: FOLFOX Male 0 31 459 2.07 30.62 1 285 744 19.01 49.63 NA 95 839 6.34 55.97 Female 0 21 860 1.40 57.37 1 198 1058 13.21 70.58 NA 61 1119 4.07 74.65 G: IROX Male 0 17 1136 1.13 75.78 1 187 1323 12.47 88.26 NA 24 1347 1.60 89.86 Female 0 14 1361 0.93 90.79 1 121 1482 8.07 98.87 NA 17 1499 1.13 100.00 # piped--much cleaner mockstudy %&gt;% tableby(arm ~ sex + age, data = .) %&gt;% set_labels(c(sex = SEX , age = Age, yrs )) %&gt;% summary() A: IFL (N=428) F: FOLFOX (N=691) G: IROX (N=380) Total (N=1499) p value SEX 0.190 Male 277 (64.7%) 411 (59.5%) 228 (60.0%) 916 (61.1%) Female 151 (35.3%) 280 (40.5%) 152 (40.0%) 583 (38.9%) Age, yrs 0.614 Mean (SD) 59.673 (11.365) 60.301 (11.632) 59.763 (11.499) 59.985 (11.519) Range 27.000 - 88.000 19.000 - 88.000 26.000 - 85.000 19.000 - 88.000 mockstudy %&gt;% modelsum(bmi ~ age, adjust = ~ sex, data = .) %&gt;% set_labels(list(sexFemale = Female , age = Age, yrs )) %&gt;% summary() estimate std.error p.value adj.r.squared (Intercept) 26.793 0.766 &lt; 0.001 0.004 Age, yrs 0.012 0.012 0.348 Female -0.718 0.291 0.014 Add labels to a data.frame tableby() and modelsum() also allow you to have label attributes on the data. Note that by default these attributes usually get dropped upon subsetting, but tableby() and modelsum() use the keep.labels() function to retain them. mockstudy.lab &lt;- keep.labels(mockstudy) You can set attributes one at a time in two ways: attr(mockstudy.lab$sex, label ) &lt;- Sex labels(mockstudy.lab$age) &lt;- Age, yrs ‚Ä¶or all at once: labels(mockstudy.lab) &lt;- list(sex = Sex , age = Age, yrs ) summary(tableby(arm ~ sex + age, data = mockstudy.lab)) A: IFL (N=428) F: FOLFOX (N=691) G: IROX (N=380) Total (N=1499) p value Sex 0.190 Male 277 (64.7%) 411 (59.5%) 228 (60.0%) 916 (61.1%) Female 151 (35.3%) 280 (40.5%) 152 (40.0%) 583 (38.9%) Age, yrs 0.614 Mean (SD) 59.673 (11.365) 60.301 (11.632) 59.763 (11.499) 59.985 (11.519) Range 27.000 - 88.000 19.000 - 88.000 26.000 - 85.000 19.000 - 88.000 You can pipe this, too. mockstudy %&gt;% set_labels(list(sex = SEX , age = Age, yrs )) %&gt;% modelsum(bmi ~ age, adjust = ~ sex, data = .) %&gt;% summary() estimate std.error p.value adj.r.squared (Intercept) 26.793 0.766 &lt; 0.001 0.004 Age, yrs 0.012 0.012 0.348 SEX Female -0.718 0.291 0.014 To extract labels from a data.frame, simply use the labels() function: labels(mockstudy.lab) ## $case ## NULL ## ## $age ## [1] Age, yrs ## ## $arm ## [1] Treatment Arm ## ## $sex ## [1] Sex ## ## $race ## [1] Race ## ## $fu.time ## NULL ## ## $fu.stat ## NULL ## ## $ps ## NULL ## ## $hgb ## NULL ## ## $bmi ## [1] Body Mass Index (kg/m^2) ## ## $alk.phos ## NULL ## ## $ast ## NULL ## ## $mdquality.s ## NULL ## ## $age.ord ## NULL ## The modelsum function https://cran.r-project.org/web/packages/arsenal/vignettes/modelsum.html The modelsum function Beth Atkinson, Ethan Heinzen, Pat Votruba, Jason Sinnwell, Shannon McDonnell and Greg Dougherty 09 November, 2018 Introduction Simple Example Pretty text version of table Pretty Rmarkdown version of table Data frame version of table Add an adjustor to the model Models for each endpoint type Gaussian Fit and summarize linear regression model Extract data using the broom package Create a summary table using modelsum Binomial Fit and summarize logistic regression model Extract data using broom package Create a summary table using modelsum Survival Fit and summarize a Cox regression model Extract data using broom package Create a summary table using modelsum Poisson Example 1: fit and summarize a Poisson regression model Extract data using broom package Create a summary table using modelsum Example 2: fit and summarize a Poisson regression model Extract data using broom package Create a summary table using modelsum Additional Examples 1. Change summary statistics globally 2. Add labels to independent variables 3. Don‚Äôt show intercept values 4. Don‚Äôt show results for adjustment variables 5. Summarize multiple variables without typing them out 6. Subset the dataset used in the analysis 7. Create combinations of variables on the fly 8. Transform variables on the fly 9. Change the ordering of the variables or delete a variable 10. Merge two modelsum objects together 11. Add a title to the table 12. Modify how missing values are treated 13. Modify the number of digits used 14. Use case-weights in the models 15. Use modelsum within an Sweave document 16. Export modelsum results to a .CSV file 17. Write modelsum object to a separate Word or HTML file 18. Use modelsum in R Shiny 23. Use modelsum in bookdown Available Function Options Summary statistics modelsum.control settings summary.modelsum settings Introduction Very often we are asked to summarize model results from multiple fits into a nice table. The endpoint might be of different types (e.g., survival, case/control, continuous) and there may be several independent variables that we want to examine univariately or adjusted for certain variables such as age and sex. Locally at Mayo, the SAS macros %modelsum, %glmuniv, and %logisuni were written to create such summary tables. With the increasing interest in R, we have developed the function modelsum to create similar tables within the R environment. In developing the modelsum function, the goal was to bring the best features of these macros into an R function. However, the task was not simply to duplicate all the functionality, but rather to make use of R‚Äôs strengths (modeling, method dispersion, flexibility in function definition and output format) and make a tool that fits the needs of R users. Additionally, the results needed to fit within the general reproducible research framework so the tables could be displayed within an R markdown report. This report provides step-by-step directions for using the functions associated with modelsum. All functions presented here are available within the arsenal package. An assumption is made that users are somewhat familiar with R markdown documents. For those who are new to the topic, a good initial resource is available at rmarkdown.rstudio.com. Simple Example The first step when using the modelsum function is to load the arsenal package. All the examples in this report use a dataset called mockstudy made available by Paul Novotny which includes a variety of types of variables (character, numeric, factor, ordered factor, survival) to use as examples. &gt; require(arsenal) &gt; data(mockstudy) # load data &gt; dim(mockstudy) # look at how many subjects and variables are in the dataset [1] 1499 14 &gt; # help(mockstudy) # learn more about the dataset and variables &gt; str(mockstudy) # quick look at the data &#39;data.frame&#39;: 1499 obs. of 14 variables: $ case : int 110754 99706 105271 105001 112263 86205 99508 90158 88989 90515 ... $ age : atomic 67 74 50 71 69 56 50 57 51 63 ... ..- attr(*, label )= chr Age in Years $ arm : atomic F: FOLFOX A: IFL A: IFL G: IROX ... ..- attr(*, label )= chr Treatment Arm $ sex : Factor w/ 2 levels Male , Female : 1 2 2 2 2 1 1 1 2 1 ... $ race : atomic Caucasian Caucasian Caucasian Caucasian ... ..- attr(*, label )= chr Race $ fu.time : int 922 270 175 128 233 120 369 421 387 363 ... $ fu.stat : int 2 2 2 2 2 2 2 2 2 2 ... $ ps : int 0 1 1 1 0 0 0 0 1 1 ... $ hgb : num 11.5 10.7 11.1 12.6 13 10.2 13.3 12.1 13.8 12.1 ... $ bmi : atomic 25.1 19.5 NA 29.4 26.4 ... ..- attr(*, label )= chr Body Mass Index (kg/m^2) $ alk.phos : int 160 290 700 771 350 569 162 152 231 492 ... $ ast : int 35 52 100 68 35 27 16 12 25 18 ... $ mdquality.s: int NA 1 1 1 NA 1 1 1 1 1 ... $ age.ord : Ord.factor w/ 8 levels 10-19 &lt; 20-29 &lt;..: 6 7 4 7 6 5 4 5 5 6 ... To create a simple linear regression table (the default), use a formula statement to specify the variables that you want summarized. The example below predicts BMI with the variables sex and age. &gt; tab1 &lt;- modelsum(bmi ~ sex + age, data=mockstudy) If you want to take a quick look at the table, you can use summary on your modelsum object and the table will print out as text in your R console window. If you use summary without any options you will see a number of &amp;nbsp; statements which translates to ‚Äúspace‚Äù in HTML. Pretty text version of table If you want a nicer version in your console window then adding the text=TRUE option. &gt; summary(tab1, text=TRUE) | |estimate |std.error |p.value |adj.r.squared | |:|:--|:|:-|:-| |(Intercept) |27.491 |0.181 |&lt; 0.001 |0.004 | |sex Female |-0.731 |0.290 |0.012 | | |(Intercept) |26.424 |0.752 |&lt; 0.001 |0.000 | |Age in Years |0.013 |0.012 |0.290 | | Pretty Rmarkdown version of table In order for the report to look nice within an R markdown (knitr) report, you just need to specify results= asis when creating the r chunk. This changes the layout slightly (compresses it) and bolds the variable names. &gt; summary(tab1) estimate std.error p.value adj.r.squared (Intercept) 27.491 0.181 &lt; 0.001 0.004 sex Female -0.731 0.290 0.012 (Intercept) 26.424 0.752 &lt; 0.001 0.000 Age in Years 0.013 0.012 0.290 Data frame version of table If you want a data.frame version, simply use as.data.frame. &gt; as.data.frame(tab1) model term label term.type estimate std.error 1 1 (Intercept) (Intercept) Intercept 27.49147713 0.18134740 2 1 sexFemale sex Female Term -0.73105055 0.29032223 3 2 (Intercept) (Intercept) Intercept 26.42372272 0.75211474 4 2 age Age in Years Term 0.01304859 0.01231653 p.value adj.r.squared 1 0.000000e+00 3.632258e-03 2 1.190605e-02 3.632258e-03 3 1.279109e-196 8.354809e-05 4 2.895753e-01 8.354809e-05 Add an adjustor to the model The argument adjust allows the user to indicate that all the variables should be adjusted for these terms. &gt; tab2 &lt;- modelsum(alk.phos ~ arm + ps + hgb, adjust= ~age + sex, data=mockstudy) &gt; summary(tab2) estimate std.error p.value adj.r.squared Nmiss (Intercept) 175.548 20.587 &lt; 0.001 -0.001 0 Treatment Arm F: FOLFOX -13.701 8.730 0.117 Treatment Arm G: IROX -2.245 9.860 0.820 Age in Years -0.017 0.319 0.956 sex Female 3.016 7.521 0.688 (Intercept) 148.391 19.585 &lt; 0.001 0.045 266 ps 46.721 5.987 &lt; 0.001 Age in Years -0.084 0.311 0.787 sex Female 1.169 7.343 0.874 (Intercept) 336.554 32.239 &lt; 0.001 0.031 266 hgb -13.845 2.137 &lt; 0.001 Age in Years 0.095 0.314 0.763 sex Female -5.980 7.516 0.426 Models for each endpoint type To make sure the correct model is run you need to specify ‚Äúfamily‚Äù. The options available right now are : gaussian, binomial, survival, and poisson. If there is enough interest, additional models can be added. Gaussian Fit and summarize linear regression model Look at whether there is any evidence that AlkPhos values vary by study arm after adjusting for sex and age (assuming a linear age relationship). &gt; fit &lt;- lm(alk.phos ~ arm + age + sex, data=mockstudy) &gt; summary(fit) Call: lm(formula = alk.phos ~ arm + age + sex, data = mockstudy) Residuals: Min 1Q Median 3Q Max -168.80 -81.45 -47.17 37.39 853.56 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 175.54808 20.58665 8.527 &lt;2e-16 *** armF: FOLFOX -13.70062 8.72963 -1.569 0.117 armG: IROX -2.24498 9.86004 -0.228 0.820 age -0.01741 0.31878 -0.055 0.956 sexFemale 3.01598 7.52097 0.401 0.688 Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 128.5 on 1228 degrees of freedom (266 observations deleted due to missingness) Multiple R-squared: 0.002552, Adjusted R-squared: -0.0006969 F-statistic: 0.7855 on 4 and 1228 DF, p-value: 0.5346 &gt; plot(fit) The results suggest that the endpoint may need to be transformed. Calculating the Box-Cox transformation suggests a log transformation. &gt; require(MASS) &gt; boxcox(fit) &gt; fit2 &lt;- lm(log(alk.phos) ~ arm + age + sex, data=mockstudy) &gt; summary(fit2) Call: lm(formula = log(alk.phos) ~ arm + age + sex, data = mockstudy) Residuals: Min 1Q Median 3Q Max -3.0098 -0.4470 -0.1065 0.4205 2.0620 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 4.9692474 0.1025239 48.469 &lt;2e-16 *** armF: FOLFOX -0.0766798 0.0434746 -1.764 0.078 . armG: IROX -0.0192828 0.0491041 -0.393 0.695 age -0.0004058 0.0015876 -0.256 0.798 sexFemale 0.0179253 0.0374553 0.479 0.632 Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 0.6401 on 1228 degrees of freedom (266 observations deleted due to missingness) Multiple R-squared: 0.003121, Adjusted R-squared: -0.0001258 F-statistic: 0.9613 on 4 and 1228 DF, p-value: 0.4278 &gt; plot(fit2) Finally, look to see whether there there is a non-linear relationship with age. &gt; require(gam) &gt; fit3 &lt;- lm(log(alk.phos) ~ arm + ns(age, df=2) + sex, data=mockstudy) &gt; &gt; # test whether there is a difference between models &gt; stats::anova(fit2,fit3) Analysis of Variance Table Model 1: log(alk.phos) ~ arm + age + sex Model 2: log(alk.phos) ~ arm + ns(age, df = 2) + sex Res.Df RSS Df Sum of Sq F Pr(&gt;F) 1 1228 503.19 2 1227 502.07 1 1.1137 2.7218 0.09924 . Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 &gt; &gt; # look at functional form of age &gt; termplot(fit3, term=2, se=T, rug=T) In this instance it looks like there isn‚Äôt enough evidence to say that the relationship is non-linear. Extract data using the broom package The broom package makes it easy to extract information from the fit. &gt; tmp &lt;- tidy(fit3) # coefficients, p-values &gt; class(tmp) [1] tbl_df tbl data.frame &gt; tmp # A tibble: 6 x 5 term estimate std.error statistic p.value &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 (Intercept) 4.76 0.141 33.8 1.93e-177 2 armF: FOLFOX -0.0767 0.0434 -1.77 7.78e- 2 3 armG: IROX -0.0195 0.0491 -0.396 6.92e- 1 4 ns(age, df = 2)1 0.330 0.260 1.27 2.04e- 1 5 ns(age, df = 2)2 -0.101 0.0935 -1.08 2.82e- 1 6 sexFemale 0.0183 0.0374 0.489 6.25e- 1 &gt; &gt; glance(fit3) # A tibble: 1 x 11 r.squared adj.r.squared sigma statistic p.value df logLik AIC BIC * &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 0.00533 0.00127 0.640 1.31 0.255 6 -1196. 2405. 2441. # ... with 2 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt; Create a summary table using modelsum &gt; ms.logy &lt;- modelsum(log(alk.phos) ~ arm + ps + hgb, data=mockstudy, adjust= ~age + sex, + family=gaussian, + gaussian.stats=c( estimate , CI.lower.estimate , CI.upper.estimate , p.value )) &gt; summary(ms.logy) estimate CI.lower.estimate CI.upper.estimate p.value (Intercept) 4.969 4.768 5.170 &lt; 0.001 Treatment Arm F: FOLFOX -0.077 -0.162 0.009 0.078 Treatment Arm G: IROX -0.019 -0.116 0.077 0.695 Age in Years -0.000 -0.004 0.003 0.798 sex Female 0.018 -0.056 0.091 0.632 (Intercept) 4.832 4.640 5.023 &lt; 0.001 ps 0.226 0.167 0.284 &lt; 0.001 Age in Years -0.001 -0.004 0.002 0.636 sex Female 0.009 -0.063 0.081 0.814 (Intercept) 5.765 5.450 6.080 &lt; 0.001 hgb -0.069 -0.090 -0.048 &lt; 0.001 Age in Years 0.000 -0.003 0.003 0.925 sex Female -0.027 -0.101 0.046 0.468 Binomial Fit and summarize logistic regression model &gt; boxplot(age ~ mdquality.s, data=mockstudy, ylab=attr(mockstudy$age,&#39;label&#39;), xlab=&#39;mdquality.s&#39;) &gt; &gt; fit &lt;- glm(mdquality.s ~ age + sex, data=mockstudy, family=binomial) &gt; summary(fit) Call: glm(formula = mdquality.s ~ age + sex, family = binomial, data = mockstudy) Deviance Residuals: Min 1Q Median 3Q Max -2.1832 0.4500 0.4569 0.4626 0.4756 Coefficients: Estimate Std. Error z value Pr(&gt;|z|) (Intercept) 2.329442 0.514684 4.526 6.01e-06 *** age -0.002353 0.008256 -0.285 0.776 sexFemale 0.039227 0.195330 0.201 0.841 Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 (Dispersion parameter for binomial family taken to be 1) Null deviance: 807.68 on 1246 degrees of freedom Residual deviance: 807.55 on 1244 degrees of freedom (252 observations deleted due to missingness) AIC: 813.55 Number of Fisher Scoring iterations: 4 &gt; &gt; # create Odd&#39;s ratio w/ confidence intervals &gt; tmp &lt;- data.frame(summary(fit)$coef) &gt; tmp Estimate Std..Error z.value Pr...z.. (Intercept) 2.329441734 0.514683688 4.5259677 6.011977e-06 age -0.002353404 0.008255814 -0.2850602 7.755980e-01 sexFemale 0.039227292 0.195330166 0.2008256 8.408350e-01 &gt; &gt; tmp$OR &lt;- round(exp(tmp[,1]),2) &gt; tmp$lower.CI &lt;- round(exp(tmp[,1] - 1.96* tmp[,2]),2) &gt; tmp$upper.CI &lt;- round(exp(tmp[,1] + 1.96* tmp[,2]),2) &gt; names(tmp)[4] &lt;- &#39;P-value&#39; &gt; &gt; kable(tmp[,c(&#39;OR&#39;,&#39;lower.CI&#39;,&#39;upper.CI&#39;,&#39;P-value&#39;)]) OR lower.CI upper.CI P-value (Intercept) 10.27 3.75 28.17 0.000006 age 1.00 0.98 1.01 0.775598 sexFemale 1.04 0.71 1.53 0.840835 &gt; &gt; # Assess the predictive ability of the model &gt; &gt; # code using the pROC package &gt; require(pROC) &gt; pred &lt;- predict(fit, type=&#39;response&#39;) &gt; tmp &lt;- pROC::roc(mockstudy$mdquality.s[!is.na(mockstudy$mdquality.s)]~ pred, plot=TRUE, percent=TRUE) &gt; tmp$auc Area under the curve: 50.69% Extract data using broom package The broom package makes it easy to extract information from the fit. &gt; tidy(fit, exp=T, conf.int=T) # coefficients, p-values, conf.intervals # A tibble: 3 x 7 term estimate std.error statistic p.value conf.low conf.high &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 (Intercept) 10.3 0.515 4.53 0.00000601 3.83 28.9 2 age 0.998 0.00826 -0.285 0.776 0.981 1.01 3 sexFemale 1.04 0.195 0.201 0.841 0.712 1.53 &gt; &gt; glance(fit) # model summary statistics # A tibble: 1 x 7 null.deviance df.null logLik AIC BIC deviance df.residual &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; 1 808. 1246 -404. 814. 829. 808. 1244 Create a summary table using modelsum &gt; summary(modelsum(mdquality.s ~ age + bmi, data=mockstudy, adjust=~sex, family=binomial)) OR CI.lower.OR CI.upper.OR p.value concordance Nmiss (Intercept) 10.272 3.831 28.876 &lt; 0.001 0.507 0 Age in Years 0.998 0.981 1.014 0.776 sex Female 1.040 0.712 1.534 0.841 (Intercept) 4.814 1.709 13.221 0.003 0.550 33 Body Mass Index (kg/m^2) 1.023 0.987 1.063 0.220 sex Female 1.053 0.717 1.561 0.794 &gt; &gt; fitall &lt;- modelsum(mdquality.s ~ age, data=mockstudy, family=binomial, + binomial.stats=c( Nmiss2 , OR , p.value )) &gt; summary(fitall) OR p.value Nmiss2 (Intercept) 10.493 &lt; 0.001 0 Age in Years 0.998 0.766 Survival Fit and summarize a Cox regression model &gt; require(survival) Loading required package: survival Attaching package: &#39;survival&#39; The following object is masked from &#39;package:rpart&#39;: solder &gt; &gt; # multivariable model with all 3 terms &gt; fit &lt;- coxph(Surv(fu.time, fu.stat) ~ age + sex + arm, data=mockstudy) &gt; summary(fit) Call: coxph(formula = Surv(fu.time, fu.stat) ~ age + sex + arm, data = mockstudy) n= 1499, number of events= 1356 coef exp(coef) se(coef) z Pr(&gt;|z|) age 0.004600 1.004611 0.002501 1.839 0.0659 . sexFemale 0.039893 1.040699 0.056039 0.712 0.4765 armF: FOLFOX -0.454650 0.634670 0.064878 -7.008 2.42e-12 *** armG: IROX -0.140785 0.868676 0.072760 -1.935 0.0530 . Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 exp(coef) exp(-coef) lower .95 upper .95 age 1.0046 0.9954 0.9997 1.0095 sexFemale 1.0407 0.9609 0.9324 1.1615 armF: FOLFOX 0.6347 1.5756 0.5589 0.7207 armG: IROX 0.8687 1.1512 0.7532 1.0018 Concordance= 0.563 (se = 0.009 ) Rsquare= 0.037 (max possible= 1 ) Likelihood ratio test= 56.21 on 4 df, p=2e-11 Wald test = 56.26 on 4 df, p=2e-11 Score (logrank) test = 56.96 on 4 df, p=1e-11 &gt; &gt; # check proportional hazards assumption &gt; fit.z &lt;- cox.zph(fit) &gt; fit.z rho chisq p age -0.0311 1.46 0.226 sexFemale -0.0325 1.44 0.230 armF: FOLFOX 0.0343 1.61 0.205 armG: IROX 0.0337 1.54 0.214 GLOBAL NA 4.59 0.332 &gt; plot(fit.z[1], resid=FALSE) # makes for a cleaner picture in this case &gt; abline(h=coef(fit)[1], col=&#39;red&#39;) &gt; &gt; # check functional form for age using pspline (penalized spline) &gt; # results are returned for the linear and non-linear components &gt; fit2 &lt;- coxph(Surv(fu.time, fu.stat) ~ pspline(age) + sex + arm, data=mockstudy) &gt; fit2 Call: coxph(formula = Surv(fu.time, fu.stat) ~ pspline(age) + sex + arm, data = mockstudy) coef se(coef) se2 Chisq DF p pspline(age), linear 0.00443 0.00237 0.00237 3.48989 1.00 0.0617 pspline(age), nonlin 13.11270 3.08 0.0047 sexFemale 0.03993 0.05610 0.05607 0.50663 1.00 0.4766 armF: FOLFOX -0.46240 0.06494 0.06493 50.69608 1.00 1.1e-12 armG: IROX -0.15243 0.07301 0.07299 4.35876 1.00 0.0368 Iterations: 6 outer, 16 Newton-Raphson Theta= 0.954 Degrees of freedom for terms= 4.1 1.0 2.0 Likelihood ratio test=70.1 on 7.08 df, p=2e-12 n= 1499, number of events= 1356 &gt; &gt; # plot smoothed age to visualize why significant &gt; termplot(fit2, se=T, terms=1) &gt; abline(h=0) &gt; &gt; # The c-statistic comes out in the summary of the fit &gt; summary(fit2)$concordance C se(C) 0.5684325 0.5684325 &gt; &gt; # It can also be calculated using the survConcordance function &gt; survConcordance(Surv(fu.time, fu.stat) ~ predict(fit2), data=mockstudy) Call: survConcordance(formula = Surv(fu.time, fu.stat) ~ predict(fit2), data = mockstudy) n= 1499 Concordance= 0.5684325 se= 0.008779125 concordant discordant tied.risk tied.time std(c-d) 620221.00 470282.00 5021.00 766.00 19235.49 Extract data using broom package The broom package makes it easy to extract information from the fit. &gt; tidy(fit) # coefficients, p-values # A tibble: 4 x 7 term estimate std.error statistic p.value conf.low conf.high &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 age 0.00460 0.00250 1.84 6.59e- 2 -0.000302 0.00950 2 sexFemale 0.0399 0.0560 0.712 4.77e- 1 -0.0699 0.150 3 armF: FOLFOX -0.455 0.0649 -7.01 2.42e-12 -0.582 -0.327 4 armG: IROX -0.141 0.0728 -1.93 5.30e- 2 -0.283 0.00182 &gt; &gt; glance(fit) # model summary statistics # A tibble: 1 x 15 n nevent statistic.log p.value.log statistic.sc p.value.sc &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 1499 1356 56.2 1.81e-11 57.0 1.26e-11 # ... with 9 more variables: statistic.wald &lt;dbl&gt;, p.value.wald &lt;dbl&gt;, # r.squared &lt;dbl&gt;, r.squared.max &lt;dbl&gt;, concordance &lt;dbl&gt;, # std.error.concordance &lt;dbl&gt;, logLik &lt;dbl&gt;, AIC &lt;dbl&gt;, BIC &lt;dbl&gt; Create a summary table using modelsum &gt; ##Note: You must use quotes when specifying family= survival &gt; ## family=survival will not work &gt; summary(modelsum(Surv(fu.time, fu.stat) ~ arm, + adjust=~age + sex, data=mockstudy, family= survival )) HR CI.lower.HR CI.upper.HR p.value concordance Treatment Arm F: FOLFOX 0.635 0.559 0.721 &lt; 0.001 0.563 Treatment Arm G: IROX 0.869 0.753 1.002 0.053 Age in Years 1.005 1.000 1.010 0.066 sex Female 1.041 0.932 1.162 0.477 &gt; &gt; ##Note: the pspline term is not working yet &gt; #summary(modelsum(Surv(fu.time, fu.stat) ~ arm, &gt; # adjust=~pspline(age) + sex, data=mockstudy, family=&#39;survival&#39;)) Poisson Poisson regression is useful when predicting an outcome variable representing counts. It can also be useful when looking at survival data. Cox models and Poisson models are very closely related and survival data can be summarized using Poisson regression. If you have overdispersion (see if the residual deviance is much larger than degrees of freedom), you may want to use quasipoisson() instead of poisson(). Some of these diagnostics need to be done outside of modelsum. Example 1: fit and summarize a Poisson regression model For the first example, use the solder dataset available in the rpart package. The endpoint skips has a definite Poisson look. &gt; require(rpart) ##just to get access to solder dataset &gt; data(solder) &gt; hist(solder$skips) &gt; &gt; fit &lt;- glm(skips ~ Opening + Solder + Mask , data=solder, family=poisson) &gt; stats::anova(fit, test=&#39;Chi&#39;) Analysis of Deviance Table Model: poisson, link: log Response: skips Terms added sequentially (first to last) Df Deviance Resid. Df Resid. Dev Pr(&gt;Chi) NULL 899 8788.2 Opening 2 2920.5 897 5867.7 &lt; 2.2e-16 *** Solder 1 1168.4 896 4699.3 &lt; 2.2e-16 *** Mask 4 2015.7 892 2683.7 &lt; 2.2e-16 *** Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 &gt; summary(fit) Call: glm(formula = skips ~ Opening + Solder + Mask, family = poisson, data = solder) Deviance Residuals: Min 1Q Median 3Q Max -6.1251 -1.4720 -0.7826 0.5986 6.6031 Coefficients: Estimate Std. Error z value Pr(&gt;|z|) (Intercept) -1.12220 0.07742 -14.50 &lt; 2e-16 *** OpeningM 0.57161 0.05707 10.02 &lt; 2e-16 *** OpeningS 1.81475 0.05044 35.98 &lt; 2e-16 *** SolderThin 0.84682 0.03327 25.45 &lt; 2e-16 *** MaskA3 0.51315 0.07098 7.23 4.83e-13 *** MaskA6 1.81103 0.06609 27.40 &lt; 2e-16 *** MaskB3 1.20225 0.06697 17.95 &lt; 2e-16 *** MaskB6 1.86648 0.06310 29.58 &lt; 2e-16 *** Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 (Dispersion parameter for poisson family taken to be 1) Null deviance: 8788.2 on 899 degrees of freedom Residual deviance: 2683.7 on 892 degrees of freedom AIC: 4802.2 Number of Fisher Scoring iterations: 5 Overdispersion is when the Residual deviance is larger than the degrees of freedom. This can be tested, approximately using the following code. The goal is to have a p-value that is &gt;0.05. &gt; 1-pchisq(fit$deviance, fit$df.residual) [1] 0 One possible solution is to use the quasipoisson family instead of the poisson family. This adjusts for the overdispersion. &gt; fit2 &lt;- glm(skips ~ Opening + Solder + Mask, data=solder, family=quasipoisson) &gt; summary(fit2) Call: glm(formula = skips ~ Opening + Solder + Mask, family = quasipoisson, data = solder) Deviance Residuals: Min 1Q Median 3Q Max -6.1251 -1.4720 -0.7826 0.5986 6.6031 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) -1.12220 0.13483 -8.323 3.19e-16 *** OpeningM 0.57161 0.09939 5.751 1.22e-08 *** OpeningS 1.81475 0.08784 20.660 &lt; 2e-16 *** SolderThin 0.84682 0.05794 14.615 &lt; 2e-16 *** MaskA3 0.51315 0.12361 4.151 3.62e-05 *** MaskA6 1.81103 0.11510 15.735 &lt; 2e-16 *** MaskB3 1.20225 0.11663 10.308 &lt; 2e-16 *** MaskB6 1.86648 0.10989 16.984 &lt; 2e-16 *** Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 (Dispersion parameter for quasipoisson family taken to be 3.033198) Null deviance: 8788.2 on 899 degrees of freedom Residual deviance: 2683.7 on 892 degrees of freedom AIC: NA Number of Fisher Scoring iterations: 5 Extract data using broom package The broom package makes it easy to extract information from the fit. &gt; tidy(fit) # coefficients, p-values # A tibble: 8 x 5 term estimate std.error statistic p.value &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 (Intercept) -1.12 0.0774 -14.5 1.29e- 47 2 OpeningM 0.572 0.0571 10.0 1.29e- 23 3 OpeningS 1.81 0.0504 36.0 1.66e-283 4 SolderThin 0.847 0.0333 25.5 6.47e-143 5 MaskA3 0.513 0.0710 7.23 4.83e- 13 6 MaskA6 1.81 0.0661 27.4 2.45e-165 7 MaskB3 1.20 0.0670 18.0 4.55e- 72 8 MaskB6 1.87 0.0631 29.6 2.71e-192 &gt; &gt; glance(fit) # model summary statistics # A tibble: 1 x 7 null.deviance df.null logLik AIC BIC deviance df.residual &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; 1 8788. 899 -2393. 4802. 4841. 2684. 892 Create a summary table using modelsum &gt; summary(modelsum(skips~Opening + Solder + Mask, data=solder, family= quasipoisson )) RR CI.lower.RR CI.upper.RR p.value (Intercept) 1.533 1.179 1.952 &lt; 0.001 Opening M 2.328 1.733 3.167 &lt; 0.001 Opening S 7.491 5.780 9.888 &lt; 0.001 (Intercept) 2.904 2.423 3.446 &lt; 0.001 Solder Thin 2.808 2.295 3.458 &lt; 0.001 (Intercept) 1.611 1.135 2.204 0.005 Mask A3 1.469 0.995 2.214 0.059 Mask A6 8.331 5.839 12.222 &lt; 0.001 Mask B3 3.328 2.309 4.920 &lt; 0.001 Mask B6 6.466 4.598 9.378 &lt; 0.001 &gt; summary(modelsum(skips~Opening + Solder + Mask, data=solder, family= poisson )) RR CI.lower.RR CI.upper.RR p.value (Intercept) 1.533 1.397 1.678 &lt; 0.001 Opening M 2.328 2.089 2.599 &lt; 0.001 Opening S 7.491 6.805 8.267 &lt; 0.001 (Intercept) 2.904 2.750 3.065 &lt; 0.001 Solder Thin 2.808 2.637 2.992 &lt; 0.001 (Intercept) 1.611 1.433 1.804 &lt; 0.001 Mask A3 1.469 1.280 1.690 &lt; 0.001 Mask A6 8.331 7.341 9.487 &lt; 0.001 Mask B3 3.328 2.923 3.800 &lt; 0.001 Mask B6 6.466 5.724 7.331 &lt; 0.001 Example 2: fit and summarize a Poisson regression model This second example uses the survival endpoint available in the mockstudy dataset. There is a close relationship between survival and Poisson models, and often it is easier to fit the model using Poisson regression, especially if you want to present absolute risk. &gt; # add .01 to the follow-up time (.01*1 day) in order to keep everyone in the analysis &gt; fit &lt;- glm(fu.stat ~ offset(log(fu.time+.01)) + age + sex + arm, data=mockstudy, family=poisson) &gt; summary(fit) Call: glm(formula = fu.stat ~ offset(log(fu.time + 0.01)) + age + sex + arm, family = poisson, data = mockstudy) Deviance Residuals: Min 1Q Median 3Q Max -3.1188 -0.4041 0.3242 0.9727 4.3588 Coefficients: Estimate Std. Error z value Pr(&gt;|z|) (Intercept) -5.875627 0.108984 -53.913 &lt; 2e-16 *** age 0.003724 0.001705 2.184 0.0290 * sexFemale 0.027321 0.038575 0.708 0.4788 armF: FOLFOX -0.335141 0.044600 -7.514 5.72e-14 *** armG: IROX -0.107776 0.050643 -2.128 0.0333 * Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 (Dispersion parameter for poisson family taken to be 1) Null deviance: 2113.5 on 1498 degrees of freedom Residual deviance: 2048.0 on 1494 degrees of freedom AIC: 5888.2 Number of Fisher Scoring iterations: 5 &gt; 1-pchisq(fit$deviance, fit$df.residual) [1] 0 &gt; &gt; coef(coxph(Surv(fu.time,fu.stat) ~ age + sex + arm, data=mockstudy)) age sexFemale armF: FOLFOX armG: IROX 0.004600011 0.039892735 -0.454650445 -0.140784996 &gt; coef(fit)[-1] age sexFemale armF: FOLFOX armG: IROX 0.003723763 0.027320917 -0.335141090 -0.107775577 &gt; &gt; # results from the Poisson model can then be described as risk ratios (similar to the hazard ratio) &gt; exp(coef(fit)[-1]) age sexFemale armF: FOLFOX armG: IROX 1.0037307 1.0276976 0.7152372 0.8978291 &gt; &gt; # As before, we can model the dispersion which alters the standard error &gt; fit2 &lt;- glm(fu.stat ~ offset(log(fu.time+.01)) + age + sex + arm, + data=mockstudy, family=quasipoisson) &gt; summary(fit2) Call: glm(formula = fu.stat ~ offset(log(fu.time + 0.01)) + age + sex + arm, family = quasipoisson, data = mockstudy) Deviance Residuals: Min 1Q Median 3Q Max -3.1188 -0.4041 0.3242 0.9727 4.3588 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) -5.875627 0.566666 -10.369 &lt;2e-16 *** age 0.003724 0.008867 0.420 0.675 sexFemale 0.027321 0.200572 0.136 0.892 armF: FOLFOX -0.335141 0.231899 -1.445 0.149 armG: IROX -0.107776 0.263318 -0.409 0.682 Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 (Dispersion parameter for quasipoisson family taken to be 27.03493) Null deviance: 2113.5 on 1498 degrees of freedom Residual deviance: 2048.0 on 1494 degrees of freedom AIC: NA Number of Fisher Scoring iterations: 5 Extract data using broom package The broom package makes it easy to extract information from the fit. &gt; tidy(fit) ##coefficients, p-values # A tibble: 5 x 5 term estimate std.error statistic p.value &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 (Intercept) -5.88 0.109 -53.9 0. 2 age 0.00372 0.00171 2.18 2.90e- 2 3 sexFemale 0.0273 0.0386 0.708 4.79e- 1 4 armF: FOLFOX -0.335 0.0446 -7.51 5.72e-14 5 armG: IROX -0.108 0.0506 -2.13 3.33e- 2 &gt; &gt; glance(fit) ##model summary statistics # A tibble: 1 x 7 null.deviance df.null logLik AIC BIC deviance df.residual &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; 1 2114. 1498 -2939. 5888. 5915. 2048. 1494 Create a summary table using modelsum Remember that the result from modelsum is different from the fit above. The modelsum summary shows the results for age + offset(log(fu.time+.01)) then sex + offset(log(fu.time+.01)) instead of age + sex + arm + offset(log(fu.time+.01)). &gt; summary(modelsum(fu.stat ~ age, adjust=~offset(log(fu.time+.01))+ sex + arm, + data=mockstudy, family=poisson)) RR CI.lower.RR CI.upper.RR p.value (Intercept) 0.003 0.002 0.003 &lt; 0.001 Age in Years 1.004 1.000 1.007 0.029 sexFemale 1.028 0.953 1.108 0.479 armF: FOLFOX 0.715 0.656 0.781 &lt; 0.001 armG: IROX 0.898 0.813 0.991 0.033 Additional Examples Here are multiple examples showing how to use some of the different options. 1. Change summary statistics globally There are standard settings for each type of model regarding what information is summarized in the table. This behavior can be modified using the modelsum.control function. In fact, you can save your standard settings and use that for future tables. &gt; mycontrols &lt;- modelsum.control(gaussian.stats=c( estimate , std.error , adj.r.squared , Nmiss ), + show.adjust=FALSE, show.intercept=FALSE) &gt; tab2 &lt;- modelsum(bmi ~ age, adjust=~sex, data=mockstudy, control=mycontrols) &gt; summary(tab2) estimate std.error adj.r.squared Age in Years 0.012 0.012 0.004 You can also change these settings directly in the modelsum call. &gt; tab3 &lt;- modelsum(bmi ~ age, adjust=~sex, data=mockstudy, + gaussian.stats=c( estimate , std.error , adj.r.squared , Nmiss ), + show.intercept=FALSE, show.adjust=FALSE) &gt; summary(tab3) estimate std.error adj.r.squared Age in Years 0.012 0.012 0.004 2. Add labels to independent variables In the above example, age is shown with a label (Age in Years), but sex is listed ‚Äúas is‚Äù. This is because the data was created in SAS and in the SAS dataset, age had a label but sex did not. The label is stored as an attribute within R. &gt; ## Look at one variable&#39;s label &gt; attr(mockstudy$age,&#39;label&#39;) [1] Age in Years &gt; &gt; ## See all the variables with a label &gt; unlist(lapply(mockstudy,&#39;attr&#39;,&#39;label&#39;)) age arm Age in Years Treatment Arm race bmi Race Body Mass Index (kg/m^2) &gt; &gt; ## or &gt; cbind(sapply(mockstudy,attr,&#39;label&#39;)) [,1] case NULL age Age in Years arm Treatment Arm sex NULL race Race fu.time NULL fu.stat NULL ps NULL hgb NULL bmi Body Mass Index (kg/m^2) alk.phos NULL ast NULL mdquality.s NULL age.ord NULL If you want to add labels to other variables, there are a couple of options. First, you could add labels to the variables in your dataset. &gt; attr(mockstudy$age,&#39;label&#39;) &lt;- &#39;Age, yrs&#39; &gt; &gt; tab1 &lt;- modelsum(bmi ~ age, adjust=~sex, data=mockstudy) &gt; summary(tab1) estimate std.error p.value adj.r.squared (Intercept) 26.793 0.766 &lt; 0.001 0.004 Age, yrs 0.012 0.012 0.348 sex Female -0.718 0.291 0.014 You can also use the built-in data.frame method for labels&lt;-: &gt; labels(mockstudy) &lt;- c(age = &#39;Age, yrs&#39;) &gt; &gt; tab1 &lt;- modelsum(bmi ~ age, adjust=~sex, data=mockstudy) &gt; summary(tab1) estimate std.error p.value adj.r.squared (Intercept) 26.793 0.766 &lt; 0.001 0.004 Age, yrs 0.012 0.012 0.348 sex Female -0.718 0.291 0.014 Another option is to add labels after you have created the table &gt; mylabels &lt;- list(sexFemale = Female , age = Age, yrs ) &gt; summary(tab1, labelTranslations = mylabels) estimate std.error p.value adj.r.squared (Intercept) 26.793 0.766 &lt; 0.001 0.004 Age, yrs 0.012 0.012 0.348 Female -0.718 0.291 0.014 Alternatively, you can check the variable labels and manipulate them with a function called labels, which works on the modelsum object. &gt; labels(tab1) bmi age Body Mass Index (kg/m^2) Age, yrs sexFemale sex Female &gt; labels(tab1) &lt;- c(sexFemale= Female , age= Baseline Age (yrs) ) &gt; labels(tab1) bmi age Body Mass Index (kg/m^2) Baseline Age (yrs) sexFemale Female &gt; summary(tab1) estimate std.error p.value adj.r.squared (Intercept) 26.793 0.766 &lt; 0.001 0.004 Baseline Age (yrs) 0.012 0.012 0.348 Female -0.718 0.291 0.014 3. Don‚Äôt show intercept values &gt; summary(modelsum(age~mdquality.s+sex, data=mockstudy), show.intercept=FALSE) estimate std.error p.value adj.r.squared Nmiss mdquality.s -0.326 1.093 0.766 -0.001 252 sex Female -1.208 0.610 0.048 0.002 0 4. Don‚Äôt show results for adjustment variables &gt; summary(modelsum(mdquality.s ~ age + bmi, data=mockstudy, adjust=~sex, family=binomial), + show.adjust=FALSE) OR CI.lower.OR CI.upper.OR p.value concordance Nmiss (Intercept) 10.272 3.831 28.876 &lt; 0.001 0.507 0 Age, yrs 0.998 0.981 1.014 0.776 (Intercept) 4.814 1.709 13.221 0.003 0.550 33 Body Mass Index (kg/m^2) 1.023 0.987 1.063 0.220 5. Summarize multiple variables without typing them out Often one wants to summarize a number of variables. Instead of typing by hand each individual variable, an alternative approach is to create a formula using the paste command with the collapse= + option. &gt; # create a vector specifying the variable names &gt; myvars &lt;- names(mockstudy) &gt; &gt; # select the 8th through the 12th &gt; # paste them together, separated by the + sign &gt; RHS &lt;- paste(myvars[8:12], collapse= + ) &gt; RHS [1] ‚Äúps+hgb+bmi+alk.phos+ast‚Äù &gt; &gt; # create a formula using the as.formula function &gt; as.formula(paste(&#39;mdquality.s ~ &#39;, RHS)) mdquality.s ~ ps + hgb + bmi + alk.phos + ast &gt; &gt; # use the formula in the modelsum function &gt; summary(modelsum(as.formula(paste(&#39;mdquality.s ~&#39;, RHS)), family=binomial, data=mockstudy)) OR CI.lower.OR CI.upper.OR p.value concordance Nmiss (Intercept) 14.628 10.755 20.399 &lt; 0.001 0.620 266 ps 0.461 0.332 0.639 &lt; 0.001 (Intercept) 1.236 0.272 5.560 0.783 0.573 266 hgb 1.176 1.040 1.334 0.011 (Intercept) 4.963 1.818 13.292 0.002 0.549 33 Body Mass Index (kg/m^2) 1.023 0.987 1.062 0.225 (Intercept) 10.622 7.687 14.794 &lt; 0.001 0.552 266 alk.phos 0.999 0.998 1.000 0.159 (Intercept) 10.936 7.912 15.232 &lt; 0.001 0.545 266 ast 0.995 0.988 1.001 0.099 These steps can also be done using the formulize function. &gt; ## The formulize function does the paste and as.formula steps &gt; tmp &lt;- formulize(&#39;mdquality.s&#39;,myvars[8:10]) &gt; tmp mdquality.s ~ ps + hgb + bmi &gt; &gt; ## More complex formulas could also be written using formulize &gt; tmp2 &lt;- formulize(&#39;mdquality.s&#39;,c(&#39;ps&#39;,&#39;hgb&#39;,&#39;sqrt(bmi)&#39;)) &gt; &gt; ## use the formula in the modelsum function &gt; summary(modelsum(tmp, data=mockstudy, family=binomial)) OR CI.lower.OR CI.upper.OR p.value concordance Nmiss (Intercept) 14.628 10.755 20.399 &lt; 0.001 0.620 266 ps 0.461 0.332 0.639 &lt; 0.001 (Intercept) 1.236 0.272 5.560 0.783 0.573 266 hgb 1.176 1.040 1.334 0.011 (Intercept) 4.963 1.818 13.292 0.002 0.549 33 Body Mass Index (kg/m^2) 1.023 0.987 1.062 0.225 6. Subset the dataset used in the analysis Here are two ways to get the same result (limit the analysis to subjects age&gt;50 and in the F: FOLFOX treatment group). The first approach uses the subset function applied to the dataset mockstudy. This example also selects a subset of variables. The modelsum function is then applied to this subsetted data. &gt; newdata &lt;- subset(mockstudy, subset=age&gt;50 &amp; arm==&#39;F: FOLFOX&#39;, select = c(age,sex, bmi:alk.phos)) &gt; dim(mockstudy) [1] 1499 14 &gt; table(mockstudy$arm) A: IFL F: FOLFOX G: IROX 428 691 380 &gt; dim(newdata) [1] 557 4 &gt; names(newdata) [1] age sex bmi alk.phos &gt; summary(modelsum(alk.phos ~ ., data=newdata)) estimate std.error p.value adj.r.squared Nmiss (Intercept) 122.577 46.924 0.009 -0.001 0 age 0.619 0.719 0.390 (Intercept) 164.814 7.673 &lt; 0.001 -0.002 0 sex Female -5.497 12.118 0.650 (Intercept) 238.658 33.705 &lt; 0.001 0.010 15 bmi -2.776 1.207 0.022 The second approach does the same analysis but uses the subset argument within modelsum to subset the data. &gt; summary(modelsum(log(alk.phos) ~ sex + ps + bmi, subset=age&gt;50 &amp; arm== F: FOLFOX , data=mockstudy)) estimate std.error p.value adj.r.squared Nmiss (Intercept) 4.872 0.039 &lt; 0.001 -0.002 0 sex Female -0.005 0.062 0.931 (Intercept) 4.770 0.040 &lt; 0.001 0.027 108 ps 0.183 0.050 &lt; 0.001 (Intercept) 5.207 0.172 &lt; 0.001 0.007 15 Body Mass Index (kg/m^2) -0.012 0.006 0.044 &gt; summary(modelsum(alk.phos ~ ps + bmi, adjust=~sex, subset = age&gt;50 &amp; bmi&lt;24, data=mockstudy)) estimate std.error p.value adj.r.squared Nmiss (Intercept) 178.812 14.550 &lt; 0.001 0.007 77 ps 20.834 13.440 0.122 sex Female -17.542 16.656 0.293 (Intercept) 373.008 104.272 &lt; 0.001 0.009 24 Body Mass Index (kg/m^2) -8.239 4.727 0.083 sex Female -24.058 16.855 0.155 &gt; summary(modelsum(alk.phos ~ ps + bmi, adjust=~sex, subset=1:30, data=mockstudy)) estimate std.error p.value adj.r.squared Nmiss (Intercept) 169.112 57.013 0.006 0.294 0 ps 254.901 68.100 &lt; 0.001 sex Female 49.566 67.643 0.470 (Intercept) 453.070 200.651 0.033 -0.049 1 Body Mass Index (kg/m^2) -5.993 7.408 0.426 sex Female -22.308 79.776 0.782 7. Create combinations of variables on the fly &gt; ## create a variable combining the levels of mdquality.s and sex &gt; with(mockstudy, table(interaction(mdquality.s,sex))) 0.Male 1.Male 0.Female 1.Female 77 686 47 437 &gt; summary(modelsum(age ~ interaction(mdquality.s,sex), data=mockstudy)) estimate std.error p.value adj.r.squared Nmiss (Intercept) 59.714 1.314 &lt; 0.001 0.003 252 interaction(mdquality.s, sex) 1.Male 0.730 1.385 0.598 interaction(mdquality.s, sex) 0.Female 0.988 2.134 0.643 interaction(mdquality.s, sex) 1.Female -1.021 1.425 0.474 8. Transform variables on the fly Certain transformations need to be surrounded by I() so that R knows to treat it as a variable transformation and not some special model feature. If the transformation includes any of the symbols / - + ^ * then surround the new variable by I(). &gt; summary(modelsum(arm== F: FOLFOX ~ I(age/10) + log(bmi) + mdquality.s, + data=mockstudy, family=binomial)) OR CI.lower.OR CI.upper.OR p.value concordance Nmiss (Intercept) 0.656 0.382 1.124 0.126 0.514 0 Age, yrs 1.045 0.957 1.142 0.326 (Intercept) 0.633 0.108 3.698 0.611 0.508 33 Body Mass Index (kg/m^2) 1.092 0.638 1.867 0.748 (Intercept) 0.722 0.503 1.029 0.074 0.502 252 mdquality.s 1.045 0.719 1.527 0.819 9. Change the ordering of the variables or delete a variable &gt; mytab &lt;- modelsum(bmi ~ sex + alk.phos + age, data=mockstudy) &gt; mytab2 &lt;- mytab[c(&#39;age&#39;,&#39;sex&#39;,&#39;alk.phos&#39;)] &gt; summary(mytab2) estimate std.error p.value adj.r.squared Nmiss (Intercept) 26.424 0.752 &lt; 0.001 0.000 0 Age, yrs 0.013 0.012 0.290 (Intercept) 27.491 0.181 &lt; 0.001 0.004 0 sex Female -0.731 0.290 0.012 (Intercept) 27.944 0.253 &lt; 0.001 0.011 266 alk.phos -0.005 0.001 &lt; 0.001 &gt; summary(mytab[c(&#39;age&#39;,&#39;sex&#39;)]) estimate std.error p.value adj.r.squared (Intercept) 26.424 0.752 &lt; 0.001 0.000 Age, yrs 0.013 0.012 0.290 (Intercept) 27.491 0.181 &lt; 0.001 0.004 sex Female -0.731 0.290 0.012 &gt; summary(mytab[c(3,1)]) estimate std.error p.value adj.r.squared (Intercept) 26.424 0.752 &lt; 0.001 0.000 Age, yrs 0.013 0.012 0.290 (Intercept) 27.491 0.181 &lt; 0.001 0.004 sex Female -0.731 0.290 0.012 10. Merge two modelsum objects together It is possible to combine two modelsum objects so that they print out together, however you need to pay attention to the columns that are being displayed. It is easier to combine two models of the same family (such as two sets of linear models). If you want to combine linear and logistic model results then you would want to display the beta coefficients for the logistic model. &gt; ## demographics &gt; tab1 &lt;- modelsum(bmi ~ sex + age, data=mockstudy) &gt; ## lab data &gt; tab2 &lt;- modelsum(mdquality.s ~ hgb + alk.phos, data=mockstudy, family=binomial) &gt; &gt; tab12 &lt;- merge(tab1,tab2) &gt; class(tab12) [1] ‚ÄúmodelsumList‚Äù &gt; &gt; ##ERROR: The merge works, but not the summary &gt; #summary(tab12) 11. Add a title to the table When creating a pdf the tables are automatically numbered and the title appears below the table. In Word and HTML, the titles appear un-numbered and above the table. &gt; t1 &lt;- modelsum(bmi ~ sex + age, data=mockstudy) &gt; summary(t1, title=&#39;Demographics&#39;) Demographics estimate std.error p.value adj.r.squared (Intercept) 27.491 0.181 &lt; 0.001 0.004 sex Female -0.731 0.290 0.012 (Intercept) 26.424 0.752 &lt; 0.001 0.000 Age, yrs 0.013 0.012 0.290 12. Modify how missing values are treated Depending on the report you are writing you have the following options: Use all values available for each variable Use only those subjects who have measurements available for all the variables &gt; ## look at how many missing values there are for each variable &gt; apply(is.na(mockstudy),2,sum) case age arm sex race fu.time 0 0 0 0 7 0 fu.stat ps hgb bmi alk.phos ast 0 266 266 33 266 266 mdquality.s age.ord 252 0 &gt; ## Show how many subjects have each variable (non-missing) &gt; summary(modelsum(bmi ~ ast + age, data=mockstudy, + control=modelsum.control(gaussian.stats=c( N , estimate )))) estimate N (Intercept) 27.331 1233 ast -0.005 (Intercept) 26.424 1499 Age, yrs 0.013 &gt; &gt; ## Always list the number of missing values &gt; summary(modelsum(bmi ~ ast + age, data=mockstudy, + control=modelsum.control(gaussian.stats=c( Nmiss2 , estimate )))) estimate Nmiss2 (Intercept) 27.331 266 ast -0.005 (Intercept) 26.424 0 Age, yrs 0.013 &gt; &gt; ## Only show the missing values if there are some (default) &gt; summary(modelsum(bmi ~ ast + age, data=mockstudy, + control=modelsum.control(gaussian.stats=c( Nmiss , estimate )))) estimate Nmiss (Intercept) 27.331 266 ast -0.005 (Intercept) 26.424 0 Age, yrs 0.013 &gt; &gt; ## Don&#39;t show N at all &gt; summary(modelsum(bmi ~ ast + age, data=mockstudy, + control=modelsum.control(gaussian.stats=c( estimate )))) estimate (Intercept) 27.331 ast -0.005 (Intercept) 26.424 Age, yrs 0.013 13. Modify the number of digits used Within modelsum.control function there are 3 options for controlling the number of significant digits shown. digits: controls the number of digits after the decimal point for continuous values digits.ratio: controls the number of digits after the decimal point for continuous values digits.p: controls the number of digits after the decimal point for continuous values &gt; summary(modelsum(bmi ~ sex + age + fu.time, data=mockstudy), digits=4, digits.test=2) Warning: Using &#39;digits.test = &#39; is deprecated. Use &#39;digits.p = &#39; instead. estimate std.error p.value adj.r.squared (Intercept) 27.4915 0.1813 &lt; 0.001 0.0036 sex Female -0.7311 0.2903 0.012 (Intercept) 26.4237 0.7521 &lt; 0.001 0.0001 Age, yrs 0.0130 0.0123 0.290 (Intercept) 26.4937 0.2447 &lt; 0.001 0.0079 fu.time 0.0011 0.0003 &lt; 0.001 14. Use case-weights in the models Occasionally it is of interest to fit models using case weights. The modelsum function allows you to pass on the weights to the models and it will do the appropriate fit. &gt; mockstudy$agegp &lt;- cut(mockstudy$age, breaks=c(18,50,60,70,90), right=FALSE) &gt; &gt; ## create weights based on agegp and sex distribution &gt; tab1 &lt;- with(mockstudy,table(agegp, sex)) &gt; tab1 sex agegp Male Female [18,50) 152 110 [50,60) 258 178 [60,70) 295 173 [70,90) 211 122 &gt; tab2 &lt;- with(mockstudy, table(agegp, sex, arm)) &gt; gpwts &lt;- rep(tab1, length(unique(mockstudy$arm)))/tab2 &gt; &gt; ## apply weights to subjects &gt; index &lt;- with(mockstudy, cbind(as.numeric(agegp), as.numeric(sex), as.numeric(as.factor(arm)))) &gt; mockstudy$wts &lt;- gpwts[index] &gt; &gt; ## show weights by treatment arm group &gt; tapply(mockstudy$wts,mockstudy$arm, summary) $`A: IFL` Min. 1st Qu. Median Mean 3rd Qu. Max. 2.923 3.225 3.548 3.502 3.844 4.045 $`F: FOLFOX` Min. 1st Qu. Median Mean 3rd Qu. Max. 2.033 2.070 2.201 2.169 2.263 2.303 $`G: IROX` Min. 1st Qu. Median Mean 3rd Qu. Max. 3.667 3.734 4.023 3.945 4.031 4.471 &gt; mockstudy$newvarA &lt;- as.numeric(mockstudy$arm==&#39;A: IFL&#39;) &gt; tab1 &lt;- modelsum(newvarA ~ ast + bmi + hgb, data=mockstudy, subset=(arm !=&#39;G: IROX&#39;), + family=binomial) &gt; summary(tab1, title=&#39;No Case Weights used&#39;) No Case Weights used OR CI.lower.OR CI.upper.OR p.value concordance Nmiss (Intercept) 0.590 0.473 0.735 &lt; 0.001 0.550 210 ast 1.003 0.998 1.008 0.258 (Intercept) 0.578 0.306 1.093 0.091 0.500 29 Body Mass Index (kg/m^2) 1.003 0.980 1.026 0.808 (Intercept) 1.006 0.386 2.631 0.990 0.514 210 hgb 0.965 0.894 1.043 0.372 &gt; &gt; suppressWarnings({ + tab2 &lt;- modelsum(newvarA ~ ast + bmi + hgb, data=mockstudy, subset=(arm !=&#39;G: IROX&#39;), + weights=wts, family=binomial) + summary(tab2, title=&#39;Case Weights used&#39;) + }) Case Weights used OR CI.lower.OR CI.upper.OR p.value concordance Nmiss (Intercept) 0.956 0.837 1.091 0.504 0.550 210 ast 1.003 1.000 1.006 0.068 (Intercept) 0.957 0.658 1.393 0.820 0.500 29 Body Mass Index (kg/m^2) 1.002 0.988 1.016 0.780 (Intercept) 1.829 1.031 3.248 0.039 0.514 210 hgb 0.956 0.913 1.001 0.058 15. Use modelsum within an Sweave document For those users who wish to create tables within an Sweave document, the following code seems to work. \\documentclass{article} \\usepackage{longtable} \\usepackage{pdfpages} \\begin{document} \\section{Read in Data} &lt;&lt;echo=TRUE&gt;&gt;= require(arsenal) require(knitr) require(rmarkdown) data(mockstudy) tab1 &lt;- modelsum(bmi~sex+age, data=mockstudy) @ \\section{Convert Summary.modelsum to LaTeX} &lt;&lt;echo=TRUE, results=&#39;hide&#39;, message=FALSE&gt;&gt;= capture.output(summary(tab1), file= Test.md ) ## Convert R Markdown Table to LaTeX render( Test.md , pdf_document(keep_tex=TRUE)) @ \\includepdf{Test.pdf} \\end{document} 16. Export modelsum results to a .CSV file When looking at multiple variables it is sometimes useful to export the results to a csv file. The as.data.frame function creates a data frame object that can be exported or further manipulated within R. &gt; summary(tab2, text=T) | |OR |CI.lower.OR |CI.upper.OR |p.value |concordance |Nmiss | |:|:--|:--|:--|:-|:--|:--| |(Intercept) |0.956 |0.837 |1.091 |0.504 |0.550 |210 | |ast |1.003 |1.000 |1.006 |0.068 | | | |(Intercept) |0.957 |0.658 |1.393 |0.820 |0.500 |29 | |Body Mass Index (kg/m^2) |1.002 |0.988 |1.016 |0.780 | | | |(Intercept) |1.829 |1.031 |3.248 |0.039 |0.514 |210 | |hgb |0.956 |0.913 |1.001 |0.058 | | | &gt; tmp &lt;- as.data.frame(tab2) &gt; tmp model term label term.type OR 1 1 (Intercept) (Intercept) Intercept 0.9559704 2 1 ast ast Term 1.0027311 3 2 (Intercept) (Intercept) Intercept 0.9573694 4 2 bmi Body Mass Index (kg/m^2) Term 1.0019251 5 3 (Intercept) (Intercept) Intercept 1.8287083 6 3 hgb hgb Term 0.9563507 CI.lower.OR CI.upper.OR p.value concordance Nmiss 1 0.8373522 1.090904 0.50443340 0.5499494 210 2 0.9998110 1.005696 0.06813456 0.5499494 210 3 0.6579225 1.392859 0.81981779 0.5002561 29 4 0.9884804 1.015561 0.78019163 0.5002561 29 5 1.0311954 3.247941 0.03911088 0.5138162 210 6 0.9132041 1.001419 0.05770821 0.5138162 210 &gt; # write.csv(tmp, &#39;/my/path/here/mymodel.csv&#39;) 17. Write modelsum object to a separate Word or HTML file &gt; ## write to an HTML document &gt; write2html(tab2, ~/ibm/trash.html ) &gt; &gt; ## write to a Word document &gt; write2word(tab2, ~/ibm/trash.doc , title= My table in Word ) 18. Use modelsum in R Shiny The easiest way to output a modelsum() object in an R Shiny app is to use the tableOutput() UI in combination with the renderTable() server function and as.data.frame(summary(modelsum())): &gt; # A standalone shiny app &gt; library(shiny) &gt; library(arsenal) &gt; data(mockstudy) &gt; &gt; shinyApp( + ui = fluidPage(tableOutput( table )), + server = function(input, output) { + output$table &lt;- renderTable({ + as.data.frame(summary(modelsum(age ~ sex, data = mockstudy), text = html )) + }, sanitize.text.function = function(x) x) + } + ) This can be especially powerful if you feed the selections from a selectInput(multiple = TRUE) into formulize() to make the table dynamic! 23. Use modelsum in bookdown Since the backbone of modelsum() is knitr::kable(), tables still render well in bookdown. However, print.summary.modelsum() doesn‚Äôt use the caption= argument of kable(), so some tables may not have a properly numbered caption. To fix this, use the method described on the bookdown site to give the table a tag/ID. &gt; summary(modelsum(age ~ sex, data = mockstudy), title= (\\\\#tab:mytableby) Caption here ) Available Function Options Summary statistics The available summary statistics, by varible type, are: ordinal: Ordinal logistic regression models default: Nmiss, OR, CI.lower.OR, CI.upper.OR, p.value optional: estimate, CI.OR, CI.estimate, CI.lower.estimate, CI.upper.estimate, N, Nmiss2, endpoint, std.error, statistic, logLik, AIC, BIC, edf, deviance, df.residual binomial,quasibinomial: Logistic regression models default: OR, CI.lower.OR, CI.upper.OR, p.value, concordance, Nmiss optional: estimate, CI.OR, CI.estimate, CI.lower.estimate, CI.upper.estimate, N, Nmiss2, endpoint, std.error, statistic, logLik, AIC, BIC, null.deviance, deviance, df.residual, df.null gaussian: Linear regression models default: estimate, std.error, p.value, adj.r.squared, Nmiss optional: CI.estimate, CI.lower.estimate, CI.upper.estimate, N, Nmiss2, statistic, standard.estimate, endpoint, r.squared, AIC, BIC, logLik, statistic.F, p.value.F poisson, quasipoisson: Poisson regression models default: RR, CI.lower.RR, CI.upper.RR, p.value, Nmiss optional: CI.RR, CI.estimate, CI.lower.estimate, CI.upper.estimate, CI.RR, Nmiss2, std.error, estimate, statistic, endpoint, AIC, BIC, logLik, dispersion, null.deviance, deviance, df.residual, df.null negbin: Negative binomial regression models default: RR, CI.lower.RR, CI.upper.RR, p.value, Nmiss optional: CI.RR, CI.estimate, CI.lower.estimate, CI.upper.estimate, CI.RR, Nmiss2, std.error, estimate, statistic, endpoint, AIC, BIC, logLik, dispersion, null.deviance, deviance, df.residual, df.null, theta, SE.theta survival: Cox models default: HR, CI.lower.HR, CI.upper.HR, p.value, concordance, Nmiss optional: CI.HR, CI.estimate, CI.lower.estimate, CI.upper.estimate, N, Nmiss2, estimate, std.error, endpoint, Nevents, statistic, r.squared, logLik, AIC, BIC, statistic.sc, p.value.sc, p.value.log, p.value.wald, N, std.error.concordance The full description of these parameters that can be shown for models include: N: a count of the number of observations used in the analysis Nmiss: only show the count of the number of missing values if there are some missing values Nmiss2: always show a count of the number of missing values for a model endpoint: dependent variable used in the model std.err: print the standard error statistic: test statistic statistic.F: test statistic (F test) p.value: print the p-value r.squared: print the model R-square adj.r.squared: print the model adjusted R-square r.squared: print the model R-square concordance: print the model C statistic (which is the AUC for logistic models) logLik: print the loglikelihood value p.value.log: print the p-value for the overall model likelihood test p.value.wald: print the p-value for the overall model wald test p.value.sc: print the p-value for overall model score test AIC: print the Akaike information criterion BIC: print the Bayesian information criterion null.deviance: null deviance deviance: model deviance df.residual: degrees of freedom for the residual df.null: degrees of freedom for the null model dispersion: This is used in Poisson models and is defined as the deviance/df.residual statistic.sc: overall model score statistic std.error.concordance: standard error for the C statistic HR: print the hazard ratio (for survival models), i.e. exp(beta) CI.lower.HR, CI.upper.HR: print the confidence interval for the HR OR: print the odd‚Äôs ratio (for logistic models), i.e. exp(beta) CI.lower.OR, CI.upper.OR: print the confidence interval for the OR RR: print the risk ratio (for poisson models), i.e. exp(beta) CI.lower.RR, CI.upper.RR: print the confidence interval for the RR estimate: print beta coefficient standardized.estimate: print the standardized beta coefficient CI.lower.estimate, CI.upper.estimate: print the confidence interval for the beta coefficient edf: print the effective degrees of freedom. theta: print the estimate of theta. SE.theta: print the estimate of theta‚Äôs standard error. modelsum.control settings A quick way to see what arguments are possible to utilize in a function is to use the args() command. Settings involving the number of digits can be set in modelsum.control or in summary.modelsum. &gt; args(modelsum.control) function (digits = 3L, digits.ratio = 3L, digits.p = 3L, format.p = TRUE, show.adjust = TRUE, show.intercept = TRUE, conf.level = 0.95, ordinal.stats = c( OR , CI.lower.OR , CI.upper.OR , p.value , Nmiss ), binomial.stats = c( OR , CI.lower.OR , CI.upper.OR , p.value , concordance , Nmiss ), gaussian.stats = c( estimate , std.error , p.value , adj.r.squared , Nmiss ), poisson.stats = c( RR , CI.lower.RR , CI.upper.RR , p.value , Nmiss ), negbin.stats = c( RR , CI.lower.RR , CI.upper.RR , p.value , Nmiss ), survival.stats = c( HR , CI.lower.HR , CI.upper.HR , p.value , concordance , Nmiss ), stat.labels = list(), ...) NULL summary.modelsum settings The summary.modelsum function has options that modify how the table appears (such as adding a title or modifying labels). &gt; args(arsenal:::summary.modelsum) function (object, ..., labelTranslations = NULL, text = FALSE, title = NULL, term.name = ) NULL ## The paired function https://cran.r-project.org/web/packages/arsenal/vignettes/paired.html The paired function Ethan Heinzen, Beth Atkinson, Jason Sinnwell 09 November, 2018 Introduction Simple Example NAs Available Function Options Testing options paired.control settings summary.tableby settings Introduction Another one of the most common tables in medical literature includes summary statistics for a set of variables paired across two time points. Locally at Mayo, the SAS macro %paired was written to create summary tables with a single call. With the increasing interest in R, we have developed the function paired() to create similar tables within the R environment. This vignette is light on purpose; paired() piggybacks off of tableby, so most documentation there applies here, too. Simple Example The first step when using the paired() function is to load the arsenal package. We can‚Äôt use mockstudy here because we need a dataset with paired observations, so we‚Äôll create our own dataset. library(arsenal) dat &lt;- data.frame( tp = paste0( Time Point , c(1, 2, 1, 2, 1, 2, 1, 2, 1, 2)), id = c(1, 1, 2, 2, 3, 3, 4, 4, 5, 6), Cat = c( A , A , A , B , B , B , B , A , NA, B ), Fac = factor(c( A , B , C , A , B , C , A , B , C , A )), Num = c(1, 2, 3, 4, 4, 3, 3, 4, 0, NA), Ord = ordered(c( I , II , II , III , III , III , I , III , II , I )), Lgl = c(TRUE, TRUE, FALSE, TRUE, FALSE, TRUE, TRUE, FALSE, FALSE, FALSE), Dat = as.Date( 2018-05-01 ) + c(1, 1, 2, 2, 3, 4, 5, 6, 3, 4), stringsAsFactors = FALSE ) To create a simple table stratified by time point, use a formula= statement to specify the variables that you want summarized and the id= argument to specify the paired observations. p &lt;- paired(tp ~ Cat + Fac + Num + Ord + Lgl + Dat, data = dat, id = id, signed.rank.exact = FALSE) summary(p) Time Point 1 (N=4) Time Point 2 (N=4) Difference (N=4) p value Cat 1.000 A 2 (50.0%) 2 (50.0%) 1 (50.0%) B 2 (50.0%) 2 (50.0%) 1 (50.0%) Fac 0.261 A 2 (50.0%) 1 (25.0%) 2 (100.0%) B 1 (25.0%) 2 (50.0%) 1 (100.0%) C 1 (25.0%) 1 (25.0%) 1 (100.0%) Num 0.391 Mean (SD) 2.750 (1.258) 3.250 (0.957) 0.500 (1.000) Range 1.000 - 4.000 2.000 - 4.000 -1.000 - 1.000 Ord 0.174 I 2 (50.0%) 0 (0.0%) 2 (100.0%) II 1 (25.0%) 1 (25.0%) 1 (100.0%) III 1 (25.0%) 3 (75.0%) 0 (0.0%) Lgl 1.000 FALSE 2 (50.0%) 1 (25.0%) 2 (100.0%) TRUE 2 (50.0%) 3 (75.0%) 1 (50.0%) Dat 0.182 median 2018-05-03 2018-05-04 0.500 Range 2018-05-02 - 2018-05-06 2018-05-02 - 2018-05-07 0.000 - 1.000 The third column shows the difference between time point 1 and time point 2. For categorical variables, it reports the percent of observations from time point 1 which changed in time point 2. NAs Note that by default, observations which do not have both timepoints are removed. This is easily changed using the na.action = na.paired( &lt;arg&gt; ) argument. For example: p &lt;- paired(tp ~ Cat + Fac + Num + Ord + Lgl + Dat, data = dat, id = id, signed.rank.exact = FALSE, na.action = na.paired( fill )) summary(p) Time Point 1 (N=6) Time Point 2 (N=6) Difference (N=6) p value Cat 1.000 N-Miss 2 1 2 A 2 (50.0%) 2 (40.0%) 1 (50.0%) B 2 (50.0%) 3 (60.0%) 1 (50.0%) Fac 0.261 N-Miss 1 1 2 A 2 (40.0%) 2 (40.0%) 2 (100.0%) B 1 (20.0%) 2 (40.0%) 1 (100.0%) C 2 (40.0%) 1 (20.0%) 1 (100.0%) Num 0.391 N-Miss 1 2 2 Mean (SD) 2.200 (1.643) 3.250 (0.957) 0.500 (1.000) Range 0.000 - 4.000 2.000 - 4.000 -1.000 - 1.000 Ord 0.174 N-Miss 1 1 2 I 2 (40.0%) 1 (20.0%) 2 (100.0%) II 2 (40.0%) 1 (20.0%) 1 (100.0%) III 1 (20.0%) 3 (60.0%) 0 (0.0%) Lgl 1.000 N-Miss 1 1 2 FALSE 3 (60.0%) 2 (40.0%) 2 (100.0%) TRUE 2 (40.0%) 3 (60.0%) 1 (50.0%) Dat 0.182 N-Miss 1 1 2 median 2018-05-04 2018-05-05 0.500 Range 2018-05-02 - 2018-05-06 2018-05-02 - 2018-05-07 0.000 - 1.000 For more details, see the help page for na.paired(). Available Function Options Testing options The tests used to calculate p-values differ by the variable type, but can be specified explicitly in the formula statement or in the control function. The following tests are accepted: paired.t: A paired t-test. mcnemar: McNemar‚Äôs test. signed.rank: the signed-rank test. sign.test: the sign test. notest: Don‚Äôt perform a test. paired.control settings A quick way to see what arguments are possible to utilize in a function is to use the args() command. Settings involving the number of digits can be set in paired.control or in summary.tableby. args(paired.control) ## function (test = TRUE, diff = TRUE, test.pname = NULL, numeric.test = paired.t , ## cat.test = mcnemar , ordered.test = signed.rank , date.test = paired.t , ## numeric.stats = c( Nmiss , meansd , range ), cat.stats = c( Nmiss , ## countpct ), ordered.stats = c( Nmiss , countpct ), ## date.stats = c( Nmiss , median , range ), stats.labels = list(Nmiss = N-Miss , ## Nmiss2 = N-Miss , meansd = Mean (SD) , medianq1q3 = Median (Q1, Q3) , ## q1q3 = Q1, Q3 , range = Range , countpct = Count (Pct) ), ## digits = 3L, digits.count = 0L, digits.p = 3L, format.p = TRUE, ## conf.level = 0.95, mcnemar.correct = TRUE, signed.rank.exact = NULL, ## signed.rank.correct = TRUE, ...) ## NULL summary.tableby settings Since the ‚Äúpaired‚Äù object inherits ‚Äútableby‚Äù, the summary.tableby function is what‚Äôs actually used to format and print the table. args(arsenal:::summary.tableby) ## function (object, ..., labelTranslations = NULL, text = FALSE, ## title = NULL, pfootnote = FALSE, term.name = ) ## NULL ## The tableby function https://cran.r-project.org/web/packages/arsenal/vignettes/tableby.html The tableby function Beth Atkinson, Ethan Heinzen, Jason Sinnwell, Shannon McDonnell and Greg Dougherty 09 November, 2018 Introduction Simple Example Pretty text version of table Pretty Rmarkdown version of table Data frame version of table Summaries using standard R code Modifying Output Add labels Change summary statistics globally Change summary statistics within the formula Controlling Options for Categorical Tests (Chisq and Fisher‚Äôs) Modifying the look &amp; feel in Word documents Additional Examples 1. Summarize without a group/by variable 2. Display footnotes indicating which ‚Äútest‚Äù was used 3. Summarize an ordered factor 4. Summarize a survival variable 5. Summarize date variables 6. Summarize multiple variables without typing them out 7. Subset the dataset used in the analysis 8. Create combinations of variables on the fly 9. Transform variables on the fly 10. Subsetting (change the ordering of the variables, delete a variable, sort by p-value, filter by p-value) 11. Merge two tableby objects together 12. Add a title to the table 13. Modify how missing values are displayed 14. Modify the number of digits used 15. Create a user-defined summary statistic 16. Use case-weights for creating summary statistics 17. Create your own p-value and add it to the table 18. For two-level categorical variables or one-line numeric variables, simplify the output. 19. Use tableby within an Sweave document 20. Export tableby object to a .CSV file 21. Write tableby object to a separate Word or HTML file 22. Use tableby in R Shiny 23. Use tableby in bookdown 24. Adjust tableby for multiple p-values Available Function Options Summary statistics Testing options tableby.control settings summary.tableby settings Introduction One of the most common tables in medical literature includes summary statistics for a set of variables, often stratified by some group (e.g. treatment arm). Locally at Mayo, the SAS macros %table and %summary were written to create summary tables with a single call. With the increasing interest in R, we have developed the function tableby to create similar tables within the R environment. In developing the tableby() function, the goal was to bring the best features of these macros into an R function. However, the task was not simply to duplicate all the functionality, but rather to make use of R‚Äôs strengths (modeling, method dispersion, flexibility in function definition and output format) and make a tool that fits the needs of R users. Additionally, the results needed to fit within the general reproducible research framework so the tables could be displayed within an R markdown report. This report provides step-by-step directions for using the functions associated with tableby(). All functions presented here are available within the arsenal package. An assumption is made that users are somewhat familiar with R Markdown documents. For those who are new to the topic, a good initial resource is available at rmarkdown.rstudio.com. Simple Example The first step when using the tableby function is to load the arsenal package. All the examples in this report use a dataset called mockstudy made available by Paul Novotny which includes a variety of types of variables (character, numeric, factor, ordered factor, survival) to use as examples. require(arsenal) require(knitr) require(survival) data(mockstudy) ##load data dim(mockstudy) ##look at how many subjects and variables are in the dataset ## [1] 1499 14 # help(mockstudy) ##learn more about the dataset and variables str(mockstudy) ##quick look at the data ## &#39;data.frame&#39;: 1499 obs. of 14 variables: ## $ case : int 110754 99706 105271 105001 112263 86205 99508 90158 88989 90515 ... ## $ age : atomic 67 74 50 71 69 56 50 57 51 63 ... ## ..- attr(*, label )= chr Age in Years ## $ arm : atomic F: FOLFOX A: IFL A: IFL G: IROX ... ## ..- attr(*, label )= chr Treatment Arm ## $ sex : Factor w/ 2 levels Male , Female : 1 2 2 2 2 1 1 1 2 1 ... ## $ race : atomic Caucasian Caucasian Caucasian Caucasian ... ## ..- attr(*, label )= chr Race ## $ fu.time : int 922 270 175 128 233 120 369 421 387 363 ... ## $ fu.stat : int 2 2 2 2 2 2 2 2 2 2 ... ## $ ps : int 0 1 1 1 0 0 0 0 1 1 ... ## $ hgb : num 11.5 10.7 11.1 12.6 13 10.2 13.3 12.1 13.8 12.1 ... ## $ bmi : atomic 25.1 19.5 NA 29.4 26.4 ... ## ..- attr(*, label )= chr Body Mass Index (kg/m^2) ## $ alk.phos : int 160 290 700 771 350 569 162 152 231 492 ... ## $ ast : int 35 52 100 68 35 27 16 12 25 18 ... ## $ mdquality.s: int NA 1 1 1 NA 1 1 1 1 1 ... ## $ age.ord : Ord.factor w/ 8 levels 10-19 &lt; 20-29 &lt;..: 6 7 4 7 6 5 4 5 5 6 ... To create a simple table stratified by treament arm, use a formula statement to specify the variables that you want summarized. The example below uses age (a continuous variable) and sex (a factor). tab1 &lt;- tableby(arm ~ sex + age, data=mockstudy) If you want to take a quick look at the table, you can use summary() on your tableby object and the table will print out as text in your R console window. If you use summary() without any options you will see a number of &amp;nbsp; statements which translates to ‚Äúspace‚Äù in HTML. Pretty text version of table If you want a nicer version in your console window then add the text=TRUE option. summary(tab1, text=TRUE) ## ## ## | | A: IFL (N=428) | F: FOLFOX (N=691) | G: IROX (N=380) | Total (N=1499) | p value| ## |:|::|:--:|::|::|-:| ## |sex | | | | | 0.190| ## |- Male | 277 (64.7%) | 411 (59.5%) | 228 (60.0%) | 916 (61.1%) | | ## |- Female | 151 (35.3%) | 280 (40.5%) | 152 (40.0%) | 583 (38.9%) | | ## |Age in Years | | | | | 0.614| ## |- Mean (SD) | 59.673 (11.365) | 60.301 (11.632) | 59.763 (11.499) | 59.985 (11.519) | | ## |- Range | 27.000 - 88.000 | 19.000 - 88.000 | 26.000 - 85.000 | 19.000 - 88.000 | | Pretty Rmarkdown version of table In order for the report to look nice within an R markdown (knitr) report, you just need to specify results= asis when creating the r chunk. This changes the layout slightly (compresses it) and bolds the variable names. summary(tab1) A: IFL (N=428) F: FOLFOX (N=691) G: IROX (N=380) Total (N=1499) p value sex 0.190 Male 277 (64.7%) 411 (59.5%) 228 (60.0%) 916 (61.1%) Female 151 (35.3%) 280 (40.5%) 152 (40.0%) 583 (38.9%) Age in Years 0.614 Mean (SD) 59.673 (11.365) 60.301 (11.632) 59.763 (11.499) 59.985 (11.519) Range 27.000 - 88.000 19.000 - 88.000 26.000 - 85.000 19.000 - 88.000 Data frame version of table If you want a data.frame version, simply use as.data.frame. as.data.frame(tab1) ## variable term label variable.type A: IFL F: FOLFOX ## 1 sex sex sex categorical ## 2 sex countpct Male categorical 277.00000, 64.71963 411.00000, 59.47902 ## 3 sex countpct Female categorical 151.00000, 35.28037 280.00000, 40.52098 ## 4 age age Age in Years numeric ## 5 age meansd Mean (SD) numeric 59.67290, 11.36454 60.30101, 11.63225 ## 6 age range Range numeric 27, 88 19, 88 ## G: IROX Total test p.value ## 1 Pearson&#39;s Chi-squared test 0.1904388 ## 2 228, 60 916.0000, 61.1074 Pearson&#39;s Chi-squared test 0.1904388 ## 3 152, 40 583.0000, 38.8926 Pearson&#39;s Chi-squared test 0.1904388 ## 4 Linear Model ANOVA 0.6143859 ## 5 59.76316, 11.49930 59.98532, 11.51877 Linear Model ANOVA 0.6143859 ## 6 26, 85 19, 88 Linear Model ANOVA 0.6143859 Summaries using standard R code ## base R frequency example tmp &lt;- table(Gender=mockstudy$sex, Study Arm =mockstudy$arm) tmp ## Study Arm ## Gender A: IFL F: FOLFOX G: IROX ## Male 277 411 228 ## Female 151 280 152 # Note: The continuity correction is applied by default in R (not used in %table) chisq.test(tmp) ## ## Pearson&#39;s Chi-squared test ## ## data: tmp ## X-squared = 3.3168, df = 2, p-value = 0.1904 ## base R numeric summary example tapply(mockstudy$age, mockstudy$arm, summary) ## $`A: IFL` ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 27.00 53.00 61.00 59.67 68.00 88.00 ## ## $`F: FOLFOX` ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 19.0 52.0 61.0 60.3 69.0 88.0 ## ## $`G: IROX` ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 26.00 52.00 61.00 59.76 68.00 85.00 summary(aov(age ~ arm, data=mockstudy)) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## arm 2 129 64.7 0.487 0.614 ## Residuals 1496 198628 132.8 Modifying Output Add labels In the above example, age is shown with a label (Age in Years), but sex is listed ‚Äúas is‚Äù with lower case letters. This is because the data was created in SAS and in the SAS dataset, age had a label but sex did not. The label is stored as an attribute within R. ## Look at one variable&#39;s label attr(mockstudy$age,&#39;label&#39;) ## [1] Age in Years ## See all the variables with a label unlist(lapply(mockstudy,&#39;attr&#39;,&#39;label&#39;)) ## age arm race ## Age in Years Treatment Arm Race ## bmi ## Body Mass Index (kg/m^2) # Can also use labels(mockstudy) If you want to add labels to other variables, there are a couple of options. First, you could add labels to the variables in your dataset. attr(mockstudy$sex,&#39;label&#39;) &lt;- &#39;Gender&#39; tab1 &lt;- tableby(arm ~ sex + age, data=mockstudy) summary(tab1) A: IFL (N=428) F: FOLFOX (N=691) G: IROX (N=380) Total (N=1499) p value Gender 0.190 Male 277 (64.7%) 411 (59.5%) 228 (60.0%) 916 (61.1%) Female 151 (35.3%) 280 (40.5%) 152 (40.0%) 583 (38.9%) Age in Years 0.614 Mean (SD) 59.673 (11.365) 60.301 (11.632) 59.763 (11.499) 59.985 (11.519) Range 27.000 - 88.000 19.000 - 88.000 26.000 - 85.000 19.000 - 88.000 You can also use the built-in data.frame method for labels&lt;-: labels(mockstudy) &lt;- c(age = &#39;Age, yrs&#39;, sex = Gender ) tab1 &lt;- tableby(arm ~ sex + age, data=mockstudy) summary(tab1) A: IFL (N=428) F: FOLFOX (N=691) G: IROX (N=380) Total (N=1499) p value Gender 0.190 Male 277 (64.7%) 411 (59.5%) 228 (60.0%) 916 (61.1%) Female 151 (35.3%) 280 (40.5%) 152 (40.0%) 583 (38.9%) Age, yrs 0.614 Mean (SD) 59.673 (11.365) 60.301 (11.632) 59.763 (11.499) 59.985 (11.519) Range 27.000 - 88.000 19.000 - 88.000 26.000 - 85.000 19.000 - 88.000 Another option is to add labels after you have created the table mylabels &lt;- list(sex = SEX , age = Age, yrs ) summary(tab1, labelTranslations = mylabels) A: IFL (N=428) F: FOLFOX (N=691) G: IROX (N=380) Total (N=1499) p value SEX 0.190 Male 277 (64.7%) 411 (59.5%) 228 (60.0%) 916 (61.1%) Female 151 (35.3%) 280 (40.5%) 152 (40.0%) 583 (38.9%) Age, yrs 0.614 Mean (SD) 59.673 (11.365) 60.301 (11.632) 59.763 (11.499) 59.985 (11.519) Range 27.000 - 88.000 19.000 - 88.000 26.000 - 85.000 19.000 - 88.000 Alternatively, you can check the variable labels and manipulate them with a function called labels, which works on the tableby object. labels(tab1) ## arm sex age ## arm Gender Age, yrs labels(tab1) &lt;- c(arm= Treatment Assignment , age= Baseline Age (yrs) ) labels(tab1) ## arm sex age ## Treatment Assignment Gender Baseline Age (yrs) summary(tab1) A: IFL (N=428) F: FOLFOX (N=691) G: IROX (N=380) Total (N=1499) p value Gender 0.190 Male 277 (64.7%) 411 (59.5%) 228 (60.0%) 916 (61.1%) Female 151 (35.3%) 280 (40.5%) 152 (40.0%) 583 (38.9%) Baseline Age (yrs) 0.614 Mean (SD) 59.673 (11.365) 60.301 (11.632) 59.763 (11.499) 59.985 (11.519) Range 27.000 - 88.000 19.000 - 88.000 26.000 - 85.000 19.000 - 88.000 Change summary statistics globally Currently the default behavior is to summarize continuous variables with: Number of missing values, Mean (SD), 25th - 75th quantiles, and Minimum-Maximum values with an ANOVA (t-test with equal variances) p-value. For categorical variables the default is to show: Number of missing values and count (column percent) with a chi-square p-value. This behavior can be modified using the tableby.control function. In fact, you can save your standard settings and use that for future tables. Note that test=FALSE and total=FALSE results in the total column and p-value column not being printed. mycontrols &lt;- tableby.control(test=FALSE, total=FALSE, numeric.test= kwt , cat.test= chisq , numeric.stats=c( N , median , q1q3 ), cat.stats=c( countpct ), stats.labels=list(N=&#39;Count&#39;, median=&#39;Median&#39;, q1q3=&#39;Q1,Q3&#39;)) tab2 &lt;- tableby(arm ~ sex + age, data=mockstudy, control=mycontrols) summary(tab2) A: IFL (N=428) F: FOLFOX (N=691) G: IROX (N=380) Gender Male 277 (64.7%) 411 (59.5%) 228 (60.0%) Female 151 (35.3%) 280 (40.5%) 152 (40.0%) Age, yrs Count 428 691 380 Median 61.000 61.000 61.000 Q1,Q3 53.000, 68.000 52.000, 69.000 52.000, 68.000 You can also change these settings directly in the tableby call. tab3 &lt;- tableby(arm ~ sex + age, data=mockstudy, test=FALSE, total=FALSE, numeric.stats=c( median , q1q3 ), numeric.test= kwt ) summary(tab3) A: IFL (N=428) F: FOLFOX (N=691) G: IROX (N=380) Gender Male 277 (64.7%) 411 (59.5%) 228 (60.0%) Female 151 (35.3%) 280 (40.5%) 152 (40.0%) Age, yrs Median 61.000 61.000 61.000 Q1, Q3 53.000, 68.000 52.000, 69.000 52.000, 68.000 Change summary statistics within the formula In addition to modifying summary options globally, it is possible to modify the test and summary statistics for specific variables within the formula statement. For example, both the kwt (Kruskal-Wallis rank-based) and anova (asymptotic analysis of variance) tests apply to numeric variables, and we can use one for the variable ‚Äúage‚Äù, another for the variable ‚Äúbmi‚Äù, and no test for the variable ‚Äúast‚Äù. A list of all the options is shown at the end of the vignette. The tests function can do a quick check on what tests were performed on each variable in tableby. tab.test &lt;- tableby(arm ~ kwt(age) + anova(bmi) + notest(ast), data=mockstudy) tests(tab.test) ## Variable p.value Method ## age Age, yrs 0.6390614 Kruskal-Wallis rank sum test ## bmi Body Mass Index (kg/m^2) 0.8916552 Linear Model ANOVA ## ast ast NA No test summary(tab.test) A: IFL (N=428) F: FOLFOX (N=691) G: IROX (N=380) Total (N=1499) p value Age, yrs 0.639 Mean (SD) 59.673 (11.365) 60.301 (11.632) 59.763 (11.499) 59.985 (11.519) Range 27.000 - 88.000 19.000 - 88.000 26.000 - 85.000 19.000 - 88.000 Body Mass Index (kg/m^2) 0.892 N-Miss 9 20 4 33 Mean (SD) 27.290 (5.552) 27.210 (5.173) 27.106 (5.751) 27.206 (5.432) Range 14.053 - 53.008 16.649 - 49.130 15.430 - 60.243 14.053 - 60.243 ast N-Miss 69 141 56 266 Mean (SD) 37.292 (28.036) 35.202 (26.659) 35.670 (25.807) 35.933 (26.843) Range 10.000 - 205.000 7.000 - 174.000 5.000 - 176.000 5.000 - 205.000 Summary statistics for any individual variable can also be modified, but it must be done as secondary arguments to the test function. The function names must be strings that are functions already written for tableby, built-in R functions like mean and range, or user-defined functions. tab.test &lt;- tableby(arm ~ kwt(ast, Nmiss2 , median ) + anova(age, N , mean ) + notest(bmi, Nmiss , median ), data=mockstudy) summary(tab.test) A: IFL (N=428) F: FOLFOX (N=691) G: IROX (N=380) Total (N=1499) p value ast 0.039 N-Miss 69 141 56 266 Median 29.000 25.500 27.000 27.000 Age, yrs 0.614 N 428 691 380 1499 mean 59.7 60.3 59.8 60 Body Mass Index (kg/m^2) N-Miss 9 20 4 33 Median 26.234 26.525 25.978 26.325 Controlling Options for Categorical Tests (Chisq and Fisher‚Äôs) The formal tests for categorical variables against the levels of the by variable, chisq and fe, have options to simulate p-values. We show how to turn on the simulations for these with 500 replicates for the Fisher‚Äôs test (fe). set.seed(100) tab.catsim &lt;- tableby(arm ~ sex + race, cat.test= fe , simulate.p.value=TRUE, B=500, data=mockstudy) tests(tab.catsim) Variable p.value sex Gender 0.2195609 race Race 0.3093812 Method sex Fisher‚Äôs Exact Test for Count Data with simulated p-value(based on 500 replicates) race Fisher‚Äôs Exact Test for Count Data with simulated p-value(based on 500 replicates) The chis-square test on 2x2 tables applies Yates‚Äô continuity correction by default, so we provide an option to turn off the correction. We show the results with and without the correction that is applied to treatment arm by sex, if we use subset to ignore one of the three treatment arms. cat.correct &lt;- tableby(arm ~ sex + race, cat.test= chisq , subset = !grepl( ^F , arm), data=mockstudy) tests(cat.correct) Variable p.value Method sex Gender 0.1666280 Pearson‚Äôs Chi-squared test race Race 0.8108543 Pearson‚Äôs Chi-squared test cat.nocorrect &lt;- tableby(arm ~ sex + race, cat.test= chisq , subset = !grepl( ^F , arm), chisq.correct=FALSE, data=mockstudy) tests(cat.nocorrect) Variable p.value Method sex Gender 0.1666280 Pearson‚Äôs Chi-squared test race Race 0.8108543 Pearson‚Äôs Chi-squared test Modifying the look &amp; feel in Word documents You can easily create Word versions of tableby output via an Rmarkdown report and the default options will give you a reasonable table in Word - just select the ‚ÄúKnit Word‚Äù option in RStudio. The functionality listed in this next paragraph is coming soon but needs an upgraded version of RStudio If you want to modify fonts used for the table, then you‚Äôll need to add an extra line to your header at the beginning of your file. You can take the WordStylesReference01.docx file and modify the fonts (storing the format preferences in your project directory). To see how this works, run your report once using WordStylesReference01.docx and then WordStylesReference02.docx. output: word_document reference_docx: /projects/bsi/gentools/R/lib320/arsenal/doc/WordStylesReference01.docx For more informating on changing the look/feel of your Word document, see the Rmarkdown documentation website. Additional Examples Here are multiple examples showing how to use some of the different options. 1. Summarize without a group/by variable tab.noby &lt;- tableby(~ bmi + sex + age, data=mockstudy) summary(tab.noby) Overall (N=1499) Body Mass Index (kg/m^2) N-Miss 33 Mean (SD) 27.206 (5.432) Range 14.053 - 60.243 Gender Male 916 (61.1%) Female 583 (38.9%) Age, yrs Mean (SD) 59.985 (11.519) Range 19.000 - 88.000 2. Display footnotes indicating which ‚Äútest‚Äù was used summary(tab.test) #, pfootnote=TRUE) A: IFL (N=428) F: FOLFOX (N=691) G: IROX (N=380) Total (N=1499) p value ast 0.039 N-Miss 69 141 56 266 Median 29.000 25.500 27.000 27.000 Age, yrs 0.614 N 428 691 380 1499 mean 59.7 60.3 59.8 60 Body Mass Index (kg/m^2) N-Miss 9 20 4 33 Median 26.234 26.525 25.978 26.325 3. Summarize an ordered factor When comparing groups of ordered data there are a couple of options. The default uses a general independence test available from the coin package. For two-group comparisons, this is essentially the Armitage trend test. The other option is to specify the Kruskal Wallis test. The example below shows both options. mockstudy$age.ordnew &lt;- ordered(c( a ,NA,as.character(mockstudy$age.ord[-(1:2)]))) table(mockstudy$age.ord, mockstudy$sex) ## ## Male Female ## 10-19 1 0 ## 20-29 8 11 ## 30-39 37 30 ## 40-49 127 83 ## 50-59 257 179 ## 60-69 298 170 ## 70-79 168 101 ## 80-89 20 9 table(mockstudy$age.ordnew, mockstudy$sex) ## ## Male Female ## 10-19 1 0 ## 20-29 8 11 ## 30-39 37 30 ## 40-49 127 83 ## 50-59 257 179 ## 60-69 297 170 ## 70-79 168 100 ## 80-89 20 9 ## a 1 0 class(mockstudy$age.ord) ## [1] ordered factor summary(tableby(sex ~ age.ordnew, data = mockstudy)) #, pfootnote = TRUE) Male (N=916) Female (N=583) Total (N=1499) p value age.ordnew 0.040 N-Miss 0 1 1 10-19 1 (0.1%) 0 (0.0%) 1 (0.1%) 20-29 8 (0.9%) 11 (1.9%) 19 (1.3%) 30-39 37 (4.0%) 30 (5.2%) 67 (4.5%) 40-49 127 (13.9%) 83 (14.3%) 210 (14.0%) 50-59 257 (28.1%) 179 (30.8%) 436 (29.1%) 60-69 297 (32.4%) 170 (29.2%) 467 (31.2%) 70-79 168 (18.3%) 100 (17.2%) 268 (17.9%) 80-89 20 (2.2%) 9 (1.5%) 29 (1.9%) a 1 (0.1%) 0 (0.0%) 1 (0.1%) summary(tableby(sex ~ kwt(age.ord), data = mockstudy)) #) #, pfootnote = TRUE) Male (N=916) Female (N=583) Total (N=1499) p value age.ord 0.067 10-19 1 (0.1%) 0 (0.0%) 1 (0.1%) 20-29 8 (0.9%) 11 (1.9%) 19 (1.3%) 30-39 37 (4.0%) 30 (5.1%) 67 (4.5%) 40-49 127 (13.9%) 83 (14.2%) 210 (14.0%) 50-59 257 (28.1%) 179 (30.7%) 436 (29.1%) 60-69 298 (32.5%) 170 (29.2%) 468 (31.2%) 70-79 168 (18.3%) 101 (17.3%) 269 (17.9%) 80-89 20 (2.2%) 9 (1.5%) 29 (1.9%) 4. Summarize a survival variable First look at the information that is presented by the survfit() function, then see how the same results can be seen with tableby. The default is to show the median survival (time at which the probability of survival = 50%). survfit(Surv(fu.time, fu.stat)~sex, data=mockstudy) ## Call: survfit(formula = Surv(fu.time, fu.stat) ~ sex, data = mockstudy) ## ## n events median 0.95LCL 0.95UCL ## sex=Male 916 829 550 515 590 ## sex=Female 583 527 543 511 575 survdiff(Surv(fu.time, fu.stat)~sex, data=mockstudy) ## Call: ## survdiff(formula = Surv(fu.time, fu.stat) ~ sex, data = mockstudy) ## ## N Observed Expected (O-E)^2/E (O-E)^2/V ## sex=Male 916 829 830 0.000370 0.000956 ## sex=Female 583 527 526 0.000583 0.000956 ## ## Chisq= 0 on 1 degrees of freedom, p= 1 summary(tableby(sex ~ Surv(fu.time, fu.stat), data=mockstudy)) Male (N=916) Female (N=583) Total (N=1499) p value Surv(fu.time, fu.stat) 0.975 Events 829 527 1356 Median Survival 550.000 543.000 546.000 It is also possible to obtain summaries of the % survival at certain time points (say the probability of surviving 1-year). summary(survfit(Surv(fu.time/365.25, fu.stat)~sex, data=mockstudy), times=1:5) ## Call: survfit(formula = Surv(fu.time/365.25, fu.stat) ~ sex, data = mockstudy) ## ## sex=Male ## time n.risk n.event survival std.err lower 95% CI upper 95% CI ## 1 626 286 0.6870 0.0153 0.6576 0.7177 ## 2 309 311 0.3437 0.0158 0.3142 0.3761 ## 3 152 151 0.1748 0.0127 0.1516 0.2015 ## 4 57 61 0.0941 0.0104 0.0759 0.1168 ## 5 24 16 0.0628 0.0095 0.0467 0.0844 ## ## sex=Female ## time n.risk n.event survival std.err lower 95% CI upper 95% CI ## 1 380 202 0.6531 0.0197 0.6155 0.693 ## 2 190 189 0.3277 0.0195 0.2917 0.368 ## 3 95 90 0.1701 0.0157 0.1420 0.204 ## 4 51 32 0.1093 0.0133 0.0861 0.139 ## 5 18 12 0.0745 0.0126 0.0534 0.104 summary(tableby(sex ~ Surv(fu.time/365.25, fu.stat), data=mockstudy, times=1:5, surv.stats=c( NeventsSurv , NriskSurv ))) Male (N=916) Female (N=583) Total (N=1499) p value Surv(fu.time/365.25, fu.stat) 0.975 time = 1 286 (68.7) 202 (65.3) 488 (67.4) time = 2 597 (34.4) 391 (32.8) 988 (33.7) time = 3 748 (17.5) 481 (17.0) 1229 (17.3) time = 4 809 (9.4) 513 (10.9) 1322 (10.1) time = 5 825 (6.3) 525 (7.4) 1350 (6.8) time = 1 626 380 1006 time = 2 309 190 499 time = 3 152 95 247 time = 4 57 51 108 time = 5 24 18 42 5. Summarize date variables Date variables by default are summarized with the number of missing values, the median, and the range. For example purposes we‚Äôve created a random date. Missing values are introduced for impossible February dates. set.seed(100) N &lt;- nrow(mockstudy) mockstudy$dtentry &lt;- mdy.Date(month=sample(1:12,N,replace=T), day=sample(1:29,N,replace=T), year=sample(2005:2009,N,replace=T)) summary(tableby(sex ~ dtentry, data=mockstudy)) Male (N=916) Female (N=583) Total (N=1499) p value dtentry 0.554 N-Miss 3 2 5 Median 2007-06-16 2007-06-15 2007-06-15 Range 2005-01-03 - 2009-12-27 2005-01-01 - 2009-12-28 2005-01-01 - 2009-12-28 6. Summarize multiple variables without typing them out Often one wants to summarize a number of variables. Instead of typing by hand each individual variable, an alternative approach is to create a formula using the paste command with the collapse= + option. ## create a vector specifying the variable names myvars &lt;- names(mockstudy) ## select the 8th through the last variables ## paste them together, separated by the + sign RHS &lt;- paste(myvars[8:10], collapse= + ) RHS [1] ‚Äúps+hgb+bmi‚Äù ## create a formula using the as.formula function as.formula(paste(&#39;arm ~ &#39;, RHS)) arm ~ ps + hgb + bmi ## use the formula in the tableby function summary(tableby(as.formula(paste(&#39;arm ~&#39;, RHS)), data=mockstudy)) A: IFL (N=428) F: FOLFOX (N=691) G: IROX (N=380) Total (N=1499) p value ps 0.903 N-Miss 69 141 56 266 Mean (SD) 0.529 (0.597) 0.547 (0.595) 0.537 (0.606) 0.539 (0.598) Range 0.000 - 2.000 0.000 - 2.000 0.000 - 2.000 0.000 - 2.000 hgb 0.639 N-Miss 69 141 56 266 Mean (SD) 12.276 (1.686) 12.381 (1.763) 12.373 (1.680) 12.348 (1.719) Range 9.060 - 17.300 9.000 - 18.200 9.000 - 17.000 9.000 - 18.200 Body Mass Index (kg/m^2) 0.892 N-Miss 9 20 4 33 Mean (SD) 27.290 (5.552) 27.210 (5.173) 27.106 (5.751) 27.206 (5.432) Range 14.053 - 53.008 16.649 - 49.130 15.430 - 60.243 14.053 - 60.243 These steps can also be done using the formulize function. ## The formulize function does the paste and as.formula steps tmp &lt;- formulize(&#39;arm&#39;,myvars[8:10]) tmp arm ~ ps + hgb + bmi ## More complex formulas could also be written using formulize tmp2 &lt;- formulize(&#39;arm&#39;,c(&#39;ps&#39;,&#39;hgb^2&#39;,&#39;bmi&#39;)) ## use the formula in the tableby function summary(tableby(tmp, data=mockstudy)) A: IFL (N=428) F: FOLFOX (N=691) G: IROX (N=380) Total (N=1499) p value ps 0.903 N-Miss 69 141 56 266 Mean (SD) 0.529 (0.597) 0.547 (0.595) 0.537 (0.606) 0.539 (0.598) Range 0.000 - 2.000 0.000 - 2.000 0.000 - 2.000 0.000 - 2.000 hgb 0.639 N-Miss 69 141 56 266 Mean (SD) 12.276 (1.686) 12.381 (1.763) 12.373 (1.680) 12.348 (1.719) Range 9.060 - 17.300 9.000 - 18.200 9.000 - 17.000 9.000 - 18.200 Body Mass Index (kg/m^2) 0.892 N-Miss 9 20 4 33 Mean (SD) 27.290 (5.552) 27.210 (5.173) 27.106 (5.751) 27.206 (5.432) Range 14.053 - 53.008 16.649 - 49.130 15.430 - 60.243 14.053 - 60.243 7. Subset the dataset used in the analysis Here are two ways to get the same result (limit the analysis to subjects age&gt;5 and in the F: FOLFOX treatment group). The first approach uses the subset function applied to the dataset mockstudy. This example also selects a subset of variables. The tableby function is then applied to this subsetted data. newdata &lt;- subset(mockstudy, subset=age&gt;50 &amp; arm==&#39;F: FOLFOX&#39;, select = c(sex,ps:bmi)) dim(mockstudy) ## [1] 1499 16 table(mockstudy$arm) ## ## A: IFL F: FOLFOX G: IROX ## 428 691 380 dim(newdata) ## [1] 557 4 names(newdata) ## [1] sex ps hgb bmi summary(tableby(sex ~ ., data=newdata)) Male (N=333) Female (N=224) Total (N=557) p value ps 0.652 N-Miss 64 44 108 Mean (SD) 0.554 (0.600) 0.528 (0.602) 0.543 (0.600) Range 0.000 - 2.000 0.000 - 2.000 0.000 - 2.000 hgb &lt; 0.001 N-Miss 64 44 108 Mean (SD) 12.720 (1.925) 12.063 (1.395) 12.457 (1.760) Range 9.000 - 18.200 9.100 - 15.900 9.000 - 18.200 bmi 0.650 N-Miss 9 6 15 Mean (SD) 27.539 (4.780) 27.337 (5.508) 27.458 (5.081) Range 17.927 - 47.458 16.649 - 49.130 16.649 - 49.130 The second approach does the same analysis but uses the subset argument within tableby to subset the data. summary(tableby(sex ~ ps + hgb + bmi, subset=age&gt;50 &amp; arm== F: FOLFOX , data=mockstudy)) Male (N=333) Female (N=224) Total (N=557) p value ps 0.652 N-Miss 64 44 108 Mean (SD) 0.554 (0.600) 0.528 (0.602) 0.543 (0.600) Range 0.000 - 2.000 0.000 - 2.000 0.000 - 2.000 hgb &lt; 0.001 N-Miss 64 44 108 Mean (SD) 12.720 (1.925) 12.063 (1.395) 12.457 (1.760) Range 9.000 - 18.200 9.100 - 15.900 9.000 - 18.200 Body Mass Index (kg/m^2) 0.650 N-Miss 9 6 15 Mean (SD) 27.539 (4.780) 27.337 (5.508) 27.458 (5.081) Range 17.927 - 47.458 16.649 - 49.130 16.649 - 49.130 8. Create combinations of variables on the fly ## create a variable combining the levels of mdquality.s and sex with(mockstudy, table(interaction(mdquality.s,sex))) ## ## 0.Male 1.Male 0.Female 1.Female ## 77 686 47 437 summary(tableby(arm ~ interaction(mdquality.s,sex), data=mockstudy)) A: IFL (N=428) F: FOLFOX (N=691) G: IROX (N=380) Total (N=1499) p value interaction(mdquality.s, sex) 0.493 N-Miss 55 156 41 252 0.Male 29 (7.8%) 31 (5.8%) 17 (5.0%) 77 (6.2%) 1.Male 214 (57.4%) 285 (53.3%) 187 (55.2%) 686 (55.0%) 0.Female 12 (3.2%) 21 (3.9%) 14 (4.1%) 47 (3.8%) 1.Female 118 (31.6%) 198 (37.0%) 121 (35.7%) 437 (35.0%) ## create a new grouping variable with combined levels of arm and sex summary(tableby(interaction(mdquality.s, sex) ~ age + bmi, data=mockstudy, subset=arm== F: FOLFOX )) 0.Male (N=31) 1.Male (N=285) 0.Female (N=21) 1.Female (N=198) Total (N=535) p value Age, yrs 0.190 Mean (SD) 63.065 (11.702) 60.653 (11.833) 60.810 (10.103) 58.924 (11.366) 60.159 (11.612) Range 41.000 - 82.000 19.000 - 88.000 42.000 - 81.000 29.000 - 83.000 19.000 - 88.000 Body Mass Index (kg/m^2) 0.894 N-Miss 0 6 1 5 12 Mean (SD) 26.633 (5.094) 27.387 (4.704) 27.359 (4.899) 27.294 (5.671) 27.307 (5.100) Range 20.177 - 41.766 17.927 - 47.458 19.801 - 39.369 16.799 - 44.841 16.799 - 47.458 9. Transform variables on the fly Certain transformations need to be surrounded by I() so that R knows to treat it as a variable transformation and not some special model feature. If the transformation includes any of the symbols / - + ^ * then surround the new variable by I(). trans &lt;- tableby(arm ~ I(age/10) + log(bmi) + factor(mdquality.s, levels=0:1, labels=c(&#39;N&#39;,&#39;Y&#39;)), data=mockstudy) summary(trans) A: IFL (N=428) F: FOLFOX (N=691) G: IROX (N=380) Total (N=1499) p value Age, yrs 0.614 Mean (SD) 5.967 (1.136) 6.030 (1.163) 5.976 (1.150) 5.999 (1.152) Range 2.700 - 8.800 1.900 - 8.800 2.600 - 8.500 1.900 - 8.800 Body Mass Index (kg/m^2) 0.811 N-Miss 9 20 4 33 Mean (SD) 3.287 (0.197) 3.286 (0.183) 3.279 (0.200) 3.285 (0.192) Range 2.643 - 3.970 2.812 - 3.894 2.736 - 4.098 2.643 - 4.098 factor(mdquality.s, levels = 0:1, labels = c(‚ÄúN‚Äù, ‚ÄúY‚Äù)) 0.694 N-Miss 55 156 41 252 N 41 (11.0%) 52 (9.7%) 31 (9.1%) 124 (9.9%) Y 332 (89.0%) 483 (90.3%) 308 (90.9%) 1123 (90.1%) The labels for these variables isn‚Äôt exactly what we‚Äôd like so we can change modify those after the fact. Instead of typing out the very long variable names you can modify specific labels by position. labels(trans) ## arm ## arm ## I(age/10) ## Age, yrs ## log(bmi) ## Body Mass Index (kg/m^2) ## factor(mdquality.s, levels = 0:1, labels = c( N , Y )) ## factor(mdquality.s, levels = 0:1, labels = c(\\ N\\ , \\ Y\\ )) labels(trans)[2:4] &lt;- c(&#39;Age per 10 yrs&#39;, &#39;log(BMI)&#39;, &#39;MD Quality&#39;) labels(trans) ## arm ## arm ## I(age/10) ## Age per 10 yrs ## log(bmi) ## log(BMI) ## factor(mdquality.s, levels = 0:1, labels = c( N , Y )) ## MD Quality summary(trans) A: IFL (N=428) F: FOLFOX (N=691) G: IROX (N=380) Total (N=1499) p value Age per 10 yrs 0.614 Mean (SD) 5.967 (1.136) 6.030 (1.163) 5.976 (1.150) 5.999 (1.152) Range 2.700 - 8.800 1.900 - 8.800 2.600 - 8.500 1.900 - 8.800 log(BMI) 0.811 N-Miss 9 20 4 33 Mean (SD) 3.287 (0.197) 3.286 (0.183) 3.279 (0.200) 3.285 (0.192) Range 2.643 - 3.970 2.812 - 3.894 2.736 - 4.098 2.643 - 4.098 MD Quality 0.694 N-Miss 55 156 41 252 N 41 (11.0%) 52 (9.7%) 31 (9.1%) 124 (9.9%) Y 332 (89.0%) 483 (90.3%) 308 (90.9%) 1123 (90.1%) Note that if we had not changed mdquality.s to a factor, it would have been summarized as though it were a continuous variable. class(mockstudy$mdquality.s) [1] ‚Äúinteger‚Äù summary(tableby(arm~mdquality.s, data=mockstudy)) A: IFL (N=428) F: FOLFOX (N=691) G: IROX (N=380) Total (N=1499) p value mdquality.s 0.695 N-Miss 55 156 41 252 Mean (SD) 0.890 (0.313) 0.903 (0.297) 0.909 (0.289) 0.901 (0.299) Range 0.000 - 1.000 0.000 - 1.000 0.000 - 1.000 0.000 - 1.000 Another option would be to specify the test and summary statistics. In fact, if I had a set of variables coded 0/1 and that was all I was summarizing, then I could change the global option for continuous variables to use the chi-square test and show countpct. summary(tableby(arm ~ chisq(mdquality.s, Nmiss , countpct ), data=mockstudy)) A: IFL (N=428) F: FOLFOX (N=691) G: IROX (N=380) Total (N=1499) p value mdquality.s 0.694 N-Miss 55 156 41 252 0 41 (11.0%) 52 (9.7%) 31 (9.1%) 124 (9.9%) 1 332 (89.0%) 483 (90.3%) 308 (90.9%) 1123 (90.1%) 10. Subsetting (change the ordering of the variables, delete a variable, sort by p-value, filter by p-value) mytab &lt;- tableby(arm ~ sex + alk.phos + age, data=mockstudy) mytab2 &lt;- mytab[c(&#39;age&#39;,&#39;sex&#39;,&#39;alk.phos&#39;)] summary(mytab2) A: IFL (N=428) F: FOLFOX (N=691) G: IROX (N=380) Total (N=1499) p value Age, yrs 0.614 Mean (SD) 59.673 (11.365) 60.301 (11.632) 59.763 (11.499) 59.985 (11.519) Range 27.000 - 88.000 19.000 - 88.000 26.000 - 85.000 19.000 - 88.000 Gender 0.190 Male 277 (64.7%) 411 (59.5%) 228 (60.0%) 916 (61.1%) Female 151 (35.3%) 280 (40.5%) 152 (40.0%) 583 (38.9%) alk.phos 0.226 N-Miss 69 141 56 266 Mean (SD) 175.577 (128.608) 161.984 (121.978) 173.506 (138.564) 168.969 (128.492) Range 11.000 - 858.000 10.000 - 1014.000 7.000 - 982.000 7.000 - 1014.000 summary(mytab[c(&#39;age&#39;,&#39;sex&#39;)], digits = 2) A: IFL (N=428) F: FOLFOX (N=691) G: IROX (N=380) Total (N=1499) p value Age, yrs 0.614 Mean (SD) 59.67 (11.36) 60.30 (11.63) 59.76 (11.50) 59.99 (11.52) Range 27.00 - 88.00 19.00 - 88.00 26.00 - 85.00 19.00 - 88.00 Gender 0.190 Male 277 (64.7%) 411 (59.5%) 228 (60.0%) 916 (61.1%) Female 151 (35.3%) 280 (40.5%) 152 (40.0%) 583 (38.9%) summary(mytab[c(3,1)], digits = 3) A: IFL (N=428) F: FOLFOX (N=691) G: IROX (N=380) Total (N=1499) p value Age, yrs 0.614 Mean (SD) 59.673 (11.365) 60.301 (11.632) 59.763 (11.499) 59.985 (11.519) Range 27.000 - 88.000 19.000 - 88.000 26.000 - 85.000 19.000 - 88.000 Gender 0.190 Male 277 (64.7%) 411 (59.5%) 228 (60.0%) 916 (61.1%) Female 151 (35.3%) 280 (40.5%) 152 (40.0%) 583 (38.9%) summary(sort(mytab, decreasing = TRUE)) A: IFL (N=428) F: FOLFOX (N=691) G: IROX (N=380) Total (N=1499) p value Age, yrs 0.614 Mean (SD) 59.673 (11.365) 60.301 (11.632) 59.763 (11.499) 59.985 (11.519) Range 27.000 - 88.000 19.000 - 88.000 26.000 - 85.000 19.000 - 88.000 alk.phos 0.226 N-Miss 69 141 56 266 Mean (SD) 175.577 (128.608) 161.984 (121.978) 173.506 (138.564) 168.969 (128.492) Range 11.000 - 858.000 10.000 - 1014.000 7.000 - 982.000 7.000 - 1014.000 Gender 0.190 Male 277 (64.7%) 411 (59.5%) 228 (60.0%) 916 (61.1%) Female 151 (35.3%) 280 (40.5%) 152 (40.0%) 583 (38.9%) summary(mytab[mytab &lt; 0.5]) A: IFL (N=428) F: FOLFOX (N=691) G: IROX (N=380) Total (N=1499) p value Gender 0.190 Male 277 (64.7%) 411 (59.5%) 228 (60.0%) 916 (61.1%) Female 151 (35.3%) 280 (40.5%) 152 (40.0%) 583 (38.9%) alk.phos 0.226 N-Miss 69 141 56 266 Mean (SD) 175.577 (128.608) 161.984 (121.978) 173.506 (138.564) 168.969 (128.492) Range 11.000 - 858.000 10.000 - 1014.000 7.000 - 982.000 7.000 - 1014.000 head(mytab, 1) # can also use tail() Tableby Object Function Call: tableby(formula = arm ~ sex + alk.phos + age, data = mockstudy) y variable: [1] ‚Äúarm‚Äù x variables: [1] ‚Äúsex‚Äù 11. Merge two tableby objects together It is possible to combine two tableby objects so that they print out together. ## demographics tab1 &lt;- tableby(arm ~ sex + age, data=mockstudy, control=tableby.control(numeric.stats=c( Nmiss , meansd ), total=FALSE)) ## lab data tab2 &lt;- tableby(arm ~ hgb + alk.phos, data=mockstudy, control=tableby.control(numeric.stats=c( Nmiss , median , q1q3 ), numeric.test= kwt , total=FALSE)) names(tab1$x) [1] ‚Äúsex‚Äù ‚Äúage‚Äù names(tab2$x) [1] ‚Äúhgb‚Äù ‚Äúalk.phos‚Äù tab12 &lt;- merge(tab1,tab2) class(tab12) [1] ‚Äútableby‚Äù names(tab12$x) [1] ‚Äúsex‚Äù ‚Äúage‚Äù ‚Äúhgb‚Äù ‚Äúalk.phos‚Äù summary(tab12) #, pfootnote=TRUE) A: IFL (N=428) F: FOLFOX (N=691) G: IROX (N=380) p value Gender 0.190 Male 277 (64.7%) 411 (59.5%) 228 (60.0%) Female 151 (35.3%) 280 (40.5%) 152 (40.0%) Age, yrs 0.614 Mean (SD) 59.673 (11.365) 60.301 (11.632) 59.763 (11.499) hgb 0.570 N-Miss 69 141 56 Median 12.100 12.200 12.400 Q1, Q3 11.000, 13.450 11.100, 13.600 11.175, 13.625 alk.phos 0.104 N-Miss 69 141 56 Median 133.000 116.000 122.000 Q1, Q3 89.000, 217.000 85.000, 194.750 87.750, 210.250 12. Add a title to the table When creating a pdf the tables are automatically numbered and the title appears below the table. In Word and HTML, the titles appear un-numbered and above the table. t1 &lt;- tableby(arm ~ sex + age, data=mockstudy) summary(t1, title=&#39;Demographics&#39;) Demographics A: IFL (N=428) F: FOLFOX (N=691) G: IROX (N=380) Total (N=1499) p value Gender 0.190 Male 277 (64.7%) 411 (59.5%) 228 (60.0%) 916 (61.1%) Female 151 (35.3%) 280 (40.5%) 152 (40.0%) 583 (38.9%) Age, yrs 0.614 Mean (SD) 59.673 (11.365) 60.301 (11.632) 59.763 (11.499) 59.985 (11.519) Range 27.000 - 88.000 19.000 - 88.000 26.000 - 85.000 19.000 - 88.000 13. Modify how missing values are displayed Depending on the report you are writing you have the following options: Show how many subjects have each variable Show how many subjects are missing each variable Show how many subjects are missing each variable only if there are any missing values Don‚Äôt indicate missing values at all ## look at how many missing values there are for each variable apply(is.na(mockstudy),2,sum) ## case age arm sex race fu.time fu.stat ps ## 0 0 0 0 7 0 0 266 ## hgb bmi alk.phos ast mdquality.s age.ord age.ordnew dtentry ## 266 33 266 266 252 0 1 5 ## Show how many subjects have each variable (non-missing) summary(tableby(sex ~ ast + age, data=mockstudy, control=tableby.control(numeric.stats=c( N , median ), total=FALSE))) Male (N=916) Female (N=583) p value ast 0.921 N 754 479 Median 27.000 27.000 Age, yrs 0.048 N 916 583 Median 61.000 60.000 ## Always list the number of missing values summary(tableby(sex ~ ast + age, data=mockstudy, control=tableby.control(numeric.stats=c( Nmiss2 , median ), total=FALSE))) Male (N=916) Female (N=583) p value ast 0.921 N-Miss 162 104 Median 27.000 27.000 Age, yrs 0.048 N-Miss 0 0 Median 61.000 60.000 ## Only show the missing values if there are some (default) summary(tableby(sex ~ ast + age, data=mockstudy, control=tableby.control(numeric.stats=c( Nmiss , mean ),total=FALSE))) Male (N=916) Female (N=583) p value ast 0.921 N-Miss 162 104 mean 35.9 36 Age, yrs 0.048 mean 60.5 59.2 ## Don&#39;t show N at all summary(tableby(sex ~ ast + age, data=mockstudy, control=tableby.control(numeric.stats=c( mean ),total=FALSE))) Male (N=916) Female (N=583) p value ast 0.921 mean 35.9 36 Age, yrs 0.048 mean 60.5 59.2 One might also consider the use of includeNA() to include NAs in the counts and percents for categorical variables. mockstudy$ps.cat &lt;- factor(mockstudy$ps) attr(mockstudy$ps.cat, label ) &lt;- ps summary(tableby(sex ~ includeNA(ps.cat), data = mockstudy, cat.stats = countpct )) Male (N=916) Female (N=583) Total (N=1499) p value ps 0.354 0 391 (42.7%) 244 (41.9%) 635 (42.4%) 1 329 (35.9%) 202 (34.6%) 531 (35.4%) 2 34 (3.7%) 33 (5.7%) 67 (4.5%) (Missing) 162 (17.7%) 104 (17.8%) 266 (17.7%) 14. Modify the number of digits used Within tableby.control function there are 4 options for controlling the number of significant digits shown. digits: controls the number of digits after the decimal place for continuous values digits.count: controls the number of digits after the decimal point for counts digits.pct: controls the number of digits after the decimal point for percents digits.p: controls the number of digits after the decimal point for p-values summary(tableby(arm ~ sex + age + fu.time, data=mockstudy), digits=4, digits.p=2, digits.pct=1) A: IFL (N=428) F: FOLFOX (N=691) G: IROX (N=380) Total (N=1499) p value Gender 0.19 Male 277 (64.7%) 411 (59.5%) 228 (60.0%) 916 (61.1%) Female 151 (35.3%) 280 (40.5%) 152 (40.0%) 583 (38.9%) Age, yrs 0.61 Mean (SD) 59.6729 (11.3645) 60.3010 (11.6323) 59.7632 (11.4993) 59.9853 (11.5188) Range 27.0000 - 88.0000 19.0000 - 88.0000 26.0000 - 85.0000 19.0000 - 88.0000 fu.time &lt; 0.01 Mean (SD) 553.5841 (419.6065) 731.2460 (487.7443) 607.2421 (435.5092) 649.0841 (462.5109) Range 9.0000 - 2170.0000 0.0000 - 2472.0000 17.0000 - 2118.0000 0.0000 - 2472.0000 With the exception of digits.p, all of these can be specified on a per-variable basis using the in-formula functions that specify which tests are run: summary(tableby(arm ~ chisq(sex, digits.pct=1) + anova(age, digits=4) + anova(fu.time, digits = 1), data=mockstudy)) A: IFL (N=428) F: FOLFOX (N=691) G: IROX (N=380) Total (N=1499) p value Gender 0.190 Male 277 (64.7%) 411 (59.5%) 228 (60.0%) 916 (61.1%) Female 151 (35.3%) 280 (40.5%) 152 (40.0%) 583 (38.9%) Age, yrs 0.614 Mean (SD) 59.6729 (11.3645) 60.3010 (11.6323) 59.7632 (11.4993) 59.9853 (11.5188) Range 27.0000 - 88.0000 19.0000 - 88.0000 26.0000 - 85.0000 19.0000 - 88.0000 fu.time &lt; 0.001 Mean (SD) 553.6 (419.6) 731.2 (487.7) 607.2 (435.5) 649.1 (462.5) Range 9.0 - 2170.0 0.0 - 2472.0 17.0 - 2118.0 0.0 - 2472.0 15. Create a user-defined summary statistic For purposes of this example, the code below creates a trimmed mean function (trims 10%) and use that to summarize the data. Note the use of the ... which tells R to pass extra arguments on - this is required for user-defined functions. In this case, na.rm=T is passed to myfunc. The weights argument is also required, even though it isn‚Äôt passed on to the internal function in this particular example. myfunc &lt;- function(x, weights=rep(1,length(x)), ...){ mean(x, trim=.1, ...) } summary(tableby(sex ~ hgb, data=mockstudy, control=tableby.control(numeric.stats=c( Nmiss , myfunc ), numeric.test= kwt , stats.labels=list(Nmiss=&#39;Missing values&#39;, myfunc= Trimmed Mean, 10% )))) Male (N=916) Female (N=583) Total (N=1499) p value hgb &lt; 0.001 Missing values 162 104 266 Trimmed Mean, 10% 12.6 11.9 NA 16. Use case-weights for creating summary statistics When comparing groups, they are often unbalanced when it comes to nuisances such as age and sex. The tableby function allows you to create weighted summary statistics. If this option us used then p-values are not calculated (test=FALSE). ##create fake group that is not balanced by age/sex set.seed(200) mockstudy$fake_arm &lt;- ifelse(mockstudy$age&gt;60 &amp; mockstudy$sex==&#39;Female&#39;,sample(c(&#39;A&#39;,&#39;B&#39;),replace=T, prob=c(.2,.8)), sample(c(&#39;A&#39;,&#39;B&#39;),replace=T, prob=c(.8,.4))) mockstudy$agegp &lt;- cut(mockstudy$age, breaks=c(18,50,60,70,90), right=FALSE) ## create weights based on agegp and sex distribution tab1 &lt;- with(mockstudy,table(agegp, sex)) tab2 &lt;- with(mockstudy, table(agegp, sex, fake_arm)) tab2 ## , , fake_arm = A ## ## sex ## agegp Male Female ## [18,50) 73 62 ## [50,60) 128 94 ## [60,70) 139 7 ## [70,90) 102 0 ## ## , , fake_arm = B ## ## sex ## agegp Male Female ## [18,50) 79 48 ## [50,60) 130 84 ## [60,70) 156 166 ## [70,90) 109 122 gpwts &lt;- rep(tab1, length(unique(mockstudy$fake_arm)))/tab2 gpwts[gpwts&gt;50] &lt;- 30 ## apply weights to subjects index &lt;- with(mockstudy, cbind(as.numeric(agegp), as.numeric(sex), as.numeric(as.factor(fake_arm)))) mockstudy$wts &lt;- gpwts[index] ## show weights by treatment arm group tapply(mockstudy$wts,mockstudy$fake_arm, summary) ## $A ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 1.774 1.894 2.069 2.276 2.082 24.714 ## ## $B ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 1.000 1.042 1.924 1.677 1.985 2.292 orig &lt;- tableby(fake_arm ~ age + sex + Surv(fu.time/365, fu.stat), data=mockstudy, test=FALSE) summary(orig, title=&#39;No Case Weights used&#39;) No Case Weights used A (N=605) B (N=894) Total (N=1499) Age, yrs Mean (SD) 57.413 (11.618) 61.726 (11.125) 59.985 (11.519) Range 22.000 - 85.000 19.000 - 88.000 19.000 - 88.000 Gender Male 442 (73.1%) 474 (53.0%) 916 (61.1%) Female 163 (26.9%) 420 (47.0%) 583 (38.9%) Surv(fu.time/365, fu.stat) Events 554 802 1356 Median Survival 1.504 1.493 1.496 tab1 &lt;- tableby(fake_arm ~ age + sex + Surv(fu.time/365, fu.stat), data=mockstudy, weights=wts) summary(tab1, title=&#39;Case Weights used&#39;) Case Weights used A (N=605) B (N=894) Total (N=1499) Age, yrs Mean (SD) 58.009 (10.925) 60.151 (11.428) 59.126 (11.235) Range 22.000 - 85.000 19.000 - 88.000 19.000 - 88.000 Gender Male 916 (66.5%) 916 (61.1%) 1832 (63.7%) Female 461 (33.5%) 583 (38.9%) 1044 (36.3%) Surv(fu.time/365, fu.stat) Events 1252 1348 2599 Median Survival 1.534 1.496 1.532 17. Create your own p-value and add it to the table When using weighted summary statistics, it is often desirable to then show a p-value from a model that corresponds to the weighted analysis. It is possible to add your own p-value and modify the column title for that new p-value. Another use for this would be to add standardized differences or confidence intervals instead of a p-value. To add the p-value you simply need to create a data frame and use the function modpval.tableby. The first 2 columns in the dataframe are required and are the variable name and the new p-value. The third column can be used to indicate what method was used to calculate the p-value. If you specify use.pname=TRUE then the column name indicating the p-value will be also be used in the tableby summary. mypval &lt;- data.frame(variable=c(&#39;age&#39;,&#39;sex&#39;,&#39;Surv(fu.time/365, fu.stat)&#39;), adj.pvalue=c(.953,.811,.01), method=c(&#39;Age/Sex adjusted model results&#39;)) tab2 &lt;- modpval.tableby(tab1, mypval, use.pname=TRUE) summary(tab2, title=&#39;Case Weights used, p-values added&#39;) #, pfootnote=TRUE) Case Weights used, p-values added A (N=605) B (N=894) Total (N=1499) adj.pvalue Age, yrs 0.953 Mean (SD) 58.009 (10.925) 60.151 (11.428) 59.126 (11.235) Range 22.000 - 85.000 19.000 - 88.000 19.000 - 88.000 Gender 0.811 Male 916 (66.5%) 916 (61.1%) 1832 (63.7%) Female 461 (33.5%) 583 (38.9%) 1044 (36.3%) Surv(fu.time/365, fu.stat) 0.010 Events 1252 1348 2599 Median Survival 1.534 1.496 1.532 18. For two-level categorical variables or one-line numeric variables, simplify the output. If the cat.simplify option is set to TRUE, then only the second level of two-level categorical varialbes is shown. In the example below, sex has two levels, and ‚ÄúFemale‚Äù is the second level, hence only the counts and percents for Female are shown. Similarly, ‚Äúmdquality.s‚Äù was turned to a factor, and ‚Äú1‚Äù is the second level, but since there are missings, the table ignores cat.simplify and displays all levels (since the output can no longer be displayed on one line). table2 &lt;- tableby(arm~sex + factor(mdquality.s), data=mockstudy, cat.simplify=TRUE) summary(table2, labelTranslations=c(sex= Female , factor(mdquality.s) = MD Quality )) A: IFL (N=428) F: FOLFOX (N=691) G: IROX (N=380) Total (N=1499) p value Female 151 (35.3%) 280 (40.5%) 152 (40.0%) 583 (38.9%) 0.190 MD Quality 0.694 N-Miss 55 156 41 252 0 41 (11.0%) 52 (9.7%) 31 (9.1%) 124 (9.9%) 1 332 (89.0%) 483 (90.3%) 308 (90.9%) 1123 (90.1%) Similarly, if numeric.simplify is set to TRUE, then any numerics which only have one row of summary statistics are simplified into a single row. Note again that ast has missing values and so is not simplified to a single row. summary(tableby(arm ~ age + ast, data = mockstudy, numeric.simplify=TRUE, numeric.stats=c( Nmiss , meansd ))) A: IFL (N=428) F: FOLFOX (N=691) G: IROX (N=380) Total (N=1499) p value Age, yrs 59.673 (11.365) 60.301 (11.632) 59.763 (11.499) 59.985 (11.519) 0.614 ast 0.507 N-Miss 69 141 56 266 Mean (SD) 37.292 (28.036) 35.202 (26.659) 35.670 (25.807) 35.933 (26.843) The in-formula functions to change which tests are run can also be used to specify these options for each variable at a time. summary(tableby(arm ~ anova(age, meansd , numeric.simplify=TRUE) + chisq(sex, cat.simplify=TRUE), data = mockstudy)) ## ## ## | | A: IFL (N=428) | F: FOLFOX (N=691) | G: IROX (N=380) | Total (N=1499) | p value| ## |:|::|:--:|::|::|-:| ## |**Age, yrs** | 59.673 (11.365) | 60.301 (11.632) | 59.763 (11.499) | 59.985 (11.519) | 0.614| ## |**Gender** | 151 (35.3%) | 280 (40.5%) | 152 (40.0%) | 583 (38.9%) | 0.190| 19. Use tableby within an Sweave document For those users who wish to create tables within an Sweave document, the following code seems to work. \\documentclass{article} \\usepackage{longtable} \\usepackage{pdfpages} \\begin{document} \\section{Read in Data} &lt;&lt;echo=TRUE&gt;&gt;= require(arsenal) require(knitr) require(rmarkdown) data(mockstudy) tab1 &lt;- tableby(arm~sex+age, data=mockstudy) @ \\section{Convert Summary.Tableby to LaTeX} &lt;&lt;echo=TRUE, results=&#39;hide&#39;, message=FALSE&gt;&gt;= capture.output(summary(tab1), file= Test.md ) ## Convert R Markdown Table to LaTeX render( Test.md , pdf_document(keep_tex=TRUE)) @ \\includepdf{Test.pdf} \\end{document} 20. Export tableby object to a .CSV file When looking at multiple variables it is sometimes useful to export the results to a csv file. The as.data.frame function creates a data frame object that can be exported or further manipulated within R. tab1 &lt;- tableby(arm~sex+age, data=mockstudy) as.data.frame(tab1) ## variable term label variable.type A: IFL F: FOLFOX ## 1 sex sex Gender categorical ## 2 sex countpct Male categorical 277.00000, 64.71963 411.00000, 59.47902 ## 3 sex countpct Female categorical 151.00000, 35.28037 280.00000, 40.52098 ## 4 age age Age, yrs numeric ## 5 age meansd Mean (SD) numeric 59.67290, 11.36454 60.30101, 11.63225 ## 6 age range Range numeric 27, 88 19, 88 ## G: IROX Total test p.value ## 1 Pearson&#39;s Chi-squared test 0.1904388 ## 2 228, 60 916.0000, 61.1074 Pearson&#39;s Chi-squared test 0.1904388 ## 3 152, 40 583.0000, 38.8926 Pearson&#39;s Chi-squared test 0.1904388 ## 4 Linear Model ANOVA 0.6143859 ## 5 59.76316, 11.49930 59.98532, 11.51877 Linear Model ANOVA 0.6143859 ## 6 26, 85 19, 88 Linear Model ANOVA 0.6143859 # write.csv(tmp, &#39;/my/path/here/mymodel.csv&#39;) 21. Write tableby object to a separate Word or HTML file ## write to an HTML document tab1 &lt;- tableby(arm ~ sex + age, data=mockstudy) write2html(tab1, ~/trash.html ) ## write to a Word document write2word(tab1, ~/trash.doc , title= My table in Word ) 22. Use tableby in R Shiny The easiest way to output a tableby() object in an R Shiny app is to use the tableOutput() UI in combination with the renderTable() server function and as.data.frame(summary(tableby())): # A standalone shiny app library(shiny) library(arsenal) data(mockstudy) shinyApp( ui = fluidPage(tableOutput( table )), server = function(input, output) { output$table &lt;- renderTable({ as.data.frame(summary(tableby(sex ~ age, data = mockstudy), text = html )) }, sanitize.text.function = function(x) x) } ) This can be especially powerful if you feed the selections from a selectInput(multiple = TRUE) into formulize() to make the table dynamic! 23. Use tableby in bookdown Since the backbone of tableby() is knitr::kable(), tables still render well in bookdown. However, print.summary.tableby() doesn‚Äôt use the caption= argument of kable(), so some tables may not have a properly numbered caption. To fix this, use the method described on the bookdown site to give the table a tag/ID. summary(tableby(sex ~ age, data = mockstudy), title= (\\\\#tab:mytableby) Caption here ) 24. Adjust tableby for multiple p-values The padjust() function is a new S3 generic piggybacking off of p.adjust(). It works on both tableby and summary.tableby objects: tab &lt;- summary(tableby(sex ~ age + fu.time + bmi + mdquality.s, data = mockstudy)) tab ## ## ## | | Male (N=916) | Female (N=583) | Total (N=1499) | p value| ## |:-|:--:|:--:|:--:|-:| ## |**Age, yrs** | | | | 0.048| ## |&amp;nbsp;&amp;nbsp;&amp;nbsp;Mean (SD) | 60.455 (11.369) | 59.247 (11.722) | 59.985 (11.519) | | ## |&amp;nbsp;&amp;nbsp;&amp;nbsp;Range | 19.000 - 88.000 | 22.000 - 88.000 | 19.000 - 88.000 | | ## |**fu.time** | | | | 0.978| ## |&amp;nbsp;&amp;nbsp;&amp;nbsp;Mean (SD) | 649.345 (454.332) | 648.674 (475.472) | 649.084 (462.511) | | ## |&amp;nbsp;&amp;nbsp;&amp;nbsp;Range | 0.000 - 2472.000 | 9.000 - 2441.000 | 0.000 - 2472.000 | | ## |**Body Mass Index (kg/m^2)** | | | | 0.012| ## |&amp;nbsp;&amp;nbsp;&amp;nbsp;N-Miss | 22 | 11 | 33 | | ## |&amp;nbsp;&amp;nbsp;&amp;nbsp;Mean (SD) | 27.491 (5.030) | 26.760 (5.984) | 27.206 (5.432) | | ## |&amp;nbsp;&amp;nbsp;&amp;nbsp;Range | 14.053 - 60.243 | 15.430 - 53.008 | 14.053 - 60.243 | | ## |**mdquality.s** | | | | 0.827| ## |&amp;nbsp;&amp;nbsp;&amp;nbsp;N-Miss | 153 | 99 | 252 | | ## |&amp;nbsp;&amp;nbsp;&amp;nbsp;Mean (SD) | 0.899 (0.301) | 0.903 (0.296) | 0.901 (0.299) | | ## |&amp;nbsp;&amp;nbsp;&amp;nbsp;Range | 0.000 - 1.000 | 0.000 - 1.000 | 0.000 - 1.000 | | padjust(tab, method = bonferroni ) ## ## ## | | Male (N=916) | Female (N=583) | Total (N=1499) | p value| ## |:-|:--:|:--:|:--:|-:| ## |**Age, yrs** | | | | 0.191| ## |&amp;nbsp;&amp;nbsp;&amp;nbsp;Mean (SD) | 60.455 (11.369) | 59.247 (11.722) | 59.985 (11.519) | | ## |&amp;nbsp;&amp;nbsp;&amp;nbsp;Range | 19.000 - 88.000 | 22.000 - 88.000 | 19.000 - 88.000 | | ## |**fu.time** | | | | 1.000| ## |&amp;nbsp;&amp;nbsp;&amp;nbsp;Mean (SD) | 649.345 (454.332) | 648.674 (475.472) | 649.084 (462.511) | | ## |&amp;nbsp;&amp;nbsp;&amp;nbsp;Range | 0.000 - 2472.000 | 9.000 - 2441.000 | 0.000 - 2472.000 | | ## |**Body Mass Index (kg/m^2)** | | | | 0.048| ## |&amp;nbsp;&amp;nbsp;&amp;nbsp;N-Miss | 22 | 11 | 33 | | ## |&amp;nbsp;&amp;nbsp;&amp;nbsp;Mean (SD) | 27.491 (5.030) | 26.760 (5.984) | 27.206 (5.432) | | ## |&amp;nbsp;&amp;nbsp;&amp;nbsp;Range | 14.053 - 60.243 | 15.430 - 53.008 | 14.053 - 60.243 | | ## |**mdquality.s** | | | | 1.000| ## |&amp;nbsp;&amp;nbsp;&amp;nbsp;N-Miss | 153 | 99 | 252 | | ## |&amp;nbsp;&amp;nbsp;&amp;nbsp;Mean (SD) | 0.899 (0.301) | 0.903 (0.296) | 0.901 (0.299) | | ## |&amp;nbsp;&amp;nbsp;&amp;nbsp;Range | 0.000 - 1.000 | 0.000 - 1.000 | 0.000 - 1.000 | | Available Function Options Summary statistics The default summary statistics, by varible type, are: numeric.stats: Continuous variables will show by default Nmiss, meansd, range cat.stats: Categorical and factor variables will show by default Nmiss, countpct ordered.stats: Ordered factors will show by default Nmiss, countpct surv.stats: Survival variables will show by default Nmiss, Nevents, medsurv date.stats: Date variables will show by default Nmiss, median, range Any summary statistics standardly defined in R (e.g. mean, median, sd, med, range) can be specified, however there are a number of extra functions defined specifically for the tableby function. N: a count of the number of observations for a particular group Nmiss: only show the count of the number of missing values if there are some missing values Nmiss2: always show a count of the number of missing values for a variable within each group meansd: print the mean and standard deviation in the format mean(sd) countpct: print the number of values in a category plus the column-percentage in the format N (%) countrowpct: print the number of values in a category plus the row-percentage in the format N (%) countcellpct: print the number of values in a category plus the cell-percentage in the format N (%) binomCI: print the proportion in a category plus a binomial confidence interval. rowbinomCI: print the row proportion in a category plus a binomial confidence interval. medianq1q3: print the median, 25th, and 75th quantiles median (Q1, Q3) q1q3: print the 25th and 75th quantiles Q1, Q3 iqr: print the inter-quartile range. medianrange: print the median, minimum and maximum values median (minimum, maximum) Nevents: print number of events for a survival object within each grouping level medsurv: print the median survival NeventsSurv: print number of events and survival at given times NriskSurv: print the number still at risk at given times medTime: print the median follow-up time Testing options The tests used to calculate p-values differ by the variable type, but can be specified explicitly in the formula statement or in the control function. The following tests are accepted: anova: analysis of variance test; the default test for continuous variables. When the grouping variable has two levels, it is equivalent to the two-sample t-test with equal variance. kwt: Kruskal-Wallis test, optional test for continuous variables. When the grouping variable has two levels, it is equivalent to the Wilcoxon Rank Sum test. chisq: chi-square goodness of fit test for equal counts of a categorical variable across categories; the default for categorical or factor variables fe: Fisher‚Äôs exact test for categorical variables; optional logrank: log-rank test, the default test for time-to-event variables trend: The independence_test function from the coin is used to test for trends. Whenthe grouping variable has two levels, it is equivalent to the Armitage trend test. This is the default for ordered factors notest: Don‚Äôt perform a test. tableby.control settings A quick way to see what arguments are possible to utilize in a function is to use the args() command. Settings involving the number of digits can be set in tableby.control or in summary.tableby. args(tableby.control) ## function (test = TRUE, total = TRUE, test.pname = NULL, cat.simplify = FALSE, ## numeric.simplify = FALSE, numeric.test = anova , cat.test = chisq , ## ordered.test = trend , surv.test = logrank , date.test = kwt , ## numeric.stats = c( Nmiss , meansd , range ), cat.stats = c( Nmiss , ## countpct ), ordered.stats = c( Nmiss , countpct ), ## surv.stats = c( Nevents , medSurv ), date.stats = c( Nmiss , ## median , range ), stats.labels = list(Nmiss = N-Miss , ## Nmiss2 = N-Miss , meansd = Mean (SD) , medianrange = Median (Range) , ## median = Median , medianq1q3 = Median (Q1, Q3) , q1q3 = Q1, Q3 , ## iqr = IQR , range = Range , countpct = Count (Pct) , ## Nevents = Events , medSurv = Median Survival , medTime = Median Follow-Up ), ## digits = 3L, digits.count = 0L, digits.pct = 1L, digits.p = 3L, ## format.p = TRUE, conf.level = 0.95, chisq.correct = FALSE, ## simulate.p.value = FALSE, B = 2000, ...) ## NULL summary.tableby settings The summary.tableby function has options that modify how the table appears (such as adding a title or modifying labels). args(arsenal:::summary.tableby) ## function (object, ..., labelTranslations = NULL, text = FALSE, ## title = NULL, pfootnote = FALSE, term.name = ) ## NULL ## The write2 function https://cran.r-project.org/web/packages/arsenal/vignettes/write2.html The write2 function Ethan Heinzen 09 November, 2018 Introduction A note on piping Examples Using arsenal Objects tableby modelsum freqlist compare Examples Using Other Objects knitr::kable() xtable::xtable() pander::pander_return() Output Multiple Tables to One Document Output Other Objects Monospaced (as if in a terminal) Add a YAML Header to the Output FAQs How do I suppress the note about my document getting rendered? How do I look at the temporary .md file? How do I prevent my document from being rendered? How do I output headers, raw HTML/LaTeX, paragraphs, etc.? How do I tweak the default format from write2word(), write2html(), or write2pdf()? How do I output to a file format other than word, HTML, and PDF? How do I avoid prefixes on my table captions in PDF? How do I output multiple tables with different titles? Introduction The write2*() functions were designed as an alternative to SAS‚Äôs ODS procedure for useRs who want to save R Markdown tables to separate Word, HTML, or PDF files without needing separate R Markdown programs. There are three shortcut functions for the most common output types: HTML, PDF, and Word. Each of these three functions calls write2(), an S3 function which accepts many file output types (see the help pages for rmarkdown::render()). Methods have been implemented for tableby(), modelsum(), and freqlist(), but also knitr::kable(), xtable::xtable(), and pander::pander_return(). The two most important things to recognize with write2() are the following: Which function is being used to output the object. Sometimes the write2 functions use summary(), while other times they will use print(). The details for each object specifically are described below. How the ... arguments are passed. To change the options for the summary-like or print-like function, you can pass named arguments which will in turn get passed to the appropriate function. Details for each object specifically are described below. A note on piping arsenal is piping-compatible! The write2*() functions are probably the most useful place to take advantage of the magrittr package‚Äôs piping framework, since commands are often nested several functions deep in the context of write2*(). Piping also allows the arsenal package to become a part of more standard analysis pipelines; instead of needing to write separate R Markdown programs, intermediate analysis tables and output can be easily incorporated into piped statements. This vignette will sprinkle the foward pipe (%&gt;%) throughout as a hint at the power and flexibility of arsenal and piping. Examples Using arsenal Objects library(arsenal) library(magrittr) data(mockstudy) tmpdir &lt;- tempdir() tableby For tableby objects, the output function in write2() is summary(). For summary.tableby objects, the output function is print(). For available arguments, see the help pages for summary.tableby(). Don‚Äôt use the option text = TRUE with the write2 functions. mylabels &lt;- list(sex = SEX , age = Age, yrs ) tab1 &lt;- tableby(arm ~ sex + age, data=mockstudy) write2html( tab1, paste0(tmpdir, /test.tableby.html ), quiet = TRUE, title = My test table , # passed to summary.tableby labelTranslations = mylabels, # passed to summary.tableby total = FALSE # passed to summary.tableby ) modelsum For modelsum objects, the output function in write2() is summary(). For summary.modelsum objects, the output function is print(). For available arguments, see the help pages for summary.modelsum(). Don‚Äôt use the option text = TRUE with the write2 functions. tab2 &lt;- modelsum(alk.phos ~ arm + ps + hgb, adjust= ~ age + sex, family = gaussian , data = mockstudy) write2pdf( tab2, paste0(tmpdir, /test.modelsum.pdf ), quiet = TRUE, title = My test table , # passed to summary.modelsum show.intercept = FALSE, # passed to summary.modelsum digits = 5 # passed to summary.modelsum ) freqlist For freqlist objects, the output function in write2() is summary(). For summary.freqlist objects, the output function is print(). For available arguments, see the help pages for summary.freqlist(). mockstudy[, c( arm , sex , mdquality.s )] %&gt;% table(useNA = ifany ) %&gt;% freqlist(groupBy = c( arm , sex )) %&gt;% write2word( paste0(tmpdir, /test.freqlist.doc ), quiet = TRUE, single = FALSE, # passed to summary.freqlist title = My cool title # passed to summary.freqlist ) compare For compare.data.frame objects, the output function in write2() is summary(). For summary.compare.data.frame objects, the output function is print(). Examples Using Other Objects knitr::kable() For objects resulting from a call to kable(), the output function in write2() is print(). There aren‚Äôt any arguments to the print.knitr_kable() function. mockstudy %&gt;% head() %&gt;% knitr::kable() %&gt;% write2html(paste0(tmpdir, /test.kable.html ), quiet = TRUE) xtable::xtable() For xtable objects, the output function in write2() is print(). For available arguments, see the help pages for print.xtable(). mockstudy %&gt;% head() %&gt;% xtable::xtable(caption = My xtable ) %&gt;% write2pdf( paste0(tmpdir, /test.xtable.pdf ), quiet = TRUE, comment = FALSE, # passed to print.xtable to turn off the default message about xtable version include.rownames = FALSE, # passed to print.xtable caption.placement = top # passed to print.xtable ) To make an HTML document, use the print.xtable() option type = html . mockstudy %&gt;% head() %&gt;% xtable::xtable(caption = My xtable ) %&gt;% write2html( paste0(tmpdir, /test.xtable.html ), quiet = TRUE, type = html , # passed to print.xtable comment = FALSE, # passed to print.xtable to turn off the default message about xtable version include.rownames = FALSE, # passed to print.xtable caption.placement = top # passed to print.xtable ) User beware! xtable() is not compatible with write2word(). pander::pander_return() Pander is a little bit more tricky. Since pander::pander() doesn‚Äôt return an object, the useR should instead use pander::pander_return(). For this (and for all character vectors), the the output function in write2() is cat(sep = &#39;\\n&#39;). write2word(pander::pander_return(head(mockstudy)), file = paste0(tmpdir, /test.pander.doc ), quiet = TRUE) Output Multiple Tables to One Document To output multiple tables into a document, simply make a list of them and call the same function as before. mylist &lt;- list( tableby(sex ~ age, data = mockstudy), freqlist(table(mockstudy[, c( sex , arm )])), knitr::kable(head(mockstudy)) ) write2pdf(mylist, paste0(tmpdir, /test.mylist.pdf ), quiet = TRUE) One neat side-effect of this function is that you can output text and headers, etc. The possibilities are endless! mylist2 &lt;- list( # Header 1 , This is a small paragraph introducing tableby. , tableby(sex ~ age, data = mockstudy), &lt;hr&gt; , # Header 2 , &lt;font color=&#39;red&#39;&gt;I can change color of my text!&lt;/font&gt; ) write2html(mylist2, paste0(tmpdir, /test.mylist2.html ), quiet = TRUE) In fact, you can even recurse on the lists! write2pdf(list(mylist2, mylist), paste0(tmpdir, /test.mylists.pdf ), quiet = TRUE) Output Other Objects Monospaced (as if in a terminal) It may be useful at times to write output that would normally be copied from the terminal. The default method for write2() does this automatically. To output the results of summary.lm(), for example: lm(age ~ sex, data = mockstudy) %&gt;% summary() %&gt;% write2pdf(paste0(tmpdir, /test.lm.pdf ), quiet = TRUE) The verbatim() function is another option to explicitly alert write2() to do this. This becomes particularly helpful to overrule existing S3 methods. For example, suppose you wanted to just print a tableby object (as if it were to print in the terminal): tab4 &lt;- tableby(arm ~ sex + age, data=mockstudy) write2html(verbatim(tab4), paste0(tmpdir, /test.print.tableby.html ), quiet = TRUE) Or suppose you wanted to print a character vector (as if it were to print in the terminal): chr &lt;- paste0( MyVector , 1:10) write2pdf(verbatim(chr), paste0(tmpdir, /test.character.pdf ), quiet = TRUE) Add a YAML Header to the Output You can add a YAML header to write2() output using the yaml() function. mylist3 &lt;- list( yaml(title = Test YAML Title , author = My cool author name ), # Header 1 , This is a small paragraph introducing tableby. , tableby(sex ~ age, data = mockstudy) ) write2html(mylist3, paste0(tmpdir, /test.yaml.html ), quiet = TRUE) In fact, all detected YAML pieces will be moved as the first output, so that the above code chunk gives the same output as this one: mylist4 &lt;- list( # Header 1 , This is a small paragraph introducing tableby. , yaml(title = Test YAML Title ), tableby(sex ~ age, data = mockstudy), yaml(author = My cool author name ) ) write2html(mylist3, paste0(tmpdir, /test.yaml2.html ), quiet = TRUE) FAQs How do I suppress the note about my document getting rendered? This is easily accomplished by using the argument quiet = TRUE (passed to the rmarkdown::render() function). write2html( knitr::kable(head(mockstudy)), paste0(tmpdir, /test.kable.quiet.html ), quiet = TRUE # passed to rmarkdown::render ) How do I look at the temporary .md file? This is easily accomplished by using the option keep.md = TRUE. write2html( knitr::kable(head(mockstudy)), paste0(tmpdir, /test.kable.keep.md.html ), quiet = TRUE, # passed to rmarkdown::render keep.md = TRUE ) How do I prevent my document from being rendered? This is easily accomplished by using the option render. = FALSE. Note that this will then default to keep.md = TRUE. write2html( knitr::kable(head(mockstudy)), paste0(tmpdir, /test.kable.dont.render.html ), render. = FALSE ) How do I output headers, raw HTML/LaTeX, paragraphs, etc.? One can simply abuse the list S3 method for write2()! mylist2 &lt;- list( # Header 1 , This is a small paragraph introducing tableby. , tableby(sex ~ age, data = mockstudy), &lt;hr&gt; , # Header 2 , &lt;font color=&#39;red&#39;&gt;I can change color of my text!&lt;/font&gt; ) write2html(mylist2, paste0(tmpdir, /test.mylist2.html ), quiet = TRUE) How do I tweak the default format from write2word(), write2html(), or write2pdf()? You can pass arguments to the format functions used behind the scenes. write2html( knitr::kable(head(mockstudy)), paste0(tmpdir, /test.kable.theme.html ), quiet = TRUE, # passed to rmarkdown::render theme = yeti # passed to rmarkdown::html_document ) See the help pages for rmarkdown::word_document(), rmarkdown::html_document(), and rmarkdown::pdf_document(). How do I output to a file format other than word, HTML, and PDF? This can be done using the generic write2() function. The last argument in the function can be another format specification. For details on the acceptable inputs, see the help page for write2(). write2( knitr::kable(head(mockstudy[, 1:4])), paste0(tmpdir, /test.kable.rtf ), quiet = TRUE, # passed to rmarkdown::render output_format = rmarkdown::rtf_document ) How do I avoid prefixes on my table captions in PDF? You can do this pretty easily with the yaml() function: mylist5 &lt;- list( yaml( header-includes = list( \\\\usepackage[labelformat=empty]{caption} )), # Header 1 , This is a small paragraph introducing tableby. , tableby(sex ~ age, data = mockstudy) ) write2pdf(mylist5, paste0(tmpdir, /test.noprefixes.pdf ), title = My tableby ) How do I output multiple tables with different titles? There are now write2() methods for the summary objects of arsenal functions. This allows you to specify a title for each table: mylist6 &lt;- list( summary(tableby(sex ~ age, data = mockstudy), title = A Title for tableby ), summary(modelsum(age ~ sex, data = mockstudy), title = A Title for modelsum ), summary(freqlist(~ sex, data = mockstudy), title = A Title for freqlist ) ) write2pdf(mylist6, paste0(tmpdir, /test.multiple.titles.pdf )) "],["dashboard-visualizations-in-r-deviation.html", "Chapter 32 Dashboard visualizations in R: Deviation 32.1 Row 32.2 Row", " Chapter 32 Dashboard visualizations in R: Deviation author: Kristian Larsen output: flexdashboard::flex_dashboard: orientation: rows vertical_layout: scroll from: https://datascienceplus.com/automated-dashboard-visualizations-with-deviation-in-r/?fbclid=IwAR2JcAMQ4eNRMrEBPGL79HDbS818vGZX0evs-ateBX0d9SRFIilY7U44Szw { eval=FALSE, include=FALSE, echo=TRUE} library(flexdashboard) library(ggplot2) library(plotly) theme_set(theme_bw()) # Data Prep data( mtcars ) # load data mtcars$`car name` &lt;- rownames(mtcars) # create new column for car names mtcars$mpg_z &lt;- round((mtcars$mpg - mean(mtcars$mpg))/sd(mtcars$mpg), 2) # compute normalized mpg mtcars$mpg_type &lt;- ifelse(mtcars$mpg_z &lt; 0, below , above ) # above / below avg flag mtcars &lt;- mtcars[order(mtcars$mpg_z), ] # sort mtcars$`car name` &lt;- factor(mtcars$`car name`, levels = mtcars$`car name`) # convert to factor to retain sorted order in plot. 32.1 Row 32.1.1 Chart A: Diverging Barcharts { eval=FALSE, include=FALSE, echo=TRUE} ggplot(mtcars, aes(x=`car name`, y=mpg_z, label=mpg_z)) + geom_bar(stat=&#39;identity&#39;, aes(fill=mpg_type), width=.5) + scale_fill_manual(name= Mileage , labels = c( Above Average , Below Average ), values = c( above = #00ba38 , below = #f8766d )) + labs(subtitle= Normalised mileage from &#39;mtcars&#39; , title= Diverging Bars ) + coord_flip() ggplotly(p = ggplot2::last_plot()) 32.1.2 Chart B: Diverging Lollipop Chart { eval=FALSE, include=FALSE, echo=TRUE} library(ggplot2) theme_set(theme_bw()) ggplot(mtcars, aes(x=`car name`, y=mpg_z, label=mpg_z)) + geom_point(stat=&#39;identity&#39;, fill= black , size=6) + geom_segment(aes(y = 0, x = `car name`, yend = mpg_z, xend = `car name`), color = black ) + geom_text(color= white , size=2) + labs(title= Diverging Lollipop Chart , subtitle= Normalized mileage from &#39;mtcars&#39;: Lollipop ) + ylim(-2.5, 2.5) + coord_flip() ggplotly(p = ggplot2::last_plot()) 32.2 Row 32.2.1 Cart C: Diverging Dot Plot { eval=FALSE, include=FALSE, echo=TRUE} library(ggplot2) theme_set(theme_bw()) # Plot ggplot(mtcars, aes(x=`car name`, y=mpg_z, label=mpg_z)) + geom_point(stat=&#39;identity&#39;, aes(col=mpg_type), size=6) + scale_color_manual(name= Mileage , labels = c( Above Average , Below Average ), values = c( above = #00ba38 , below = #f8766d )) + geom_text(color= white , size=2) + labs(title= Diverging Dot Plot , subtitle= Normalized mileage from &#39;mtcars&#39;: Dotplot ) + ylim(-2.5, 2.5) + coord_flip() ggplotly(p = ggplot2::last_plot()) "],["autoreport-1.html", "Chapter 33 autoreport 33.1 Describe results of analysis", " Chapter 33 autoreport print(paste0( Git Update Started at: , Sys.time())) CommitMessage &lt;- paste( updated on: , Sys.time(), sep = ) wd &lt;- ~/serdarbalci setorigin &lt;- git remote set-url origin git@github.com:sbalci/MyJournalWatch.git \\n gitCommand &lt;- paste( cd , wd, \\n git add . \\n git commit --message &#39; , CommitMessage, &#39; \\n , setorigin, git push origin master \\n , sep = ) system(command = paste(gitCommand, \\n ) , intern = TRUE, wait = TRUE) Sys.sleep(5) print(paste0( Git Update Ended at: , Sys.time())) 33.1 Describe results of analysis Copy/paste t-tests Directly to Manuscripts: https://neuropsychology.github.io/psycho.R//2018/06/19/analyze_ttest.html https://github.com/neuropsychology/psycho.R { eval=FALSE, include=FALSE, echo=TRUE} # Load packages library(tidyverse) # devtools::install_github( neuropsychology/psycho.R ) # Install the latest psycho version library(psycho) { eval=FALSE, include=FALSE, echo=TRUE} df &lt;- psycho::affective # Load the data df { eval=FALSE, include=FALSE, echo=TRUE} results &lt;- t.test(df$Age ~ df$Sex) # Perform a simple t-test results { eval=FALSE, include=FALSE, echo=TRUE} psycho::analyze(results) { eval=FALSE, include=FALSE, echo=TRUE} t.test(df$Adjusting ~ df$Sex, var.equal=TRUE, conf.level = .90) %&gt;% psycho::analyze() { eval=FALSE, include=FALSE, echo=TRUE} t.test(df$Adjusting, mu = 0, conf.level = .90) %&gt;% psycho::analyze() { eval=FALSE, include=FALSE, echo=TRUE} t.test(df$Adjusting ~ df$Sex) %&gt;% psycho::analyze() %&gt;% summary() "],["citation.html", "Chapter 34 citation", " Chapter 34 citation { PubMed references, eval=FALSE, include=FALSE, echo=TRUE} PMID_25783680 &lt;- RefManageR::ReadPubMed( 25783680 , database = PubMed ) cit_25783680 &lt;- paste0(PMID_25783680$title, , PMID_25783680$journal, , PMID: https://www.ncbi.nlm.nih.gov/pubmed/?term= , PMID_25783680$eprint, , doi: https://doi.org/ , PMID_25783680$doi) My next citation is here1. { dimension badge, eval=FALSE, include=FALSE, echo=TRUE} PMID_25783680 &lt;- RefManageR::ReadPubMed( 25783680 , database = PubMed ) dimensionBadge &lt;- paste0( &lt;script async=&#39;&#39; charset=&#39;utf-8&#39; src=&#39;https://badge.dimensions.ai/badge.js&#39;&gt;&lt;/script&gt; &lt;span class=&#39;__dimensions_badge_embed__&#39; data-doi=&#39; , PMID_25783680$doi, &#39; data-style=&#39;small_circle&#39;&gt;&lt;/span&gt; ) r dimensionBadge { eval=FALSE, include=FALSE, echo=TRUE} PMID_25783680 &lt;- RefManageR::ReadPubMed( 25783680 , database = PubMed ) altmetricBadge &lt;- paste0( &lt;script type=&#39;text/javascript&#39; src=&#39;https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js&#39;&gt;&lt;/script&gt; &lt;span class=&#39;altmetric-embed&#39; data-badge-popover=&#39;right&#39; data-badge-type=&#39;donut&#39; data-doi=&#39; , PMID_25783680$doi, &#39;&gt;&lt;/span&gt; ) r altmetricBadge r cit_25783680 ‚Ü©Ô∏é "],["bbplot.html", "Chapter 35 bbplot 35.1 BBC Visual and Data Journalism cookbook for R graphics", " Chapter 35 bbplot 35.1 BBC Visual and Data Journalism cookbook for R graphics https://bbc.github.io/rcookbook/ {r eval=FALSE, include=FALSE, echo=TRUE} # devtools::install_github(&#39;bbc/bbplot&#39;) {r eval=FALSE, include=FALSE, echo=TRUE} #This line of code installs the pacman page if you do not have it installed - if you do, it simply loads the package if(!require(pacman))install.packages( pacman ) pacman::p_load(&#39;dplyr&#39;, &#39;tidyr&#39;, &#39;gapminder&#39;, &#39;ggplot2&#39;, &#39;ggalt&#39;, &#39;forcats&#39;, &#39;R.utils&#39;, &#39;png&#39;, &#39;grid&#39;, &#39;ggpubr&#39;, &#39;scales&#39;, &#39;bbplot&#39;) "],["bibliography-1.html", "Chapter 36 Bibliography", " Chapter 36 Bibliography A brief introduction to bibliometrix https://cran.r-project.org/web/packages/bibliometrix/vignettes/bibliometrix-vignette.html Bibliographic Network Visualization for Academic Literature Reviews http://www.mburnamfink.com/blog/bibliographic-network-visualization-for-academic-literature-reviews https://embed.kumu.io/0b991b02bb20975fde904f4bf7433333#jpsp-top-50?s=%23doi-101037-0022-35147451252 More Than Words? Computer-Aided Text Analysis in Organizational Behavior and Psychology Research https://www.annualreviews.org/doi/10.1146/annurev-orgpsych-032117-104622 https://www.kumu.io/nicholasjkelley/jpsp-top-50 "],["knitcitations.html", "Chapter 37 knitcitations", " Chapter 37 knitcitations https://github.com/cboettig/knitcitations {r eval=FALSE, include=FALSE, echo=TRUE} # library(devtools) # install_github( cboettig/knitcitations ) install.packages( knitcitations ) {r eval=FALSE, include=FALSE, echo=TRUE} library( knitcitations ) cleanbib() options( citation_format = pandoc ) {r eval=FALSE, include=FALSE, echo=TRUE} knitcitations::citep( 10.1890/11-0011.1 ) citation r citep( 10.1890/11-0011.1 ) in text {r eval=FALSE, include=FALSE, echo=TRUE} knitcitations::citet( 10.1098/rspb.2013.1372 ) citation r citet( 10.1098/rspb.2013.1372 ) in text {r eval=FALSE, include=FALSE, echo=TRUE} knitcitations::citep( http://knowledgeblog.org/greycite ) write.bibtex(file= references.bib ) "],["rcrossref.html", "Chapter 38 rcrossref", " Chapter 38 rcrossref https://github.com/ropensci/rcrossref "],["rorcid-tutorial.html", "Chapter 39 rorcid tutorial", " Chapter 39 rorcid tutorial https://ropensci.org/tutorials/rorcid_tutorial/ "],["rentrez-tutorial.html", "Chapter 40 rentrez tutorial", " Chapter 40 rentrez tutorial https://ropensci.org/tutorials/rentrez_tutorial/ "],["webscicorpus.html", "Chapter 41 WebSciCorpus", " Chapter 41 WebSciCorpus https://www.clarehooper.net/WebSciCorpus/ "],["web-of-science-wos-corpus-parsing-script.html", "Chapter 42 WEB OF SCIENCE (WOS) CORPUS | PARSING SCRIPT", " Chapter 42 WEB OF SCIENCE (WOS) CORPUS | PARSING SCRIPT https://docs.cortext.net/question/web-of-science-wos-corpus-parsing-script-2/ "],["t-lab-plus-2019.html", "Chapter 43 T-LAB PLUS 2019", " Chapter 43 T-LAB PLUS 2019 https://tlab.it/en/allegati/help_en_online/mmappe2.htm "],["tools-for-bibliometric-analyses.html", "Chapter 44 Tools for bibliometric analyses", " Chapter 44 Tools for bibliometric analyses https://ju.se/library/research--teaching-support/bibliometrics/tools-for-bibliometric-analyses.html "],["evidencepartners.html", "Chapter 45 evidencepartners", " Chapter 45 evidencepartners https://www.evidencepartners.com/ "],["r-script-for-creating-a-cross-citation-network.html", "Chapter 46 R script for creating a cross-citation network", " Chapter 46 R script for creating a cross-citation network https://www.researchgate.net/publication/327790285_R_script_for_creating_a_cross-citation_network Repository: https://github.com/arsiders/citation-network # RCitation - Quick Citation Network # Fall 2018 # A.R. Siders (siders@alumni.stanford.edu) # Creates a network of the citations among a set of academic papers. # Rationale: If full title of Article 2 is present in text of Article 1, Article 1 cites Article 2. # NOTE: Will only work in fields where full, unabbreviated titles are used in reference/bibliography citation format. # NOTE: Will have high error rate if titles are very short or comprised of common words (e.g., paper Vulnerability produced many false positives). Some errors result from authors using a shortened version of a title (e.g., only text before a colon) or incorrect citations or typos. Citation networks produced are therefore approximate and to be used primarily for exploration of the data. # NOTE: Error rate may be reduced by using only reference sections of the articles of interest, rather than full texts, but this will increase work required to prepare articles. # ==&gt; FIVE STEPS TO CITATION NETWORK # STEP 1. FORMAT INPUT # a. Papers: Folder of papers in txt format (UTF-8) organized *in SAME ORDER* as Titles # b. Titles: Column of paper titles in csv spreadsheet (Column #1) *in SAME ORDER* as documents in Papers folder. Need a header cell or top title will be removed. # Recommend naming all texts in Papers folder using author last name listed alphabetically. Organize Titles using same order. # STEP 2. PREP # set working directory setwd( C:\\[name of working space] ) # make sure \\ not / in name setwd( C:/Users/User/OneDrive/Adaptive Capacity Text Mining/Citation Network Test/CitationNetwork Test Data ) # load packages install.packages(c( tm , plyr )) library(tm) library(plyr) # STEP 3. LOAD INPUTS # a. Papers papers&lt;-Corpus(DirSource( [name of folder where papers located] )) papers&lt;-Corpus(DirSource( Papers )) # b. Titles titletable&lt;-read.csv( [name of titles file].csv ) #make sure column has a header titletable&lt;-read.csv( TestTitles.csv ) titles&lt;-as.vector(titletable[,1]) # load functions at bottom of this script (below Step 5) length(papers) length(titles) # STEP 4. RUN FUNCTION CitationNetwork&lt;-CreateCitationNetwork(papers,titles) # add date currentDate &lt;- Sys.Date() csvFileName &lt;- paste( CitationEdges ,currentDate, .csv ,sep= ) # save results write.csv(CitationNetwork, file=csvFileName) # STEP 5. VISUALIZE NETWORK # Install Gephi or other network visualization software and load CitationEdges.csv # Load list of titles or other spreadsheet as nodes to visualize network # Gephi available at https://gephi.org/ # ===&gt; FUNCTIONS TO LOAD CreateCitationNetwork&lt;-function(papers,titles){ # prep papers corpus papers&lt;-tm_map(papers, content_transformer(tolower)) papers&lt;-tm_map(papers, removePunctuation) papers&lt;-tm_map(papers, removeNumbers) papers&lt;-tm_map(papers, stripWhitespace) # prep titles titles&lt;-removePunctuation(titles) titles&lt;-stripWhitespace(titles) titles&lt;-tolower(titles) # create citation true/false matrix Cites.TF&lt;-CiteMatrix(titles, papers) # format matrix into edges file CitationEdges&lt;-EdgesFormat(Cites.TF, titles) return(CitationEdges) } # format true/false matrix into edges file EdgesFormat&lt;-function(Cites.TF, titles){ #create an empty object to put information in edges&lt;-data.frame(matrix(NA), nrow=NA, ncol=NA) colnames(edges)&lt;- c( Source , Target , Weight ) for (i in 1:length(Cites.TF)){ #for each document, run through all titles accross columns for (j in 1:ncol(Cites.TF)){ # for each title, see if document [row] cited that title [column] if (Cites.TF[i,j]==TRUE){ #if document is cited temp&lt;-data.frame(matrix(NA), nrow=NA, ncol=NA) colnames(temp)&lt;- c( Source , Target , Weight ) # first column &lt;- document doing the citing temp[1,1]&lt;-titles[i] # second column &lt;- document being cited temp[1,2]&lt;-titles[j] # third column the yes/no [weight] temp[1,3]&lt;-1 temp[1,4]&lt;- Directed edges&lt;-rbind(edges,temp) } } } return(edges[-1,]) #-1 removes initial row of null values } # Citation true/false matrix CiteMatrix&lt;-function(search.vector, Ref.corpus){ # Creates a csv matrix with True/False for citation patterns citations&lt;-data.frame(matrix(NA, nrow = length(Ref.corpus), ncol=length(search.vector))) #Columns are the document being cited colnames(citations)&lt;-search.vector #Rows are the document doing the citing rownames(citations)&lt;-search.vector for (i in 1:length(search.vector)){ searchi&lt;-search.vector[i] papercite&lt;-grepl(searchi, Ref.corpus$content, fixed=TRUE) citations[,i]&lt;-papercite } return(citations) } The application of methods of social network analysis in bibliometrics and webometrics. Measures and tools https://www.researchgate.net/publication/327817518_The_application_of_methods_of_social_network_analysis_in_bibliometrics_and_webometrics_Measures_and_tools "],["scientominer-icr.html", "Chapter 47 ScientoMiner ICR", " Chapter 47 ScientoMiner ICR https://zenodo.org/record/1432557#.XItjfxO2k1J "],["onodo.html", "Chapter 48 onodo", " Chapter 48 onodo https://onodo.org/dashboard https://onodo.org/tutorials "],["bibexcel.html", "Chapter 49 BibExcel", " Chapter 49 BibExcel https://homepage.univie.ac.at/juan.gorraiz/bibexcel/ "],["scientometric-portal.html", "Chapter 50 Scientometric Portal", " Chapter 50 Scientometric Portal https://sites.google.com/site/hjamali/scientometric-portal "],["leydesdorff.html", "Chapter 51 leydesdorff", " Chapter 51 leydesdorff https://www.leydesdorff.net/software.htm "],["publish-or-perish.html", "Chapter 52 Publish or Perish", " Chapter 52 Publish or Perish https://harzing.com/resources/publish-or-perish "],["pajek-analysis-and-visualization-of-large-networks.html", "Chapter 53 Pajek: analysis and visualization of large networks", " Chapter 53 Pajek: analysis and visualization of large networks http://mrvar.fdv.uni-lj.si/pajek/ "],["r-bioconductor.html", "Chapter 54 R Bioconductor", " Chapter 54 R Bioconductor https://www.bioconductor.org/ ## try http:// if https:// URLs are not supported source( https://bioconductor.org/biocLite.R ) biocLite() The Bioconductor 2018 Workshop Compilation https://bioconductor.github.io/BiocWorkshops/index.html https://github.com/Bioconductor/BiocWorkshops {r eval=FALSE, include=FALSE, echo=TRUE} fname &lt;- file.choose() fname {r eval=FALSE, include=FALSE, echo=TRUE} file.exists(fname) https://raw.githubusercontent.com/Bioconductor/BiocWorkshops/master/100_Morgan_RBiocForAll/ALL-phenoData.csv {r eval=FALSE, include=FALSE, echo=TRUE} pdata &lt;- read.csv(fname) {r eval=FALSE, include=FALSE, echo=TRUE} pdata {r eval=FALSE, include=FALSE, echo=TRUE} dim(pdata) {r eval=FALSE, include=FALSE, echo=TRUE} head(pdata) {r eval=FALSE, include=FALSE, echo=TRUE} tail(pdata) {r eval=FALSE, include=FALSE, echo=TRUE} summary(pdata) {r eval=FALSE, include=FALSE, echo=TRUE} class(fname) class(pdata) {r eval=FALSE, include=FALSE, echo=TRUE} pdata &lt;- read.csv( fname, colClasses = c( character , factor , integer , factor ) ) summary(pdata) {r eval=FALSE, include=FALSE, echo=TRUE} pdata[1:5, c( sex , mol.biol )] {r eval=FALSE, include=FALSE, echo=TRUE} pdata[1:5, c(2, 3)] {r eval=FALSE, include=FALSE, echo=TRUE} pdata[1:5, ] {r eval=FALSE, include=FALSE, echo=TRUE} pdata$age {r eval=FALSE, include=FALSE, echo=TRUE} pdata[[ age ]] {r eval=FALSE, include=FALSE, echo=TRUE} class(pdata$age) {r eval=FALSE, include=FALSE, echo=TRUE} table(pdata$mol.biol) {r eval=FALSE, include=FALSE, echo=TRUE} table(is.na(pdata$age)) {r eval=FALSE, include=FALSE, echo=TRUE} levels(pdata$sex) {r eval=FALSE, include=FALSE, echo=TRUE} pdata$sex == F {r eval=FALSE, include=FALSE, echo=TRUE} (pdata$sex == F ) &amp; (pdata$age &gt; 50) {r eval=FALSE, include=FALSE, echo=TRUE} table( pdata$mol.biol ) {r eval=FALSE, include=FALSE, echo=TRUE} pdata$mol.biol %in% c( BCR/ABL , NEG ) {r eval=FALSE, include=FALSE, echo=TRUE} subset(pdata, sex == F &amp; age &gt; 50) {r eval=FALSE, include=FALSE, echo=TRUE} bcrabl &lt;- subset(pdata, mol.biol %in% c( BCR/ABL , NEG )) dim( bcrabl ) {r eval=FALSE, include=FALSE, echo=TRUE} table(bcrabl$mol.biol) {r eval=FALSE, include=FALSE, echo=TRUE} str(bcrabl$mol.biol) {r eval=FALSE, include=FALSE, echo=TRUE} factor(bcrabl$mol.biol) {r eval=FALSE, include=FALSE, echo=TRUE} bcrabl$mol.biol &lt;- factor(bcrabl$mol.biol) table(bcrabl$mol.biol) {r eval=FALSE, include=FALSE, echo=TRUE} str(bcrabl$mol.biol) {r eval=FALSE, include=FALSE, echo=TRUE} boxplot(age ~ mol.biol, bcrabl) {r eval=FALSE, include=FALSE, echo=TRUE} t.test(age ~ mol.biol, bcrabl) {r eval=FALSE, include=FALSE, echo=TRUE} library(ggplot2) ggplot(bcrabl, aes(x = mol.biol, y = age)) {r eval=FALSE, include=FALSE, echo=TRUE} ggplot(bcrabl, aes(x = mol.biol, y = age)) + geom_boxplot() {r eval=FALSE, include=FALSE, echo=TRUE} if (! BiocManager %in% rownames(installed.packages())) install.packages( BiocManager , repos= https://cran.r-project.org ) {r eval=FALSE, include=FALSE, echo=TRUE} BiocManager::install(c( rtracklayer , GenomicRanges )) {r eval=FALSE, include=FALSE, echo=TRUE} BiocManager::valid() {r eval=FALSE, include=FALSE, echo=TRUE} BiocManager::available( TxDb.Hsapiens ) {r eval=FALSE, include=FALSE, echo=TRUE} browseVignettes( simpleSingleCell ) https://support.bioconductor.org/ https://bioconductor.org/help/course-materials/ {r eval=FALSE, include=FALSE, echo=TRUE} library( rtracklayer ) library( GenomicRanges ) https://genome.ucsc.edu/cgi-bin/hgTables?hgsid=578954849_wF1QP81SIHdfr8b0kmZUOcsZcHYr&amp;clade=mammal&amp;org=Human&amp;db=hg38&amp;hgta_group=regulation&amp;hgta_track=knownGene&amp;hgta_table=0&amp;hgta_regionType=genome&amp;position=chr9%3A133252000-133280861&amp;hgta_outputType=primaryTable&amp;hgta_outFileName= {r eval=FALSE, include=FALSE, echo=TRUE} fname &lt;- file.choose() {r eval=FALSE, include=FALSE, echo=TRUE} cpg &lt;- rtracklayer::import(fname) {r eval=FALSE, include=FALSE, echo=TRUE} file.exists(fname) {r eval=FALSE, include=FALSE, echo=TRUE} cpg https://bioconductor.github.io/BiocWorkshops/r-and-bioconductor-for-everyone-an-introduction.html Introduction to Bioconductor https://www..com/community/tutorials/intro-bioconductor {r eval=FALSE, include=FALSE, echo=TRUE} source( https://bioconductor.org/biocLite.R ) biocLite() {r eval=FALSE, include=FALSE, echo=TRUE} source( https://bioconductor.org/biocLite.R ) biocLite(c( Biostrings , GenomicRanges , IMMAN )) {r eval=FALSE, include=FALSE, echo=TRUE} library(Biostrings) dnaSequence &lt;- DNAStringSet( c( AAACTG , CCCAACCA ) ) dnaSequence {r eval=FALSE, include=FALSE, echo=TRUE} complement(dnaSequence) Important packages: - DNAStringSet - Biostrings - GenomicRanges {r eval=FALSE, include=FALSE, echo=TRUE} library(GenomicRanges) grangeObj &lt;- GRanges(seqnames = Rle(c( chr1 , chr2 , chr1 , chr3 ), c(1, 3, 2, 4)), ranges = IRanges(1:10, end = 7:16, names = head(letters, 10)), strand = Rle(strand(c( - , + , * , + , - )), c(1, 2, 2, 3, 2)), score = 1:10, GC = seq(1, 0, length=10)) grangeObj {r eval=FALSE, include=FALSE, echo=TRUE} seqnames(grangeObj) {r eval=FALSE, include=FALSE, echo=TRUE} ranges(grangeObj) {r eval=FALSE, include=FALSE, echo=TRUE} strand(grangeObj) {r eval=FALSE, include=FALSE, echo=TRUE} library( clusterProfiler ) {r eval=FALSE, include=FALSE, echo=TRUE} library( DOSE ) {r eval=FALSE, include=FALSE, echo=TRUE} library( org.Hs.eg.db ) {r eval=FALSE, include=FALSE, echo=TRUE} data(geneList, package= DOSE ) gene &lt;- names(geneList)[abs(geneList) &gt; 2] {r eval=FALSE, include=FALSE, echo=TRUE} ego &lt;- enrichGO(gene = gene, universe = names(geneList), OrgDb = org.Hs.eg.db, ont = CC , pAdjustMethod = BH , pvalueCutoff = 0.01, qvalueCutoff = 0.05, readable = TRUE) head(ego) {r eval=FALSE, include=FALSE, echo=TRUE} emapplot(ego) {r eval=FALSE, include=FALSE, echo=TRUE} class(dnaSequence) {r eval=FALSE, include=FALSE, echo=TRUE} methods(class = DNAStringSet ) https://bioconductor.org/packages https://support.bioconductor.org/ http://bioconductor.org/help/course-materials/ "],["biyoinformatik.html", "Chapter 55 Biyoinformatik", " Chapter 55 Biyoinformatik DESeq results to pathways in 60 Seconds with the fgsea package https://stephenturner.github.io/deseq-to-fgsea/ "],["bioconductor-1.html", "Chapter 56 Bioconductor 56.1 Courses &amp; Conferences", " Chapter 56 Bioconductor https://www.youtube.com/user/bioconductor 56.1 Courses &amp; Conferences https://www.bioconductor.org/help/course-materials/ "],["neuroconductor-tutorials.html", "Chapter 57 Neuroconductor Tutorials", " Chapter 57 Neuroconductor Tutorials https://neuroconductor.org/tutorials "],["neuroconductor-courses.html", "Chapter 58 Neuroconductor Courses", " Chapter 58 Neuroconductor Courses https://neuroconductor.org/courses "],["cancerinsilico.html", "Chapter 59 CancerInSilico", " Chapter 59 CancerInSilico An R interface for computational modeling of tumor progression https://bioconductor.org/packages/release/bioc/html/CancerInSilico.html {r eval=FALSE, include=FALSE, echo=TRUE} if (!requireNamespace( BiocManager )) install.packages( BiocManager ) BiocManager::install() if (!requireNamespace( BiocManager , quietly = TRUE)) install.packages( BiocManager ) BiocManager::install( CancerInSilico , version = 3.8 ) library(CancerInSilico) {r eval=FALSE, include=FALSE, echo=TRUE} browseVignettes( CancerInSilico ) https://bioconductor.org/packages/release/bioc/vignettes/CancerInSilico/inst/doc/CancerInSilico.html "],["running-a-cell-simulation.html", "Chapter 60 Running a Cell Simulation 60.1 Run Simple Simulation 60.2 Plot CellModel Object 60.3 Query Cell Information", " Chapter 60 Running a Cell Simulation 60.1 Run Simple Simulation {r eval=FALSE, include=FALSE, echo=TRUE} simple_mod &lt;- suppressMessages(inSilicoCellModel(initialNum=30, runTime=72, density=0.1, outputIncrement=24, randSeed=123)) 60.2 Plot CellModel Object {r eval=FALSE, include=FALSE, echo=TRUE} plotCells(simple_mod, time=0) {r eval=FALSE, include=FALSE, echo=TRUE} plotCells(simple_mod, time=36) {r eval=FALSE, include=FALSE, echo=TRUE} plotCells(simple_mod, time=72) 60.3 Query Cell Information {r eval=FALSE, include=FALSE, echo=TRUE} # hours in simulation times &lt;- 0:simple_mod@runTime # plot number of cells over time nCells &lt;- sapply(times, getNumberOfCells, model=simple_mod) plot(times, nCells, type= l , xlab= hour , ylab= number of cells ) {r eval=FALSE, include=FALSE, echo=TRUE} # plot population density over time den &lt;- sapply(times, getDensity, model=simple_mod) plot(times, den, type= l , xlab= hour , ylab= population density ) "],["drugs.html", "Chapter 61 Drugs", " Chapter 61 Drugs {r eval=FALSE, include=FALSE, echo=TRUE} drug &lt;- new( Drug , name= Drug_A , timeAdded=24, cycleLengthEffect=function(type, length) length * 2) drug_mod &lt;- suppressMessages(inSilicoCellModel(initialNum=30, runTime=72, density=0.1, drugs=c(drug), outputIncrement=24, randSeed=123)) {r eval=FALSE, include=FALSE, echo=TRUE} # hours in simulation times &lt;- 0:simple_mod@runTime # plot number of cells over time nCells &lt;- sapply(times, getNumberOfCells, model=simple_mod) nCells_drug &lt;- sapply(times, getNumberOfCells, model=drug_mod) plot(times, nCells, type= l , xlab= hour , ylab= number of cells ) lines(times, nCells_drug, type= l , xlab= hour , ylab= number of cells , col= red ) "],["cell-types.html", "Chapter 62 Cell Types 62.1 Adding a Single Cell Type 62.2 Adding Multiple Cell Types 62.3 Getting Cell Type", " Chapter 62 Cell Types 62.1 Adding a Single Cell Type {r eval=FALSE, include=FALSE, echo=TRUE} type_A &lt;- new( CellType , name= A , minCycle=16, cycleLength=function() 16) fast_cells_mod &lt;- suppressMessages(inSilicoCellModel(initialNum=30, runTime=72, density=0.1, cellTypes=c(type_A), outputIncrement=24, randSeed=123)) {r eval=FALSE, include=FALSE, echo=TRUE} # hours in simulation times &lt;- 0:fast_cells_mod@runTime # plot number of cells over time nCells &lt;- sapply(times, getNumberOfCells, model=simple_mod) nCells_fast &lt;- sapply(times, getNumberOfCells, model=fast_cells_mod) plot(times, nCells, type= l , xlab= hour , ylab= number of cells ) lines(times, nCells_fast, type= l , xlab= hour , ylab= number of cells , col= red ) 62.2 Adding Multiple Cell Types {r eval=FALSE, include=FALSE, echo=TRUE} type_B &lt;- new( CellType , name= B , size=1, minCycle=16, cycleLength=function() 16 + rexp(1,1/4)) type_C &lt;- new( CellType , name= C , size=1, minCycle=32, cycleLength=function() 32 + rexp(1,1/4)) two_types_mod &lt;- suppressMessages(inSilicoCellModel(initialNum=30, runTime=72, density=0.1, cellTypes=c(type_B, type_C), cellTypeInitFreq=c(0.4,0.6), outputIncrement=24, randSeed=123)) 62.3 Getting Cell Type {r eval=FALSE, include=FALSE, echo=TRUE} getTypeBProportion &lt;- function(time) { N &lt;- getNumberOfCells(two_types_mod, time) sum(sapply(1:N, function(i) getCellType(two_types_mod, time, i) == 1)) / N } times &lt;- 0:two_types_mod@runTime Bprop &lt;- sapply(times, getTypeBProportion) plot(times, Bprop, type= l , xlab= hour , ylab= type B proportion ) "],["pathways.html", "Chapter 63 Pathways 63.1 Calibrate Gene Expression Range 63.2 Generate Pathway Activity 63.3 Visualize Pathway Activity 63.4 Accounting for Model Effects 63.5 Normalize Pathway Activity", " Chapter 63 Pathways {r eval=FALSE, include=FALSE, echo=TRUE} mitosisGeneNames &lt;- paste( m_ , letters[1:20], sep= ) mitosisExpression &lt;- function(model, cell, time) { ifelse(getCellPhase(model, time, cell) == M , 1, 0) } pwyMitosis &lt;- new( Pathway , genes=mitosisGeneNames, expressionScale=mitosisExpression) {r eval=FALSE, include=FALSE, echo=TRUE} contactInhibitionGeneNames &lt;- paste( ci_ , letters[1:15], sep= ) contactInhibitionExpression &lt;- function(model, cell, time) { getLocalDensity(model, time, cell, 3.3) } pwyContactInhibition &lt;- new( Pathway , genes=contactInhibitionGeneNames, expressionScale=contactInhibitionExpression) 63.1 Calibrate Gene Expression Range {r eval=FALSE, include=FALSE, echo=TRUE} # create simulated data set allGenes &lt;- c(mitosisGeneNames, contactInhibitionGeneNames) geneMeans &lt;- 2 + rexp(length(allGenes), 1/20) data &lt;- t(pmax(sapply(geneMeans, rnorm, n=25, sd=2), 0)) rownames(data) &lt;- allGenes # calibrate pathways pwyMitosis &lt;- calibratePathway(pwyMitosis, data) pwyContactInhibition &lt;- calibratePathway(pwyContactInhibition, data) 63.2 Generate Pathway Activity {r eval=FALSE, include=FALSE, echo=TRUE} params &lt;- new( GeneExpressionParams ) params@randSeed &lt;- 123 # control this for reporducibility params@nCells &lt;- 30 # sample 30 cells at each time point to measure activity params@sampleFreq &lt;- 6 # measure activity every 6 hours pwys &lt;- c(pwyMitosis, pwyContactInhibition) pwyActivity &lt;- inSilicoGeneExpression(simple_mod, pwys, params)$pathways 63.3 Visualize Pathway Activity {r eval=FALSE, include=FALSE, echo=TRUE} # mitosis plot(seq(0,72,6), pwyActivity[[1]], type= l , col= orange , ylim=c(0,1)) # contact inhibition lines(seq(0,72,6), pwyActivity[[2]], col= blue ) 63.4 Accounting for Model Effects {r eval=FALSE, include=FALSE, echo=TRUE} pwyMitosis@expressionScale = function(model, cell, time) { window &lt;- c(max(time - 2, 0), min(time + 2, model@runTime)) a1 &lt;- getAxisLength(model, window[1], cell) a2 &lt;- getAxisLength(model, window[2], cell) if (is.na(a1)) a1 &lt;- 0 # in case cell was just born return(ifelse(a2 &lt; a1, 1, 0)) } pwys &lt;- c(pwyMitosis, pwyContactInhibition) pwyActivity &lt;- inSilicoGeneExpression(simple_mod, pwys, params)$pathways # mitosis plot(seq(0,72,6), pwyActivity[[1]], type= l , col= orange , ylim=c(0,1)) # contact inhibition lines(seq(0,72,6), pwyActivity[[2]], col= blue ) 63.5 Normalize Pathway Activity {r eval=FALSE, include=FALSE, echo=TRUE} pwyMitosis@transformMidpoint = 0.1 pwyMitosis@transformSlope = 5 / 0.1 pwys &lt;- c(pwyMitosis, pwyContactInhibition) pwyActivity &lt;- inSilicoGeneExpression(simple_mod, pwys, params)$pathways # mitosis plot(seq(0,72,6), pwyActivity[[1]], type= l , col= orange , ylim=c(0,1)) # contact inhibition lines(seq(0,72,6), pwyActivity[[2]], col= blue ) "],["simulating-bulk-gene-expression-data.html", "Chapter 64 Simulating Bulk Gene Expression Data 64.1 Simulating Microarray Data 64.2 Visualize Bulk Gene Expression Data", " Chapter 64 Simulating Bulk Gene Expression Data 64.1 Simulating Microarray Data {r eval=FALSE, include=FALSE, echo=TRUE} params@RNAseq &lt;- FALSE # generate microarray data params@singleCell &lt;- FALSE # generate bulk data params@perError &lt;- 0.1 # parameter for simulated noise pwys &lt;- c(pwyMitosis, pwyContactInhibition) ge &lt;- inSilicoGeneExpression(simple_mod, pwys, params)$expression 64.2 Visualize Bulk Gene Expression Data {r eval=FALSE, include=FALSE, echo=TRUE} ndx &lt;- apply(ge, 1, var) == 0 # remove zero variance rows gplots::heatmap.2(ge[!ndx,], col = greenred , scale= row , trace= none , hclust=function(x) hclust(x,method = complete ), distfun=function(x) as.dist((1-cor(t(x)))/2), Colv=FALSE, dendrogram= row , RowSideColors = ifelse(rownames(ge[!ndx,]) %in% mitosisGeneNames, orange , blue ), labRow = FALSE, labCol = seq(0,72,6), main= Bulk Gene Expression from Simple Cell Simulation ) "],["simulating-single-cell-gene-expression-data.html", "Chapter 65 Simulating Single Cell Gene Expression Data 65.1 Cell Type Pathways 65.2 Simulating Single Cell RNA-seq 65.3 Visualize Single Cell Data", " Chapter 65 Simulating Single Cell Gene Expression Data 65.1 Cell Type Pathways {r eval=FALSE, include=FALSE, echo=TRUE} # gene names B_genes &lt;- paste( b. , letters[1:20], sep= ) C_genes &lt;- paste( c. , letters[1:20], sep= ) # pathway behavior pwy_B &lt;- new( Pathway , genes=B_genes, expressionScale= function(model, cell, time) ifelse(getCellType(model, time, cell)==1, 1, 0)) pwy_C &lt;- new( Pathway , genes=C_genes, expressionScale= function(model, cell, time) ifelse(getCellType(model, time, cell)==2, 1, 0)) # calibrate pathways geneMeans &lt;- 2 + rexp(length(c(B_genes, C_genes)), 1/20) data &lt;- t(pmax(sapply(geneMeans, rnorm, n=25, sd=2), 0)) rownames(data) &lt;- c(B_genes, C_genes) pwy_B &lt;- calibratePathway(pwy_B, data) pwy_C &lt;- calibratePathway(pwy_C, data) 65.2 Simulating Single Cell RNA-seq {r eval=FALSE, include=FALSE, echo=TRUE} params@RNAseq &lt;- TRUE params@singleCell &lt;- TRUE params@dropoutPresent &lt;- TRUE ge &lt;- inSilicoGeneExpression(two_types_mod, c(pwy_B, pwy_C), params)$expression 65.3 Visualize Single Cell Data {r eval=FALSE, include=FALSE, echo=TRUE} cells &lt;- unname(sapply(colnames(ge), function(x) strsplit(x, _ )[[1]][1])) cells &lt;- as.numeric(gsub( c , , cells)) type &lt;- sapply(cells, getCellType, model=two_types_mod, time=two_types_mod@runTime) type[type==1] &lt;- red type[type==2] &lt;- blue pca &lt;- prcomp(ge, center=FALSE, scale.=FALSE) plot(pca$rotation[,c(1,2)], col=type) "],["cancer-packages.html", "Chapter 66 Cancer Packages", " Chapter 66 Cancer Packages "],["bcra.html", "Chapter 67 BCRA", " Chapter 67 BCRA https://cran.r-project.org/web/packages/BCRA/index.html "],["cgdsr.html", "Chapter 68 cgdsr", " Chapter 68 cgdsr cgdsr: R-Based API for Accessing the MSKCC Cancer Genomics Data Server (CGDS) https://cran.r-project.org/web/packages/cgdsr/index.html "],["tcgabiolinksgui.html", "Chapter 69 TCGAbiolinksGUI", " Chapter 69 TCGAbiolinksGUI {r eval=FALSE, include=FALSE, echo=TRUE} if (!requireNamespace( BiocManager , quietly = TRUE)) install.packages( BiocManager ) BiocManager::install( TCGAbiolinksGUI , version = 3.8 ) {r eval=FALSE, include=FALSE, echo=TRUE} browseVignettes( TCGAbiolinksGUI ) https://bioconductor.org/packages/release/bioc/html/TCGAbiolinksGUI.html {r eval=FALSE, include=FALSE, echo=TRUE} library(TCGAbiolinksGUI) TCGAbiolinksGUI() "],["rtcga.html", "Chapter 70 RTCGA", " Chapter 70 RTCGA {r eval=FALSE, include=FALSE, echo=TRUE} if (!requireNamespace( BiocManager , quietly = TRUE)) install.packages( BiocManager ) BiocManager::install( RTCGA , version = 3.8 ) {r eval=FALSE, include=FALSE, echo=TRUE} browseVignettes( RTCGA ) {r eval=FALSE, include=FALSE, echo=TRUE} RTCGA::infoTCGA() "],["cancersubtypes.html", "Chapter 71 CancerSubtypes", " Chapter 71 CancerSubtypes {r eval=FALSE, include=FALSE, echo=TRUE} if (!requireNamespace( BiocManager , quietly = TRUE)) install.packages( BiocManager ) BiocManager::install( CancerSubtypes , version = 3.8 ) {r eval=FALSE, include=FALSE, echo=TRUE} browseVignettes( CancerSubtypes ) "],["cancermutationanalysis.html", "Chapter 72 CancerMutationAnalysis", " Chapter 72 CancerMutationAnalysis "],["cancerclass.html", "Chapter 73 cancerclass", " Chapter 73 cancerclass "],["cancer.html", "Chapter 74 canceR", " Chapter 74 canceR {r eval=FALSE, include=FALSE, echo=TRUE} if (!requireNamespace( BiocManager , quietly = TRUE)) install.packages( BiocManager ) BiocManager::install( canceR , version = 3.8 ) {r eval=FALSE, include=FALSE, echo=TRUE} browseVignettes( canceR ) {r eval=FALSE, include=FALSE, echo=TRUE} canceR::canceR() "],["biocancer.html", "Chapter 75 bioCancer", " Chapter 75 bioCancer {r eval=FALSE, include=FALSE, echo=TRUE} if (!requireNamespace( BiocManager , quietly = TRUE)) install.packages( BiocManager ) BiocManager::install( bioCancer , version = 3.8 ) {r eval=FALSE, include=FALSE, echo=TRUE} browseVignettes( bioCancer ) {r eval=FALSE, include=FALSE, echo=TRUE} bioCancer::bioCancer() "],["tcgaretriever.html", "Chapter 76 TCGAretriever", " Chapter 76 TCGAretriever TCGAretriever: Retrieve Genomic and Clinical Data from TCGA https://cran.r-project.org/web/packages/TCGAretriever/index.html "],["tcga2stat.html", "Chapter 77 TCGA2STAT", " Chapter 77 TCGA2STAT https://cran.r-project.org/web/packages/TCGA2STAT/vignettes/TCGA2STAT.html "],["tciapathfinder.html", "Chapter 78 TCIApathfinder", " Chapter 78 TCIApathfinder TCIApathfinder: Client for the Cancer Imaging Archive REST API https://cran.r-project.org/web/packages/TCIApathfinder/index.html "],["milc.html", "Chapter 79 MILC", " Chapter 79 MILC MILC: MIcrosimulation Lung Cancer (MILC) model https://cran.r-project.org/web/packages/MILC/index.html "],["infiniumpurify.html", "Chapter 80 InfiniumPurify", " Chapter 80 InfiniumPurify InfiniumPurify: Estimate and Account for Tumor Purity in Cancer Methylation Data Analysis https://cran.r-project.org/web/packages/InfiniumPurify/index.html "],["using-cloud-for-research.html", "Chapter 81 Using Cloud for Research", " Chapter 81 Using Cloud for Research "],["rclone.html", "Chapter 82 rclone", " Chapter 82 rclone https://rclone.org/drive/ "],["rmdrive.html", "Chapter 83 rmdrive", " Chapter 83 rmdrive https://github.com/ekothe/rmdrive "],["my-r-codes-for-data-analysis-1.html", "Chapter 84 My R Codes For Data Analysis", " Chapter 84 My R Codes For Data Analysis rstudioapi::selectDirectory() xaringan:::inf_mr() Load required packages Load required packages Load required packages Gerekli paketleri y√ºkle {r 1, message=FALSE, warning=FALSE} library(tidyverse) "],["tips.html", "Chapter 85 tips", " Chapter 85 tips {r eval=FALSE, include=FALSE, echo=TRUE} my_string1 &lt;- 3+4 my_string2 &lt;- plot(cars) eval(parse(text = my_string1)) eval(parse(text = my_string2)) "],["environment-memory.html", "Chapter 86 environment memory", " Chapter 86 environment memory http://r-statistics.co/R-Tutorial.html As you create new variables, by default they get store in what is called a global environment. a &lt;- 10 b &lt;- 20 ls() # list objects in global env rm(a) # delete the object ‚Äòa‚Äô rm(list = ls()) # caution: delete all objects in .GlobalEnv gc() # free system memory However if you choose, you can create a new environment and store them there. rm(list=ls()) # remove all objects in work space env1 &lt;- new.env() # create a new environment assign( a , 3, envir = env1) # store a=3 inside env1 ls() # returns objects in .GlobalEnv ls(env1) # returns objects in env1 get(‚Äòa‚Äô, envir=env1) # retrieve value from env1 sort(vec1) # ascending sort sort(vec1, decreasing = TRUE) # Descending sort Sorting can also be achieved using the order() function which returns the indices of elements in ascending order. vec1[order(vec1)] # ascending sort vec1[rev(order(vec1))] # descending sort seq(1, 10, by = 2) # diff between adj elements is 2 seq(1, 10, length=25) # length of the vector is 25 rep(1, 5) # repeat 1, five times. rep(1:3, 5) # repeat 1:3, 5 times rep(1:3, each=5) # repeat 1 to 3, each 5 times. subset(airquality, Day == 1, select = -Temp) # select Day=1 and exclude ‚ÄòTemp‚Äô airquality[which(airquality$Day==1), -c(4)] # same as above set.seed(100) trainIndex &lt;- sample(c(1:nrow(airquality)), size=nrow(airquality)*0.7, replace=F) # get test sample indices airquality[trainIndex, ] # training data airquality[-trainIndex, ] # test data if(checkConditionIfTrue) { ‚Ä¶.statements.. ‚Ä¶.statements.. } else { # place the ‚Äòelse‚Äô in same line as ‚Äò}‚Äô ‚Ä¶.statements.. ‚Ä¶.statements.. } for(counterVar in c(1:n)){ ‚Ä¶. statements.. } "],["my-r-codes-for-data-analysis-2.html", "Chapter 87 My R Codes For Data Analysis", " Chapter 87 My R Codes For Data Analysis sub# In this repository I am going to collect R codes for data analysis. Codes are from various resources and I try to give original link as much as possible. author: Serdar Balcƒ±, MD, Pathologist date: ‚Äò{r # format(Sys.Date())‚Äô 87.0.1 Compare Means {r eval=FALSE, include=FALSE, echo=TRUE} t.test(scabies$age[scabies$gender== male ],scabies$age[scabies$gender== female ]) {r eval=FALSE, include=FALSE, echo=TRUE} test &lt;- t.test(scabies$age[scabies$gender== male ],scabies$age[scabies$gender== female ]) psycho::analyze(test) "],["infer.html", "Chapter 88 infer", " Chapter 88 infer Randomization Examples using nycflights13 flights data https://cran.r-project.org/web/packages/infer/vignettes/flights_examples.html {r eval=FALSE, include=FALSE, echo=TRUE} library(nycflights13) library(dplyr) library(ggplot2) library(stringr) library(infer) set.seed(2017) {r eval=FALSE, include=FALSE, echo=TRUE} fli_small &lt;- flights %&gt;% na.omit() %&gt;% sample_n(size = 500) %&gt;% mutate(season = case_when( month %in% c(10:12, 1:3) ~ winter , month %in% c(4:9) ~ summer )) %&gt;% mutate(day_hour = case_when( between(hour, 1, 12) ~ morning , between(hour, 13, 24) ~ not morning )) %&gt;% select(arr_delay, dep_delay, season, day_hour, origin, carrier) fli_small Hypothesis tests One numerical variable (mean) {r eval=FALSE, include=FALSE, echo=TRUE} x_bar &lt;- fli_small %&gt;% summarize(mean(dep_delay)) %&gt;% pull() x_bar {r eval=FALSE, include=FALSE, echo=TRUE} null_distn &lt;- fli_small %&gt;% specify(response = dep_delay) %&gt;% hypothesize(null = point , mu = 10) %&gt;% generate(reps = 1000, type = bootstrap ) %&gt;% calculate(stat = mean ) null_distn {r eval=FALSE, include=FALSE, echo=TRUE} ggplot(data = null_distn, mapping = aes(x = stat)) + geom_density() + geom_vline(xintercept = x_bar, color = red ) {r eval=FALSE, include=FALSE, echo=TRUE} null_distn %&gt;% summarize(p_value = mean(stat &gt;= x_bar) * 2) One numerical variable (median) {r eval=FALSE, include=FALSE, echo=TRUE} x_tilde &lt;- fli_small %&gt;% summarize(median(dep_delay)) %&gt;% pull() x_tilde {r eval=FALSE, include=FALSE, echo=TRUE} null_distn &lt;- fli_small %&gt;% specify(response = dep_delay) %&gt;% hypothesize(null = point , med = -1) %&gt;% generate(reps = 1000, type = bootstrap ) %&gt;% calculate(stat = median ) null_distn {r eval=FALSE, include=FALSE, echo=TRUE} ggplot(null_distn, aes(x = stat)) + geom_bar() + geom_vline(xintercept = x_tilde, color = red ) {r eval=FALSE, include=FALSE, echo=TRUE} null_distn %&gt;% summarize(p_value = mean(stat &lt;= x_tilde) * 2) One categorical (one proportion) {r eval=FALSE, include=FALSE, echo=TRUE} p_hat &lt;- fli_small %&gt;% summarize(mean(day_hour == morning )) %&gt;% pull() p_hat null_distn &lt;- fli_small %&gt;% specify(response = day_hour, success = morning ) %&gt;% hypothesize(null = point , p = .5) %&gt;% generate(reps = 1000, type = simulate ) %&gt;% calculate(stat = prop ) ggplot(null_distn, aes(x = stat)) + geom_bar() + geom_vline(xintercept = p_hat, color = red ) null_distn %&gt;% summarize(p_value = mean(stat &lt;= p_hat) * 2) p_value 0.132 Logical variables will be coerced to factors: null_distn &lt;- fli_small %&gt;% mutate(day_hour_logical = (day_hour == morning )) %&gt;% specify(response = day_hour_logical, success = TRUE ) %&gt;% hypothesize(null = point , p = .5) %&gt;% generate(reps = 1000, type = simulate ) %&gt;% calculate(stat = prop ) Two categorical (2 level) variables d_hat &lt;- fli_small %&gt;% group_by(season) %&gt;% summarize(prop = mean(day_hour == morning )) %&gt;% summarize(diff(prop)) %&gt;% pull() null_distn &lt;- fli_small %&gt;% specify(day_hour ~ season, success = morning ) %&gt;% hypothesize(null = independence ) %&gt;% generate(reps = 1000, type = permute ) %&gt;% calculate(stat = diff in props , order = c( winter , summer )) ggplot(null_distn, aes(x = stat)) + geom_density() + geom_vline(xintercept = d_hat, color = red ) null_distn %&gt;% summarize(p_value = mean(stat &lt;= d_hat) * 2) %&gt;% pull() ## [1] 0.758 One categorical (&gt;2 level) - GoF Chisq_hat &lt;- fli_small %&gt;% specify(response = origin) %&gt;% hypothesize(null = point , p = c( EWR = .33, JFK = .33, LGA = .34)) %&gt;% calculate(stat = Chisq ) null_distn &lt;- fli_small %&gt;% specify(response = origin) %&gt;% hypothesize(null = point , p = c( EWR = .33, JFK = .33, LGA = .34)) %&gt;% generate(reps = 1000, type = simulate ) %&gt;% calculate(stat = Chisq ) ggplot(null_distn, aes(x = stat)) + geom_density() + geom_vline(xintercept = pull(Chisq_hat), color = red ) null_distn %&gt;% summarize(p_value = mean(stat &gt;= pull(Chisq_hat))) %&gt;% pull() ## [1] 0.002 Two categorical (&gt;2 level) variables Chisq_hat &lt;- fli_small %&gt;% chisq_stat(formula = day_hour ~ origin) null_distn &lt;- fli_small %&gt;% specify(day_hour ~ origin, success = morning ) %&gt;% hypothesize(null = independence ) %&gt;% generate(reps = 1000, type = permute ) %&gt;% calculate(stat = Chisq ) ggplot(null_distn, aes(x = stat)) + geom_density() + geom_vline(xintercept = pull(Chisq_hat), color = red ) null_distn %&gt;% summarize(p_value = mean(stat &gt;= pull(Chisq_hat))) %&gt;% pull() ## [1] 0.017 One numerical variable, one categorical (2 levels) (diff in means) d_hat &lt;- fli_small %&gt;% group_by(season) %&gt;% summarize(mean_stat = mean(dep_delay)) %&gt;% # Since summer - winter summarize(-diff(mean_stat)) %&gt;% pull() null_distn &lt;- fli_small %&gt;% specify(dep_delay ~ season) %&gt;% # alt: response = dep_delay, # explanatory = season hypothesize(null = independence ) %&gt;% generate(reps = 1000, type = permute ) %&gt;% calculate(stat = diff in means , order = c( summer , winter )) ggplot(null_distn, aes(x = stat)) + geom_density() + geom_vline(xintercept = d_hat, color = red ) null_distn %&gt;% summarize(p_value = mean(stat &lt;= d_hat) * 2) %&gt;% pull() ## [1] 1.574 One numerical variable, one categorical (2 levels) (diff in medians) d_hat &lt;- fli_small %&gt;% group_by(season) %&gt;% summarize(median_stat = median(dep_delay)) %&gt;% # Since summer - winter summarize(-diff(median_stat)) %&gt;% pull() null_distn &lt;- fli_small %&gt;% specify(dep_delay ~ season) %&gt;% # alt: response = dep_delay, # explanatory = season hypothesize(null = independence ) %&gt;% generate(reps = 1000, type = permute ) %&gt;% calculate(stat = diff in medians , order = c( summer , winter )) ggplot(null_distn, aes(x = stat)) + geom_bar() + geom_vline(xintercept = d_hat, color = red ) null_distn %&gt;% summarize(p_value = mean(stat &gt;= d_hat) * 2) %&gt;% pull() ## [1] 0.068 One numerical, one categorical (&gt;2 levels) - ANOVA F_hat &lt;- anova( aov(formula = arr_delay ~ origin, data = fli_small) )$`F value`[1] null_distn &lt;- fli_small %&gt;% specify(arr_delay ~ origin) %&gt;% # alt: response = arr_delay, # explanatory = origin hypothesize(null = independence ) %&gt;% generate(reps = 1000, type = permute ) %&gt;% calculate(stat = F ) ggplot(null_distn, aes(x = stat)) + geom_density() + geom_vline(xintercept = F_hat, color = red ) null_distn %&gt;% summarize(p_value = mean(stat &gt;= F_hat)) %&gt;% pull() ## [1] 0.351 Two numerical vars - SLR slope_hat &lt;- lm(arr_delay ~ dep_delay, data = fli_small) %&gt;% broom::tidy() %&gt;% filter(term == dep_delay ) %&gt;% pull(estimate) null_distn &lt;- fli_small %&gt;% specify(arr_delay ~ dep_delay) %&gt;% hypothesize(null = independence ) %&gt;% generate(reps = 1000, type = permute ) %&gt;% calculate(stat = slope ) ggplot(null_distn, aes(x = stat)) + geom_density() + geom_vline(xintercept = slope_hat, color = red ) null_distn %&gt;% summarize(p_value = mean(stat &gt;= slope_hat) * 2) %&gt;% pull() ## [1] 0 Confidence intervals One numerical (one mean) x_bar &lt;- fli_small %&gt;% summarize(mean(arr_delay)) %&gt;% pull() boot &lt;- fli_small %&gt;% specify(response = arr_delay) %&gt;% generate(reps = 1000, type = bootstrap ) %&gt;% calculate(stat = mean ) %&gt;% pull() c(lower = x_bar - 2 * sd(boot), upper = x_bar + 2 * sd(boot)) ## lower upper ## 1.122209 8.021791 One categorical (one proportion) p_hat &lt;- fli_small %&gt;% summarize(mean(day_hour == morning )) %&gt;% pull() boot &lt;- fli_small %&gt;% specify(response = day_hour, success = morning ) %&gt;% generate(reps = 1000, type = bootstrap ) %&gt;% calculate(stat = prop ) %&gt;% pull() c(lower = p_hat - 2 * sd(boot), upper = p_hat + 2 * sd(boot)) ## lower upper ## 0.4194756 0.5125244 One numerical variable, one categorical (2 levels) (diff in means) d_hat &lt;- fli_small %&gt;% group_by(season) %&gt;% summarize(mean_stat = mean(arr_delay)) %&gt;% # Since summer - winter summarize(-diff(mean_stat)) %&gt;% pull() boot &lt;- fli_small %&gt;% specify(arr_delay ~ season) %&gt;% generate(reps = 1000, type = bootstrap ) %&gt;% calculate(stat = diff in means , order = c( summer , winter )) %&gt;% pull() c(lower = d_hat - 2 * sd(boot), upper = d_hat + 2 * sd(boot)) ## lower upper ## -7.704370 6.213971 Two categorical variables (diff in proportions) d_hat &lt;- fli_small %&gt;% group_by(season) %&gt;% summarize(prop = mean(day_hour == morning )) %&gt;% # Since summer - winter summarize(-diff(prop)) %&gt;% pull() boot &lt;- fli_small %&gt;% specify(day_hour ~ season, success = morning ) %&gt;% generate(reps = 1000, type = bootstrap ) %&gt;% calculate(stat = diff in props , order = c( summer , winter )) %&gt;% pull() c(lower = d_hat - 2 * sd(boot), upper = d_hat + 2 * sd(boot)) ## lower upper ## -0.07149487 0.11258550 Two numerical vars - SLR slope_hat &lt;- lm(arr_delay ~ dep_delay, data = fli_small) %&gt;% broom::tidy() %&gt;% filter(term == dep_delay ) %&gt;% pull(estimate) boot &lt;- fli_small %&gt;% specify(arr_delay ~ dep_delay) %&gt;% generate(reps = 1000, type = bootstrap ) %&gt;% calculate(stat = slope ) %&gt;% pull() c(lower = slope_hat - 2 * sd(boot), upper = slope_hat + 2 * sd(boot)) ## lower upper ## 0.9657595 1.0681384 Examples using mtcars data https://cran.r-project.org/web/packages/infer/vignettes/mtcars_examples.html Examples using mtcars data Chester Ismay and Andrew Bray 2018-01-05 Note: The type argument in generate() is automatically filled based on the entries for specify() and hypothesize(). It can be removed throughout the examples that follow. It is left in to reiterate the type of generation process being performed. Data preparation library(infer) library(dplyr) mtcars &lt;- mtcars %&gt;% mutate(cyl = factor(cyl), vs = factor(vs), am = factor(am), gear = factor(gear), carb = factor(carb)) # For reproducibility set.seed(2018) One numerical variable (mean) mtcars %&gt;% specify(response = mpg) %&gt;% # formula alt: mpg ~ NULL hypothesize(null = point , mu = 25) %&gt;% generate(reps = 100, type = bootstrap ) %&gt;% calculate(stat = mean ) ## # A tibble: 100 x 2 ## replicate stat ## &lt;int&gt; &lt;dbl&gt; ## 1 1 26.6 ## 2 2 25.1 ## 3 3 25.2 ## 4 4 24.7 ## 5 5 24.6 ## 6 6 25.8 ## 7 7 24.7 ## 8 8 25.6 ## 9 9 25.0 ## 10 10 25.1 ## # ... with 90 more rows One numerical variable (median) mtcars %&gt;% specify(response = mpg) %&gt;% # formula alt: mpg ~ NULL hypothesize(null = point , med = 26) %&gt;% generate(reps = 100, type = bootstrap ) %&gt;% calculate(stat = median ) ## # A tibble: 100 x 2 ## replicate stat ## &lt;int&gt; &lt;dbl&gt; ## 1 1 28.2 ## 2 2 27.2 ## 3 3 26.2 ## 4 4 26 ## 5 5 26.5 ## 6 6 24.5 ## 7 7 26 ## 8 8 28.2 ## 9 9 28.2 ## 10 10 23.2 ## # ... with 90 more rows One categorical (2 level) variable mtcars %&gt;% specify(response = am, success = 1 ) %&gt;% # formula alt: am ~ NULL hypothesize(null = point , p = .25) %&gt;% generate(reps = 100, type = simulate ) %&gt;% calculate(stat = prop ) ## # A tibble: 100 x 2 ## replicate stat ## &lt;fct&gt; &lt;dbl&gt; ## 1 1 0.375 ## 2 2 0.0625 ## 3 3 0.125 ## 4 4 0.25 ## 5 5 0.188 ## 6 6 0.406 ## 7 7 0.219 ## 8 8 0.375 ## 9 9 0.344 ## 10 10 0.188 ## # ... with 90 more rows Two categorical (2 level) variables mtcars %&gt;% specify(am ~ vs, success = 1 ) %&gt;% # alt: response = am, explanatory = vs hypothesize(null = independence ) %&gt;% generate(reps = 100, type = permute ) %&gt;% calculate(stat = diff in props , order = c( 0 , 1 )) ## # A tibble: 100 x 2 ## replicate stat ## &lt;int&gt; &lt;dbl&gt; ## 1 1 -0.421 ## 2 2 -0.167 ## 3 3 -0.421 ## 4 4 -0.0397 ## 5 5 0.0873 ## 6 6 -0.0397 ## 7 7 -0.0397 ## 8 8 -0.0397 ## 9 9 0.0873 ## 10 10 -0.167 ## # ... with 90 more rows One categorical (&gt;2 level) - GoF mtcars %&gt;% specify(cyl ~ NULL) %&gt;% # alt: response = cyl hypothesize(null = point , p = c( 4 = .5, 6 = .25, 8 = .25)) %&gt;% generate(reps = 100, type = simulate ) %&gt;% calculate(stat = Chisq ) ## # A tibble: 100 x 2 ## replicate stat ## &lt;fct&gt; &lt;dbl&gt; ## 1 1 6.75 ## 2 2 1.69 ## 3 3 3.19 ## 4 4 1.69 ## 5 5 6 ## 6 6 2.69 ## 7 7 4.75 ## 8 8 0.75 ## 9 9 0.688 ## 10 10 3.69 ## # ... with 90 more rows Two categorical (&gt;2 level) variables mtcars %&gt;% specify(cyl ~ am) %&gt;% # alt: response = cyl, explanatory = am hypothesize(null = independence ) %&gt;% generate(reps = 100, type = permute ) %&gt;% calculate(stat = Chisq ) ## # A tibble: 100 x 2 ## replicate stat ## &lt;int&gt; &lt;dbl&gt; ## 1 1 1.34 ## 2 2 1.63 ## 3 3 1.63 ## 4 4 2.63 ## 5 5 3.90 ## 6 6 1.74 ## 7 7 0.126 ## 8 8 1.74 ## 9 9 1.34 ## 10 10 1.34 ## # ... with 90 more rows One numerical variable one categorical (2 levels) (diff in means) mtcars %&gt;% specify(mpg ~ am) %&gt;% # alt: response = mpg, explanatory = am hypothesize(null = independence ) %&gt;% generate(reps = 100, type = permute ) %&gt;% calculate(stat = diff in means , order = c( 0 , 1 )) ## # A tibble: 100 x 2 ## replicate stat ## &lt;int&gt; &lt;dbl&gt; ## 1 1 -1.10 ## 2 2 0.217 ## 3 3 -1.08 ## 4 4 -3.80 ## 5 5 3.08 ## 6 6 0.489 ## 7 7 2.34 ## 8 8 4.10 ## 9 9 -1.86 ## 10 10 -0.210 ## # ... with 90 more rows One numerical variable one categorical (2 levels) (diff in medians) mtcars %&gt;% specify(mpg ~ am) %&gt;% # alt: response = mpg, explanatory = am hypothesize(null = independence ) %&gt;% generate(reps = 100, type = permute ) %&gt;% calculate(stat = diff in medians , order = c( 0 , 1 )) ## # A tibble: 100 x 2 ## replicate stat ## &lt;int&gt; &lt;dbl&gt; ## 1 1 0.5 ## 2 2 -1.10 ## 3 3 5.20 ## 4 4 1.8 ## 5 5 0.5 ## 6 6 3.3 ## 7 7 -1.60 ## 8 8 -2.3 ## 9 9 2.90 ## 10 10 -0.5 ## # ... with 90 more rows One numerical one categorical (&gt;2 levels) - ANOVA mtcars %&gt;% specify(mpg ~ cyl) %&gt;% # alt: response = mpg, explanatory = cyl hypothesize(null = independence ) %&gt;% generate(reps = 100, type = permute ) %&gt;% calculate(stat = F ) ## # A tibble: 100 x 2 ## replicate stat ## &lt;int&gt; &lt;dbl&gt; ## 1 1 1.43 ## 2 2 1.65 ## 3 3 0.318 ## 4 4 0.393 ## 5 5 1.05 ## 6 6 0.826 ## 7 7 1.32 ## 8 8 0.833 ## 9 9 0.144 ## 10 10 0.365 ## # ... with 90 more rows Two numerical vars - SLR mtcars %&gt;% specify(mpg ~ hp) %&gt;% # alt: response = mpg, explanatory = cyl hypothesize(null = independence ) %&gt;% generate(reps = 100, type = permute ) %&gt;% calculate(stat = slope ) ## # A tibble: 100 x 2 ## replicate stat ## &lt;int&gt; &lt;dbl&gt; ## 1 1 -0.0151 ## 2 2 0.00224 ## 3 3 -0.0120 ## 4 4 0.00292 ## 5 5 0.0203 ## 6 6 -0.00730 ## 7 7 -0.0246 ## 8 8 0.00555 ## 9 9 0.0109 ## 10 10 0.0176 ## # ... with 90 more rows One numerical variable (standard deviation) Not currently implemented mtcars %&gt;% specify(response = mpg) %&gt;% # formula alt: mpg ~ NULL hypothesize(null = point , sigma = 5) %&gt;% generate(reps = 100, type = bootstrap ) %&gt;% calculate(stat = sd ) Confidence intervals One numerical (one mean) mtcars %&gt;% specify(response = mpg) %&gt;% generate(reps = 100, type = bootstrap ) %&gt;% calculate(stat = mean ) ## # A tibble: 100 x 2 ## replicate stat ## &lt;int&gt; &lt;dbl&gt; ## 1 1 19.6 ## 2 2 21.8 ## 3 3 18.7 ## 4 4 19.2 ## 5 5 21.6 ## 6 6 19.9 ## 7 7 20.7 ## 8 8 19.3 ## 9 9 21.2 ## 10 10 21.3 ## # ... with 90 more rows One numerical (one median) mtcars %&gt;% specify(response = mpg) %&gt;% generate(reps = 100, type = bootstrap ) %&gt;% calculate(stat = median ) ## # A tibble: 100 x 2 ## replicate stat ## &lt;int&gt; &lt;dbl&gt; ## 1 1 19.2 ## 2 2 20.1 ## 3 3 21 ## 4 4 17.8 ## 5 5 20.1 ## 6 6 19.2 ## 7 7 18.4 ## 8 8 19.2 ## 9 9 19.2 ## 10 10 18.0 ## # ... with 90 more rows One numerical (standard deviation) mtcars %&gt;% specify(response = mpg) %&gt;% generate(reps = 100, type = bootstrap ) %&gt;% calculate(stat = sd ) ## # A tibble: 100 x 2 ## replicate stat ## &lt;int&gt; &lt;dbl&gt; ## 1 1 5.28 ## 2 2 6.74 ## 3 3 5.29 ## 4 4 5.41 ## 5 5 5.56 ## 6 6 5.65 ## 7 7 6.17 ## 8 8 6.40 ## 9 9 6.31 ## 10 10 6.11 ## # ... with 90 more rows One categorical (one proportion) mtcars %&gt;% specify(response = am, success = 1 ) %&gt;% generate(reps = 100, type = bootstrap ) %&gt;% calculate(stat = prop ) ## # A tibble: 100 x 2 ## replicate stat ## &lt;int&gt; &lt;dbl&gt; ## 1 1 0.375 ## 2 2 0.406 ## 3 3 0.406 ## 4 4 0.312 ## 5 5 0.312 ## 6 6 0.469 ## 7 7 0.438 ## 8 8 0.281 ## 9 9 0.438 ## 10 10 0.5 ## # ... with 90 more rows One numerical variable one categorical (2 levels) (diff in means) mtcars %&gt;% specify(mpg ~ am) %&gt;% generate(reps = 100, type = bootstrap ) %&gt;% calculate(stat = diff in means , order = c( 0 , 1 )) ## # A tibble: 100 x 2 ## replicate stat ## &lt;int&gt; &lt;dbl&gt; ## 1 1 -9.38 ## 2 2 -5.11 ## 3 3 -4.88 ## 4 4 -5.39 ## 5 5 -9.19 ## 6 6 -7.20 ## 7 7 -5.34 ## 8 8 -3.20 ## 9 9 -5.95 ## 10 10 -11.0 ## # ... with 90 more rows Two categorical variables (diff in proportions) mtcars %&gt;% specify(am ~ vs, success = 1 ) %&gt;% generate(reps = 100, type = bootstrap ) %&gt;% calculate(stat = diff in props , order = c( 0 , 1 )) ## # A tibble: 100 x 2 ## replicate stat ## &lt;int&gt; &lt;dbl&gt; ## 1 1 -0.352 ## 2 2 -0.15 ## 3 3 -0.294 ## 4 4 -0.254 ## 5 5 -0.438 ## 6 6 -0.126 ## 7 7 -0.188 ## 8 8 0.167 ## 9 9 -0.143 ## 10 10 -0.5 ## # ... with 90 more rows Two numerical vars - SLR mtcars %&gt;% specify(mpg ~ hp) %&gt;% generate(reps = 100, type = bootstrap ) %&gt;% calculate(stat = slope ) ## # A tibble: 100 x 2 ## replicate stat ## &lt;int&gt; &lt;dbl&gt; ## 1 1 -0.0850 ## 2 2 -0.0512 ## 3 3 -0.0736 ## 4 4 -0.0569 ## 5 5 -0.0930 ## 6 6 -0.0659 ## 7 7 -0.0710 ## 8 8 -0.0767 ## 9 9 -0.0556 ## 10 10 -0.0627 ## # ... with 90 more rows Two numerical vars - correlation mtcars %&gt;% specify(mpg ~ hp) %&gt;% generate(reps = 100, type = bootstrap ) %&gt;% calculate(stat = correlation ) ## # A tibble: 100 x 2 ## replicate stat ## &lt;int&gt; &lt;dbl&gt; ## 1 1 -0.821 ## 2 2 -0.812 ## 3 3 -0.802 ## 4 4 -0.723 ## 5 5 -0.885 ## 6 6 -0.777 ## 7 7 -0.752 ## 8 8 -0.758 ## 9 9 -0.826 ## 10 10 -0.779 ## # ... with 90 more rows Two sample t test example using nycflights13 flights data https://cran.r-project.org/web/packages/infer/vignettes/two_sample_t.html Two sample t test example using nycflights13 flights data Chester Ismay 2018-11-15 Note: The type argument in generate() is automatically filled based on the entries for specify() and hypothesize(). It can be removed throughout the examples that follow. It is left in to reiterate the type of generation process being performed. Data preparation library(nycflights13) library(dplyr) library(stringr) library(infer) set.seed(2017) fli_small &lt;- flights %&gt;% sample_n(size = 500) %&gt;% mutate(half_year = case_when( between(month, 1, 6) ~ h1 , between(month, 7, 12) ~ h2 )) %&gt;% mutate(day_hour = case_when( between(hour, 1, 12) ~ morning , between(hour, 13, 24) ~ not morning )) %&gt;% select(arr_delay, dep_delay, half_year, day_hour, origin, carrier) Two numeric - arr_delay, dep_delay Two categories half_year ( h1 , h2 ), day_hour ( morning , not morning ) Three categories - origin ( EWR , JFK , LGA ) Sixteen categories - carrier One numerical variable, one categorical (2 levels) Calculate observed statistic The recommended approach is to use specify() %&gt;% calculate(): obs_t &lt;- fli_small %&gt;% specify(arr_delay ~ half_year) %&gt;% calculate(stat = t , order = c( h1 , h2 )) ## Warning: Removed 15 rows containing missing values. The observed t statistic is stat 0.8685 . Or using t_test in infer obs_t &lt;- fli_small %&gt;% t_test(formula = arr_delay ~ half_year, alternative = two_sided , order = c( h1 , h2 )) %&gt;% dplyr::pull(statistic) The observed t statistic is 0.8685. Or using another shortcut function in infer: obs_t &lt;- fli_small %&gt;% t_stat(formula = arr_delay ~ half_year, order = c( h1 , h2 )) The observed t statistic is statistic 0.8685 . Randomization approach to t-statistic t_null_perm &lt;- fli_small %&gt;% # alt: response = arr_delay, explanatory = half_year specify(arr_delay ~ half_year) %&gt;% hypothesize(null = independence ) %&gt;% generate(reps = 1000, type = permute ) %&gt;% calculate(stat = t , order = c( h1 , h2 )) ## Warning: Removed 15 rows containing missing values. visualize(t_null_perm) + shade_p_value(obs_stat = obs_t, direction = two_sided ) Calculate the randomization-based p-value t_null_perm %&gt;% get_p_value(obs_stat = obs_t, direction = two_sided ) p_value 0.408 Theoretical distribution t_null_theor &lt;- fli_small %&gt;% # alt: response = arr_delay, explanatory = half_year specify(arr_delay ~ half_year) %&gt;% hypothesize(null = independence ) %&gt;% # generate() ## Not used for theoretical calculate(stat = t , order = c( h1 , h2 )) ## Warning: Removed 15 rows containing missing values. visualize(t_null_theor, method = theoretical ) + shade_p_value(obs_stat = obs_t, direction = two_sided ) ## Warning: Check to make sure the conditions have been met for the ## theoretical method. {infer} currently does not check these for you. Overlay appropriate t distribution on top of permuted t-statistics visualize(t_null_perm, method = both ) + shade_p_value(obs_stat = obs_t, direction = two_sided ) ## Warning: Check to make sure the conditions have been met for the ## theoretical method. {infer} currently does not check these for you. Compute theoretical p-value fli_small %&gt;% t_test(formula = arr_delay ~ half_year, alternative = two_sided , order = c( h1 , h2 )) %&gt;% dplyr::pull(p_value) ## [1] 0.3855 "],["compare-proportions-1.html", "Chapter 89 Compare Proportions", " Chapter 89 Compare Proportions {r eval=FALSE, include=FALSE, echo=TRUE} prop.test(numerator,denominator) {r eval=FALSE, include=FALSE, echo=TRUE} table(impetigo = scabies$impetigo_active, scabies = scabies$scabies_infestation) # dependent ~ independent {r eval=FALSE, include=FALSE, echo=TRUE} #See that because &#39;no&#39; is the &#39;base&#39; level the table is laid out # No Disease Has Disease # Not-exposed # Exposed #This is dependent on how your data is coded so you need to check this before using epi.2by2 #If the table is laid out correctly then you can input straight into epi.2by2, otherwise you #need to recode or re-order the variables so that the table will be laid out correctly #epi.2by2 wants the data with the exposed/disease group in top right corner #So we just tell R to order the variables differently when we draw the table epiR::epi.2by2(table(relevel(scabies$scabies_infestation, yes ), relevel(scabies$impetigo_active, yes ))) "],["contingency-tables-1.html", "Chapter 90 contingency tables", " Chapter 90 contingency tables {r eval=FALSE, include=FALSE, echo=TRUE} ?chisq.test() chisq.test {stats} R Documentation Pearson&#39;s Chi-squared Test for Count Data Description chisq.test performs chi-squared contingency table tests and goodness-of-fit tests. Usage chisq.test(x, y = NULL, correct = TRUE, p = rep(1/length(x), length(x)), rescale.p = FALSE, simulate.p.value = FALSE, B = 2000) Arguments x a numeric vector or matrix. x and y can also both be factors. y a numeric vector; ignored if x is a matrix. If x is a factor, y should be a factor of the same length. correct a logical indicating whether to apply continuity correction when computing the test statistic for 2 by 2 tables: one half is subtracted from all |O - E| differences; however, the correction will not be bigger than the differences themselves. No correction is done if simulate.p.value = TRUE. p a vector of probabilities of the same length of x. An error is given if any entry of p is negative. rescale.p a logical scalar; if TRUE then p is rescaled (if necessary) to sum to 1. If rescale.p is FALSE, and p does not sum to 1, an error is given. simulate.p.value a logical indicating whether to compute p-values by Monte Carlo simulation. B an integer specifying the number of replicates used in the Monte Carlo test. Details If x is a matrix with one row or column, or if x is a vector and y is not given, then a goodness-of-fit test is performed (x is treated as a one-dimensional contingency table). The entries of x must be non-negative integers. In this case, the hypothesis tested is whether the population probabilities equal those in p, or are all equal if p is not given. If x is a matrix with at least two rows and columns, it is taken as a two-dimensional contingency table: the entries of x must be non-negative integers. Otherwise, x and y must be vectors or factors of the same length; cases with missing values are removed, the objects are coerced to factors, and the contingency table is computed from these. Then Pearson&#39;s chi-squared test is performed of the null hypothesis that the joint distribution of the cell counts in a 2-dimensional contingency table is the product of the row and column marginals. If simulate.p.value is FALSE, the p-value is computed from the asymptotic chi-squared distribution of the test statistic; continuity correction is only used in the 2-by-2 case (if correct is TRUE, the default). Otherwise the p-value is computed for a Monte Carlo test (Hope, 1968) with B replicates. In the contingency table case simulation is done by random sampling from the set of all contingency tables with given marginals, and works only if the marginals are strictly positive. Continuity correction is never used, and the statistic is quoted without it. Note that this is not the usual sampling situation assumed for the chi-squared test but rather that for Fisher&#39;s exact test. In the goodness-of-fit case simulation is done by random sampling from the discrete distribution specified by p, each sample being of size n = sum(x). This simulation is done in R and may be slow. Value A list with class htest containing the following components: statistic the value the chi-squared test statistic. parameter the degrees of freedom of the approximate chi-squared distribution of the test statistic, NA if the p-value is computed by Monte Carlo simulation. p.value the p-value for the test. method a character string indicating the type of test performed, and whether Monte Carlo simulation or continuity correction was used. data.name a character string giving the name(s) of the data. observed the observed counts. expected the expected counts under the null hypothesis. residuals the Pearson residuals, (observed - expected) / sqrt(expected). stdres standardized residuals, (observed - expected) / sqrt(V), where V is the residual cell variance (Agresti, 2007, section 2.4.5 for the case where x is a matrix, n * p * (1 - p) otherwise). Source The code for Monte Carlo simulation is a C translation of the Fortran algorithm of Patefield (1981). References Hope, A. C. A. (1968). A simplified Monte Carlo significance test procedure. Journal of the Royal Statistical Society Series B, 30, 582‚Äì598. http://www.jstor.org/stable/2984263. Patefield, W. M. (1981). Algorithm AS 159: An efficient method of generating r x c tables with given row and column totals. Applied Statistics, 30, 91‚Äì97. doi: 10.2307/2346669. Agresti, A. (2007). An Introduction to Categorical Data Analysis, 2nd ed. New York: John Wiley &amp; Sons. Page 38. See Also For goodness-of-fit testing, notably of continuous distributions, ks.test. Examples ## From Agresti(2007) p.39 M &lt;- as.table(rbind(c(762, 327, 468), c(484, 239, 477))) dimnames(M) &lt;- list(gender = c( F , M ), party = c( Democrat , Independent , Republican )) (Xsq &lt;- chisq.test(M)) # Prints test summary Xsq$observed # observed counts (same as M) Xsq$expected # expected counts under the null Xsq$residuals # Pearson residuals Xsq$stdres # standardized residuals ## Effect of simulating p-values x &lt;- matrix(c(12, 5, 7, 7), ncol = 2) chisq.test(x)$p.value # 0.4233 chisq.test(x, simulate.p.value = TRUE, B = 10000)$p.value # around 0.29! ## Testing for population probabilities ## Case A. Tabulated data x &lt;- c(A = 20, B = 15, C = 25) chisq.test(x) chisq.test(as.table(x)) # the same x &lt;- c(89,37,30,28,2) p &lt;- c(40,20,20,15,5) try( chisq.test(x, p = p) # gives an error ) chisq.test(x, p = p, rescale.p = TRUE) # works p &lt;- c(0.40,0.20,0.20,0.19,0.01) # Expected count in category 5 # is 1.86 &lt; 5 ==&gt; chi square approx. chisq.test(x, p = p) # maybe doubtful, but is ok! chisq.test(x, p = p, simulate.p.value = TRUE) ## Case B. Raw data x &lt;- trunc(5 * runif(100)) chisq.test(table(x)) # NOT &#39;chisq.test(x)&#39;! [Package stats version 3.5.1 Index] {r eval=FALSE, include=FALSE, echo=TRUE} observed_table &lt;- matrix(c(35, 15, 50, 10, 30, 60), nrow = 2, ncol = 3, byrow = T) rownames(observed_table) &lt;- c(&#39;Female&#39;, &#39;Male&#39;) colnames(observed_table) &lt;- c(&#39;Archery&#39;, &#39;Boxing&#39;, &#39;Cycling&#39;) observed_table {r eval=FALSE, include=FALSE, echo=TRUE} X &lt;- chisq.test(observed_table) X {r eval=FALSE, include=FALSE, echo=TRUE} X$expected "],["infer-1.html", "Chapter 91 infer", " Chapter 91 infer Chi-squared test example using nycflights13 flights data https://cran.r-project.org/web/packages/infer/vignettes/chisq_test.html {r eval=FALSE, include=FALSE, echo=TRUE} library(nycflights13) library(dplyr) library(ggplot2) library(stringr) library(infer) set.seed(2017) {r eval=FALSE, include=FALSE, echo=TRUE} fli_small &lt;- flights %&gt;% na.omit() %&gt;% sample_n(size = 500) %&gt;% mutate(season = case_when( month %in% c(10:12, 1:3) ~ winter , month %in% c(4:9) ~ summer )) %&gt;% mutate(day_hour = case_when( between(hour, 1, 12) ~ morning , between(hour, 13, 24) ~ not morning )) %&gt;% select(arr_delay, dep_delay, season, day_hour, origin, carrier) fli_small {r eval=FALSE, include=FALSE, echo=TRUE} obs_chisq &lt;- fli_small %&gt;% specify(origin ~ season) %&gt;% # alt: response = origin, explanatory = season calculate(stat = Chisq ) obs_chisq {r eval=FALSE, include=FALSE, echo=TRUE} obs_chisq &lt;- fli_small %&gt;% chisq_test(formula = origin ~ season) %&gt;% dplyr::select(statistic) obs_chisq {r eval=FALSE, include=FALSE, echo=TRUE} obs_chisq &lt;- fli_small %&gt;% chisq_stat(formula = origin ~ season) obs_chisq {r eval=FALSE, include=FALSE, echo=TRUE} chisq_null_perm &lt;- fli_small %&gt;% specify(origin ~ season) %&gt;% # alt: response = origin, explanatory = season hypothesize(null = independence ) %&gt;% generate(reps = 1000, type = permute ) %&gt;% calculate(stat = Chisq ) visualize(chisq_null_perm) + shade_p_value(obs_stat = obs_chisq, direction = greater ) {r eval=FALSE, include=FALSE, echo=TRUE} chisq_null_perm %&gt;% get_p_value(obs_stat = obs_chisq, direction = greater ) {r eval=FALSE, include=FALSE, echo=TRUE} chisq_null_theor &lt;- fli_small %&gt;% specify(origin ~ season) %&gt;% hypothesize(null = independence ) %&gt;% # generate() ## Not used for theoretical calculate(stat = Chisq ) chisq_null_theor {r eval=FALSE, include=FALSE, echo=TRUE} visualize(chisq_null_theor, method = theoretical ) + shade_p_value(obs_stat = obs_chisq, direction = right ) {r eval=FALSE, include=FALSE, echo=TRUE} visualize(chisq_null_perm, method = both ) + shade_p_value(obs_stat = obs_chisq, direction = right ) {r eval=FALSE, include=FALSE, echo=TRUE} fli_small %&gt;% chisq_test(formula = origin ~ season) %&gt;% dplyr::pull(p_value) "],["correlations.html", "Chapter 92 Correlations", " Chapter 92 Correlations "],["comparisons-between-correlations.html", "Chapter 93 comparisons between correlations", " Chapter 93 comparisons between correlations http://comparingcorrelations.org/ "],["exploring-correlations-in-r-with-corrr.html", "Chapter 94 Exploring correlations in R with corrr", " Chapter 94 Exploring correlations in R with corrr https://drsimonj.svbtle.com/exploring-correlations-in-r-with-corrr "],["d3rain.html", "Chapter 95 d3rain", " Chapter 95 d3rain {r eval=FALSE, include=FALSE, echo=TRUE} library(dplyr) library(d3rain) armed_levels &lt;- rev(c(&#39;Unarmed&#39;, &#39;Knife&#39;, &#39;Non-lethal firearm&#39;, &#39;Firearm&#39;)) pk &lt;- fivethirtyeight::police_killings %&gt;% mutate(armed = recode(armed, No = Unarmed )) %&gt;% mutate(armed = factor(armed, levels = armed_levels)) %&gt;% filter(armed %in% armed_levels, !is.na(age)) pk %&gt;% arrange(age) %&gt;% d3rain(age, armed, toolTip = age, title = 2015 Police Killings by Age, Armed Status ) %&gt;% drip_settings(dripSequence = &#39;iterate&#39;, ease = &#39;linear&#39;, jitterWidth = 25, dripSpeed = 500, dripFill = &#39;firebrick&#39;, iterationSpeedX = 20) %&gt;% chart_settings(fontFamily = &#39;times&#39;, yAxisTickLocation = &#39;left&#39;) "],["data-list-1.html", "Chapter 96 Data List", " Chapter 96 Data List Learning Clinical Epidemiology with R http://datacompass.lshtm.ac.uk/599/ ISLR {r eval=FALSE, include=FALSE, echo=TRUE} data(package = ISLR ) acs Download, Manipulate, and Present American Community Survey and Decennial Data from the US Census https://cran.r-project.org/web/packages/acs/index.html eurostat Tools for Eurostat Open Data https://cran.r-project.org/web/packages/eurostat/index.html Rilostat https://github.com/ilostat/Rilostat OECD https://cran.r-project.org/web/packages/OECD/vignettes/oecd_vignette_main.pdf gapminder Factfulness: Building Gapminder Income Mountains http://staff.math.su.se/hoehle/blog/2018/07/02/factfulness.html nycflights13 fivethirtyeight projects https://www.analyticsvidhya.com/blog/2014/11/data-science-projects-learn/ Miscellaneous Datasets http://users.stat.ufl.edu/~winner/datasets.html datasets https://www.rdocumentation.org/packages/datasets/versions/3.5.1 "],["data-science-live-book.html", "Chapter 97 Data Science Live Book", " Chapter 97 Data Science Live Book https://livebook.datascienceheroes.com/ {r eval=FALSE, include=FALSE, echo=TRUE} # Loading funModeling! library(funModeling) library(dplyr) data(heart_disease) {r eval=FALSE, include=FALSE, echo=TRUE} # Profiling the data input df_status(heart_disease) "],["data-table-package.html", "Chapter 98 data.table package", " Chapter 98 data.table package "],["rdatatable.html", "Chapter 99 Rdatatable 99.1 Introduction to data.table", " Chapter 99 Rdatatable https://github.com/Rdatatable {r eval=FALSE, include=FALSE, echo=TRUE} library(data.table) 99.1 Introduction to data.table https://cloud.r-project.org/web/packages/data.table/vignettes/datatable-intro.html {r eval=FALSE, include=FALSE, echo=TRUE} input &lt;- if (file.exists( flights14.csv )) { flights14.csv } else { https://raw.githubusercontent.com/Rdatatable/data.table/master/vignettes/flights14.csv } flights &lt;- fread(input) flights {r eval=FALSE, include=FALSE, echo=TRUE} ?fread DT = data.table( ID = c( b , b , b , a , a , c ), a = 1:6, b = 7:12, c = 13:18 ) DT class(DT$ID) getOption( datatable.print.nrows ) ans &lt;- flights[origin == JFK &amp; month == 6L] head(ans) ans &lt;- flights[1:2] ans ans &lt;- flights[origin == JFK &amp; month == 6L][1:2] head(ans) ans &lt;- flights[order(origin, -dest)] head(ans) ans &lt;- flights[, arr_delay] head(ans) ans &lt;- flights[, arr_delay, dest] head(ans) ans &lt;- flights[, list(arr_delay)] head(ans) ans &lt;- flights[, .(arr_delay)] head(ans) ans &lt;- flights[, .(arr_delay, dep_delay)] head(ans) ans &lt;- flights[, .(delay_arr = arr_delay, delay_dep = dep_delay)] head(ans) ans &lt;- flights[, sum( (arr_delay + dep_delay) &lt; 0 )] ans ans &lt;- flights[origin == JFK &amp; month == 6L, .(m_arr = mean(arr_delay), m_dep = mean(dep_delay))] ans ans &lt;- flights[origin == JFK &amp; month == 6L, length(dest)] ans ans &lt;- flights[origin == JFK &amp; month == 6L, .N] ans ans &lt;- flights[, c( arr_delay , dep_delay )] head(ans) select_cols = c( arr_delay , dep_delay ) flights[ , ..select_cols] flights[ , select_cols, with = FALSE] ans &lt;- flights[, !c( arr_delay , dep_delay )] ans &lt;- flights[, -c( arr_delay , dep_delay )] ans &lt;- flights[, year:day] ans &lt;- flights[, day:year] ans &lt;- flights[, -(year:day)] ans &lt;- flights[, !(year:day)] ans &lt;- flights[, .(.N), by = .(origin)] ans ans &lt;- flights[, .(.N), by = origin ] ans ans &lt;- flights[, .N, by = origin] ans ans &lt;- flights[carrier == AA , .N, by = origin] ans ans &lt;- flights[carrier == AA , .N, by = .(origin, dest)] head(ans) ans &lt;- flights[carrier == AA , .N, by = c( origin , dest )] ans ans &lt;- flights[carrier == AA , .(mean(arr_delay), mean(dep_delay)), by = .(origin, dest, month)] ans ans &lt;- flights[carrier == AA , .(mean(arr_delay), mean(dep_delay)), keyby = .(origin, dest, month)] ans ans &lt;- flights[carrier == AA , .N, by = .(origin, dest)] ans ans &lt;- flights[carrier == AA , .N, by = .(origin, dest)][order(origin, -dest)] head(ans, 10) ans &lt;- flights[, .N, .(dep_delay&gt;0, arr_delay&gt;0)] ans flights[, .N, .(dep_delayed = dep_delay&gt;0, arr_delayed = arr_delay&gt;0)] "],["cheat-sheet.html", "Chapter 100 cheat sheet 100.1 Subsetting Rows Using i 100.2 Manipulating on Columns in j 100.3 Doing j by Group 100.4 Adding/Updating Columns By Reference in j Using := 100.5 Indexing And Keys", " Chapter 100 cheat sheet https://www..com/community/tutorials/data-table-cheat-sheet https://s3.amazonaws.com/assets..com/blog_assets/datatable_Cheat_Sheet_R.pdf {r eval=FALSE, include=FALSE, echo=TRUE} library(data.table) http://r-datatable.com https://github.com/Rdatatable/data.table/wiki {r eval=FALSE, include=FALSE, echo=TRUE} set.seed(45L) DT &lt;- data.table(V1 = c(1L,2L), V2 = LETTERS[1:3], V3 = round(rnorm(4),4), V4 = 1:12) {r eval=FALSE, include=FALSE, echo=TRUE} DT {r eval=FALSE, include=FALSE, echo=TRUE} typeof(DT) {r eval=FALSE, include=FALSE, echo=TRUE} class(DT) 100.1 Subsetting Rows Using i {r eval=FALSE, include=FALSE, echo=TRUE} DT[3:5,] {r eval=FALSE, include=FALSE, echo=TRUE} DT[3:5] {r eval=FALSE, include=FALSE, echo=TRUE} DT[V2== A ] {r eval=FALSE, include=FALSE, echo=TRUE} DT[V2 %in% c( A , C )] 100.2 Manipulating on Columns in j sonu√ß vekt√∂r olarak alƒ±nacaksa sadece s√ºtun ismi yazƒ±lƒ±yor {r eval=FALSE, include=FALSE, echo=TRUE} DT[,V2] sonu√ß data.frame olarak alƒ±nacaksa s√ºtun ismi √∂n√ºnde . yazƒ±lƒ±yor {r eval=FALSE, include=FALSE, echo=TRUE} DT[,.(V2)] {r eval=FALSE, include=FALSE, echo=TRUE} DT[,.(V2,V3)] tek s√ºtun √ºzerinden √∂zet alma {r eval=FALSE, include=FALSE, echo=TRUE} DT[,sum(V1)] birden fazla s√ºtun √ºzerinden √∂zet alma {r eval=FALSE, include=FALSE, echo=TRUE} DT[,.(sum(V1),sd(V3))] {r eval=FALSE, include=FALSE, echo=TRUE} DT[,.(Aggregate = sum(V1), Sd.V3 = sd(V3))] {r eval=FALSE, include=FALSE, echo=TRUE} DT[,.(V1,Sd.V3=sd(V3))] {r eval=FALSE, include=FALSE, echo=TRUE} DT[,.(print(V2), plot(V3), NULL)] 100.3 Doing j by Group {r eval=FALSE, include=FALSE, echo=TRUE} DT[,.(V4.Sum = sum(V4)),by = V1] {r eval=FALSE, include=FALSE, echo=TRUE} DT[,.(V4.Sum = sum(V4)), by = .(V1,V2)] {r eval=FALSE, include=FALSE, echo=TRUE} DT[,.(V4.Sum = sum(V4)), by = sign(V1-1)] {r eval=FALSE, include=FALSE, echo=TRUE} DT[,.(V4.Sum = sum(V4)), by = .(V1.01 = sign(V1 - 1))] {r eval=FALSE, include=FALSE, echo=TRUE} DT[1:5,.(V4.Sum = sum(V4)), by = V1] {r eval=FALSE, include=FALSE, echo=TRUE} DT[,.N,by = V1] 100.4 Adding/Updating Columns By Reference in j Using := {r eval=FALSE, include=FALSE, echo=TRUE} DT[,V1:=round(exp(V1),2)] DT {r eval=FALSE, include=FALSE, echo=TRUE} DT[,V5:=round(exp(V1),2)] DT {r eval=FALSE, include=FALSE, echo=TRUE} DT[,c( V1 , V2 ):=list(round(exp(V1),2), LETTERS[4:6])] DT {r eval=FALSE, include=FALSE, echo=TRUE} DT[,&#39;:=&#39;(V1=round(exp(V1),2), V2=LETTERS[4:6])][] {r eval=FALSE, include=FALSE, echo=TRUE} DT[,V1:=NULL] DT {r eval=FALSE, include=FALSE, echo=TRUE} DT[,c( V1 , V2 ):=NULL][] {r eval=FALSE, include=FALSE, echo=TRUE} Cols.chosen = c( A , B ) DT[,Cols.Chosen:=NULL] {r eval=FALSE, include=FALSE, echo=TRUE} Cols.chosen = c( A , B ) DT[,(Cols.Chosen):=NULL] 100.5 Indexing And Keys {r eval=FALSE, include=FALSE, echo=TRUE} setkey(DT,V2) {r eval=FALSE, include=FALSE, echo=TRUE} DT[ A ] {r eval=FALSE, include=FALSE, echo=TRUE} DT[c( A , C )] {r eval=FALSE, include=FALSE, echo=TRUE} DT[ A ,mult= first ] {r eval=FALSE, include=FALSE, echo=TRUE} DT[ A ,mult= last ] {r eval=FALSE, include=FALSE, echo=TRUE} DT[c( A , D )] {r eval=FALSE, include=FALSE, echo=TRUE} DT[c( A , D ),nomatch=0] {r eval=FALSE, include=FALSE, echo=TRUE} DT[c( A , C ),sum(V4)] {r eval=FALSE, include=FALSE, echo=TRUE} DT[c( A , C ), sum(V4), by=.EACHI] {r eval=FALSE, include=FALSE, echo=TRUE} setkey(DT,V1,V2) {r eval=FALSE, include=FALSE, echo=TRUE} DT[.(2, C )] {r eval=FALSE, include=FALSE, echo=TRUE} DT[.(2,c( A , C ))] "],["data-tools-1.html", "Chapter 101 Data Tools", " Chapter 101 Data Tools Installations for Data Science. Anaconda, RStudio, Spark, TensorFlow, AWS (Amazon Web Services). https://medium.com/@GalarnykMichael https://github.com/mGalarnyk/Installations_Mac_Ubuntu_Windows Google Cloud for Data Science: Beginner‚Äôs Guide https://www..com/community/tutorials/google-cloud-data-science Deep Learning With Jupyter Notebooks In The Cloud https://www..com/community/tutorials/deep-learning-jupyter-aws https://www..com/community/tutorials/homebrew-install-use system() function works when I use R from terminal but not from RStudio #2193 https://github.com/rstudio/rstudio/issues/2193 myTerm &lt;- rstudioapi::terminalCreate(show = FALSE) rstudioapi::terminalSend(myTerm, esearch -db pubmed -query &#39;(diabetes AND pregnancy) AND (\\ 2017/01/01\\ [PDAT] : \\ 2017/12/31\\ [PDAT])&#39; | efetch -format xml | xtract -pattern Grant -element Agency | sort-uniq-count-rank | head -n 10 &gt; myquery.txt \\n ) Sys.sleep(1) repeat{ Sys.sleep(0.1) if(rstudioapi::terminalBusy(myTerm) == FALSE){ print( Code Executed ) break } } {r eval=FALSE, include=FALSE, echo=TRUE} library(datasets) # initialize library(help=datasets) # display the datasets {r eval=FALSE, include=FALSE, echo=TRUE} class(airquality) # get class sapply(airquality, class) # get class of all columns str(airquality) # structure summary(airquality) # summary of airquality head(airquality) # view the first 6 obs fix(airquality) # view spreadsheet like grid View(airquality) rownames(airquality) # row names colnames(airquality) # columns names nrow(airquality) # number of rows ncol(airquality) # number of columns "],["my-r-codes-for-data-analysis-3.html", "Chapter 102 My R Codes For Data Analysis", " Chapter 102 My R Codes For Data Analysis "],["decision-trees-1.html", "Chapter 103 Decision Trees", " Chapter 103 Decision Trees {r eval=FALSE, include=FALSE, echo=TRUE} # install.packages( ISLR ) library(ISLR) data(package = ISLR ) carseats &lt;- Carseats carseats {r eval=FALSE, include=FALSE, echo=TRUE} # install.packages( tree ) library(tree) require(tree) {r eval=FALSE, include=FALSE, echo=TRUE} names(carseats) {r eval=FALSE, include=FALSE, echo=TRUE} hist(carseats$Sales) {r eval=FALSE, include=FALSE, echo=TRUE} High &lt;- ifelse(carseats$Sales &lt;= 8, No , Yes ) carseats &lt;- data.frame(carseats, High) carseats {r eval=FALSE, include=FALSE, echo=TRUE} tree.carseats &lt;- tree::tree(High~.-Sales, data = carseats) {r eval=FALSE, include=FALSE, echo=TRUE} tree.carseats {r eval=FALSE, include=FALSE, echo=TRUE} set.seed(101) train &lt;- sample(1:nrow(carseats), 250) {r eval=FALSE, include=FALSE, echo=TRUE} train {r eval=FALSE, fig.height=6, fig.width=12, include=FALSE} tree.carseats &lt;- tree(High~.-Sales, carseats, subset=train) plot(tree.carseats) text(tree.carseats, pretty=0) {r eval=FALSE, include=FALSE, echo=TRUE} tree.pred &lt;- predict(tree.carseats, carseats[-train,], type = class ) {r eval=FALSE, include=FALSE, echo=TRUE} tree.pred {r eval=FALSE, include=FALSE, echo=TRUE} with(carseats[-train,], table(tree.pred, High)) {r eval=FALSE, include=FALSE, echo=TRUE} cv.carseats &lt;- cv.tree(tree.carseats, FUN = prune.misclass) cv.carseats {r eval=FALSE, include=FALSE, echo=TRUE} plot(cv.carseats) prune.carseats = prune.misclass(tree.carseats, best = 12) plot(prune.carseats) text(prune.carseats, pretty=0) It&#39;s a bit shallower than previous trees, and you can actually read the labels. Let&#39;s evaluate it on the test dataset again. tree.pred = predict(prune.carseats, carseats[-train,], type= class ) with(carseats[-train,], table(tree.pred, High)) (74 + 39) / 150 Seems like the correct classifications dropped a little bit. It has done about the same as your original tree, so pruning did not hurt much with respect to misclassification errors, and gave a simpler tree. Often case, trees don&#39;t give very good prediction errors, so let&#39;s go ahead take a look at random forests and boosting, which tend to outperform trees as far as prediction and misclassification are concerned. Random Forests For this part, you will use the Boston housing data to explore random forests and boosting. The dataset is located in the MASS package. It gives housing values and other statistics in each of 506 suburbs of Boston based on a 1970 census. library(MASS) data(package= MASS ) boston&lt;-Boston dim(boston) names(boston) Let&#39;s also load the randomForest package. require(randomForest) To prepare data for random forest, let&#39;s set the seed and create a sample training set of 300 observations. set.seed(101) train = sample(1:nrow(boston), 300) In this dataset, there are 506 surburbs of Boston. For each surburb, you have variables such as crime per capita, types of industry, average # of rooms per dwelling, average proportion of age of the houses etc. Let&#39;s use medv - the median value of owner-occupied homes for each of these surburbs, as the response variable. Let&#39;s fit a random forest and see how well it performs. As being said, you use the response medv, the median housing value (in $1K dollars), and the training sample set. rf.boston = randomForest(medv~., data = boston, subset = train) rf.boston Printing out the random forest gives its summary: the # of trees (500 were grown), the mean squared residuals (MSR), and the percentage of variance explained. The MSR and % variance explained are based on the out-of-bag estimates, a very clever device in random forests to get honest error estimates. The only tuning parameter in a random Forests is the argument called mtry, which is the number of variables that are selected at each split of each tree when you make a split. As seen here, mtry is 4 of the 13 exploratory variables (excluding medv) in the Boston Housing data - meaning that each time the tree comes to split a node, 4 variables would be selected at random, then the split would be confined to 1 of those 4 variables. That&#39;s how randomForests de-correlates the trees. You&#39;re going to fit a series of random forests. There are 13 variables, so let&#39;s have mtry range from 1 to 13: In order to record the errors, you set up 2 variables oob.err and test.err. In a loop of mtry from 1 to 13, you first fit the randomForest with that value of mtry on the train dataset, restricting the number of trees to be 350. Then you extract the mean-squared-error on the object (the out-of-bag error). Then you predict on the test dataset (boston[-train]) using fit (the fit of randomForest). Lastly, you compute the test error: mean-squared error, which is equals to mean( (medv - pred) ^ 2 ). oob.err = double(13) test.err = double(13) for(mtry in 1:13){ fit = randomForest(medv~., data = boston, subset=train, mtry=mtry, ntree = 350) oob.err[mtry] = fit$mse[350] pred = predict(fit, boston[-train,]) test.err[mtry] = with(boston[-train,], mean( (medv-pred)^2 )) } Basically you just grew 4550 trees (13 times 350). Now let&#39;s make a plot using the matplot command. The test error and the out-of-bag error are binded together to make a 2-column matrix. There are a few other arguments in the matrix, including the plotting character values (pch = 23 means filled diamond), colors (red and blue), type equals both (plotting both points and connecting them with the lines), and name of y-axis (Mean Squared Error). You can also put a legend at the top right corner of the plot. matplot(1:mtry, cbind(test.err, oob.err), pch = 23, col = c( red , blue ), type = b , ylab= Mean Squared Error ) legend( topright , legend = c( OOB , Test ), pch = 23, col = c( red , blue )) Ideally, these 2 curves should line up, but it seems like the test error is a bit lower. However, there&#39;s a lot of variability in these test error estimates. Since the out-of-bag error estimate was computed on one dataset and the test error estimate was computed on another dataset, these differences are pretty much well within the standard errors. Notice that the red curve is smoothly above the blue curve? These error estimates are very correlated, because the randomForest with mtry = 4 is very similar to the one with mtry = 5. That&#39;s why each of the curves is quite smooth. What you see is that mtry around 4 seems to be the most optimal choice, at least for the test error. This value of mtry for the out-of-bag error equals 9. So with very few tiers, you have fitted a very powerful prediction model using random forests. How so? The left-hand side shows the performance of a single tree. The mean squared error on out-of-bag is 26, and you&#39;ve dropped down to about 15 (just a bit above half). This means you reduced the error by half. Likewise for the test error, you reduced the error from 20 to 12. Boosting Compared to random forests, boosting grows smaller and stubbier trees and goes at the bias. You will use the package GBM (Gradient Boosted Modeling), in R. require(gbm) GBM asks for the distribution, which is Gaussian, because you&#39;ll be doing squared error loss. You&#39;re going to ask GBM for 10,000 trees, which sounds like a lot, but these are going to be shallow trees. Interaction depth is the number of splits, so you want 4 splits in each tree. Shrinkage is 0.01, which is how much you&#39;re going to shrink the tree step back. boost.boston = gbm(medv~., data = boston[train,], distribution = gaussian , n.trees = 10000, shrinkage = 0.01, interaction.depth = 4) summary(boost.boston) The summary function gives a variable importance plot. It seems like there are 2 variables that have high relative importance: rm (number of rooms) and lstat (percentage of lower economic status people in the community). Let&#39;s plot these 2 variables: plot(boost.boston,i= lstat ) plot(boost.boston,i= rm ) The 1st plot shows that the higher the proportion of lower status people in the suburb, the lower the value of the housing prices. The 2nd plot shows the reversed relationship with the number of rooms: the average number of rooms in the house increases as the price increases. It&#39;s time to predict a boosted model on the test dataset. Let&#39;s look at the test performance as a function of the number of trees: First, you make a grid of number of trees in steps of 100 from 100 to 10,000. Then, you run the predict function on the boosted model. It takes n.trees as an argument, and produces a matrix of predictions on the test data. The dimensions of the matrix are 206 test observations and 100 different predict vectors at the 100 different values of tree. n.trees = seq(from = 100, to = 10000, by = 100) predmat = predict(boost.boston, newdata = boston[-train,], n.trees = n.trees) dim(predmat) It&#39;s time to compute the test error for each of the predict vectors: predmat is a matrix, medv is a vector, thus (predmat - medv) is a matrix of differences. You can use the apply function to the columns of these square differences (the mean). That would compute the column-wise mean squared error for the predict vectors. Then you make a plot using similar parameters to that one used for Random Forest. It would show a boosting error plot. boost.err = with(boston[-train,], apply( (predmat - medv)^2, 2, mean) ) plot(n.trees, boost.err, pch = 23, ylab = Mean Squared Error , xlab = # Trees , main = Boosting Test Error ) abline(h = min(test.err), col = red ) The boosting error pretty much drops down as the number of trees increases. This is an evidence showing that boosting is reluctant to overfit. Let&#39;s also include the best test error from the randomForest into the plot. Boosting actually gets a reasonable amount below the test error for randomForest. Conclusion So that&#39;s the end of this R tutorial on building decision tree models: classification trees, random forests, and boosted trees. The latter 2 are powerful methods that you can use anytime as needed. In my experience, boosting usually outperforms RandomForest, but RandomForest is easier to implement. In RandomForest, the only tuning parameter is the number of trees; while in boosting, more tuning parameters are required besides the number of trees, including the shrinkage and the interaction depth. If you would like to learn more, be sure to take a look at our Machine Learning Toolbox course for R. "],["decision-tree.html", "Chapter 104 decision tree", " Chapter 104 decision tree https://analytics4all.org/2016/11/23/r-decision-trees-regression/ "],["decision-tree-classifier-implementation-in-r.html", "Chapter 105 DECISION TREE CLASSIFIER IMPLEMENTATION IN R", " Chapter 105 DECISION TREE CLASSIFIER IMPLEMENTATION IN R https://dataaspirant.com/2017/01/30/how-decision-tree-algorithm-works/ https://dataaspirant.com/2017/02/03/decision-tree-classifier-implementation-in-r/ "],["caret.html", "Chapter 106 caret", " Chapter 106 caret Classification And REgression Training {r eval=FALSE, include=FALSE, echo=TRUE} library(caret) library(rpart.plot) {r eval=FALSE, include=FALSE, echo=TRUE} data_url &lt;- c( https://archive.ics.uci.edu/ml/machine-learning-databases/car/car.data ) download.file(url = data_url, destfile = data/car.data ) car_df &lt;- read.csv( data/car.data , sep = &#39;,&#39;, header = FALSE) {r eval=FALSE, include=FALSE, echo=TRUE} set.seed(3033) intrain &lt;- createDataPartition(y = car_df$V7, p= 0.7, list = FALSE) training &lt;- car_df[intrain,] testing &lt;- car_df[-intrain,] {r eval=FALSE, include=FALSE, echo=TRUE} #check dimensions of train &amp; test set dim(training); dim(testing); {r eval=FALSE, include=FALSE, echo=TRUE} anyNA(car_df) {r eval=FALSE, include=FALSE, echo=TRUE} summary(car_df) {r eval=FALSE, include=FALSE, echo=TRUE} trctrl &lt;- trainControl(method = repeatedcv , number = 10, repeats = 3) # The ‚Äúmethod‚Äù parameter holds the details about resampling method. We can set ‚Äúmethod‚Äù with many values like ‚Äúboot‚Äù, ‚Äúboot632‚Äù, ‚Äúcv‚Äù, ‚Äúrepeatedcv‚Äù, ‚ÄúLOOCV‚Äù, ‚ÄúLGOCV‚Äù etc. For this tutorial, let‚Äôs try to use repeatedcv i.e, repeated cross-validation. # # The ‚Äúnumber‚Äù parameter holds the number of resampling iterations. The ‚Äúrepeats ‚Äù parameter contains the complete sets of folds to compute for our repeated cross-validation. We are using setting number =10 and repeats =3. This trainControl() methods returns a list. We are going to pass this on our train() method. set.seed(3333) dtree_fit &lt;- train(V7 ~., data = training, method = rpart , parms = list(split = information ), trControl=trctrl, tuneLength = 10) # train() method should be passed with ‚Äúmethod‚Äù parameter as ‚Äúrpart‚Äù. There is another package ‚Äúrpart‚Äù, it is specifically available for decision tree implementation. Caret links its train function with others to make our work simple. # # We are passing our target variable V7. The ‚ÄúV7~.‚Äù denotes a formula for using all attributes in our classifier and V7 as the target variable. The ‚ÄútrControl‚Äù parameter should be passed with results from our trianControl() method. {r eval=FALSE, include=FALSE, echo=TRUE} ?rpart {r eval=FALSE, include=FALSE, echo=TRUE} dtree_fit {r eval=FALSE, include=FALSE, echo=TRUE} prp(dtree_fit$finalModel, box.palette = Reds , tweak = 1.2) {r eval=FALSE, include=FALSE, echo=TRUE} testing[1,] {r eval=FALSE, include=FALSE, echo=TRUE} predict(dtree_fit, newdata = testing[1,]) {r eval=FALSE, include=FALSE, echo=TRUE} test_pred &lt;- predict(dtree_fit, newdata = testing) {r eval=FALSE, include=FALSE, echo=TRUE} confusionMatrix(test_pred, testing$V7 ) #check accuracy {r eval=FALSE, include=FALSE, echo=TRUE} set.seed(3333) dtree_fit_gini &lt;- train(V7 ~., data = training, method = rpart , parms = list(split = gini ), trControl=trctrl, tuneLength = 10) dtree_fit_gini {r eval=FALSE, include=FALSE, echo=TRUE} prp(dtree_fit_gini$finalModel, box.palette = Blues , tweak = 1.2) {r eval=FALSE, include=FALSE, echo=TRUE} test_pred_gini &lt;- predict(dtree_fit_gini, newdata = testing) confusionMatrix(test_pred_gini, testing$V7 ) #check accuracy "],["my-r-codes-for-data-analysis-4.html", "Chapter 107 My R Codes For Data Analysis 107.1 Descriptive Statistics 107.2 skimr", " Chapter 107 My R Codes For Data Analysis 107.1 Descriptive Statistics {r eval=FALSE, include=FALSE, echo=TRUE} Epi::stat.table(gender,mean(age), data = scabies) {r eval=FALSE, include=FALSE, echo=TRUE} table &lt;- Epi::stat.table(gender,mean(age), data = scabies) pander::pander(table) {r eval=FALSE, include=FALSE, echo=TRUE} #Tabulate, by gender, the mean age from the scabies dataset Epi::stat.table(gender,list(mean(age),median(age)), data = scabies) {r eval=FALSE, include=FALSE, echo=TRUE} summary_data &lt;- arsenal::tableby(gender~age+scabies_infestation,data=scabies) summary(summary_data) 107.2 skimr https://cran.r-project.org/web/packages/skimr/vignettes/Using_skimr.html {r eval=FALSE, include=FALSE, echo=TRUE} require(skimr) {r eval=FALSE, include=FALSE, echo=TRUE} summary(iris) {r eval=FALSE, include=FALSE, echo=TRUE} summary(iris$Sepal.Length) {r eval=FALSE, include=FALSE, echo=TRUE} fivenum(iris$Sepal.Length) {r eval=FALSE, include=FALSE, echo=TRUE} summary(iris$Species) {r eval=FALSE, include=FALSE, echo=TRUE} skim(iris) {r eval=FALSE, include=FALSE, echo=TRUE} iris_results &lt;- skim(iris) str(iris_results) iris_results$variable iris_results$type {r eval=FALSE, include=FALSE, echo=TRUE} skimr::skim(iris) %&gt;% dplyr::filter(stat == mean ) {r eval=FALSE, include=FALSE, echo=TRUE} head(iris_results, n=15) {r eval=FALSE, include=FALSE, echo=TRUE} mtcars %&gt;% dplyr::group_by(gear) %&gt;% skim() {r eval=FALSE, include=FALSE, echo=TRUE} skim(iris, Sepal.Length, Species) {r eval=FALSE, include=FALSE, echo=TRUE} skim(iris, starts_with( Sepal )) {r eval=FALSE, include=FALSE, echo=TRUE} skim(datasets::lynx) Exploratory Data Analysis in R (introduction) https://blog.datascienceheroes.com/exploratory-data-analysis-in-r-intro/ {r eval=FALSE, include=FALSE, echo=TRUE} library(tidyverse) library(summarytools) # library(funModeling) library(tidyverse) library(Hmisc) basic_eda &lt;- function(data) { glimpse(data) # df_status(data) # freq(data) # profiling_num(data) # plot_num(data) describe(data) } basic_eda(irisdata) What‚Äôs so hard about histograms? http://tinlizzie.org/~aran/histograms/ "],["dataexplorer.html", "Chapter 108 DataExplorer", " Chapter 108 DataExplorer "],["webinar-tidyverse-exploratory-analysis-emily-robinson.html", "Chapter 109 Webinar: Tidyverse Exploratory Analysis (Emily Robinson)", " Chapter 109 Webinar: Tidyverse Exploratory Analysis (Emily Robinson) &lt;iframe width= 560 height= 315 src= https://www.youtube.com/embed/uG3igAGX7UE frameborder= 0 allow= accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture allowfullscreen&gt; https://hookedondata.org/the-lesser-known-stars-of-the-tidyverse/ https://www.rstudio.com/resources/videos/the-lesser-known-stars-of-the-tidyverse/ https://github.com/robinsones/robinsones_blog/blob/master/content/post/multipleChoiceResponses.csv https://github.com/robinsones/robinsones_blog/blob/master/content/post/2018-11-16-the-lesser-known-stars-of-the-tidyverse.Rmd "],["i-only-use-r-for-descriptive-stats-and-thats-ok.html", "Chapter 110 I ‚Äúonly‚Äù use R for descriptive stats ‚Äî and that‚Äôs OK", " Chapter 110 I ‚Äúonly‚Äù use R for descriptive stats ‚Äî and that‚Äôs OK https://rforeval.com/descriptive-stats-r/ "],["histograms.html", "Chapter 111 histograms", " Chapter 111 histograms http://tinlizzie.org/histograms/ "],["bibliographic-studies-2.html", "Chapter 112 Bibliographic Studies", " Chapter 112 Bibliographic Studies {r eval=FALSE, include=FALSE, echo=TRUE} knitr::opts_chunk$set(fig.width = 12, fig.height = 8, fig.path = &#39;figure/&#39;, echo = TRUE, warning = FALSE, message = FALSE, error = FALSE, eval = TRUE, tidy = TRUE, comment = NA) {r , include=FALSE} library(tidyverse) {r eval=FALSE, include=FALSE, echo=TRUE} state.name {r eval=FALSE, include=FALSE, echo=TRUE} # install.packages( maps ) # library(maps) # x &lt;- map( world , plot=FALSE) # glimpse(x) # x$names {r eval=FALSE, include=FALSE, echo=TRUE} install.packages( rworldmap ) library(rworldmap) vignette(&#39;rworldmap&#39;) data(countryExData) countryExData SEER China vs others https://www.rdocumentation.org/packages/bayesTFR/versions/6.1-2/topics/country.names https://stat.ethz.ch/R-manual/R-devel/library/datasets/html/state.html "],["who-works-on-seer.html", "Chapter 113 Who works on SEER 113.1 Aim 113.2 Data retriveal from PubMed using EDirect", " Chapter 113 Who works on SEER If you want to see the code used in the analysis please click the code button on the right upper corner or throughout the page. Select from the tabs below. 113.1 Aim Aim: 113.2 Data retriveal from PubMed using EDirect Articles are downloaded as xml. {r Search PubMed write all data as xml, eval=FALSE, include=FALSE, echo=TRUE} myTerm &lt;- rstudioapi::terminalCreate(show = FALSE) rstudioapi::terminalSend( myTerm, esearch -db pubmed -query \\ &#39;SEER Program&#39;[Mesh] \\ -datetype PDAT -mindate 1800 -maxdate 3000 | efetch -format xml &gt; data/pubmed_result_SEER_MeSH.xml \\n ) Sys.sleep(1) repeat { Sys.sleep(0.1) if (rstudioapi::terminalBusy(myTerm) == FALSE) { print( Code Executed ) break } } {r extract journal names from all data xml, eval=FALSE, message=FALSE, warning=FALSE, include=FALSE} myTerm &lt;- rstudioapi::terminalCreate(show = FALSE) rstudioapi::terminalSend( myTerm, xtract -input data/pubmed_result_SEER_MeSH.xml -pattern PubmedArticle -sep &#39; &#39; -def &#39;na&#39; -element MedlineCitation/PMID PubDate/Year Affiliation&gt; data/SEER_countries.csv \\n ) Sys.sleep(1) repeat { Sys.sleep(0.1) if (rstudioapi::terminalBusy(myTerm) == FALSE) { print( Code Executed ) break } } {r eval=FALSE, include=FALSE, echo=TRUE} library(readr) SEER_countries &lt;- read_delim( data/SEER_countries.csv , \\t , escape_double = FALSE, col_names = c( PMID , year , Affiliations ), na = NA , trim_ws = TRUE) # View(SEER_countries) {r eval=FALSE, include=FALSE, echo=TRUE} countries &lt;- read_delim( data/countries.txt , delim = | , col_names = c( abb , country )) country &lt;- countries$country country &lt;- c(country, state.name) country[80] &lt;- Georgia_ {r eval=FALSE, include=FALSE, echo=TRUE} # SEER_countries &lt;- cbind(SEER_countries, setNames(lapply(country, function(x) x=NA), country)) # names(SEER_countries)[254] &lt;- GeorgiaUSA {r eval=FALSE, include=FALSE, echo=TRUE} # grepl(pattern = China , x = SEER_countries$Affiliations) {r eval=FALSE, include=FALSE, echo=TRUE} # deneme1 &lt;- grepl(pattern = country[44], x = SEER_countries$Affiliations) # deneme2 &lt;- sapply(country, function(x) grepl(x, SEER_countries$Affiliations)) # sum(deneme1 != deneme2[,44]) {r eval=FALSE, include=FALSE, echo=TRUE} # deneme2 &lt;- as.data.frame(deneme2) # sum(deneme2$Turkey) {r eval=FALSE, include=FALSE, echo=TRUE} SEER_countries &lt;- cbind(SEER_countries, sapply(country, function(x) grepl(x, SEER_countries$Affiliations))) {r eval=FALSE, include=FALSE, echo=TRUE} dim(SEER_countries)[1] At the time of the research the number of articles with ‚ÄòSEER Program‚Äô[Mesh] formula is r dim(SEER_countries)[1] . {r eval=FALSE, include=FALSE, echo=TRUE} # deneme &lt;- colSums(SEER_countries[,-(1:3)]) # deneme &lt;- as.data.frame(deneme) # deneme &lt;- rownames_to_column(deneme, var = countries ) # names(deneme) &lt;- c( countries , number ) # deneme %&gt;% arrange(desc(number)) {r eval=FALSE, include=FALSE, echo=TRUE} SEER_countries[SEER_countries == FALSE] &lt;- 0 SEER_countries[SEER_countries == TRUE] &lt;- 1 {r eval=FALSE, include=FALSE, echo=TRUE} countryTotals &lt;- SEER_countries %&gt;% select(-c(1:3)) %&gt;% summarise_all(funs(sum)) countryTotals[which(countryTotals&gt;0)] publisherCountries &lt;- names(countryTotals[which(countryTotals&gt;0)]) SEER_countries &lt;- SEER_countries %&gt;% select(c(1:3, publisherCountries)) {r eval=FALSE, include=FALSE, echo=TRUE} deneme &lt;- SEER_countries %&gt;% gather(key = Country , value = Number , -c(1:3)) %&gt;% group_by(Country, year) %&gt;% summarise(total = sum(Number)) {r eval=FALSE, include=FALSE, echo=TRUE} deneme %&gt;% filter(year != na ) %&gt;% filter(year != 2017 ) %&gt;% filter(year != 2018 ) %&gt;% ggplot() + aes(y = total, x = year, group = Country, color = Country) + geom_line() + guides(fill=FALSE, color=FALSE) + theme(axis.text.x = element_text(angle = 90, hjust = 1)) {r eval=FALSE, include=FALSE, echo=TRUE} USAnames &lt;- names(SEER_countries) %in% state.name Others &lt;- setdiff(names(SEER_countries[-c(1:3)]), c(USAnames, United States , China )) deneme2 &lt;- SEER_countries %&gt;% mutate( sumUSA = rowSums( select(., one_of(USAnames), `United States`) ) ) %&gt;% mutate( sumOthers = rowSums( select(., one_of(Others)) ) ) %&gt;% select(PMID, year, China, USA = sumUSA, Others = sumOthers) {r eval=FALSE, include=FALSE, echo=TRUE} deneme3 &lt;- deneme2 %&gt;% gather(key = Country , value = Number , -c(1:2)) %&gt;% group_by(PMID, Country, year) %&gt;% summarise(total = sum(Number)) %&gt;% filter(year != na ) %&gt;% filter(year != 2017 ) %&gt;% filter(year != 2018 ) %&gt;% filter(total != 0 ) {r eval=FALSE, include=FALSE, echo=TRUE} # which(duplicated(deneme3$PMID)) # which(duplicated(deneme3$PMID))-1 # deneme3[which(duplicated(deneme3$PMID)),] together &lt;- bind_cols( First = deneme3$Country[which(duplicated(deneme3$PMID))], Second = deneme3$Country[which(duplicated(deneme3$PMID))-1] ) table(together$First, together$Second) %&gt;% addmargins() bind_cols( {r eval=FALSE, include=FALSE, echo=TRUE} deneme4 &lt;- deneme2 %&gt;% gather(key = Country , value = Number , -c(1:2)) %&gt;% group_by(Country, year) %&gt;% summarise(total = sum(Number)) %&gt;% filter(year != na ) %&gt;% filter(year != 2017 ) %&gt;% filter(year != 2018 ) %&gt;% filter(total != 0 ) {r eval=FALSE, include=FALSE, echo=TRUE} deneme4 %&gt;% ggplot() + aes(y = total, x = year, group = Country, color = Country) + geom_line() + # guides(fill=FALSE, color=FALSE) + theme(axis.text.x = element_text(angle = 90, hjust = 1)) While helping the preparation of #PBPath Journal Watch (https://t.co/WiBsJixzlc) I thought that many SEER (???) studies are from China. So using edirect (???) and #RStats I draw the attached graph. What do you think? Do Chinese do research on SEER that much? pic.twitter.com/3Op5r9ofbK ‚Äî Serdar Balcƒ± ((???)) October 6, 2018 {r eval=FALSE, include=FALSE, echo=TRUE} p &lt;- deneme4 %&gt;% ggplot() + aes(y = total, x = year, group = Country, color = Country) + geom_line() + # guides(fill=FALSE, color=FALSE) + theme(axis.text.x = element_text(angle = 90, hjust = 1)) "],["eurostat.html", "Chapter 114 Eurostat", " Chapter 114 Eurostat eurostat http://ec.europa.eu/eurostat http://ec.europa.eu/eurostat/data/database eurostat R package http://ropengov.github.io/eurostat/ Retrieval and Analysis of Eurostat Open Data with the eurostat Package https://journal.r-project.org/archive/2017/RJ-2017-019/index.html CheatSheet https://github.com/rOpenGov/eurostat/blob/master/vignettes/cheatsheet/eurostat_cheatsheet.pdf https://github.com/rstudio/cheatsheets/raw/master/eurostat.pdf Searching, downloading and manipulating Eurostat data with R http://ropengov.github.io/r/2015/05/01/eurostat-package-examples/ Mapping Eurostat information https://www.mytinyshinys.com/2017/07/11/eurostat/ eurostat-package published https://rpubs.com/muuankarski/27120 Tutorial (vignette) for the eurostat R package http://ropengov.github.io/eurostat/articles/eurostat_tutorial.html {r eval=FALSE, include=FALSE, echo=TRUE} # install.packages( eurostat ) library(eurostat) {r eval=FALSE, include=FALSE, echo=TRUE} TOC &lt;- get_eurostat_toc() {r eval=FALSE, include=FALSE, echo=TRUE} TOC {r eval=FALSE, include=FALSE, echo=TRUE} query &lt;- search_eurostat( road accidents , type = table ) query {r eval=FALSE, include=FALSE, echo=TRUE} query$code[[1]] {r eval=FALSE, include=FALSE, echo=TRUE} query$title[[1]] {r eval=FALSE, include=FALSE, echo=TRUE} dat &lt;- get_eurostat(id = sdg_11_40 , time_format = num ) {r eval=FALSE, include=FALSE, echo=TRUE} dat {r eval=FALSE, include=FALSE, echo=TRUE} countries &lt;- c( UK , SK , FR , PL , ES , PT , TR ) t1 &lt;- get_eurostat( sdg_11_40 , filters = list(geo = countries)) {r eval=FALSE, include=FALSE, echo=TRUE} t1 {r eval=FALSE, include=FALSE, echo=TRUE} t2 &lt;- get_eurostat(id = sdg_11_40 , time_format = num ) {r eval=FALSE, include=FALSE, echo=TRUE} table(t2$geo) "],["evidence-synthesis-projects.html", "Chapter 115 Evidence Synthesis Projects", " Chapter 115 Evidence Synthesis Projects "],["revtools.html", "Chapter 116 revtools", " Chapter 116 revtools revtools: Tools to Support Evidence Synthesis https://cran.r-project.org/package=revtools https://revtools.net/ https://revtools.net/user_manual/1_introduction.html {r eval=FALSE, include=FALSE, echo=TRUE} # install.packages( revtools ) # devtools:: install_github( mjwestgate/revtools ) library(revtools) data1 &lt;- read_bibliography( my_data.ris ) data2 &lt;- read_bibliography( my_data.bib ) {r eval=FALSE, include=FALSE, echo=TRUE} # data1 &lt;- read_bibliography(file.choose()) data1 &lt;- read_bibliography( data/citations.nbib ) # If the files are in the working directory: file_names &lt;- list.files() # Or if they are in a subdirectory: file_names &lt;- paste0( ./raw_data/ , list.files(path = ./raw_data/ ) ) # Then import to a list data_list &lt;- lapply( file_names, function(x){read_bibliography(x)} ) {r eval=FALSE, include=FALSE, echo=TRUE} data2 &lt;- read_bibliography( data/citations.nbib , return_df = FALSE ) class(data2) class(data2[[1]]) names(data2[[1]]) {r eval=FALSE, include=FALSE, echo=TRUE} write_bibliography(data2, data/denemeRIS , format = ris ) {r eval=FALSE, include=FALSE, echo=TRUE} # revtools::format_citation() data &lt;- read_bibliography( my_data.ris ) matches &lt;- find_duplicates( data = data, match_variable = title , group_variable = NULL, match_function = fuzzdist , method = fuzz_partial_ratio , threshold = 0 ) data_unique &lt;- extract_unique_references(data, matches) "],["screen-duplicates.html", "Chapter 117 screen_duplicates", " Chapter 117 screen_duplicates https://revtools.net/user_manual/4_removing_duplicates.html {r eval=FALSE, include=FALSE, echo=TRUE} screen_duplicates(data1) # 1. standalone; load in data in the app screen_titles() # 2. the same, but save back to workspace on exit result &lt;- screen_titles() # ditto, data &lt;- read_bibliography( my_data.ris ) # load in data # 3. launch the app using data from the workspace screen_titles(data) # 4. specify an object to return data to result &lt;- screen_titles(data) {r eval=FALSE, include=FALSE, echo=TRUE} screen_titles(data1) {r eval=FALSE, include=FALSE, echo=TRUE} screen_abstracts(data1) {r eval=FALSE, include=FALSE, echo=TRUE} screen_topics(data1) {r eval=FALSE, include=FALSE, echo=TRUE} library(revtools) data &lt;- read_bibliography( data/deneme2.ris ) dtm &lt;- make_DTM(data) model &lt;- topicmodels::LDA( dtm, k = 15, LDA.control = list( burnin = 1000, iter = 6000, keep = 100 ) ) {r eval=FALSE, include=FALSE, echo=TRUE} articles &lt;- as.data.frame(data) articles$topic &lt;- topics(model) # cross-tabulate to show number of articles per topic per year popularity &lt;- as.data.frame( xtabs( ~ year + topic, data = articles, drop.unused.levels = FALSE ), stringsAsFactors = FALSE ) popularity$year &lt;- scale( as.numeric(popularity$year) ) popularity$topic &lt;- as.factor(popularity$topic) # create a mixed model library(lme4) popularity_model &lt;- glmer(Freq ~ 1 + (1 | topic) + (year -1 | topic), family = poisson(link = log ), data = popularity ) # export the results of this model popularity_results &lt;- ranef(popularity_model)$topic colnames(popularity_results) &lt;- c( intercept , slope ) {r eval=FALSE, include=FALSE, echo=TRUE} library(ggplot2) p &lt;- ggplot(popularity_results, aes(x = intercept, y = slope) ) + geom_point() p "],["refmanager.html", "Chapter 118 RefManageR", " Chapter 118 RefManageR RefManageR: Straightforward ‚ÄòBibTeX‚Äô and ‚ÄòBibLaTeX‚Äô Bibliography Management https://cran.r-project.org/web/packages/RefManageR/index.html "],["bibtex.html", "Chapter 119 bibtex", " Chapter 119 bibtex bibtex: Bibtex Parser https://cran.r-project.org/web/packages/bibtex/index.html "],["explatory-data-analysis-summary-statistics.html", "Chapter 120 Explatory Data Analysis &amp; Summary Statistics", " Chapter 120 Explatory Data Analysis &amp; Summary Statistics "],["dataexplorer-1.html", "Chapter 121 DataExplorer", " Chapter 121 DataExplorer https://cran.r-project.org/web/packages/DataExplorer/vignettes/dataexplorer-intro.html https://boxuancui.github.io/DataExplorer/ "],["my-r-codes-for-data-analysis-5.html", "Chapter 122 My R Codes For Data Analysis 122.1 File organization best practices", " Chapter 122 My R Codes For Data Analysis 122.1 File organization best practices This page summarises how to organize files and analysis before everything gets jumbled up: Setting up a reproducible data analysis workflow in R Basically they suggest: - using a project and project folder in RStudio for each analysis - using packrat as much as possible setwd() and getwd() is not necesary when you use projects. Why should I use the here package when I‚Äôm already using projects? https://malco.io/2018/11/05/why-should-i-use-the-here-package/ {r eval=FALSE, include=FALSE, echo=TRUE} library(here) here() {r eval=FALSE, include=FALSE, echo=TRUE} dr_here() {r eval=FALSE, include=FALSE, echo=TRUE} here( figure , figure.png ) {r eval=FALSE, include=FALSE, echo=TRUE} file.path( figure , figure.png ) {r eval=FALSE, include=FALSE, echo=TRUE} read_csv(here( data , mtcars.csv )) "],["all-tables-examples.html", "Chapter 123 All tables examples 123.1 1 Cross tables 123.2 2 Model tables with finalfit() 123.3 3 Model tables manually using ff_merge() 123.4 4 Support for complex survey structures via library(survey)", " Chapter 123 All tables examples author: Ewen Harrison output: rmarkdown::html_vignette vignette: &gt; %\\VignetteIndexEntry{All tables examples} %\\VignetteEngine{knitr::rmarkdown} %\\VignetteEncoding{UTF-8} 123.1 1 Cross tables Two-way tables are used extensively in healthcare research, e.g. a 2x2 table comparing two factors with two levels each, or table 1 from a typical clinical study or trial The main functions all take a dependent variable - the outcome (maximum of 5 levels) - and explanatory variables - predictors or exposures (any number categorical or continuous variables). 123.1.1 1.01 Default {r eval=FALSE, include=FALSE, echo=TRUE} library(finalfit) explanatory = c( age , age.factor , sex.factor , obstruct.factor ) dependent = perfor.factor colon_s %&gt;% summary_factorlist(dependent, explanatory) -&gt; t {r eval=FALSE, include=FALSE, echo=TRUE} library(knitr) kable(t, row.names=FALSE, align = c( l , l , r , r )) Note, chi-squared warnings will be generated when the expected count in any cell is less than 5. Fisher‚Äôs exact test can be used as below, or go straight to a univariable logistic regression, e.g. colon_s %&gt;% finalfit(dependent, explanatory) 123.1.2 1.02 Add or edit variable labels {r eval=FALSE, include=FALSE, echo=TRUE} library(finalfit) library(dplyr) explanatory = c( age , age.factor , sex.factor , obstruct.factor ) dependent = perfor.factor colon_s %&gt;% mutate( sex.factor = ff_label(sex.factor, Gender ) ) %&gt;% summary_factorlist(dependent, explanatory) -&gt; t {r eval=FALSE, include=FALSE, echo=TRUE} library(knitr) kable(t, row.names=FALSE, align = c( l , l , r , r )) 123.1.3 1.03 P-value for hypothesis test Chi-squared for categorical, Kruskal-Wallis/Mann-Whitney for continuous {r eval=FALSE, include=FALSE, echo=TRUE, warning=FALSE} library(finalfit) explanatory = c( age , age.factor , sex.factor , obstruct.factor ) dependent = perfor.factor colon_s %&gt;% summary_factorlist(dependent, explanatory, p = TRUE) -&gt; t {r eval=FALSE, include=FALSE, echo=TRUE} library(knitr) kable(t, row.names=FALSE, align = c( l , l , r , r , r )) 123.1.4 1.04 With Fisher‚Äôs exact test {r eval=FALSE, include=FALSE, echo=TRUE} library(finalfit) explanatory = c( age , age.factor , sex.factor , obstruct.factor ) dependent = perfor.factor colon_s %&gt;% summary_factorlist(dependent, explanatory, p = TRUE, catTest = catTestfisher) -&gt; t {r eval=FALSE, include=FALSE, echo=TRUE} library(knitr) kable(t, row.names=FALSE, align = c( l , l , r , r )) 123.1.5 1.05 Median (interquartile range) instead of mean (standard deviation) ‚Ä¶ for continuous variables. {r eval=FALSE, include=FALSE, echo=TRUE, warning=FALSE} library(finalfit) explanatory = c( age , age.factor , sex.factor , obstruct.factor ) dependent = perfor.factor colon_s %&gt;% summary_factorlist(dependent, explanatory, p = TRUE, cont = median ) -&gt; t {r eval=FALSE, include=FALSE, echo=TRUE} library(knitr) kable(t, row.names=FALSE, align = c( l , l , r , r , r )) 123.1.6 1.06 Missing values for the explanatory variables Always do this when describing your data. {r eval=FALSE, include=FALSE, echo=TRUE, warning=FALSE} library(finalfit) explanatory = c( age , age.factor , sex.factor , obstruct.factor ) dependent = perfor.factor colon_s %&gt;% summary_factorlist(dependent, explanatory, p = TRUE, cont = median , na_include = TRUE) -&gt; t {r eval=FALSE, include=FALSE, echo=TRUE} library(knitr) kable(t, row.names=FALSE, align = c( l , l , r , r , r )) 123.1.7 1.07 Column proportions (rather than row) {r eval=FALSE, include=FALSE, echo=TRUE, warning=FALSE} library(finalfit) explanatory = c( age , age.factor , sex.factor , obstruct.factor ) dependent = perfor.factor colon_s %&gt;% summary_factorlist(dependent, explanatory, p = TRUE, cont = median , na_include = TRUE, column = TRUE) -&gt; t {r eval=FALSE, include=FALSE, echo=TRUE} library(knitr) kable(t, row.names=FALSE, align = c( l , l , r , r , r )) 123.1.8 1.08 Total column {r eval=FALSE, include=FALSE, echo=TRUE, warning=FALSE} library(finalfit) explanatory = c( age , age.factor , sex.factor , obstruct.factor ) dependent = perfor.factor colon_s %&gt;% summary_factorlist(dependent, explanatory, p = TRUE, cont = median , na_include = TRUE, column = TRUE, total_col = TRUE) -&gt; t {r eval=FALSE, include=FALSE, echo=TRUE} library(knitr) kable(t, row.names=FALSE, align = c( l , l , r , r , r , r )) 123.1.9 1.09 Order a variable by total This is intended for when there is only one explanatory variable. {r eval=FALSE, include=FALSE, echo=TRUE, warning=FALSE} library(finalfit) explanatory = c( extent.factor ) dependent = perfor.factor colon_s %&gt;% summary_factorlist(dependent, explanatory, p = TRUE, cont = median , na_include = TRUE, column = TRUE, total_col = TRUE, orderbytotal = TRUE) -&gt; t {r eval=FALSE, include=FALSE, echo=TRUE} library(knitr) kable(t, row.names=FALSE, align = c( l , l , r , r , r , r )) 123.1.10 1.10 Label with dependent name {r eval=FALSE, include=FALSE, echo=TRUE, warning=FALSE} library(finalfit) explanatory = c( age , age.factor , sex.factor , obstruct.factor ) dependent = perfor.factor colon_s %&gt;% summary_factorlist(dependent, explanatory, p = TRUE, cont = median , na_include = TRUE, column = TRUE, total_col = TRUE, add_dependent_label = TRUE) -&gt; t {r eval=FALSE, include=FALSE, echo=TRUE} library(knitr) kable(t, row.names=FALSE, align = c( l , l , r , r , r , r )) The dependent name cannot be passed directly to the table intentionally. This is to avoid errors when code is copied and the name is not updated. Change the dependent label using the following. The prefix ( Dependent: ) and any suffix can be altered. {r eval=FALSE, include=FALSE, echo=TRUE, warning=FALSE} library(finalfit) explanatory = c( age , age.factor , sex.factor , obstruct.factor ) dependent = perfor.factor colon_s %&gt;% dplyr::mutate( perfor.factor = ff_label(perfor.factor, Perforated cancer ) ) %&gt;% summary_factorlist(dependent, explanatory, p = TRUE, cont = median , na_include = TRUE, column = TRUE, total_col = TRUE, add_dependent_label = TRUE, dependent_label_prefix = ) -&gt; t {r eval=FALSE, include=FALSE, echo=TRUE} library(knitr) kable(t, row.names=FALSE, align = c( l , l , r , r , r , r )) 123.1.11 1.11 Dependent variable with any number of factor levels supported {r eval=FALSE, include=FALSE, echo=TRUE, warning=FALSE} library(finalfit) explanatory = c( age , age.factor , sex.factor , obstruct.factor ) dependent = extent.factor colon_s %&gt;% dplyr::mutate( perfor.factor = ff_label(perfor.factor, Perforated cancer ) ) %&gt;% summary_factorlist(dependent, explanatory, p = TRUE, cont = median , na_include = TRUE, column = TRUE, total_col = TRUE, add_dependent_label = TRUE, dependent_label_prefix = ) -&gt; t {r eval=FALSE, include=FALSE, echo=TRUE} library(knitr) kable(t, row.names=FALSE, align = c( l , l , r , r , r , r , r , r )) 123.1.12 1.12 Explanatory variable defaults to factor when ‚â§5 distinct values {r eval=FALSE, include=FALSE, echo=TRUE, warning=FALSE} library(finalfit) # Here, `extent` is a continuous variable with 4 distinct values. # Any continuous variable with 5 or fewer unique values is converted silently to factor # e.g. explanatory = c( extent ) dependent = mort_5yr colon_s %&gt;% summary_factorlist(dependent, explanatory) -&gt; t {r eval=FALSE, include=FALSE, echo=TRUE} library(knitr) kable(t, row.names=FALSE, align = c( l , l , r , r , r , r , r , r )) 123.1.13 1.13 Keep as continous variable when ‚â§5 distinct values {r eval=FALSE, include=FALSE, echo=TRUE, warning=FALSE} library(finalfit) explanatory = c( extent ) dependent = mort_5yr colon_s %&gt;% summary_factorlist(dependent, explanatory, cont_cut = 3) -&gt; t {r eval=FALSE, include=FALSE, echo=TRUE} library(knitr) kable(t, row.names=FALSE, align = c( l , l , r , r , r , r , r , r )) 123.1.14 1.14 Stratified crosstables I‚Äôve been meaning to include support for table stratification for a while. I have delayed for a good reason. Perhaps the most straightforward way to implement stratificiation is with dplyr::group_by(). However, the non-standard evaluation required for multiple strata may confuse as it is not implemented else where in the package (doesn‚Äôt work with group_by_). This translates to whether variable names are passed in quotes or not. Finally,. dplyr::do() is planned for deprecation, but there is no good alternative at the moment. Anyway, here is a solution, which while not that pretty, is very effective. {r eval=FALSE, include=FALSE, echo=TRUE, warning=FALSE} library(dplyr) # Piped function to generate stratified crosstabs table explanatory = c( age.factor , sex.factor ) dependent = rx.factor # Pick option below split = rx.factor split = c( perfor.factor , node4.factor ) colon_s %&gt;% group_by(!!! syms(split)) %&gt;% #Looks awkward, but this keeps quoted var names (rather than unquoted) do( summary_factorlist(., dependent, explanatory, p = TRUE) ) %&gt;% data.frame() %&gt;% dependent_label(colon_s, dependent, prefix = ) %&gt;% colname2label(split) -&gt; t {r eval=FALSE, include=FALSE, echo=TRUE} library(knitr) kable(t, row.names=FALSE, align = c( l , l , l , l , r , r , r )) 123.2 2 Model tables with finalfit() 123.2.1 2.01 Default Logistic regression first. {r eval=FALSE, include=FALSE, echo=TRUE, warning=FALSE, message=FALSE} library(finalfit) explanatory = c( age.factor , sex.factor , obstruct.factor , perfor.factor ) dependent = mort_5yr colon_s %&gt;% finalfit(dependent, explanatory) -&gt; t {r eval=FALSE, include=FALSE, echo=TRUE} library(knitr) kable(t, row.names=FALSE, align = c( l , l , r , r , r , r , r , r )) 123.2.2 2.02 Hide reference levels Most appropriate when all explanatory variables are continuous or well-known binary variables, such as sex. {r eval=FALSE, include=FALSE, echo=TRUE, warning=FALSE, message=FALSE} library(finalfit) explanatory = c( age , sex.factor ) dependent = mort_5yr colon_s %&gt;% finalfit(dependent, explanatory, add_dependent_label = FALSE) %&gt;% ff_remove_ref() %&gt;% dependent_label(colon_s, dependent)-&gt; t {r eval=FALSE, include=FALSE, echo=TRUE} library(knitr) kable(t, row.names=FALSE, align = c( l , l , r , r , r , r , r , r )) 123.2.3 2.03 Model metrics {r eval=FALSE, include=FALSE, echo=TRUE, warning=FALSE, message=FALSE} library(finalfit) explanatory = c( age.factor , sex.factor , obstruct.factor , perfor.factor ) dependent = mort_5yr colon_s %&gt;% finalfit(dependent, explanatory, metrics = TRUE) -&gt; t {r eval=FALSE, include=FALSE, echo=TRUE} library(knitr) kable(t[[1]], row.names=FALSE, align = c( l , l , r , r , r , r , r , r )) kable(t[[2]], row.names=FALSE, align = c( l , l , r , r , r , r , r , r ), col.names = ) 123.2.4 2.04 Model metrics can be applied to all supported base models {r eval=FALSE, include=FALSE, echo=TRUE, warning=FALSE, message=FALSE} library(finalfit) glm(mort_5yr ~ age.factor + sex.factor + obstruct.factor + perfor.factor, data = colon_s, family = binomial ) %&gt;% ff_metrics() -&gt; t {r eval=FALSE, include=FALSE, echo=TRUE} library(knitr) kable(t, row.names=FALSE, align = c( l , l , r , r , r , r , r , r ), col.names = ) 123.2.5 2.05 Reduced model {r eval=FALSE, include=FALSE, echo=TRUE, warning=FALSE, message=FALSE} library(finalfit) explanatory = c( age.factor , sex.factor , obstruct.factor , perfor.factor ) explanatory_multi = c( age.factor , obstruct.factor ) dependent = mort_5yr colon_s %&gt;% finalfit(dependent, explanatory, explanatory_multi) -&gt; t {r eval=FALSE, include=FALSE, echo=TRUE} library(knitr) kable(t, row.names=FALSE, align = c( l , l , r , r , r , r , r , r )) 123.2.6 2.06 Include all models {r eval=FALSE, include=FALSE, echo=TRUE, warning=FALSE, message=FALSE} library(finalfit) explanatory = c( age.factor , sex.factor , obstruct.factor , perfor.factor ) explanatory_multi = c( age.factor , obstruct.factor ) dependent = mort_5yr colon_s %&gt;% finalfit(dependent, explanatory, explanatory_multi, metrics = TRUE, keep_models = TRUE) -&gt; t {r eval=FALSE, include=FALSE, echo=TRUE} library(knitr) kable(t[[1]], row.names=FALSE, align = c( l , l , r , r , r , r , r , r )) kable(t[[2]], row.names=FALSE, align = c( l , l , r , r , r , r , r , r ), col.names = ) 123.2.7 2.06 Interactions Interactions can be specified in the normal way. Formatting the output is trickier. At the moment, we have left the default model output. This can be adjusted as necessary. {r eval=FALSE, include=FALSE, echo=TRUE, warning=FALSE, message=FALSE} library(finalfit) explanatory = c( age.factor*sex.factor , obstruct.factor , perfor.factor ) dependent = mort_5yr colon_s %&gt;% finalfit(dependent, explanatory) -&gt; t {r eval=FALSE, include=FALSE, echo=TRUE} library(knitr) kable(t, row.names=FALSE, align = c( l , l , r , r , r , r , r , r )) 123.2.8 2.07 Interactions: create interaction variable with two factors {r eval=FALSE, include=FALSE, echo=TRUE, warning=FALSE, message=FALSE} library(finalfit) #explanatory = c( age.factor*sex.factor , obstruct.factor , perfor.factor ) explanatory = c( obstruct.factor , perfor.factor ) dependent = mort_5yr colon_s %&gt;% ff_interaction(age.factor, sex.factor) %&gt;% finalfit(dependent, c(explanatory, age.factor__sex.factor )) -&gt; t {r eval=FALSE, include=FALSE, echo=TRUE} library(knitr) kable(t, row.names=FALSE, align = c( l , l , r , r , r , r , r , r )) 123.2.9 2.08 Dependent name The dependent name cannot be specified directly intentionally. This is to prevent errors when copying code. Re-label using ff_label(). The dependent prefix and suffix can also be altered. {r eval=FALSE, include=FALSE, echo=TRUE, warning=FALSE, message=FALSE} library(finalfit) explanatory = c( age.factor , sex.factor , obstruct.factor , perfor.factor ) dependent = mort_5yr colon_s %&gt;% dplyr::mutate( mort_5yr = ff_label(mort_5yr, 5-year mortality ) ) %&gt;% finalfit(dependent, explanatory, dependent_label_prefix = , dependent_label_suffix = (full model) ) -&gt; t {r eval=FALSE, include=FALSE, echo=TRUE} library(knitr) kable(t, row.names=FALSE, align = c( l , l , r , r , r , r , r , r )) 123.2.10 2.09 Estimate name {r eval=FALSE, include=FALSE, echo=TRUE, warning=FALSE, message=FALSE} library(finalfit) explanatory = c( age.factor , sex.factor , obstruct.factor , perfor.factor ) dependent = mort_5yr colon_s %&gt;% finalfit(dependent, explanatory, estimate_name = Odds ratio ) -&gt; t {r eval=FALSE, include=FALSE, echo=TRUE} library(knitr) kable(t, row.names=FALSE, align = c( l , l , r , r , r , r , r , r )) 123.2.11 2.10 Digits / decimal places Number of digits to round to regression results. (1) estimate, (2) confidence interval limits, (3) p-value. Default is c(2,2,3). Trailing zeros are preserved. Number of decimal places for counts and mean (sd) / median (IQR) not currently supported. Defaults are senisble :) {r eval=FALSE, include=FALSE, echo=TRUE, warning=FALSE, message=FALSE} library(finalfit) explanatory = c( age.factor , sex.factor , obstruct.factor , perfor.factor ) dependent = mort_5yr colon_s %&gt;% finalfit(dependent, explanatory, digits = c(3,3,4)) -&gt; t {r eval=FALSE, include=FALSE, echo=TRUE} library(knitr) kable(t, row.names=FALSE, align = c( l , l , r , r , r , r , r , r )) 123.2.12 2.11 Confidence interval type One of c( profile , default ) for GLM models (confint.glm()). Note, a little awkwardly, the ‚Äòdefault‚Äô setting is profile, rather than default. Profile levels are probably a little more accurate. Only go to default if taking a significant length of time for profile, i.e. data is greater than hundreds of thousands of lines. For glmer/lmer models (confint.merMod()), c( profile , Wald , boot ). Not implemented for lm(), coxph() or coxphlist, which use default. {r eval=FALSE, include=FALSE, echo=TRUE, warning=FALSE, message=FALSE} library(finalfit) explanatory = c( age.factor , sex.factor , obstruct.factor , perfor.factor ) dependent = mort_5yr colon_s %&gt;% finalfit(dependent, explanatory, confint_type = default ) -&gt; t {r eval=FALSE, include=FALSE, echo=TRUE} library(knitr) kable(t, row.names=FALSE, align = c( l , l , r , r , r , r , r , r )) 123.2.13 2.12 Confidence interval level Probably never change this :) Note, the p-value is intentionally not included for confidence levels other than 95% to avoid confusion. {r eval=FALSE, include=FALSE, echo=TRUE, warning=FALSE, message=FALSE} library(finalfit) explanatory = c( age.factor , sex.factor , obstruct.factor , perfor.factor ) dependent = mort_5yr colon_s %&gt;% finalfit(dependent, explanatory, confint_level = 0.90) -&gt; t {r eval=FALSE, include=FALSE, echo=TRUE} library(knitr) kable(t, row.names=FALSE, align = c( l , l , r , r , r , r , r , r )) 123.2.14 2.13 Confidence interval separation Some like to avoid the hyphen so as not to confuse with minus sign. Obviously not an issue in logistic regression. {r eval=FALSE, include=FALSE, echo=TRUE, warning=FALSE, message=FALSE} library(finalfit) explanatory = c( age.factor , sex.factor , obstruct.factor , perfor.factor ) dependent = mort_5yr colon_s %&gt;% finalfit(dependent, explanatory, confint_sep = to ) -&gt; t {r eval=FALSE, include=FALSE, echo=TRUE} library(knitr) kable(t, row.names=FALSE, align = c( l , l , r , r , r , r , r , r )) 123.2.15 2.14 Mixed effects random-intercept model At its simplest, a random-intercept model can be specified using a single quoted variable. In this example, it is the equivalent of quoting {r # andom_effect = (1 | hospital). {r eval=FALSE, include=FALSE, echo=TRUE, warning=FALSE, message=FALSE} library(finalfit) explanatory = c( age.factor , sex.factor , obstruct.factor , perfor.factor ) dependent = mort_5yr random_effect = hospital colon_s %&gt;% finalfit(dependent, explanatory, random_effect = random_effect, dependent_label_suffix = (random intercept) ) -&gt; t {r eval=FALSE, include=FALSE, echo=TRUE} library(knitr) kable(t, row.names=FALSE, align = c( l , l , r , r , r , r , r , r )) 123.2.16 2.15 Mixed effects random-slope model In the example below, allow the effect of age on outcome to vary by hospital. Note, this specification must have parentheses included. {r eval=FALSE, include=FALSE, echo=TRUE, warning=FALSE, message=FALSE} library(finalfit) explanatory = c( age.factor , sex.factor , obstruct.factor , perfor.factor ) dependent = mort_5yr random_effect = (age.factor | hospital) colon_s %&gt;% finalfit(dependent, explanatory, random_effect = random_effect, dependent_label_suffix = (random slope: age) ) -&gt; t {r eval=FALSE, include=FALSE, echo=TRUE} library(knitr) kable(t, row.names=FALSE, align = c( l , l , r , r , r , r , r , r )) 123.2.17 2.16 Mixed effects random-slope model directly from lme4 Clearly, as models get more complex, parameters such as random effect group variances may require to be extracted directly from model outputs. {r eval=FALSE, include=FALSE, echo=TRUE, warning=FALSE, message=FALSE} library(finalfit) explanatory = c( age.factor , sex.factor , obstruct.factor , perfor.factor ) dependent = mort_5yr random_effect = (age.factor | hospital) colon_s %&gt;% lme4::glmer(mort_5yr ~ age.factor + (age.factor | hospital), family = binomial , data = .) %&gt;% broom::tidy() -&gt; t {r eval=FALSE, include=FALSE, echo=TRUE} library(knitr) kable(t, row.names=FALSE, align = c( l , l , r , r , r , r , r , r )) 123.2.18 2.17 Exclude all missing data in final model from univariable analyses This can be useful if you want the numbers in the final table to match the final multivariable model. However, be careful to include a full explanation of this in the methods and the reason for exluding the missing data. {r eval=FALSE, include=FALSE, echo=TRUE, warning=FALSE, message=FALSE} library(finalfit) explanatory = c( age.factor , sex.factor , obstruct.factor , perfor.factor ) dependent = &#39;mort_5yr&#39; colon_s %&gt;% dplyr::select(explanatory, dependent) %&gt;% na.omit() %&gt;% finalfit(dependent, explanatory) -&gt; t {r eval=FALSE, include=FALSE, echo=TRUE} library(knitr) kable(t, row.names=FALSE, align = c( l , l , r , r , r , r , r , r )) 123.2.19 2.18 Linear regression {r eval=FALSE, include=FALSE, echo=TRUE, warning=FALSE, message=FALSE} library(finalfit) explanatory = c( age.factor , sex.factor , obstruct.factor , perfor.factor ) dependent = &#39;nodes&#39; colon_s %&gt;% finalfit(dependent, explanatory) -&gt; t {r eval=FALSE, include=FALSE, echo=TRUE} library(knitr) kable(t, row.names=FALSE, align = c( l , l , r , r , r , r , r , r )) 123.2.20 2.19 Mixed effects random-intercept linear regression {r eval=FALSE, include=FALSE, echo=TRUE, warning=FALSE, message=FALSE} library(finalfit) explanatory = c( age.factor , sex.factor , obstruct.factor , perfor.factor ) dependent = nodes random_effect = hospital colon_s %&gt;% finalfit(dependent, explanatory, random_effect = random_effect, dependent_label_suffix = (random intercept) ) -&gt; t {r eval=FALSE, include=FALSE, echo=TRUE} library(knitr) kable(t, row.names=FALSE, align = c( l , l , r , r , r , r , r , r )) 123.2.21 2.20 Mixed effects random-slope linear regression {r eval=FALSE, include=FALSE, echo=TRUE, warning=FALSE, message=FALSE} library(finalfit) explanatory = c( age.factor , sex.factor , obstruct.factor , perfor.factor ) dependent = nodes random_effect = (age.factor | hospital) colon_s %&gt;% finalfit(dependent, explanatory, random_effect = random_effect, dependent_label_suffix = (random slope: age) ) -&gt; t {r eval=FALSE, include=FALSE, echo=TRUE} library(knitr) kable(t, row.names=FALSE, align = c( l , l , r , r , r , r , r , r )) 123.2.22 2.21 Cox proportional hazards model (survival / time to event) {r eval=FALSE, include=FALSE, echo=TRUE, warning=FALSE, message=FALSE} library(finalfit) explanatory = c( age.factor , sex.factor , obstruct.factor , perfor.factor ) dependent = Surv(time, status) colon_s %&gt;% finalfit(dependent, explanatory) -&gt; t {r eval=FALSE, include=FALSE, echo=TRUE} library(knitr) kable(t, row.names=FALSE, align = c( l , l , r , r , r , r , r , r )) 123.2.23 2.22 Cox proportional hazards model: change dependent label As above, the dependent label cannot be specfied directly in the model to avoid errors. However, in survival modelling the surivial object specification can be long or awkward. Therefore, here is the work around. {r eval=FALSE, include=FALSE, echo=TRUE, warning=FALSE, message=FALSE} library(finalfit) explanatory = c( age.factor , sex.factor , obstruct.factor , perfor.factor ) dependent = Surv(time, status) colon_s %&gt;% finalfit(dependent, explanatory, add_dependent_label = FALSE) %&gt;% dplyr::rename( Overall survival = label) %&gt;% dplyr::rename( = levels) -&gt; t {r eval=FALSE, include=FALSE, echo=TRUE} library(knitr) kable(t, row.names=FALSE, align = c( l , l , r , r , r , r , r , r )) 123.3 3 Model tables manually using ff_merge() 123.3.1 3.1 Basic table Note summary_factorlist() needs argument, fit_id = TRUE. {r eval=FALSE, include=FALSE, echo=TRUE, warning=FALSE, message=FALSE} library(finalfit) library(dplyr) explanatory = c( age.factor , sex.factor , obstruct.factor , perfor.factor ) dependent = mort_5yr ## Crosstable colon_s %&gt;% summary_factorlist(dependent, explanatory, fit_id=TRUE) -&gt; table_1 ## Univariable colon_s %&gt;% glmuni(dependent, explanatory) %&gt;% fit2df(estimate_suffix= (univariable) ) -&gt; table_2 ## Merge table_1 %&gt;% ff_merge(table_2) %&gt;% select(-c(fit_id, index)) %&gt;% dependent_label(colon_s, dependent)-&gt; t {r eval=FALSE, include=FALSE, echo=TRUE} library(knitr) kable(t, row.names=FALSE, align = c( l , l , r , r , r , r , r , r )) 123.3.2 3.2 Complex table (all in single pipe) {r eval=FALSE, include=FALSE, echo=TRUE, warning=FALSE, message=FALSE} library(finalfit) library(dplyr) explanatory = c( age.factor , sex.factor , obstruct.factor , perfor.factor ) random_effect = hospital dependent = mort_5yr # All in one pipe colon_s %&gt;% ## Crosstable summary_factorlist(dependent, explanatory, fit_id=TRUE) %&gt;% ## Add univariable ff_merge( glmuni(colon_s, dependent, explanatory) %&gt;% fit2df(estimate_suffix= (univariable) ) ) %&gt;% ## Add multivariable ff_merge( glmmulti(colon_s, dependent, explanatory) %&gt;% fit2df(estimate_suffix= (multivariable) ) ) %&gt;% ## Add mixed effects ff_merge( glmmixed(colon_s, dependent, explanatory, random_effect) %&gt;% fit2df(estimate_suffix= (multilevel) ) ) %&gt;% select(-c(fit_id, index)) %&gt;% dependent_label(colon_s, dependent) -&gt; t {r eval=FALSE, include=FALSE, echo=TRUE} library(knitr) kable(t, row.names=FALSE, align = c( l , l , r , r , r , r , r , r )) 123.3.3 3.3 Other GLM models 123.3.3.1 Poisson {r eval=FALSE, include=FALSE, echo=TRUE, warning=FALSE, message=FALSE} library(finalfit) library(dplyr) ## Dobson (1990) Page 93: Randomized Controlled Trial : counts = c(18,17,15,20,10,20,25,13,12) outcome = gl(3,1,9) treatment = gl(3,3) d.AD &lt;- data.frame(treatment, outcome, counts) dependent = counts explanatory = c( outcome , treatment ) fit_uni = d.AD %&gt;% glmuni(dependent, explanatory, family = poisson) %&gt;% fit2df(estimate_name = Rate ratio (univariable) ) fit_multi = d.AD %&gt;% glmmulti(dependent, explanatory, family = poisson) %&gt;% fit2df(estimate_name = Rate ratio (multivariable) ) # All in one pipe d.AD %&gt;% ## Crosstable summary_factorlist(dependent, explanatory, cont = median , fit_id=TRUE) %&gt;% ## Add univariable ff_merge(fit_uni, estimate_name = Rate ratio ) %&gt;% ## Add multivariable ff_merge(fit_multi, estimate_name = Rate ratio ) %&gt;% select(-c(fit_id, index)) %&gt;% dependent_label(d.AD, dependent) -&gt; t {r eval=FALSE, include=FALSE, echo=TRUE} library(knitr) kable(t, row.names=FALSE, align = c( l , l , r , r , r , r , r , r )) 123.3.3.2 Gamma {r eval=FALSE, include=FALSE, echo=TRUE, warning=FALSE, message=FALSE} library(finalfit) library(dplyr) # A Gamma example, from McCullagh &amp; Nelder (1989, pp. 300-2) clotting &lt;- data.frame( u = c(5,10,15,20,30,40,60,80,100), lot1 = c(118,58,42,35,27,25,21,19,18), lot2 = c(69,35,26,21,18,16,13,12,12)) dependent = lot1 explanatory = log(u) fit_uni = clotting %&gt;% glmuni(dependent, explanatory, family = Gamma) %&gt;% fit2df(estimate_name = Coefficient , exp = FALSE, digits = c(3,3,4)) # All in one pipe clotting %&gt;% ## Crosstable summary_factorlist(dependent, explanatory, cont = median , fit_id=TRUE) %&gt;% ## Add fit ff_merge(fit_uni) %&gt;% select(-c(fit_id, index)) %&gt;% dependent_label(colon_s, dependent) -&gt; t {r eval=FALSE, include=FALSE, echo=TRUE} library(knitr) kable(t, row.names=FALSE, align = c( l , l , r , r , r , r , r , r )) 123.3.4 3.4 Weighted regression {r eval=FALSE, include=FALSE, echo=TRUE, warning=FALSE, message=FALSE} library(finalfit) library(dplyr) explanatory = c( age.factor , sex.factor , obstruct.factor , perfor.factor ) dependent = mort_5yr weights = runif(dim(colon_s)[1]) # random just for example # All in one pipe colon_s %&gt;% ## Crosstable summary_factorlist(dependent, explanatory, fit_id=TRUE) %&gt;% ## Add univariable ff_merge( glmuni(colon_s, dependent, explanatory, weights = weights, family = quasibinomial) %&gt;% fit2df(estimate_suffix= (univariable) ) ) %&gt;% ## Add multivariable ff_merge( glmmulti(colon_s, dependent, explanatory, weights = weights, family = quasibinomial) %&gt;% fit2df(estimate_suffix= (multivariable) ) ) %&gt;% select(-c(fit_id, index)) %&gt;% dependent_label(colon_s, dependent) -&gt; t {r eval=FALSE, include=FALSE, echo=TRUE} library(knitr) kable(t, row.names=FALSE, align = c( l , l , r , r , r , r , r , r )) 123.3.5 3.5 Using base R functions Note ff_formula() convenience function to make multivariable formula (y ~ x1 + x2 + x3 etc.) from a dependent and explanatory vector of names. {r eval=FALSE, include=FALSE, echo=TRUE, warning=FALSE, message=FALSE} library(finalfit) library(dplyr) explanatory = c( age.factor , sex.factor , obstruct.factor , perfor.factor ) dependent = mort_5yr # All in one pipe colon_s %&gt;% ## Crosstable summary_factorlist(dependent, explanatory, fit_id=TRUE) %&gt;% ## Add univariable ff_merge( glmuni(colon_s, dependent, explanatory) %&gt;% fit2df(estimate_suffix= (univariable) ) ) %&gt;% ## Add multivariable ff_merge( glm( ff_formula(dependent, explanatory), data = colon_s, family = binomial , weights = NULL ) %&gt;% fit2df(estimate_suffix= (multivariable) ) ) %&gt;% select(-c(fit_id, index)) %&gt;% dependent_label(colon_s, dependent) -&gt; t {r eval=FALSE, include=FALSE, echo=TRUE} library(knitr) kable(t, row.names=FALSE, align = c( l , l , r , r , r , r , r , r )) 123.3.6 3.6 Edit table rows This can be done as any dataframe would be edited. {r eval=FALSE, include=FALSE, echo=TRUE, warning=FALSE, message=FALSE} library(finalfit) library(dplyr) explanatory = c( age.factor*sex.factor , obstruct.factor , perfor.factor ) dependent = mort_5yr # Run model for term test fit &lt;- glm( ff_formula(dependent, explanatory), data=colon_s, family = binomial ) # Not run #term_test &lt;- survey::regTermTest(fit, age.factor:sex.factor ) # Run final table with results of term test colon_s %&gt;% finalfit(dependent, explanatory) %&gt;% rbind(c( age.factor:sex.factor (overall) , Interaction , - , - , - , paste0( p = 0.775 ) ))-&gt; t {r eval=FALSE, include=FALSE, echo=TRUE} library(knitr) kable(t, row.names=FALSE, align = c( l , l , r , r , r , r , r , r )) 123.3.7 3.7 Base model + individual explanatory variables This was an email enquiry about how to build on a base model. The example request was in a survival context. {r eval=FALSE, include=FALSE, echo=TRUE, warning=FALSE, message=FALSE} library(finalfit) library(dplyr) mydata = colon_s base_explanatory = c( age.factor , sex.factor ) explanatory = c( obstruct.factor , perfor.factor , node4.factor ) dependent = Surv(time, status) mydata %&gt;% # Counts summary_factorlist(dependent, c(base_explanatory, explanatory), column = TRUE, fit_id = TRUE) %&gt;% # Univariable ff_merge( coxphuni(mydata, dependent, c(base_explanatory, explanatory)) %&gt;% fit2df(estimate_suffix = (Univariable) ) ) %&gt;% # Base ff_merge( coxphmulti(mydata, dependent, base_explanatory) %&gt;% fit2df(estimate_suffix = (Base model) ) ) %&gt;% # Model 1 ff_merge( coxphmulti(mydata, dependent, c(base_explanatory, explanatory[1])) %&gt;% fit2df(estimate_suffix = (Model 1) ) ) %&gt;% # Model 2 ff_merge( coxphmulti(mydata, dependent, c(base_explanatory, explanatory[2])) %&gt;% fit2df(estimate_suffix = (Model 2) ) ) %&gt;% # Model 3 ff_merge( coxphmulti(mydata, dependent, c(base_explanatory, explanatory[3])) %&gt;% fit2df(estimate_suffix = (Model 3) ) ) %&gt;% # Full ff_merge( coxphmulti(mydata, dependent, c(base_explanatory, explanatory)) %&gt;% fit2df(estimate_suffix = (Full) ) ) %&gt;% # Tidy-up select(-c(fit_id, index)) %&gt;% rename( Overall survival = label) %&gt;% rename( = levels) %&gt;% rename(`n (%)` = all) -&gt; t {r eval=FALSE, include=FALSE, echo=TRUE} library(knitr) kable(t, row.names=FALSE, align = c( l , l , r , r , r , r , r , r , r , r )) 123.4 4 Support for complex survey structures via library(survey) 123.4.1 4.1 Linear regression Examples taken from survey::svyglm() help page. {r eval=FALSE, include=FALSE, echo=TRUE, warning=FALSE, message=FALSE} library(survey) library(dplyr) data(api) dependent = api00 explanatory = c( ell , meals , mobility ) # Label data frame apistrat = apistrat %&gt;% mutate( api00 = ff_label(api00, API in 2000 (api00) ), ell = ff_label(ell, English language learners (percent)(ell) ), meals = ff_label(meals, Meals eligible (percent)(meals) ), mobility = ff_label(mobility, First year at the school (percent)(mobility) ), sch.wide = ff_label(sch.wide, School-wide target met (sch.wide) ) ) # Linear example dependent = api00 explanatory = c( ell , meals , mobility ) # Stratified design dstrat = svydesign(id=~1,strata=~stype, weights=~pw, data=apistrat, fpc=~fpc) # Univariable fit fit_uni = dstrat %&gt;% svyglmuni(dependent, explanatory) %&gt;% fit2df(estimate_suffix = (univariable) ) # Multivariable fit fit_multi = dstrat %&gt;% svyglmmulti(dependent, explanatory) %&gt;% fit2df(estimate_suffix = (multivariable) ) # Pipe together apistrat %&gt;% summary_factorlist(dependent, explanatory, fit_id = TRUE) %&gt;% ff_merge(fit_uni) %&gt;% ff_merge(fit_multi) %&gt;% select(-fit_id, -index) %&gt;% dependent_label(apistrat, dependent) -&gt; t {r eval=FALSE, include=FALSE, echo=TRUE} library(knitr) kable(t, row.names=FALSE, align = c( l , l , r , r , r , r , r , r , r , r )) 123.4.2 4.2 Binomial example Note model family needs specified and exponentiation set to TRUE if desired. {r eval=FALSE, include=FALSE, echo=TRUE, warning=FALSE, message=FALSE} library(survey) library(dplyr) data(api) dependent = sch.wide explanatory = c( ell , meals , mobility ) # Label data frame apistrat = apistrat %&gt;% mutate( api00 = ff_label(api00, API in 2000 (api00) ), ell = ff_label(ell, English language learners (percent)(ell) ), meals = ff_label(meals, Meals eligible (percent)(meals) ), mobility = ff_label(mobility, First year at the school (percent)(mobility) ), sch.wide = ff_label(sch.wide, School-wide target met (sch.wide) ) ) # Univariable fit fit_uni = dstrat %&gt;% svyglmuni(dependent, explanatory, family = quasibinomial ) %&gt;% fit2df(exp = TRUE, estimate_name = OR , estimate_suffix = (univariable) ) # Multivariable fit fit_multi = dstrat %&gt;% svyglmmulti(dependent, explanatory, family = quasibinomial ) %&gt;% fit2df(exp = TRUE, estimate_name = OR , estimate_suffix = (multivariable) ) # Pipe together apistrat %&gt;% summary_factorlist(dependent, explanatory, fit_id = TRUE) %&gt;% ff_merge(fit_uni) %&gt;% ff_merge(fit_multi) %&gt;% select(-fit_id, -index) %&gt;% dependent_label(apistrat, dependent) -&gt; t {r eval=FALSE, include=FALSE, echo=TRUE} library(knitr) kable(t, row.names=FALSE, align = c( l , l , r , r , r , r , r , r , r , r )) "],["finalfit.html", "Chapter 124 finalfit 124.1 Table 1 - Demographics 124.2 Table 2 - Association between tumour factors and 5 year mortality 124.3 Figure 1 - Association between tumour factors and 5 year mortality", " Chapter 124 finalfit devtools::install_github( ewenharrison/finalfit ) {r eval=FALSE, include=FALSE, echo=TRUE} library(finalfit) library(dplyr) {r eval=FALSE, include=FALSE, echo=TRUE} colon_s {r eval=FALSE, include=FALSE, echo=TRUE} dependent &lt;- differ.factor # Specify explanatory variables of interest explanatory &lt;- c( age , sex.factor , extent.factor , obstruct.factor , nodes ) {r eval=FALSE, include=FALSE, echo=TRUE} # colon_s %&gt;% # select(age, sex.factor, # extent.factor, obstruct.factor, nodes) %&gt;% # names() -&gt; explanatory {r eval=FALSE, include=FALSE, echo=TRUE} colon_s %&gt;% summary_factorlist(dependent, explanatory, p=TRUE, na_include=FALSE) {r eval=FALSE, include=FALSE, echo=TRUE} Hmisc::label(colon_s$nodes) &lt;- Lymph nodes involved explanatory = c( age , sex.factor , extent.factor , nodes ) colon_s %&gt;% summary_factorlist(dependent, explanatory, p=TRUE, na_include=FALSE, add_dependent_label=TRUE) -&gt; table1 table1 {r eval=FALSE, include=FALSE, echo=TRUE} explanatory &lt;- c( age , sex.factor , extent.factor , nodes , differ.factor ) dependent &lt;- mort_5yr colon_s %&gt;% finalfit(dependent = dependent, explanatory = explanatory, fit_id=TRUE, dependent_label_prefix = ) -&gt; table2 kableExtra::kable(table2) {r eval=FALSE, include=FALSE, echo=TRUE} colon_s %&gt;% or_plot(dependent, explanatory, breaks = c(0.5, 1, 5, 10, 20, 30)) {r eval=FALSE, include=FALSE, echo=TRUE} # Save objects for knitr/markdown save(table1, table2, dependent, explanatory, file = out.rda ) {r eval=FALSE, include=FALSE, echo=TRUE} # Load data into global environment. library(finalfit) library(dplyr) library(knitr) load( out.rda ) 124.1 Table 1 - Demographics {r table1x, echo = TRUE, results=&#39;asis&#39;} kable(table1, row.names=FALSE, align=c( l , l , r , r , r , r )) 124.2 Table 2 - Association between tumour factors and 5 year mortality {r table2x, echo = TRUE, results=&#39;asis&#39;} kable(table2, row.names=FALSE, align=c( l , l , r , r , r , r )) 124.3 Figure 1 - Association between tumour factors and 5 year mortality {r figure1x, eval=FALSE, include=FALSE, echo=TRUE, warning=FALSE, message=FALSE} colon_s %&gt;% or_plot(dependent, explanatory) "],["finalfit-1.html", "Chapter 125 finalfit 125.1 Table 1 - Demographics 125.2 Table 2 - Association between tumour factors and 5 year mortality 125.3 Figure 1 - Association between tumour factors and 5 year mortality", " Chapter 125 finalfit devtools::install_github( ewenharrison/finalfit ) {r eval=FALSE, include=FALSE, echo=TRUE} library(finalfit) library(dplyr) {r eval=FALSE, include=FALSE, echo=TRUE} colon_s {r eval=FALSE, include=FALSE, echo=TRUE} dependent &lt;- differ.factor # Specify explanatory variables of interest explanatory &lt;- c( age , sex.factor , extent.factor , obstruct.factor , nodes ) {r eval=FALSE, include=FALSE, echo=TRUE} # colon_s %&gt;% # select(age, sex.factor, # extent.factor, obstruct.factor, nodes) %&gt;% # names() -&gt; explanatory {r eval=FALSE, include=FALSE, echo=TRUE} colon_s %&gt;% summary_factorlist(dependent, explanatory, p=TRUE, na_include=FALSE) {r eval=FALSE, include=FALSE, echo=TRUE} Hmisc::label(colon_s$nodes) &lt;- Lymph nodes involved explanatory = c( age , sex.factor , extent.factor , nodes ) colon_s %&gt;% summary_factorlist(dependent, explanatory, p=TRUE, na_include=FALSE, add_dependent_label=TRUE) -&gt; table1 table1 {r eval=FALSE, include=FALSE, echo=TRUE} explanatory &lt;- c( age , sex.factor , extent.factor , nodes , differ.factor ) dependent &lt;- mort_5yr colon_s %&gt;% finalfit(dependent = dependent, explanatory = explanatory, fit_id=TRUE, dependent_label_prefix = ) -&gt; table2 kableExtra::kable(table2) {r eval=FALSE, include=FALSE, echo=TRUE} colon_s %&gt;% or_plot(dependent, explanatory, breaks = c(0.5, 1, 5, 10, 20, 30)) {r eval=FALSE, include=FALSE, echo=TRUE} # Save objects for knitr/markdown save(table1, table2, dependent, explanatory, file = out.rda ) {r eval=FALSE, include=FALSE, echo=TRUE} # Load data into global environment. library(finalfit) library(dplyr) library(knitr) load( out.rda ) 125.1 Table 1 - Demographics {r table1 4, echo = TRUE, results=&#39;asis&#39;} kable(table1, row.names=FALSE, align=c( l , l , r , r , r , r )) 125.2 Table 2 - Association between tumour factors and 5 year mortality {r table2 4, echo = TRUE, results=&#39;asis&#39;} kable(table2, row.names=FALSE, align=c( l , l , r , r , r , r )) 125.3 Figure 1 - Association between tumour factors and 5 year mortality {r figure1 4, eval=FALSE, include=FALSE, echo=TRUE, warning=FALSE, message=FALSE} colon_s %&gt;% or_plot(dependent, explanatory) "],["example-knitrr-markdown-document.html", "Chapter 126 Example knitr/R Markdown document 126.1 Table 1 - Demographics 126.2 Table 2 - Association between tumour factors and 5 year mortality 126.3 Figure 1 - Association between tumour factors and 5 year mortality", " Chapter 126 Example knitr/R Markdown document author: Ewen Harrison date: 21/5/2018 output: pdf_document: default geometry: margin=0.75in {r eval=FALSE, include=FALSE, echo=TRUE} # Load data into global environment. library(finalfit) library(dplyr) library(knitr) library(kableExtra) load( out.rda ) 126.1 Table 1 - Demographics {r table1 3, echo = TRUE, results=&#39;asis&#39;} kable(table1, row.names=FALSE, align=c( l , l , r , r , r , r ), booktabs=TRUE) 126.2 Table 2 - Association between tumour factors and 5 year mortality {r table2 3, eval=FALSE, include=FALSE, results=&#39;asis&#39;} kable(table2, row.names=FALSE, align=c( l , l , r , r , r , r ), booktabs=TRUE) %&gt;% kable_styling(font_size=8) 126.3 Figure 1 - Association between tumour factors and 5 year mortality {r figure1-, warning=FALSE, message=FALSE, fig.width=10, eval=FALSE, include=FALSE, echo=TRUE, warning=FALSE, message=FALSE} colon_s %&gt;% or_plot(dependent, explanatory) "],["finalfit-2.html", "Chapter 127 finalfit 127.1 Table 1 - Demographics 127.2 Table 2 - Association between tumour factors and 5 year mortality 127.3 Figure 1 - Association between tumour factors and 5 year mortality", " Chapter 127 finalfit devtools::install_github( ewenharrison/finalfit ) {r eval=FALSE, include=FALSE, echo=TRUE} library(finalfit) library(dplyr) {r eval=FALSE, include=FALSE, echo=TRUE} colon_s {r eval=FALSE, include=FALSE, echo=TRUE} dependent &lt;- differ.factor # Specify explanatory variables of interest explanatory &lt;- c( age , sex.factor , extent.factor , obstruct.factor , nodes ) {r eval=FALSE, include=FALSE, echo=TRUE} # colon_s %&gt;% # select(age, sex.factor, # extent.factor, obstruct.factor, nodes) %&gt;% # names() -&gt; explanatory {r eval=FALSE, include=FALSE, echo=TRUE} colon_s %&gt;% summary_factorlist(dependent, explanatory, p=TRUE, na_include=FALSE) {r eval=FALSE, include=FALSE, echo=TRUE} Hmisc::label(colon_s$nodes) &lt;- Lymph nodes involved explanatory = c( age , sex.factor , extent.factor , nodes ) colon_s %&gt;% summary_factorlist(dependent, explanatory, p=TRUE, na_include=FALSE, add_dependent_label=TRUE) -&gt; table1 table1 {r eval=FALSE, include=FALSE, echo=TRUE} explanatory &lt;- c( age , sex.factor , extent.factor , nodes , differ.factor ) dependent &lt;- mort_5yr colon_s %&gt;% finalfit(dependent = dependent, explanatory = explanatory, fit_id=TRUE, dependent_label_prefix = ) -&gt; table2 kableExtra::kable(table2) {r eval=FALSE, include=FALSE, echo=TRUE} colon_s %&gt;% or_plot(dependent, explanatory, breaks = c(0.5, 1, 5, 10, 20, 30)) {r eval=FALSE, include=FALSE, echo=TRUE} # Save objects for knitr/markdown save(table1, table2, dependent, explanatory, file = out.rda ) {r eval=FALSE, include=FALSE, echo=TRUE} # Load data into global environment. library(finalfit) library(dplyr) library(knitr) load( out.rda ) 127.1 Table 1 - Demographics {r table1y 2, echo = TRUE, results=&#39;asis&#39;} kable(table1, row.names=FALSE, align=c( l , l , r , r , r , r )) 127.2 Table 2 - Association between tumour factors and 5 year mortality {r table2 2, echo = TRUE, results=&#39;asis&#39;} kable(table2, row.names=FALSE, align=c( l , l , r , r , r , r )) 127.3 Figure 1 - Association between tumour factors and 5 year mortality {r figure1, eval=FALSE, include=FALSE, echo=TRUE, warning=FALSE, message=FALSE} colon_s %&gt;% or_plot(dependent, explanatory) "],["r-notebook.html", "Chapter 128 R Notebook 128.1 Flipping Coin", " Chapter 128 R Notebook 128.1 Flipping Coin {r eval=FALSE, include=FALSE, echo=TRUE} rbinom(n = 1, size = 1, prob = 0.5) {r eval=FALSE, include=FALSE, echo=TRUE} rbinom(n = 10, size = 1, prob = 0.5) {r eval=FALSE, include=FALSE, echo=TRUE} rbinom(n = 1, size = 10, prob = 0.5) {r eval=FALSE, include=FALSE, echo=TRUE} rbinom(n = 100, size = 100, prob = 0.5) {r eval=FALSE, include=FALSE, echo=TRUE} rbinom(n = 10, size = 10, prob = 0.3) "],["formattable.html", "Chapter 129 formattable", " Chapter 129 formattable https://www.littlemissdata.com/blog/prettytables {r eval=FALSE, include=FALSE, echo=TRUE} library(data.table) library(dplyr) library(formattable) library(tidyr) {r eval=FALSE, include=FALSE, echo=TRUE} #Set a few color variables to make our table more visually appealing customGreen0 = #DeF7E9 customGreen = #71CA97 customRed = #ff7f7f "],["general-linear-models-1.html", "Chapter 130 General Linear Models", " Chapter 130 General Linear Models "],["alternatives-to-the-default-r-outputs-for-glms-and-linear-models.html", "Chapter 131 5 Alternatives to the Default R Outputs for GLMs and Linear Models 131.1 Classic Output 131.2 stargazer 131.3 formattable 131.4 flipRegression", " Chapter 131 5 Alternatives to the Default R Outputs for GLMs and Linear Models https://www.displayr.com/5-alternatives-to-the-default-r-outputs-for-glms-and-linear-models/?utm_medium=Feed&amp;utm_source=Syndication 131.1 Classic Output {r eval=FALSE, include=FALSE, echo=TRUE} churn &lt;- read.csv( https://community.watsonanalytics.com/wp-content/uploads/2015/03/WA_Fn-UseC_-Telco-Customer-Churn.csv ) my.glm &lt;- glm(Churn ~ SeniorCitizen + tenure + InternetService + MonthlyCharges, data = churn, family = binomial(logit)) summary(my.glm) 131.2 stargazer {r eval=FALSE, include=FALSE, echo=TRUE} write(stargazer::stargazer(my.glm, type = html ), stargazer.html ) 131.3 formattable {r eval=FALSE, include=FALSE, echo=TRUE} library(formattable) my.glm 131.4 flipRegression {r eval=FALSE, include=FALSE, echo=TRUE} # devtools::install_github( Displayr/flipPlots ) # devtools::install_github( Displayr/flipRegression ) library(flipPlots) library(flipRegression) my.regression &lt;- Regression(Churn ~ SeniorCitizen + tenure + InternetService + MonthlyCharges, data = churn, show.labels = TRUE, type = Binary Logit ) my.regression {r eval=FALSE, include=FALSE, echo=TRUE} library(flipRegression) Regression(Churn ~ SeniorCitizen + tenure + InternetService + MonthlyCharges, data = churn, show.labels = TRUE, output = Relative Importance Analysis , type = Binary Logit ) {r eval=FALSE, include=FALSE, echo=TRUE} library(effects) my.glm = glm(Churn ~ SeniorCitizen + tenure + InternetService + MonthlyCharges, data = churn, family = binomial(logit)) effects = allEffects(my.glm) plot(effects, col = 2, ylab = Probability(Churn) , ylim = c(0, .6), type = response ) {r eval=FALSE, include=FALSE, echo=TRUE} library(httr) # GET( https://docs.displayr.com/images/f/f0/Churn.xlsx , # write_disk(tf &lt;- tempfile(fileext = .xlsx ))) # df &lt;- readxl::read_excel(tf, 1L) library(mgcv) my.gam &lt;- gam(Churn ~ SeniorCitizen + s(tenure) + InternetService + s(MonthlyCharges), data = churn, family = binomial(logit)) my.gam 131.4.1 Building Online Interactive Simulators for Predictive Models in R https://www.displayr.com/building-online-interactive-simulators-for-predictive-models-in-r/ "],["general-resources-1.html", "Chapter 132 General Resources", " Chapter 132 General Resources "],["data-science-live-book-1.html", "Chapter 133 Data Science Live Book 133.1 master course links", " Chapter 133 Data Science Live Book https://livebook.datascienceheroes.com/ https://toolbox.google.com/datasetsearch http://archive.ics.uci.edu/ml/index.php http://asdfree.com/ https://rstudio-education.github.io/hopr/ What I Wish I Knew When I Started R https://www.williamrchase.com/slides/intro_r_anthropology_2018 https://sbalci.gitbooks.io/pathology-notes/content/pathology-residents/computational-pathology.html http://web.stanford.edu/class/bios221/book/ https://kbroman.org/minimal_make/ https://www.gnu.org/software/make/ https://kbroman.org/minimal_make/ https://www..com/community/tutorials/shell-commands-data-scientist https://moderndive.com/3-viz.html https://www.causeweb.org/cause/ecots/ecots18/breakouts/7 https://plotly-book.cpsievert.me/ http://r-bio.github.io/01-intro-R/ https://www.rdatagen.net/post/by-vs-within/?platform=hootsuite http://www.biomart.org/download.html https://ropensci.org/blog/2018/07/24/educollab-challenges/ https://www..com/community/tutorials/data-science-pitfalls https://serialmentor.com/dataviz/preface.html https://news.codecademy.com/errors-in-code-think-differently/?utm_source=customer.io&amp;utm_medium=email&amp;utm_campaign=fortnightly_8-1-18&amp;utm_content=ErrorFortnightly Data Science Live Book https://livebook.datascienceheroes.com/ School of Psychology at the University of New South Wales http://www.compcogscisydney.org/teaching/ Of Minds and Machines http://www.compcogscisydney.org/mm/ psyr: Using R in Psychological Science http://www.compcogscisydney.org/psyr/ Perception and Cognition http://www.compcogscisydney.org/psyc2071/ Learning Statistics with R http://www.compcogscisydney.org/learning-statistics-with-r/ Computational Cognitive Science http://www.compcogscisydney.org/ccs/ Advanced R https://adv-r.hadley.nz/ One Page R https://togaware.com/onepager/ htmlwidgets for R http://www.htmlwidgets.org/ http://gallery.htmlwidgets.org/ Learning R for Clinical Epidemiologists http://rpubs.com/michaelmarks/R-Clin-Epi r-tutor http://www.r-tutor.com/ Statistics Meets Big Data http://www.statsoft.org/ ModernDive https://moderndive.com/ Laerd Statistics https://statistics.laerd.com/ statpages http://statpages.info/index.html The R class R programming for biologists http://r-bio.github.io/ Sosyal Bilimler Ara≈ütƒ±rmalarƒ± ƒ∞√ßin R https://bookdown.org/connect/#/apps/1531/access R for Psychological Science An introductory resource http://compcogscisydney.org/psyr/ Jamovi tutorial https://datalab.cc/tools/jamovi https://www.youtube.com/playlist?list=PLkk92zzyru5OAtc_ItUubaSSq6S_TGfRn 133.1 master course links "],["do-more-with-r.html", "Chapter 134 Do More with R", " Chapter 134 Do More with R https://www.infoworld.com/video/series/8563/do-more-with-r "],["my-r-codes-for-data-analysis-6.html", "Chapter 135 My R Codes For Data Analysis", " Chapter 135 My R Codes For Data Analysis "],["getting-data-into-r-veriyi-ra-y√ºkleme-1.html", "Chapter 136 Getting Data into R / Veriyi R‚Äôa y√ºkleme 136.1 Import Data", " Chapter 136 Getting Data into R / Veriyi R‚Äôa y√ºkleme 136.1 Import Data 136.1.1 Import using RStudio 136.1.2 Import CSV File {r eval=FALSE, include=FALSE, echo=TRUE} scabies &lt;- read.csv(file = http://datacompass.lshtm.ac.uk/607/2/S1-Dataset_CSV.csv , header = TRUE, sep = , ) scabies 136.1.2.1 How to import multiple .csv files at once? https://stackoverflow.com/questions/11433432/how-to-import-multiple-csv-files-at-once temp = list.files(pattern= *.csv ) myfiles = lapply(temp, read.delim) temp = list.files(pattern= *.csv ) for (i in 1:length(temp)) assign(temp[i], read.csv(temp[i])) temp = list.files(pattern= *.csv ) list2env( lapply(setNames(temp, make.names(gsub( *.csv$ , , temp))), read.csv), envir = .GlobalEnv) # Get the files names files = list.files(pattern= *.csv ) # First apply read.csv, then rbind myfiles = do.call(rbind, lapply(files, function(x) read.csv(x, stringsAsFactors = FALSE))) library(data.table) DT = do.call(rbind, lapply(files, fread)) # The same using `{r # bindlist` DT = rbindlist(lapply(files, fread)) library(readr) library(dplyr) tbl = lapply(files, read_csv) %&gt;% bind_rows() data &lt;- read.csv( switch(animal, dog = dogdata.csv , cat = catdata.csv , rabbit = rabbitdata.csv ) ) 136.1.3 Import TXT File {r eval=FALSE, include=FALSE, echo=TRUE} ebola &lt;- read.csv(file = http://datacompass.lshtm.ac.uk/608/1/mmc1.txt , header = TRUE, sep = , ) ebola 136.1.4 Import Excel File my_data &lt;- read_excel(file.choose()) files &lt;- list.files(pattern = .xlsx ) data_xlsx_df &lt;- map_df(set_names(files), function(file) { file %&gt;% excel_sheets() %&gt;% set_names() %&gt;% map_df( ~ read_xlsx(path = file, sheet = .x, range = H3 ), .id = sheet ) }, .id = file ) 136.1.4.1 Import Sheets 136.1.5 Import SPSS File 136.1.6 Keep SPSS labels {r eval=FALSE, include=FALSE, echo=TRUE} library(foreign) # foreign paketi y√ºkleniyor read.spss komutu ile deƒüer etiketlerini almasƒ±nƒ± ve bunu liste olarak deƒüil de data.frame olarak kaydetmesini istiyoruz {r eval=FALSE, include=FALSE, echo=TRUE} mydata &lt;- read.spss( mydata.sav , use.value.labels = TRUE, to.data.frame = TRUE) aktardƒ±ƒüƒ±mƒ±z data.frame‚Äôin √∂zellikleri (attr) i√ßinde deƒüi≈ükenlerin etiketleri var, bunlarƒ± dƒ±≈üarƒ± √ßƒ±kartƒ±yoruz {r eval=FALSE, include=FALSE, echo=TRUE} VariableLabels &lt;- as.data.frame(attr(mydata, variable.labels )) elde ettiƒüimiz data.frame‚Äôdeki satƒ±r isimleri deƒüi≈ükenlerin isimleri oluyor, kar≈üƒ±larƒ±nda da deƒüi≈üken etiketleri var satƒ±r isimlerini de dƒ±≈üarƒ± √ßƒ±kartƒ±yoruz {r eval=FALSE, include=FALSE, echo=TRUE} VariableLabels$original &lt;- rownames(VariableLabels) Deƒüi≈üken etiketi olanlarƒ± etiketleri ile diƒüerlerini olduƒüu gibi saklƒ±yoruz {r eval=FALSE, include=FALSE, echo=TRUE} VariableLabels$label[VariableLabels$label == ] &lt;- NA VariableLabels$colname &lt;- VariableLabels$original VariableLabels$colname[!is.na(VariableLabels$label)] &lt;- as.vector(VariableLabels$label[!is.na(VariableLabels$label)]) son olarak da data.frame‚Äôdeki s√ºtun isimlerini deƒüi≈ütiriyoruz {r eval=FALSE, include=FALSE, echo=TRUE} names(mydata) &lt;- VariableLabels$colname "],["export-data.html", "Chapter 137 Export Data", " Chapter 137 Export Data 137.0.1 Export to SPSS, while keeping labels R‚Äôda factor olan label verdiƒüiniz deƒüi≈ükenleri SPSS ya da diƒüer istatistik programlarƒ±na aktardƒ±ƒüƒ±nƒ±zda bu tanƒ±mlamalarƒ± korumak i≈üimize yarar. Bunun i√ßin foreign paketi ile bir txt dosyasƒ± ve bir sps dosyasƒ± olu≈üturuyoruz. SPSS‚Äôte sps dosyasƒ±nƒ± a√ßƒ±p kodu √ßalƒ±≈ütƒ±rarak tekrar atanan deƒüerler geri y√ºkleniyor. {r eval=FALSE, include=FALSE, echo=TRUE} library(foreign) write.foreign(mydata, mydata.txt , mydata.sps , package = SPSS ) https://twitter.com/WeAreRLadies/status/1034817323922804737 f &lt;- list.files( my_folder , pattern = *.csv , full.names = TRUE) d &lt;- purrr::map_df(f, readr::read_csv, .id = id ) m &lt;- lm(mpg ~ qsec + wt, data = mtcars) broom::tidy(m) Import a Directory of CSV Files at Once Using {purrr} and {readr} https://www.gerkelab.com/blog/2018/09/import-directory-csv-purrr-readr/ data_dir %&gt;% dir_ls(regexp = \\\\.csv$ ) %&gt;% map_dfr(read_csv, .id = source ) %&gt;% mutate(Month_Year = myd(Month_Year, truncated = 1)) https://suatatan.wordpress.com/2017/10/07/bulk-replacing-turkish-characters-in-r/ Turkish character sometimes became the menace for the data scientist. To avoid the risks you may want to change it with safe characters. To do that you can use this code: #turkce karakter donusumu to.plain &lt;- function(s) { # 1 character substitutions old1 &lt;- ‚Äú√ßƒü≈üƒ±√º√∂√áƒû≈ûƒ∞√ñ√ú‚Äù new1 &lt;- ‚Äúcgsiuocgsiou‚Äù s1 &lt;- chartr(old1, new1, s) # 2 character substitutions old2 &lt;- c(‚Äú≈ì‚Äù, ‚Äú√ü‚Äù, ‚Äú√¶‚Äù, ‚Äú√∏‚Äù) new2 &lt;- c(‚Äúoe‚Äù, ‚Äúss‚Äù, ‚Äúae‚Äù, ‚Äúoe‚Äù) s2 &lt;- s1 for(i in seq_along(old2)) s2 &lt;- gsub(old2[i], new2[i], s2, fixed = TRUE) s2 } df$source=as.vector(sapply(df$source,to.plain)) to.plain(make.names(tolower(names(df)))) Remove all special characters from a string in R? https://stackoverflow.com/questions/10294284/remove-all-special-characters-from-a-string-in-r x &lt;- a1~!@#$%^&amp;*(){}_+:\\ &lt;&gt;?,./;&#39;[]-= stringr::str_replace_all(x, [[:punct:]] , ) stringr::str_replace_all(x, [^[:alnum:]] , ) astr &lt;- √Åbcd√™√£√ßo√†√∫√º iconv(astr, from = &#39;UTF-8&#39;, to = &#39;ASCII//TRANSLIT&#39;) Data &lt;- gsub( [^0-9A-Za-z///&#39; ] , &#39; , Data ,ignore.case = TRUE) Data &lt;- gsub( &#39;&#39; , , Data ,ignore.case = TRUE) "],["pdftables.html", "Chapter 138 pdftables", " Chapter 138 pdftables https://cran.r-project.org/web/packages/pdftables/vignettes/convert_pdf_tables.html "],["tabulizer.html", "Chapter 139 tabulizer", " Chapter 139 tabulizer Extract Tables from PDFs https://github.com/ropensci/tabulizer "],["rio.html", "Chapter 140 rio", " Chapter 140 rio Import, Export, and Convert Data Files https://thomasleeper.com/rio/index.html https://cran.r-project.org/web/packages/rio/vignettes/rio.html "],["read-with-purrr.html", "Chapter 141 read with purrr", " Chapter 141 read with purrr R tip: Iterate with purrr‚Äôs map_df function https://www.infoworld.com/video/89075/r-tip-iterate-with-purrrs-map-df-function "],["the-janitor-package.html", "Chapter 142 The janitor package 142.1 convert excel number into date", " Chapter 142 The janitor package https://garthtarr.github.io/meatR/janitor.html {r eval=FALSE, include=FALSE, echo=TRUE} # install.packages( janitor ) {r eval=FALSE, include=FALSE, echo=TRUE} library(tidyverse) library(janitor) library(xlsx) {r eval=FALSE, include=FALSE, echo=TRUE} # mymsa &lt;- data.table::fread( https://garthtarr.com/data/mymsa.xlsx , fill = TRUE) mymsa &lt;- read_excel( data/mymsa.xlsx ) mymsa$√ßƒü≈ü√º√∂ &lt;- 2 x &lt;- janitor::clean_names(mymsa) {r eval=FALSE, include=FALSE, echo=TRUE} data.frame(mymsa = colnames(mymsa), x = colnames(x)) {r eval=FALSE, include=FALSE, echo=TRUE} tabyl(x, meat_colour) %&gt;% knitr::kable() {r eval=FALSE, include=FALSE, echo=TRUE} table(x$meat_colour) {r eval=FALSE, include=FALSE, echo=TRUE} # Load dplyr for the %&gt;% pipe library(dplyr) x %&gt;% tabyl(meat_colour) %&gt;% knitr::kable() {r eval=FALSE, include=FALSE, echo=TRUE} x %&gt;% tabyl(meat_colour) %&gt;% adorn_pct_formatting(digits = 0, affix_sign = TRUE) %&gt;% knitr::kable() {r eval=FALSE, include=FALSE, echo=TRUE} x %&gt;% tabyl(spare) {r eval=FALSE, include=FALSE, echo=TRUE} x = remove_empty(x, which = c( rows , cols )) {r eval=FALSE, include=FALSE, echo=TRUE} x = read_excel( data/mymsa.xlsx ) %&gt;% clean_names() %&gt;% remove_empty() {r eval=FALSE, include=FALSE, echo=TRUE} x %&gt;% tabyl(meat_colour, plant) %&gt;% knitr::kable() # can also make 3 way tables {r eval=FALSE, include=FALSE, echo=TRUE} # row totals x %&gt;% tabyl(meat_colour, plant) %&gt;% adorn_totals(where = row ) %&gt;% knitr::kable() {r eval=FALSE, include=FALSE, echo=TRUE} # column totals x %&gt;% tabyl(meat_colour, plant) %&gt;% adorn_totals(where = col ) %&gt;% knitr::kable() {r eval=FALSE, include=FALSE, echo=TRUE} # row and column totals x %&gt;% tabyl(meat_colour, plant) %&gt;% adorn_totals(where = c( row , col )) {r eval=FALSE, include=FALSE, echo=TRUE} x %&gt;% tabyl(meat_colour, plant) %&gt;% adorn_totals(where = c( row , col )) %&gt;% adorn_percentages(denominator = col ) %&gt;% adorn_pct_formatting(digits = 0) {r eval=FALSE, include=FALSE, echo=TRUE} x %&gt;% tabyl(meat_colour, plant) %&gt;% adorn_totals(where = c( row , col )) %&gt;% adorn_percentages(denominator = col ) %&gt;% adorn_pct_formatting(digits = 0) %&gt;% adorn_ns(position = front ) {r eval=FALSE, include=FALSE, echo=TRUE} adorn_cumulative &lt;- function(dat, colname, dir = down ){ if(!missing(colname)){ colname &lt;- rlang::enquo(colname) } else if( valid_percent %in% names(dat)) { colname &lt;- rlang::sym( valid_percent ) } else if( percent %in% names(dat)){ colname &lt;- rlang::sym( percent ) } else { stop( \\ colname\\ not specified and default columns valid_percent and percent are not present in data.frame dat ) } target &lt;- dplyr::pull(dat, !! colname) if(dir == up ){ target &lt;- rev(target) } dat$cumulative &lt;- cumsum(ifelse(is.na(target), 0, target)) + target*0 # an na.rm version of cumsum, from https://stackoverflow.com/a/25576972 if(dir == up ){ dat$cumulative &lt;- rev(dat$cumulative) names(dat)[names(dat) %in% cumulative ] &lt;- cumulative_up } dat } {r eval=FALSE, include=FALSE, echo=TRUE} x %&gt;% get_dupes(rfid) {r eval=FALSE, include=FALSE, echo=TRUE} x1 = x %&gt;% slice(1:3) x2 = bind_rows(x1,x) x2 %&gt;% get_dupes(rfid) 142.1 convert excel number into date {r eval=FALSE, include=FALSE, echo=TRUE} janitor::excel_numeric_to_date(41103) output: pdf_document: default html_document: default header-includes: - \\usepackage{pdflscape} - \\usepackage{xcolor} - \\newcommand{\\blandscape}{\\begin{landscape}} - \\newcommand{\\elandscape}{\\end{landscape}} "],["ggplot2-.html", "Chapter 143 ggplot2 -", " Chapter 143 ggplot2 - mpg {r, background=&#39;#fff5e6&#39;} library( tidyverse ) ggplot(mpg) + geom_point(aes(x = displ, y = hwy)) ggplot(mpg, aes(model, manufacturer)) + geom_point() ggplot(mpg, aes(displ, cty, colour = year)) + geom_point() ggplot(mpg, aes(displ, hwy)) + geom_point(aes(shape = year)) ggplot(mpg, aes(displ, hwy)) + geom_point() + geom_smooth(span = 0.2) ggplot(mpg, aes(hwy)) + geom_histogram() + geom_freqpoly() ggplot(mpg, aes(cty, hwy)) + geom_point() + geom_smooth() ggplot(mpg, aes(class, hwy)) + geom_boxplot() ggplot(mpg, aes(reorder(class, hwy), hwy)) + geom_boxplot() "],["gganimate-.html", "Chapter 144 gganimate -", " Chapter 144 gganimate - library(gganimate) p &lt;- ggplot(iris, aes(x = Petal.Width, y = Petal.Length)) + geom_point() plot(p) anim &lt;- p + transition_states(Species, transition_length = 2, state_length = 1) anim p + enter_appear() {r eval=FALSE, include=FALSE, echo=TRUE} sometext &lt;-strsplit( paste0( You can even try to make some crazy things like this paragraph. , It may seem like a useless feature right now but it&#39;s so cool , and nobody can resist. ;) ), )[[1]] text_formatted &lt;-paste( kableExtra::text_spec(sometext, latex , color = kableExtra::spec_color(1:length(sometext), end = 0.9), font_size =kableExtra::spec_font_size(1:length(sometext), begin = 5, end = 20)),collapse = ) mytext &lt;- kableExtra::text_spec( Serdar , color = blue , background = black ) {r # mytext To display the text, type {r # text_formatted outside of the chunk {r eval=FALSE, include=FALSE, echo=TRUE} library(kableExtra) my_text &lt;- paste0( ƒ∞statistik Metod: , S√ºrekli verilerin ortalama, standart sapma, median, minimum ve maksimum deƒüerleri verildi. , R Core Team (2019). R: A language and environment for statistical computing. R Foundation for Statistical Computing, Vienna, Austria. URL https://www.R-project.org/. , Therneau T (2015). A Package for Survival Analysis in S. version 2.38, URL:https://CRAN.R-project.org/package=survival , Terry M. Therneau, Patricia M. Grambsch (2000). Modeling Survival Data: Extending the Cox Model. Springer, New York. ISBN 0-387-98784-3. , Ewen Harrison, Tom Drake and Riinu Ots (2019). finalfit: Quickly Create Elegant Regression Results Tables and Plots when Modelling. R package version 0.9.6. https://github.com/ewenharrison/finalfit , sep = \\n ) my_text &lt;- paste0( You can even try to make some crazy things like this paragraph. , It may seem like a useless feature right now but it&#39;s so cool , and nobody can resist. ;) ) my_text_html &lt;- paste( text_spec( my_text, html , color = red , background = yellow ), collapse = ) sometext &lt;-strsplit(my_text, )[[1]] my_text_latex &lt;- paste( text_spec( sometext, latex , color = red , background = yellow ), collapse = ) "],["ggpubr.html", "Chapter 145 ggpubr", " Chapter 145 ggpubr "],["ggpubr-1.html", "Chapter 146 ggpubr", " Chapter 146 ggpubr https://rpkgs.datanovia.com/ggpubr if(!require(devtools)) install.packages( devtools ) devtools::install_github( kassambara/ggpubr ) Distribution library(ggpubr) set.seed(1234) wdata = data.frame( sex = factor(rep(c( F , M ), each=200)), weight = c(rnorm(200, 55), rnorm(200, 58))) head(wdata, 4) ggdensity(wdata, x = weight , add = mean , rug = TRUE, color = sex , fill = sex , palette = c( #00AFBB , #E7B800 )) gghistogram(wdata, x = weight , add = mean , rug = TRUE, color = sex , fill = sex , palette = c( #00AFBB , #E7B800 )) data( ToothGrowth ) df &lt;- ToothGrowth head(df, 4) p &lt;- ggboxplot(df, x = dose , y = len , color = dose , palette =c( #00AFBB , #E7B800 , #FC4E07 ), add = jitter , shape = dose ) p # Add p-values comparing groups # Specify the comparisons you want my_comparisons &lt;- list( c( 0.5 , 1 ), c( 1 , 2 ), c( 0.5 , 2 ) ) p + stat_compare_means(comparisons = my_comparisons)+ # Add pairwise comparisons p-value stat_compare_means(label.y = 50) # Add global p-value ggviolin(df, x = dose , y = len , fill = dose , palette = c( #00AFBB , #E7B800 , #FC4E07 ), add = boxplot , add.params = list(fill = white ))+ stat_compare_means(comparisons = my_comparisons, label = p.signif )+ # Add significance levels stat_compare_means(label.y = 50) # Add global the p-value data( mtcars ) dfm &lt;- mtcars dfm$cyl &lt;- as.factor(dfm$cyl) dfm$name &lt;- rownames(dfm) head(dfm[, c( name , wt , mpg , cyl )]) ggbarplot(dfm, x = name , y = mpg , fill = cyl , # change fill color by cyl color = white , # Set bar border colors to white palette = jco , # jco journal color palett. see ?ggpar sort.val = desc , # Sort the value in dscending order sort.by.groups = FALSE, # Don&#39;t sort inside each group x.text.angle = 90 # Rotate vertically x axis texts ) ggbarplot(dfm, x = name , y = mpg , fill = cyl , # change fill color by cyl color = white , # Set bar border colors to white palette = jco , # jco journal color palett. see ?ggpar sort.val = asc , # Sort the value in dscending order sort.by.groups = TRUE, # Sort inside each group x.text.angle = 90 # Rotate vertically x axis texts ) dfm$mpg_z &lt;- (dfm$mpg -mean(dfm$mpg))/sd(dfm$mpg) dfm$mpg_grp &lt;- factor(ifelse(dfm$mpg_z &lt; 0, low , high ), levels = c( low , high )) head(dfm[, c( name , wt , mpg , mpg_z , mpg_grp , cyl )]) ggbarplot(dfm, x = name , y = mpg_z , fill = mpg_grp , # change fill color by mpg_level color = white , # Set bar border colors to white palette = jco , # jco journal color palett. see ?ggpar sort.val = asc , # Sort the value in ascending order sort.by.groups = FALSE, # Don&#39;t sort inside each group x.text.angle = 90, # Rotate vertically x axis texts ylab = MPG z-score , xlab = FALSE, legend.title = MPG Group ) ggbarplot(dfm, x = name , y = mpg_z , fill = mpg_grp , # change fill color by mpg_level color = white , # Set bar border colors to white palette = jco , # jco journal color palett. see ?ggpar sort.val = desc , # Sort the value in descending order sort.by.groups = FALSE, # Don&#39;t sort inside each group x.text.angle = 90, # Rotate vertically x axis texts ylab = MPG z-score , legend.title = MPG Group , rotate = TRUE, ggtheme = theme_minimal() ) ggdotchart(dfm, x = name , y = mpg , color = cyl , # Color by groups palette = c( #00AFBB , #E7B800 , #FC4E07 ), # Custom color palette sorting = ascending , # Sort value in descending order add = segments , # Add segments from y = 0 to dots ggtheme = theme_pubr() # ggplot2 theme ) ggdotchart(dfm, x = name , y = mpg , color = cyl , # Color by groups palette = c( #00AFBB , #E7B800 , #FC4E07 ), # Custom color palette sorting = descending , # Sort value in descending order add = segments , # Add segments from y = 0 to dots rotate = TRUE, # Rotate vertically group = cyl , # Order by groups dot.size = 6, # Large dot size label = round(dfm$mpg), # Add mpg values as dot labels font.label = list(color = white , size = 9, vjust = 0.5), # Adjust label parameters ggtheme = theme_pubr() # ggplot2 theme ) ggdotchart(dfm, x = name , y = mpg_z , color = cyl , # Color by groups palette = c( #00AFBB , #E7B800 , #FC4E07 ), # Custom color palette sorting = descending , # Sort value in descending order add = segments , # Add segments from y = 0 to dots add.params = list(color = lightgray , size = 2), # Change segment color and size group = cyl , # Order by groups dot.size = 6, # Large dot size label = round(dfm$mpg_z,1), # Add mpg values as dot labels font.label = list(color = white , size = 9, vjust = 0.5), # Adjust label parameters ggtheme = theme_pubr() # ggplot2 theme )+ geom_hline(yintercept = 0, linetype = 2, color = lightgray ) ggdotchart(dfm, x = name , y = mpg , color = cyl , # Color by groups palette = c( #00AFBB , #E7B800 , #FC4E07 ), # Custom color palette sorting = descending , # Sort value in descending order rotate = TRUE, # Rotate vertically dot.size = 2, # Large dot size y.text.col = TRUE, # Color y text by groups ggtheme = theme_pubr() # ggplot2 theme )+ theme_cleveland() # Add dashed grids "],["r-notebook-1.html", "Chapter 147 R Notebook", " Chapter 147 R Notebook print(paste0( Git Update Started at: , Sys.time())) CommitMessage &lt;- paste( updated on: , Sys.time(), sep = ) wd &lt;- ~/serdarbalci setorigin &lt;- git remote set-url origin git@github.com:sbalci/MyJournalWatch.git \\n gitCommand &lt;- paste( cd , wd, \\n git add . \\n git commit --message &#39; , CommitMessage, &#39; \\n , setorigin, git push origin master \\n , sep = ) system(command = paste(gitCommand, \\n ) , intern = TRUE, wait = TRUE) Sys.sleep(5) print(paste0( Git Update Ended at: , Sys.time())) "],["happy-git-and-github-for-the-user.html", "Chapter 148 Happy Git and GitHub for the useR", " Chapter 148 Happy Git and GitHub for the useR https://happygitwithr.com An introduction to Git and how to use it with RStudio http://r-bio.github.io/intro-git-rstudio/ https://andrewbtran.github.io/NICAR/2018/workflow/docs/03-integrating_github.html https://aberdeenstudygroup.github.io/studyGroup/lessons/SG-T1-GitHubVersionControl/VersionControl/ http://r-bio.github.io/intro-git-rstudio/ https://stackoverflow.com/questions/41688164/using-rstudio-to-make-pull-requests-in-git https://bookdown.org/rdpeng/RProgDA/version-control-and-github.html https://www.r-bloggers.com/rstudio-and-github/ http://happygitwithr.com/fork.html https://kbroman.org/github_tutorial/ https://kbroman.org/simple_site/ Helping you make your first pull request! https://github.com/thisisnic/first-contributions {r eval=FALSE, include=FALSE, echo=TRUE} require(rstudioapi) CommitMessage &lt;- paste( updated on , Sys.time(), sep = ) wd &lt;- getwd() gitCommand &lt;- paste( cd , wd, \\n git add . \\n git commit --message &#39; , CommitMessage, &#39; \\n git push origin master \\n , sep = ) Sys.sleep(time = 1) gitTerm &lt;- rstudioapi::terminalCreate(show = FALSE) Sys.sleep(time = 1) rstudioapi::terminalSend(gitTerm, gitCommand) {r eval=FALSE, include=FALSE, echo=TRUE} CommitMessage &lt;- paste( updated on , Sys.time(), sep = ) wd &lt;- getwd() gitCommand &lt;- paste( cd , wd, \\n git add . \\n git commit --message &#39; , CommitMessage, &#39; \\n git push origin master \\n , sep = ) system(command = gitCommand, intern = TRUE) "],["r-notebook-2.html", "Chapter 149 R Notebook 149.1 scholar.shiny", " Chapter 149 R Notebook 149.0.1 scholar Analyse citation data from Google Scholar: https://github.com/jkeirstead/scholar/ 149.0.2 coauthornetwork Exploring Google Scholar coauthorship: https://cimentadaj.github.io/blog/2018-06-19-exploring-google-scholar-coauthorship/exploring-google-scholar-coauthorship/ {r eval=FALSE, include=FALSE, echo=TRUE} # devtools::install_github( cimentadaj/coauthornetwork ) library(coauthornetwork) {r eval=FALSE, include=FALSE, echo=TRUE} network &lt;- grab_network( citations?user=q40DcqYAAAAJ&amp;hl=en ) network {r eval=FALSE, include=FALSE, echo=TRUE} plot_coauthors(grab_network( citations?user=q40DcqYAAAAJ&amp;hl=en , n_coauthors = 15), size_labels = 2) {r eval=FALSE, include=FALSE, echo=TRUE} plot_coauthors(grab_network( citations?user=RJNKLHgAAAAJ&amp;hl=en , n_coauthors = 15), size_labels = 2) {r eval=FALSE, fig.height=5, fig.width=6, include=FALSE} plot_coauthors(grab_network( citations?user=VYE2H0wAAAAJ&amp;hl=en , n_coauthors = 15), size_labels = 3) {r eval=FALSE, include=FALSE, echo=TRUE} plot_coauthors(grab_network( citations?user=joN_UxsAAAAJ&amp;hl=en , n_coauthors = 15), size_labels = 1) 149.1 scholar.shiny A shiny application that interacts with Google Scholar https://github.com/agbarnett/scholar.shiny "],["graphs.html", "Chapter 150 Graphs", " Chapter 150 Graphs "],["flatly.html", "Chapter 151 flatly", " Chapter 151 flatly Texas Housing Prices: flatly theme https://elastic-lovelace-155848.netlify.com/gallery/themes/flatly.html "],["easyalluvial.html", "Chapter 152 easyalluvial", " Chapter 152 easyalluvial https://github.com/erblast/easyalluvial https://www.datisticsblog.com/2018/10/intro_easyalluvial/#features https://cran.r-project.org/web/packages/easyalluvial/index.html {r eval=FALSE, include=FALSE, echo=TRUE} # install.packages(&#39;easyalluvial&#39;) {r eval=FALSE, include=FALSE, echo=TRUE} suppressPackageStartupMessages(require(tidyverse)) suppressPackageStartupMessages(require(easyalluvial)) {r eval=FALSE, include=FALSE, echo=TRUE} ## mtcars2 is included in the current development version # mtcars2 &lt;- within(mtcars, { # vs &lt;- factor(vs, labels = c( V , S )) # am &lt;- factor(am, labels = c( automatic , manual )) # cyl &lt;- ordered(cyl) # gear &lt;- ordered(gear) # carb &lt;- ordered(carb) # }) # # mtcars2$id = row.names(mtcars) # # mtcars2 = dplyr::as_tibble(mtcars2) knitr::kable(head(mtcars2)) {r eval=FALSE, include=FALSE, echo=TRUE} library(easyalluvial) alluvial_wide(data = mtcars2 , max_variables = 5 , fill_by = &#39;first_variable&#39; ) {r eval=FALSE, include=FALSE, echo=TRUE} knitr::kable( head(quarterly_flights) ) {r eval=FALSE, include=FALSE, echo=TRUE} alluvial_long( quarterly_flights , key = qu , value = mean_arr_delay , id = tailnum , fill = carrier ) "],["rcolorbrewer.html", "Chapter 153 RColorBrewer", " Chapter 153 RColorBrewer How to expand color palette with ggplot and RColorBrewer https://www.r-bloggers.com/how-to-expand-color-palette-with-ggplot-and-rcolorbrewer/ "],["highcharter.html", "Chapter 154 highcharter", " Chapter 154 highcharter http://jkunst.com/highcharter/ https://github.com/jbkunst/highcharter http://www.htmlwidgets.org/index.html https://cran.r-project.org/web/packages/highcharter/index.html https://www..com/community/tutorials/data-visualization-highcharter-r {r eval=FALSE, include=FALSE, echo=TRUE} library(tidyverse) library(highcharter) {r eval=FALSE, include=FALSE, echo=TRUE} data( pokemon ) # glimpse(pokemon) hchart works like ggplot2&#39;s qplot. hc_add_series works like ggplot2&#39;s geom_S. hcaes works like ggplot2&#39;s aes. {r eval=FALSE, include=FALSE, echo=TRUE} pokemon %&gt;% count(type_1) %&gt;% arrange(n) %&gt;% hchart(type = bar , hcaes(x = type_1 , y = n )) {r eval=FALSE, include=FALSE, echo=TRUE} pokemon %&gt;% count(type_1) %&gt;% arrange(n) %&gt;% hchart(type = column , hcaes(x = type_1 , y = n )) {r eval=FALSE, include=FALSE, echo=TRUE} pokemon %&gt;% count(type_1) %&gt;% arrange(n) %&gt;% hchart(type = treemap , hcaes(x = type_1 , value = n , color = n )) {r eval=FALSE, include=FALSE, echo=TRUE} highchart() %&gt;% hc_add_series(pokemon, scatter , hcaes(x = height , y = weight )) {r eval=FALSE, include=FALSE, echo=TRUE} data(diamonds, package = ggplot2 ) set.seed(123) data &lt;- sample_n(diamonds, 300) modlss &lt;- loess(price ~ carat, data = data) fit &lt;- arrange(broom::augment(modlss), carat) highchart() %&gt;% hc_add_series(data, type = scatter , hcaes(x = carat , y = price , size = depth , group = cut )) %&gt;% hc_add_series(fit, type = line , hcaes(x = carat , y = .fitted ), name = Fit , id = fit ) %&gt;% hc_add_series(fit, type = arearange , hcaes(x = carat , low = .fitted - 2*.se.fit , high = .fitted + 2*.se.fit ), linkedTo = fit ) {r eval=FALSE, include=FALSE, echo=TRUE} highchart() %&gt;% hc_chart(type = area ) %&gt;% hc_title(text = Historic and Estimated Worldwide Population Distribution by Region ) %&gt;% hc_subtitle(text = Source: Wikipedia.org ) %&gt;% hc_xAxis(categories = c( 1750 , 1800 , 1850 , 1900 , 1950 , 1999 , 2050 ), tickmarkPlacement = on , title = list(enabled = FALSE)) %&gt;% hc_yAxis(title = list(text = Percent )) %&gt;% hc_tooltip(pointFormat = &lt;span style=\\ color:{series.color}\\ &gt;{series.name}&lt;/span&gt;: &lt;b&gt;{point.percentage:.1f}%&lt;/b&gt; ({point.y:,.0f} millions)&lt;br/&gt; , shared = TRUE) %&gt;% hc_plotOptions(area = list( stacking = percent , lineColor = #ffffff , lineWidth = 1, marker = list( lineWidth = 1, lineColor = #ffffff )) ) %&gt;% hc_add_series(name = Asia , data = c(502, 635, 809, 947, 1402, 3634, 5268)) %&gt;% hc_add_series(name = Africa , data = c(106, 107, 111, 133, 221, 767, 1766)) %&gt;% hc_add_series(name = Europe , data = c(163, 203, 276, 408, 547, 729, 628)) %&gt;% hc_add_series(name = America , data = c(18, 31, 54, 156, 339, 818, 1201)) %&gt;% hc_add_series(name = Oceania , data = c(2, 2, 2, 6, 13, 30, 46)) {r eval=FALSE, include=FALSE, echo=TRUE} x &lt;- quantmod::getSymbols( GOOG , auto.assign = FALSE) hchart(x) {r eval=FALSE, include=FALSE, echo=TRUE} y &lt;- quantmod::getSymbols( AMZN , auto.assign = FALSE) highchart(type = stock ) %&gt;% hc_add_series(x) %&gt;% hc_add_series(y, type = ohlc ) Highmaps - Map Collection https://code.highcharts.com/mapdata/ {r eval=FALSE, include=FALSE, echo=TRUE} hcmap( https://code.highcharts.com/mapdata/countries/in/in-all.js )%&gt;% hc_title(text = India ) {r eval=FALSE, include=FALSE, echo=TRUE} hcmap( https://code.highcharts.com/mapdata/countries/tr/tr-all.js )%&gt;% hc_title(text = Turkey ) download_map_data: Download the geojson data from the highcharts collection. get_data_from_map: Get the properties for each region in the map, as the keys from the map data. {r eval=FALSE, include=FALSE, echo=TRUE} mapdata &lt;- get_data_from_map(download_map_data( https://code.highcharts.com/mapdata/countries/in/in-all.js )) # glimpse(mapdata) {r eval=FALSE, include=FALSE, echo=TRUE} #population state wise pop &lt;- as.data.frame(c(84673556, 1382611, 31169272, 103804637, 1055450, 25540196, 342853, 242911, 18980000, 1457723, 60383628, 25353081, 6864602, 12548926, 32966238, 61130704, 33387677, 64429, 72597565, 112372972, 2721756, 2964007, 1091014, 1980602, 41947358, 1244464, 27704236, 68621012, 607688, 72138958, 3671032, 207281477, 10116752,91347736)) state &lt;- mapdata%&gt;% select(`hc-a2`)%&gt;% arrange(`hc-a2`) State_pop &lt;- as.data.frame(c(state, pop)) names(State_pop)= c( State , Population ) hcmap( https://code.highcharts.com/mapdata/countries/in/in-all.js , data = State_pop, value = Population , joinBy = c( hc-a2 , State ), name = Fake data , dataLabels = list(enabled = TRUE, format = &#39;{point.name}&#39;), borderColor = #FAFAFA , borderWidth = 0.1, tooltip = list(valueDecimals = 0)) {r eval=FALSE, include=FALSE, echo=TRUE} data(mpg, package = ggplot2 ) mpgg &lt;- mpg %&gt;% filter(class %in% c( suv , compact , midsize )) %&gt;% group_by(class, manufacturer) %&gt;% summarize(count = n()) categories_grouped &lt;- mpgg %&gt;% group_by(name = class) %&gt;% do(categories = .$manufacturer) %&gt;% list_parse() highchart() %&gt;% hc_xAxis(categories = categories_grouped) %&gt;% hc_add_series(data = mpgg, type = bar , hcaes(y = count , color = manufacturer ), showInLegend = FALSE) {r eval=FALSE, include=FALSE, echo=TRUE} df &lt;- data_frame( name = c( Animals , Fruits , Cars ), y = c(5, 2, 4), drilldown = tolower(name) ) ds &lt;- list_parse(df) names(ds) &lt;- NULL hc &lt;- highchart() %&gt;% hc_chart(type = column ) %&gt;% hc_title(text = Basic drilldown ) %&gt;% hc_xAxis(type = category ) %&gt;% hc_legend(enabled = FALSE) %&gt;% hc_plotOptions( series = list( boderWidth = 0, dataLabels = list(enabled = TRUE) ) ) %&gt;% hc_add_series( name = Things , colorByPoint = TRUE, data = ds ) dfan &lt;- data_frame( name = c( Cats , Dogs , Cows , Sheep , Pigs ), value = c(4, 3, 1, 2, 1) ) dffru &lt;- data_frame( name = c( Apple , Organes ), value = c(4, 2) ) dfcar &lt;- data_frame( name = c( Toyota , Opel , Volkswage ), value = c(4, 2, 2) ) second_el_to_numeric &lt;- function(ls){ map(ls, function(x){ x[[2]] &lt;- as.numeric(x[[2]]) x }) } dsan &lt;- second_el_to_numeric(list_parse2(dfan)) dsfru &lt;- second_el_to_numeric(list_parse2(dffru)) dscar &lt;- second_el_to_numeric(list_parse2(dfcar)) hc %&gt;% hc_drilldown( allowPointDrilldown = TRUE, series = list( list( id = animals , data = dsan ), list( id = fruits , data = dsfru ), list( id = cars , data = dscar ) ) ) {r eval=FALSE, include=FALSE, echo=TRUE} tm &lt;- pokemon %&gt;% mutate(type_2 = ifelse(is.na(type_2), paste( only , type_1), type_2), type_1 = type_1) %&gt;% group_by(type_1, type_2) %&gt;% summarise(n = n()) %&gt;% ungroup() %&gt;% treemap::treemap(index = c( type_1 , type_2 ), vSize = n , vColor = type_1 ) tm$tm &lt;- tm$tm %&gt;% tbl_df() %&gt;% left_join(pokemon %&gt;% select(type_1, type_2, color_f) %&gt;% distinct(), by = c( type_1 , type_2 )) %&gt;% left_join(pokemon %&gt;% select(type_1, color_1) %&gt;% distinct(), by = c( type_1 )) %&gt;% mutate(type_1 = paste0( Main , type_1), color = ifelse(is.na(color_f), color_1, color_f)) highchart() %&gt;% hc_add_series_treemap(tm, allowDrillToNode = TRUE, layoutAlgorithm = squarified ) {r eval=FALSE, include=FALSE, echo=TRUE} pokemon%&gt;% count(type_1)%&gt;% arrange(n)%&gt;% hchart(type = bar , hcaes(x = type_1 , y = n , color = type_1 ))%&gt;% hc_exporting(enabled = TRUE) {r eval=FALSE, include=FALSE, echo=TRUE} pokemon%&gt;% count(type_1)%&gt;% arrange(n)%&gt;% hchart(type = bar , hcaes(x = type_1 , y = n , color = type_1 ))%&gt;% hc_exporting(enabled = TRUE)%&gt;% hc_add_theme(hc_theme_chalk()) {r eval=FALSE, include=FALSE, echo=TRUE} data( weather ) x &lt;- c( Min , Mean , Max ) y &lt;- sprintf( {point.%s} , c( min_temperaturec , mean_temperaturec , max_temperaturec )) tltip &lt;- tooltip_table(x, y) hchart(weather, type = columnrange , hcaes(x = date , low = min_temperaturec , high = max_temperaturec , color = mean_temperaturec )) %&gt;% hc_chart(polar = TRUE) %&gt;% hc_yAxis( max = 30, min = -10, labels = list(format = {value} C ), showFirstLabel = FALSE) %&gt;% hc_xAxis( title = list(text = ), gridLineWidth = 0.5, labels = list(format = {value: %b} )) %&gt;% hc_tooltip(useHTML = TRUE, pointFormat = tltip, headerFormat = as.character(tags$small( {point.x:%d %B, %Y} ))) "],["taucharts.html", "Chapter 155 taucharts", " Chapter 155 taucharts https://www.infoworld.com/video/87337/r-tip-how-to-create-easy-interactive-scatter-plots-with-taucharts {r eval=FALSE, include=FALSE, echo=TRUE} devtools::install_github( hrbrmstr/taucharts ) # githubinstall::githubinstall( taucharts ) {r eval=FALSE, include=FALSE, echo=TRUE} library(tidyverse) library(taucharts) data( mtcars ) {r eval=FALSE, include=FALSE, echo=TRUE} mtcars2 &lt;- mtcars %&gt;% select(wt, mpg) %&gt;% mutate(model = row.names(mtcars)) {r eval=FALSE, include=FALSE, echo=TRUE} taucharts::tauchart(mtcars2) %&gt;% tau_point(x = wt , y = mpg ) %&gt;% tau_tooltip() %&gt;% tau_trendline() "],["gganimate.html", "Chapter 156 gganimate", " Chapter 156 gganimate https://www.infoworld.com/video/89987/r-tip-animations-in-r "],["ggplot2.html", "Chapter 157 ggplot2", " Chapter 157 ggplot2 http://r-statistics.co/ggplot2-Tutorial-With-R.html {r eval=FALSE, include=FALSE, echo=TRUE} library(ggplot2) diamonds ggplot(diamonds) # if only the dataset is known. {r eval=FALSE, include=FALSE, echo=TRUE} ggplot(diamonds, aes(x=carat)) # if only X-axis is known. The Y-axis can be specified in respective geoms. {r eval=FALSE, include=FALSE, echo=TRUE} ggplot(diamonds, aes(x=carat, y=price)) # if both X and Y axes are fixed for all layers. {r eval=FALSE, include=FALSE, echo=TRUE} ggplot(diamonds, aes(x=carat, color=cut)) # Each category of the &#39;cut&#39; variable will now have a distinct color, once a geom is added. {r eval=FALSE, include=FALSE, echo=TRUE} ggplot(diamonds, aes(x=carat), color= steelblue ) https://ggplot2.tidyverse.org/reference/ {r eval=FALSE, include=FALSE, echo=TRUE} ggplot(diamonds, aes(x=carat, y=price, color=cut)) + geom_point() + geom_smooth() # Adding scatterplot geom (layer1) and smoothing geom (layer2). {r eval=FALSE, include=FALSE, echo=TRUE} ggplot(diamonds) + geom_point(aes(x=carat, y=price, color=cut)) + geom_smooth(aes(x=carat, y=price, color=cut)) # Same as above but specifying the aesthetics inside the geoms. {r eval=FALSE, include=FALSE, echo=TRUE} library(ggplot2) ggplot(diamonds) + geom_point(aes(x=carat, y=price, color=cut)) + geom_smooth(aes(x=carat, y=price)) # Remove color from geom_smooth ggplot(diamonds, aes(x=carat, y=price)) + geom_point(aes(color=cut)) + geom_smooth() # same but simpler continue from here http://r-statistics.co/ggplot2-Tutorial-With-R.html "],["gganimate-1.html", "Chapter 158 gganimate", " Chapter 158 gganimate https://cran.r-project.org/web/packages/gganimate/vignettes/gganimate.html {r eval=FALSE, include=FALSE, echo=TRUE} library(gganimate) #&gt; Loading required package: ggplot2 # We&#39;ll start with a static plot p &lt;- ggplot(iris, aes(x = Petal.Width, y = Petal.Length)) + geom_point() plot(p) {r eval=FALSE, include=FALSE, echo=TRUE} anim &lt;- p + transition_states(Species, transition_length = 2, state_length = 1) anim {r eval=FALSE, include=FALSE, echo=TRUE} anim + ease_aes(&#39;cubic-in-out&#39;) # Slow start and end for a smoother look {r eval=FALSE, include=FALSE, echo=TRUE} anim + ease_aes(&#39;cubic-in-out&#39;, y = &#39;bounce-out&#39;) # Sets special ease for y aesthetic {r eval=FALSE, include=FALSE, echo=TRUE} anim + ggtitle(&#39;Now showing {closest_state}&#39;, subtitle = &#39;Frame {frame} of {nframes}&#39;) {r eval=FALSE, include=FALSE, echo=TRUE} ggplot(iris, aes(x = Petal.Width, y = Petal.Length)) + geom_line(aes(group = rep(1:50, 3)), colour = &#39;grey&#39;) + geom_point() {r eval=FALSE, include=FALSE, echo=TRUE} ggplot(iris, aes(x = Petal.Width, y = Petal.Length)) + geom_point(aes(colour = Species)) + transition_states(Species, transition_length = 2, state_length = 1) "],["ggforce.html", "Chapter 159 ggforce", " Chapter 159 ggforce {r eval=FALSE, include=FALSE, echo=TRUE} install.packages( ggforce ) library(ggforce) {r eval=FALSE, include=FALSE, echo=TRUE} Titanic {r eval=FALSE, include=FALSE, echo=TRUE} titanic &lt;- reshape2::melt(Titanic) head(titanic) {r eval=FALSE, include=FALSE, echo=TRUE} titanic &lt;- gather_set_data(titanic, 1:4) head(titanic) # View(titanic) {r eval=FALSE, include=FALSE, echo=TRUE} ggplot(titanic, aes(x, id = id, split = y, value = value)) + geom_parallel_sets(aes(fill = Sex), alpha = 0.3, axis.width = 0.1) + geom_parallel_sets_axes(axis.width = 0.1) + geom_parallel_sets_labels(colour = &#39;white&#39;) "],["g2r.html", "Chapter 160 g2r", " Chapter 160 g2r remotes::install_github( JohnCoene/g2r ) {r eval=FALSE, include=FALSE, echo=TRUE} library(g2r) g2(iris, asp(Petal.Length, Petal.Width, color = Species)) %&gt;% fig_point() %&gt;% plane_wrap(planes(Species)) "],["h2o.html", "Chapter 161 h2o", " Chapter 161 h2o http://h2o-release.s3.amazonaws.com/h2o/rel-wright/10/docs-website/h2o-r/docs/articles/getting_started.html {r eval=FALSE, include=FALSE, echo=TRUE} if ( package:h2o %in% search()) { detach( package:h2o , unload=TRUE) } if ( h2o %in% rownames(installed.packages())) { remove.packages( h2o ) } # Next, download packages that H2O depends on. pkgs &lt;- c( RCurl , jsonlite ) for (pkg in pkgs) { if (! (pkg %in% rownames(installed.packages()))) { install.packages(pkg) } } # Download and install the latest H2O package for R. install.packages( h2o , type= source , repos=(c( http://h2o-release.s3.amazonaws.com/h2o/latest_stable_R ))) # Initialize H2O and run a demo to see H2O at work. library(h2o) h2o.init() demo(h2o.kmeans) "],["hierarchical-clustering-1.html", "Chapter 162 Hierarchical Clustering", " Chapter 162 Hierarchical Clustering https://datascienceplus.com/hierarchical-clustering-in-r/ "],["how-to-prepare-data-for-histopathology-research.html", "Chapter 163 How to Prepare Data for Histopathology Research?", " Chapter 163 How to Prepare Data for Histopathology Research? author: &#39;[Serdar Balcƒ±, MD, Pathologist](https://sbalci.github.io/)&#39; date: `{r # format(Sys.Date())` output: revealjs::revealjs_presentation: incremental: yes theme: sky highlight: pygments center: no smart: yes transition: fade self_contained: yes ig_width: 7 fig_height: 6 fig_caption: yes reveal_options: slideNumber: yes previewLinks: yes prettydoc::html_pretty: theme: leonids highlight: github rmdshower::shower_presentation: null beamer_presentation: incremental: yes highlight: tango html_notebook: fig_caption: yes highlight: kate number_sections: yes theme: flatly toc: yes toc_depth: 5 toc_float: yes slidy_presentation: null pdf_document: toc: yes toc_depth: &#39;5&#39; html_document: fig_caption: yes keep_md: yes toc: yes toc_depth: 5 toc_float: yes xaringan::moon_reader: lib_dir: libs nature: beforeInit: - macros.js - https://platform.twitter.com/widgets.js highlightStyle: github highlightLines: yes countIncrementalSlides: no self_contained: yes ioslides_presentation: incremental: yes highlight: github institute: &#39;[serdarbalci.com](https://www.serdarbalci.com)&#39; editor_options: chunk_output_type: inline {r eval=FALSE, include=FALSE, echo=TRUE} knitr::opts_chunk$set(fig.width = 12, fig.height = 8, fig.path = &#39;Figs/&#39;, echo = TRUE, warning = FALSE, message = FALSE, error = FALSE, eval = TRUE, tidy = TRUE, comment = NA, cache = TRUE) {r strings , include=FALSE} PubMedString &lt;- PubMed: https://www.ncbi.nlm.nih.gov/pubmed/?term= doiString &lt;- doi: https://doi.org/ dimensionString1 &lt;- &lt;script async=&#39;&#39; charset=&#39;utf-8&#39; src=&#39;https://badge.dimensions.ai/badge.js&#39;&gt;&lt;/script&gt; &lt;span class=&#39;__dimensions_badge_embed__&#39; data-doi=&#39; dimensionString2 &lt;- &#39; data-style=&#39;small_circle&#39; data-hide-zero-citations=&#39;true&#39; data-legend=&#39;always&#39;&gt;&lt;/span&gt; altmetricString1 &lt;- &lt;script type=&#39;text/javascript&#39; src=&#39;https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js&#39;&gt;&lt;/script&gt; &lt;span class=&#39;altmetric-embed&#39; data-link-target=&#39;_blank&#39; data-badge-details=&#39;right&#39; data-badge-type=&#39;donut&#39; data-doi=&#39; altmetricString2 &lt;- &#39; data-hide-no-mentions=&#39;true&#39;&gt;&lt;/span&gt; addthis_String1 &lt;- &lt;div class=&#39;addthis_inline_share_toolbox&#39; data-url=&#39;pbpath.org/current-journal-watch/&#39; data-title=&#39;See this abstract on #PBPath #JournalWatch : addthis_String2 &lt;- &#39;&gt;&lt;/div&gt; {r run xaringan, eval=FALSE, message=FALSE, warning=FALSE, include=FALSE} # xaringan::inf_mr() # servr::daemon_stop(1) "],["how-to-prepare-data-for-histopathology-research-1.html", "Chapter 164 How to Prepare Data for Histopathology Research?", " Chapter 164 How to Prepare Data for Histopathology Research? Outline Why is Data Preparation Important? Do I need a specific Software? What are the Golden Rules? What do I do with Data after analysis? I got all the tables from the biostatistician, is it enough? What is a Good (Clean/Ideal/Tidy) Data? What is a Bad (Dirty/Common/Untidy) Data? Do I need to know statistics before collecting Data? Do I need to have a hypothesis before collecting Data? Do I need a research question before collecting Data? "],["how-to-prepare-data-for-histopathology-research-2.html", "Chapter 165 How to Prepare Data for Histopathology Research?", " Chapter 165 How to Prepare Data for Histopathology Research? We Should Collect the Data Related to What We will Report Recommendations for reporting histopathology studies: a proposal {r 25846513, include=FALSE} PMID_25846513 &lt;- RefManageR::ReadPubMed(&#39;25846513&#39;, database = &#39;PubMed&#39;) citation_25846513 &lt;- paste0(PMID_25846513$journal,&#39; &#39;, PMID_25846513$year, &#39; &#39;, PMID_25846513$month,&#39;;&#39;, PMID_25846513$volume,&#39;(&#39;, PMID_25846513$number,&#39;):&#39;, PMID_25846513$pages) PubMed_25846513 &lt;- paste0(PubMedString, PMID_25846513$eprint) doi_25846513 &lt;- paste0(doiString, PMID_25846513$doi) dimensionBadge_25846513 &lt;- paste0(dimensionString1, PMID_25846513$doi,dimensionString2) altmetricBadge_25846513 &lt;- paste0(altmetricString1, PMID_25846513$doi, altmetricString2 ) addthis_inline_25846513 &lt;- paste0(addthis_String1, PMID_25846513$title , PMID: 25846513 , addthis_String2) {r # PMID_25846513$title {r # citation_25846513 {r # PubMed_25846513 {r # addthis_inline_25846513 {r # PMID_25846513$abstract {r # doi_25846513 {r # dimensionBadge_25846513 {r # altmetricBadge_25846513 "],["tables-and-graphs-to-be-formed.html", "Chapter 166 Tables and Graphs to be Formed", " Chapter 166 Tables and Graphs to be Formed Table One: Clinical Features Related to this disease and Histopathological Features (like a CAP synoptic) Cross Tables IHC Tables Survival Tables and Graphs "],["age.html", "Chapter 167 Age", " Chapter 167 Age "],["gender.html", "Chapter 168 Gender", " Chapter 168 Gender Male Female Non-binary (based on research) For missing values: {gender} üì¶ https://lincolnmullen.com/software/gender/ https://github.com/ropensci/gender "],["surgery-type.html", "Chapter 169 Surgery Type", " Chapter 169 Surgery Type "],["histopatoloji-√ßalƒ±≈ümalarƒ±nda-istatistik-analizi-i√ßin-nasƒ±l-veri-hazƒ±rlanƒ±r.html", "Chapter 170 Histopatoloji √ßalƒ±≈ümalarƒ±nda istatistik analizi i√ßin nasƒ±l veri hazƒ±rlanƒ±r?", " Chapter 170 Histopatoloji √ßalƒ±≈ümalarƒ±nda istatistik analizi i√ßin nasƒ±l veri hazƒ±rlanƒ±r? ƒ∞statistik analizlerinde en √ßok vakit alan kƒ±sƒ±m verilerin d√ºzenlenmesi ve analize hazƒ±r hale getirilmesidir. Bu durum o kadar belirgindir ki veri analizi ile ilgili eƒüitimlerin de √∂zel bir kƒ±smƒ±nƒ± veri temizleme dersleri olu≈üturmaktadƒ±r \\(Coursera\\). Analize hazƒ±r haldeki veri temiz veri olarak adlandƒ±rƒ±lƒ±r \\(Tidy Data, H.Wickham\\). ƒ∞statistik√ßilerin kƒ±sƒ±tlƒ± vakti olduƒüunu d√º≈ü√ºn√ºld√ºƒü√ºnde verinin temiz olarak teslim edilmesi onlarƒ±n veriyi rahat anlamalarƒ±na ve veri temizleme i√ßin ayƒ±racaklarƒ± vakit yerine sizin ara≈ütƒ±rmanƒ±zdaki ilgin√ß noktalara odaklanmalarƒ±na yardƒ±mcƒ± olacaktƒ±r. Ayrƒ±ca temiz veri ile √ßalƒ±≈ümanƒ±n istatistik√ßileri daha mutlu ettiƒüini ve √∂zen g√∂sterilmi≈ü bir veride onlarƒ±n da daha √∂zenli √ßalƒ±≈ütƒ±ƒüƒ±nƒ± g√∂zlemlediƒüimi belirtmek isterim. Bu yazƒ±da hipotetik bir histopatoloji √ßalƒ±≈ümasƒ± i√ßin basamak basamak veri hazƒ±rlanma s√ºreci anlatƒ±lacaktƒ±r. Histopatolojik makalelerde bulunmasƒ± gereken minimum bilgiler Bu yazƒ±da kendi kar≈üƒ±la≈ütƒ±ƒüƒ±m problemleri ve literat√ºrdeki √∂nerileri \\(Virchows Arch \\(2015\\) 466:611‚Äì615) derlemeye √ßalƒ±≈ütƒ±m. Statistical Problems to Document and to Avoid Manuscript Checklist for Authors http://biostat.mc.vanderbilt.edu/wiki/Main/ManuscriptChecklist Aslƒ±nda bir eski t√ºm√∂re yeni boya olarak adlandƒ±rƒ±lan ve sƒ±k yapƒ±lan bir √ßalƒ±≈üma t√ºr√ºn√º inceleyeceƒüiz. Bunun i√ßin yapƒ±lacak ilk i≈ü √ßalƒ±≈üƒ±lacak t√ºm√∂rle ilgili CAP protokol√ºn√º dikkatlice okumaktƒ±r. CAP protokollerinin √∂zellikle not ve a√ßƒ±klama kƒ±sƒ±mlarƒ±ndaki detaylar √ßok faydalƒ± olacaktƒ±r. Bundan sonra bir bo≈ü kaƒüƒ±t alƒ±p CAP protokol√ºnde raporda belirtilmesi gereken konular maddeler halinde sƒ±ralanmalƒ±dƒ±r. Bu maddeler √ßalƒ±≈ümanƒ±n tasarlamasƒ±ndan, analizine, yorumuna ve tartƒ±≈ümasƒ±na √ßok yardƒ±mcƒ± olacaktƒ±r. 170 Temiz veri i√ßin dikkat edilmesi gereken kurallar: Her satƒ±r tek hasta Her s√ºtun tek bilgi Her bilgi tek bir ≈üekilde ifade edilecek 170 Verinin girileceƒüi bilgisayar programƒ± Aynƒ± deƒüerin farklƒ± ≈üekilde yazƒ±lmasƒ± Veri hazƒ±rlamak i√ßin excel ya da filemaker kullanƒ±lmasƒ±nƒ± √∂neririm. 170 Vaka numarasƒ± √áalƒ±≈ümaya ka√ß vaka alƒ±nacak? Her deƒüi≈üken i√ßin 10 vaka? Vakalarƒ±n se√ßilme ≈üekli: Geli≈üig√ºzel? Randomize? Birbirini takip eden \\(consequative\\) 170 Yƒ±l Hangi yƒ±l aralƒ±ƒüƒ± tercih edilmeli? Yƒ±l aralƒ±ƒüƒ±nƒ±nƒ±n belirtilmesi nadir vakalarda vaka sayƒ±sƒ± ile ilgili bilgi verebilir. Bu nedenle klinikteki toplam vaka sayƒ±sƒ± ile kar≈üƒ±la≈ütƒ±rma yapƒ±lmasƒ± Bir klinikten √ßƒ±kan vaka sayƒ±sƒ± da o klinikte bu i≈üin ne kadar ciddi yapƒ±ldƒ±ƒüƒ±nƒ±n ve tecr√ºbenin g√∂stergesi. Kabul ≈üansƒ±nƒ± arttƒ±ran fakt√∂r. ƒ∞mm√ºnohistokimya i√ßin eski vakalar mƒ± tercih edilecek yeni vakalar mƒ±? 170 Biyopsi No 170 TC Kimlik, Hasta No, Ad Soyad hasta bazlƒ± √ßalƒ±≈üma vs √∂rnek bazlƒ± √ßalƒ±≈üma HIPAA kurallarƒ± 170 Ya≈ü Yƒ±l, ay Eƒüer t√ºm√∂r belli bir ya≈ü aralƒ±ƒüƒ±nda g√∂r√ºl√ºyor, ya da bimodal daƒüƒ±lƒ±m g√∂steriyorsa \\(osteosarkom gibi\\) bu durumu 170 Doƒüum Tarihi 170 Cinsiyet 170 T√ºm√∂r √ßapƒ± 170 T evresi 170 N evresi Lenf nodu Direk invazyon 170 M evresi 170 TNM/AJCC evresi 170 Histopatolojik tip 170 Lenfovask√ºler ƒ∞nvazyon \\(LVI\\) Lenfovask√ºler invazyon √ßoƒüu t√ºm√∂r raporlarƒ±nda belirtilmesi gereken bir √∂zelliktir. √ñnerilen kodlama ≈üekli var ise 1, yok ise 0 ≈üeklindedir. Lenfatik ve vask√ºler invazyon ayrƒ± ayrƒ± da kodlanabilir. Mesela kolon t√ºm√∂rlerinde ekstramural ven√∂z invazyonun belirtilmesi gibi. CAP protokollerinde equivocal olarak belirtilen ≈ü√ºpheli durumlardan m√ºmk√ºn olduk√ßa ka√ßƒ±nmak analizlerin daha rahat yapƒ±labilmesi i√ßin gereklidir. Extensive retraction artefact gibi √∂zellikli durumlar √ßalƒ±≈ümƒ±yorsa imm√ºnohistokimyasal √ßalƒ±≈ümalara gerek olmadan rutin H&amp;E deƒüerlendirme yeterlidir. Raporlardan elde edilen bulgular da analiz i√ßin kullanƒ±labilir. √ñzellikle rutin rapora g√∂re tedavi planlanan durumlarda, doƒüal seyri seyretmek istediƒüiniz √ßalƒ±≈ümalarda bunu yapabilirsiniz. Patologlarƒ±n ise yaptƒ±klarƒ± √ßalƒ±≈ümalarda mutlaka t√ºm vakalara yeniden bakmalarƒ± √∂nerilir. Bazen ara≈ütƒ±rmacƒ±lar sadece negatif olarak raporlanan vakalara bakƒ±p, bunlarda atlanan lenfovask√ºler invazyonu yakalamaya √ßalƒ±≈üƒ±rlar. Bu durumda lenfovask√ºler invazyon y√ºzdeniz literat√ºrden y√ºksek √ßƒ±kacaktƒ±r \\(Yanlƒ±≈ü negatifler azalacaktƒ±r\\). Pozitif olan olgulara da bakƒ±lmalƒ±, pozitif olarak raporlanan ve aslƒ±nda lenfovask√ºler invazyonu olmayan vakalar \\(yanlƒ±≈ü pozitif\\) ise negatif olarak analize alƒ±nmalƒ±dƒ±r. Lenf nodunda metastaz olan olgularda lenfovask√ºler invazyonu pozitif olarak kabul etmek uygun deƒüildir. Lenf noduna metastaz yapmanƒ±n ayrƒ± perit√ºm√∂ral lenfovask√ºler invazyon tespit edilmesinin ayrƒ± t√ºm√∂r geli≈üim basamaklarƒ± olduƒüu d√º≈ü√ºn√ºlmelidir. 170 Perin√∂ral \\(perin√∂ryal\\) invazyon 170 Cerrahi sƒ±nƒ±r 170 Ek hastalƒ±k 170 ƒ∞kinci primer Birden fazla t√ºm√∂r√º olan olgularda klinik gidi≈üi ve saƒükalƒ±mƒ± diƒüer t√ºm√∂r etkiliyor olabilir. Sistoprostatektomilerde √ºrotelyal karsinom saƒükalƒ±ma, prostat t√ºm√∂rlerinden daha fazla etki edecektir. Bu vakalarƒ±n √ßƒ±kartƒ±lmasƒ± da insidansƒ± etkileyebilir. Bu nedenle √ßalƒ±≈ümanƒ±n tasarƒ±mƒ±na g√∂re bu vakalarƒ± eklemek ya da √ßƒ±akrtmak gerekecektir. 170 ƒ∞mm√ºnohistokimya Pozitif, negatif Kayƒ±p, korunmu≈ü ≈ûiddet Yaygƒ±nlƒ±k H-skor, Allred score, Quick score Hangi h√ºcre pozitif Hangi komponent pozitif \\(n√ºkleer, sitoplazmik, membran√∂z\\) Y√ºzde Sonu√ßlarƒ± gruplama Uygun antikor klonunu se√ßmek ayrƒ± bir yazƒ± konusu olmalƒ±. 170 Ameliyat ≈üekli 170 Saƒükalƒ±m Saƒükalƒ±m verisi de hassas bilgilerdendir. √ñl√ºm bildirim sistemi Tarihler Tanƒ± tarihi Son tarih Tarih girerken neye dikkat edelim \\(ƒ∞ngilizce ve T√ºrk√ße farklƒ± tarih formatlarƒ±\\) 170 Overall survival 170 Disease Free Survival 170 Vertikal tarama 170 Bilinmeyen veriler, Eksik veriler, Missing values Her eksik h√ºcre, √ßok deƒüi≈ükenli analizden o vakanƒ±n d√º≈ümesine neden olacaktƒ±r. Eksik camlar Eksik verileri excelde kontrol etme 170 Tek merkez, √ßok merkezli √ßalƒ±≈üma 170 ƒ∞statistik√ßiye sorulmasƒ± gereken sorular √áalƒ±≈ümaya ba≈ülamadan √∂nce, hangi sorularƒ± soracaƒüƒ±nƒ±zƒ± zaten planlamƒ±≈ü olmanƒ±z ve buna g√∂re verilerinizi d√ºzenlemi≈ü olmanƒ±z gerekir. Yine de √ßalƒ±≈üma s√ºrerken ve √ßalƒ±≈ümanƒ±n sonunda yeni sorular ve d√º≈ü√ºnceler ortaya √ßƒ±kabilir. Sorulacak sorular ve yapƒ±lacak analizler i√ßin bir √∂n hazƒ±rlƒ±k yapmak ve bunlarƒ± d√ºzg√ºn c√ºmleler halinde kaydetmek √∂nemlidir. Mesela t√ºm√∂r tipleri ile X protein ekspresyonunu kar≈üƒ±la≈ütƒ±rmak istiyorum bir soru olabilir. Ama daha iyisi X proteininin ekspresyonunun A t√ºm√∂r√ºnde B t√ºm√∂r√ºne g√∂re daha fazla olduƒüunu d√º≈ü√ºn√ºyorum, bunun √∂yle olup olmadƒ±ƒüƒ±nƒ± analiz etmenizi istiyorum daha da anla≈üƒ±lƒ±r bir soru olacaktƒ±r. 170 Bana p deƒüeri ver 170 Hangi istatistik y√∂ntemlerini bilmem lazƒ±m Tƒ±p fak√ºltesinin ilk yƒ±llarƒ±nda √∂ƒürenilen istatistikle ilgili kavramlar yƒ±llar i√ßinde unutuluyor. Elbette herkesin detaylƒ± olarak istatistik metodlarƒ±nƒ± bilmesine gerek yok. Ancak yine de bir istatistik okuryazarlƒ±ƒüƒ±nƒ±n \\(statistical literacy\\) olmasƒ±nda fayda var. ANOVA testi 30 vaka sayƒ±lƒ±, tercihen verilerin normal daƒüƒ±ldƒ±ƒüƒ± durumlarda ve veriler √∂l√ß√ºlebilir ve s√ºrekli nitelikte ise kullanƒ±lƒ±r. Mesela ya≈ü, √∂zefagus l√ºmeninin, √∂zefagus duvarƒ±na oranƒ± gibi durumlarda kullanƒ±labilir. Ancak histopatolojik dercelendirme ya da evreleme gibi kesikli deƒüi≈ükenlerin olduƒüu durumlarda parametrik test olan ANOVA √∂nerilmez. Grade 1 ila grade 2 arasƒ±ndaki fark ile grade 2 ila grade 3 arasƒ±ndaki fark matematiksel olarak e≈üit deƒüildir. Grade 2, grade 1 den 2 kat k√∂t√º, grade 3 ise grade 1‚Äôden 3 kat k√∂t√ºd√ºr gibi bir yorum yapƒ±lmaz. Hastalarƒ±n kanser evresinin ortalama 2,5 , ya da t√ºm√∂r grade‚Äôinin ortalama 1,2 olarak verilmesi √∂nerilmez. Bunun yerine ortanca ve √ßeyrekler arasƒ± fark \\(median, interquartile range\\) kullanƒ±lmasƒ± daha uygun olur. Bu nedenle yapƒ±lacak test de ANOVA‚Äônƒ±n nonparametrik kar≈üƒ±lƒ±ƒüƒ± olan Kruskal Wallis testidir. √ñl√ß√ºm ≈üeklinde olan, s√ºrekli deƒüi≈ükenlerde bile vaka sayƒ±sƒ±nƒ±n 30‚Äôdan az ise ya da veriler normal daƒüƒ±lmƒ±yorsa birden fazla grubun kar≈üƒ±la≈ütƒ±rmasƒ±nda da Kruskal Wallis testi kullanƒ±lƒ±r. ƒ∞statistik dƒ±≈üƒ± bakƒ±≈ü a√ßƒ±sƒ± ile; Kanser evreleme √ßalƒ±≈ümalarƒ±nda \\(lenf nodu sayƒ±sƒ±nda\\) logaritmik d√∂n√º≈ü√ºm √ßok kullanƒ±lƒ±yor. Ve hemen t√ºm √ßalƒ±≈ümalarda i≈üe yarƒ±yor. √ñrnek https://www.ncbi.nlm.nih.gov/pubmed/28094085 Ama klinikte bilgisayar destekli bir karar sistemi kullanƒ±lmadƒ±ƒüƒ± zaman bu logaritmik deƒüerler √ßok afaki kalabiliyor. Model anlamlƒ± olsa da pratikte anlamasƒ± zor oluyor. Normallik yoksa nonparametrik testleri bir kademe daha rahat anlayabiliyorum. "],["top-ten-errors-of-statistical-analysis-in-observational-studies-for-cancer-research.html", "Chapter 171 Top ten errors of statistical analysis in observational studies for cancer research", " Chapter 171 Top ten errors of statistical analysis in observational studies for cancer research https://rd.springer.com/article/10.1007%2Fs12094-017-1817-9 "],["tweets.html", "Chapter 172 Tweets", " Chapter 172 Tweets {r eval=FALSE, include=FALSE, echo=TRUE} library(rtweet) how_to_prepare_data_tweet_1 &lt;- paste0( 1/n ) post_tweet(how_to_prepare_data_tweet_1) title: R ile analize ba≈ülarken2 author: Derleyen [Serdar Balcƒ±, MD, Pathologist](https://sbalci.github.io/) date: `{r # format(Sys.Date())` output: rmdformats::html_clean: highlight: kate html_notebook: fig_caption: yes highlight: kate number_sections: yes theme: flatly toc: yes toc_depth: 5 toc_float: yes pdf_document: toc: yes toc_depth: &#39;5&#39; html_document: fig_caption: yes keep_md: yes toc: yes toc_depth: 5 toc_float: yes {r , echo=TRUE, cache=FALSE} library(knitr) library(rmdformats) ## Global options options(max.print= 75 ) opts_chunk$set(echo=TRUE, cache=TRUE, prompt=FALSE, tidy=TRUE, comment=NA, message=FALSE, warning=FALSE) opts_knit$set(width=75) R generation https://rss.onlinelibrary.wiley.com/doi/10.1111/j.1740-9713.2018.01169.x Bu bir derlemedir, m√ºmk√ºn mertebe alƒ±ntƒ±lara referans vermeye √ßalƒ±≈ütƒ±m.‚Ü©Ô∏é "],["r-y√ºkleme.html", "Chapter 173 R y√ºkleme 173.1 R-project 173.2 RStudio 173.3 X11 173.4 Java OS", " Chapter 173 R y√ºkleme http://www.youtube.com/watch?v=XcBLEVknqvY 173.1 R-project https://cran.r-project.org/ 173.2 RStudio https://www.rstudio.com/ https://www.rstudio.com/products/rstudio/download/ https://moderndive.com/2-getting-started.html 173.2.1 RStudio eklentileri Discover and install useful RStudio addins https://cran.r-project.org/web/packages/addinslist/README.html https://rstudio.github.io/rstudioaddins/ {r eval=FALSE, include=FALSE, echo=TRUE} # devtools::install_github( rstudio/addinexamples , type = source ) 173.3 X11 https://www.xquartz.org/ 173.4 Java OS https://support.apple.com/kb/dl1572 "],["r-zor-≈üeyler-i√ßin-kolay-kolay-≈üeyler-i√ßin-zor.html", "Chapter 174 R zor ≈üeyler i√ßin kolay, kolay ≈üeyler i√ßin zor", " Chapter 174 R zor ≈üeyler i√ßin kolay, kolay ≈üeyler i√ßin zor R makes easy things hard, and hard things easy Aynƒ± ≈üeyi √ßok fazla ≈üekilde yapmak m√ºmk√ºn R Syntax Comparison::CHEAT SHEET https://www.amelia.mn/Syntax-cheatsheet.pdf "],["r-paketleri.html", "Chapter 175 R paketleri 175.1 Neden paketler var 175.2 Paketleri nereden bulabiliriz 175.3 Kendi paket evrenini olu≈ütur 175.4 R i√ßin yardƒ±m bulma 175.5 R paket y√ºkleme", " Chapter 175 R paketleri 175.1 Neden paketler var I love the #rstats community.Someone is like, ‚Äúoh hey peeps, I saw a big need for this mundane but difficult task that I infrequently do, so I created a package that will literally scrape the last bits of peanut butter out of the jar for you. It's called pbplyr.‚ÄùWhat a tribe. ‚Äî Frank Elavsky   ≥ ((???)) July 3, 2018 https://blog.mitchelloharawild.com/blog/user-2018-feature-wall/ 175.2 Paketleri nereden bulabiliriz Available CRAN Packages By Name https://cran.r-project.org/web/packages/available_packages_by_name.html Bioconductor https://www.bioconductor.org RecommendR http://recommendr.info/ pkgsearch CRAN package search https://github.com/metacran/pkgsearch Awesome R https://awesome-r.com/ 175.3 Kendi paket evrenini olu≈ütur pkgverse: Build a Meta-Package Universe https://cran.r-project.org/web/packages/pkgverse/index.html 175.4 R i√ßin yardƒ±m bulma # ?mean # ??efetch # help(merge) # example(merge) Vignette RDocumentation https://www.rdocumentation.org R Package Documentation https://rdrr.io/ GitHub Stackoverflow https://stackoverflow.com/ Google uygun anahtar kelime How I use #rstats h/t (???) pic.twitter.com/erRnTG0Ujr ‚Äî Emily Bovee ((???)) August 10, 2018 Awesome Cheatsheet https://github.com/detailyang/awesome-cheatsheet http://cran.r-project.org/doc/contrib/Baggott-refcard-v2.pdf https://www.rstudio.com/resources/cheatsheets/ Awesome R https://github.com/qinwf/awesome-R#readme https://awesome-r.com/ Twitter https://twitter.com/hashtag/rstats?src=hash Reproducible Examples Got a question to ask on (???) or post on (???)? No time to read the long post on how to use reprex? Here is a 20-second gif for you to format your R codes nicely and for others to reproduce your problem. (An example from a talk given by (???)) #rstat pic.twitter.com/gpuGXpFIsX ‚Äî ZhiYang ((???)) October 18, 2018 175.5 R paket y√ºkleme install.packages( tidyverse , dependencies = TRUE) install.packages( jmv , dependencies = TRUE) install.packages( questionr , dependencies = TRUE) install.packages( Rcmdr , dependencies = TRUE) install.packages( summarytools ) {r} # install.packages( tidyverse , dependencies = TRUE) # install.packages( jmv , dependencies = TRUE) # install.packages( questionr , dependencies = TRUE) # install.packages( Rcmdr , dependencies = TRUE) # install.packages( summarytools ) {r, error=FALSE, message = FALSE, warning = FALSE, eval = TRUE, include = TRUE} # require(tidyverse) # require(jmv) # require(questionr) # library(summarytools) # library(gganimate) "],["r-studio-ile-proje-olu≈üturma.html", "Chapter 176 R studio ile proje olu≈üturma", " Chapter 176 R studio ile proje olu≈üturma https://support.rstudio.com/hc/en-us/articles/200526207-Using-Projects "],["rstudio-ile-veri-y√ºkleme.html", "Chapter 177 RStudio ile veri y√ºkleme 177.1 Excel 177.2 SPSS 177.3 csv", " Chapter 177 RStudio ile veri y√ºkleme https://support.rstudio.com/hc/en-us/articles/218611977-Importing-Data-with-RStudio 177.1 Excel 177.2 SPSS 177.3 csv "],["veriyi-g√∂r√ºnt√ºleme.html", "Chapter 178 Veriyi g√∂r√ºnt√ºleme", " Chapter 178 Veriyi g√∂r√ºnt√ºleme Spreadsheet users using #rstats: where's the data?#rstats users using spreadsheets: where's the code? ‚Äî Leonard Kiefer ((???)) July 7, 2018 {r, results= markup } # library(nycflights13) # summary(flights) View(data) data head tail glimpse str skimr::skim() "],["veriyi-deƒüi≈ütirme.html", "Chapter 179 Veriyi deƒüi≈ütirme 179.1 Veriyi kod ile deƒüi≈ütirelim 179.2 Veriyi eklentilerle deƒüi≈ütirme 179.3 RStudio aracƒ±lƒ±ƒüƒ±yla recode", " Chapter 179 Veriyi deƒüi≈ütirme 179.1 Veriyi kod ile deƒüi≈ütirelim 179.2 Veriyi eklentilerle deƒüi≈ütirme 179.3 RStudio aracƒ±lƒ±ƒüƒ±yla recode questionr paketi kullanƒ±lacak https://juba.github.io/questionr/articles/recoding_addins.html "],["basit-tanƒ±mlayƒ±cƒ±-istatistikler.html", "Chapter 180 Basit tanƒ±mlayƒ±cƒ± istatistikler 180.1 summarytools 180.2 skimr 180.3 DataExplorer 180.4 Grafikler", " Chapter 180 Basit tanƒ±mlayƒ±cƒ± istatistikler summary() mean median min max sd table() {r, echo=TRUE, include = TRUE} library(readr) irisdata &lt;- read_csv( data/iris.csv ) jmv::descriptives( data = irisdata, vars = Sepal.Length , splitBy = Species , freq = TRUE, hist = TRUE, dens = TRUE, bar = TRUE, box = TRUE, violin = TRUE, dot = TRUE, mode = TRUE, sum = TRUE, sd = TRUE, variance = TRUE, range = TRUE, se = TRUE, skew = TRUE, kurt = TRUE, quart = TRUE, pcEqGr = TRUE) {r, echo=TRUE, include=FALSE} # install.packages( scatr ) scatr::scat( data = irisdata, x = Sepal.Length , y = Sepal.Width , group = Species , marg = dens , line = linear , se = TRUE) 180.1 summarytools https://cran.r-project.org/web/packages/summarytools/vignettes/Introduction.html {r, include=FALSE, comment=NA, prompt=FALSE, cache=FALSE, echo=TRUE, results=&#39;asis&#39;} library(summarytools) summarytools::freq(iris$Species, style = rmarkdown ) {r, include=FALSE, comment=NA, prompt=FALSE, cache=FALSE, echo=TRUE, results=&#39;asis&#39;} summarytools::freq(iris$Species, report.nas = FALSE, style = rmarkdown , headings = TRUE) {r, include=FALSE, comment=NA, prompt=FALSE, cache=FALSE, echo=TRUE, results=&#39;asis&#39;} with(tobacco, print(ctable(smoker, diseased), method = &#39;render&#39;)) {r, include=FALSE, comment=NA, prompt=FALSE, cache=FALSE, echo=TRUE, results=&#39;asis&#39;} with(tobacco, print(ctable(smoker, diseased, prop = &#39;n&#39;, totals = FALSE), headings = TRUE, method = render )) {r, include=FALSE, comment=NA, prompt=FALSE, cache=FALSE, echo=TRUE, results=&#39;asis&#39;} summarytools::descr(iris, style = rmarkdown ) {r, include=FALSE, comment=NA, prompt=FALSE, cache=FALSE, echo=TRUE, results=&#39;asis&#39;} descr(iris, stats = c( mean , sd , min , med , max ), transpose = TRUE, headings = TRUE, style = rmarkdown ) {r, include=FALSE, comment=NA, prompt=FALSE, cache=FALSE, echo=TRUE, results=&#39;asis&#39;} # view(dfSummary(iris)) {r, include=FALSE, comment=NA, prompt=FALSE, cache=FALSE, echo=TRUE, results=&#39;asis&#39;} dfSummary(tobacco, plain.ascii = FALSE, style = grid ) {r, include=FALSE, comment=NA, prompt=FALSE, cache=FALSE, echo=TRUE, results=&#39;asis&#39;} # First save the results iris_stats_by_species &lt;- by(data = iris, INDICES = iris$Species, FUN = descr, stats = c( mean , sd , min , med , max ), transpose = TRUE) # Then use view(), like so: view(iris_stats_by_species, method = pander , style = rmarkdown ) {r, include=FALSE, comment=NA, prompt=FALSE, cache=FALSE, echo=TRUE, results=&#39;asis&#39;} # view(iris_stats_by_species) {r, include=FALSE, comment=NA, prompt=FALSE, cache=FALSE, echo=TRUE, results=&#39;asis&#39;} data(tobacco) # tobacco is an example dataframe included in the package BMI_by_age &lt;- with(tobacco, by(BMI, age.gr, descr, stats = c( mean , sd , min , med , max ))) view(BMI_by_age, pander , style = rmarkdown ) {r, include=FALSE, comment=NA, prompt=FALSE, cache=FALSE, echo=TRUE, results=&#39;asis&#39;} BMI_by_age &lt;- with(tobacco, by(BMI, age.gr, descr, transpose = TRUE, stats = c( mean , sd , min , med , max ))) view(BMI_by_age, pander , style = rmarkdown , headings = TRUE) {r, include=FALSE, comment=NA, prompt=FALSE, cache=FALSE, echo=TRUE, results=&#39;asis&#39;} tobacco_subset &lt;- tobacco[ ,c( gender , age.gr , smoker )] freq_tables &lt;- lapply(tobacco_subset, freq) # view(freq_tables, footnote = NA, file = &#39;freq-tables.html&#39;) {r, include=FALSE, comment=NA, prompt=FALSE, cache=FALSE, echo=TRUE, results=&#39;asis&#39;} what.is(iris) {r eval=FALSE, include=FALSE, echo=TRUE} freq(tobacco$gender, style = &#39;rmarkdown&#39;) {r eval=FALSE, include=FALSE, echo=TRUE} print(freq(tobacco$gender), method = &#39;render&#39;) 180.2 skimr library(skimr) skim(df) 180.3 DataExplorer library(DataExplorer) DataExplorer::create_report(df) 180.4 Grafikler {r eval=FALSE, include=FALSE, echo=TRUE} # library(ggplot2) # library(mosaic) # mPlot(irisdata) {r eval=FALSE, include=FALSE, echo=TRUE} ctable(tobacco$gender, tobacco$smoker, style = &#39;rmarkdown&#39;) {r eval=FALSE, include=FALSE, echo=TRUE} print(ctable(tobacco$gender, tobacco$smoker), method = &#39;render&#39;) descr(tobacco, style = &#39;rmarkdown&#39;) print(descr(tobacco), method = &#39;render&#39;, table.classes = &#39;st-small&#39;) dfSummary(tobacco, style = &#39;grid&#39;, plain.ascii = FALSE) print(dfSummary(tobacco, graph.magnif = 0.75), method = &#39;render&#39;) Here, building up a #ggplot2 as slowly as possible, #rstats. Incremental adjustments. #rstatsteachingideas pic.twitter.com/nUulQl8bPh ‚Äî Gina Reynolds ((???)) August 13, 2018 Dreaming of a fancy #Rstats #ggplot #dataviz but still scared of typing #code? (???) esquisse package has you covered https://t.co/1vIDXcVAAF pic.twitter.com/RlTkptnrNv ‚Äî Radoslaw Panczak ((???)) October 2, 2018 "],["rcmdr.html", "Chapter 181 Rcmdr", " Chapter 181 Rcmdr library(Rcmdr) Rcmdr::Commander() A Comparative Review of the R Commander GUI for R http://r4stats.com/articles/software-reviews/r-commander/ "],["jamovi.html", "Chapter 182 jamovi", " Chapter 182 jamovi https://www.jamovi.org/ https://blog.jamovi.org/2018/07/30/rj.html "],["sonraki-konular.html", "Chapter 183 Sonraki Konular", " Chapter 183 Sonraki Konular RStudio ile GitHub Hipotez testleri R Markdown ve R Notebook ile tekrarlanabilir rapor "],["diƒüer-kodlar.html", "Chapter 184 Diƒüer kodlar", " Chapter 184 Diƒüer kodlar Diƒüer kodlar i√ßin bakƒ±nƒ±z: https://sbalci.github.io/ "],["geri-bildirim.html", "Chapter 185 Geri Bildirim", " Chapter 185 Geri Bildirim Geri bildirim i√ßin tƒ±klayƒ±nƒ±z: Geri bildirim formu Please enable JavaScript to view the comments powered by Disqus. title: R ile analize ba≈ülarken3 author: Derleyen [Serdar Balcƒ±, MD, Pathologist](https://sbalci.github.io/) date: `{r # format(Sys.Date())` output: rmdformats::html_clean: highlight: kate html_notebook: fig_caption: yes highlight: kate number_sections: yes theme: flatly toc: yes toc_depth: 5 toc_float: yes pdf_document: toc: yes toc_depth: &#39;5&#39; html_document: fig_caption: yes keep_md: yes toc: yes toc_depth: 5 toc_float: yes {r , eval=FALSE, cache=FALSE, include=FALSE} library(knitr) library(rmdformats) ## Global options options(max.print= 75 ) opts_chunk$set(echo=TRUE, cache=TRUE, prompt=FALSE, tidy=TRUE, comment=NA, message=FALSE, warning=FALSE) opts_knit$set(width=75) R generation https://rss.onlinelibrary.wiley.com/doi/10.1111/j.1740-9713.2018.01169.x Bu bir derlemedir, m√ºmk√ºn mertebe alƒ±ntƒ±lara referans vermeye √ßalƒ±≈ütƒ±m.‚Ü©Ô∏é "],["r-y√ºkleme-1.html", "Chapter 186 R y√ºkleme 186.1 R-project 186.2 RStudio 186.3 X11 186.4 Java OS", " Chapter 186 R y√ºkleme http://www.youtube.com/watch?v=XcBLEVknqvY 186.1 R-project https://cran.r-project.org/ 186.2 RStudio https://www.rstudio.com/ https://www.rstudio.com/products/rstudio/download/ https://moderndive.com/2-getting-started.html 186.2.1 RStudio eklentileri Discover and install useful RStudio addins https://cran.r-project.org/web/packages/addinslist/README.html https://rstudio.github.io/rstudioaddins/ {r eval=FALSE, include=FALSE, echo=TRUE} # devtools::install_github( rstudio/addinexamples , type = source ) 186.3 X11 https://www.xquartz.org/ 186.4 Java OS https://support.apple.com/kb/dl1572 "],["r-zor-≈üeyler-i√ßin-kolay-kolay-≈üeyler-i√ßin-zor-1.html", "Chapter 187 R zor ≈üeyler i√ßin kolay, kolay ≈üeyler i√ßin zor", " Chapter 187 R zor ≈üeyler i√ßin kolay, kolay ≈üeyler i√ßin zor R makes easy things hard, and hard things easy Aynƒ± ≈üeyi √ßok fazla ≈üekilde yapmak m√ºmk√ºn R Syntax Comparison::CHEAT SHEET https://www.amelia.mn/Syntax-cheatsheet.pdf "],["r-paketleri-1.html", "Chapter 188 R paketleri 188.1 Neden paketler var 188.2 Paketleri nereden bulabiliriz 188.3 Kendi paket evrenini olu≈ütur 188.4 R i√ßin yardƒ±m bulma 188.5 R paket y√ºkleme", " Chapter 188 R paketleri 188.1 Neden paketler var I love the #rstats community.Someone is like, ‚Äúoh hey peeps, I saw a big need for this mundane but difficult task that I infrequently do, so I created a package that will literally scrape the last bits of peanut butter out of the jar for you. It's called pbplyr.‚ÄùWhat a tribe. ‚Äî Frank Elavsky   ≥ ((???)) July 3, 2018 https://blog.mitchelloharawild.com/blog/user-2018-feature-wall/ 188.2 Paketleri nereden bulabiliriz Available CRAN Packages By Name https://cran.r-project.org/web/packages/available_packages_by_name.html Bioconductor https://www.bioconductor.org RecommendR http://recommendr.info/ pkgsearch CRAN package search https://github.com/metacran/pkgsearch Awesome R https://awesome-r.com/ 188.3 Kendi paket evrenini olu≈ütur pkgverse: Build a Meta-Package Universe https://cran.r-project.org/web/packages/pkgverse/index.html 188.4 R i√ßin yardƒ±m bulma # ?mean # ??efetch # help(merge) # example(merge) Vignette RDocumentation https://www.rdocumentation.org R Package Documentation https://rdrr.io/ GitHub Stackoverflow https://stackoverflow.com/ Google uygun anahtar kelime How I use #rstats h/t (???) pic.twitter.com/erRnTG0Ujr ‚Äî Emily Bovee ((???)) August 10, 2018 Awesome Cheatsheet https://github.com/detailyang/awesome-cheatsheet http://cran.r-project.org/doc/contrib/Baggott-refcard-v2.pdf https://www.rstudio.com/resources/cheatsheets/ Awesome R https://github.com/qinwf/awesome-R#readme https://awesome-r.com/ Twitter https://twitter.com/hashtag/rstats?src=hash Reproducible Examples Got a question to ask on (???) or post on (???)? No time to read the long post on how to use reprex? Here is a 20-second gif for you to format your R codes nicely and for others to reproduce your problem. (An example from a talk given by (???)) #rstat pic.twitter.com/gpuGXpFIsX ‚Äî ZhiYang ((???)) October 18, 2018 188.5 R paket y√ºkleme install.packages( tidyverse , dependencies = TRUE) install.packages( jmv , dependencies = TRUE) install.packages( questionr , dependencies = TRUE) install.packages( Rcmdr , dependencies = TRUE) install.packages( summarytools ) {r} # install.packages( tidyverse , dependencies = TRUE) # install.packages( jmv , dependencies = TRUE) # install.packages( questionr , dependencies = TRUE) # install.packages( Rcmdr , dependencies = TRUE) # install.packages( summarytools ) {r, error=FALSE, message = FALSE, warning = FALSE, eval = TRUE, include = TRUE} # require(tidyverse) # require(jmv) # require(questionr) # library(summarytools) # library(gganimate) "],["r-studio-ile-proje-olu≈üturma-1.html", "Chapter 189 R studio ile proje olu≈üturma", " Chapter 189 R studio ile proje olu≈üturma https://support.rstudio.com/hc/en-us/articles/200526207-Using-Projects "],["rstudio-ile-veri-y√ºkleme-1.html", "Chapter 190 RStudio ile veri y√ºkleme 190.1 Excel 190.2 SPSS 190.3 csv", " Chapter 190 RStudio ile veri y√ºkleme https://support.rstudio.com/hc/en-us/articles/218611977-Importing-Data-with-RStudio 190.1 Excel 190.2 SPSS 190.3 csv "],["veriyi-g√∂r√ºnt√ºleme-1.html", "Chapter 191 Veriyi g√∂r√ºnt√ºleme", " Chapter 191 Veriyi g√∂r√ºnt√ºleme Spreadsheet users using #rstats: where's the data?#rstats users using spreadsheets: where's the code? ‚Äî Leonard Kiefer ((???)) July 7, 2018 {r, results= markup } # library(nycflights13) # summary(flights) View(data) data head tail glimpse str skimr::skim() "],["veriyi-deƒüi≈ütirme-1.html", "Chapter 192 Veriyi deƒüi≈ütirme 192.1 Veriyi kod ile deƒüi≈ütirelim 192.2 Veriyi eklentilerle deƒüi≈ütirme 192.3 RStudio aracƒ±lƒ±ƒüƒ±yla recode", " Chapter 192 Veriyi deƒüi≈ütirme 192.1 Veriyi kod ile deƒüi≈ütirelim 192.2 Veriyi eklentilerle deƒüi≈ütirme 192.3 RStudio aracƒ±lƒ±ƒüƒ±yla recode questionr paketi kullanƒ±lacak https://juba.github.io/questionr/articles/recoding_addins.html "],["basit-tanƒ±mlayƒ±cƒ±-istatistikler-1.html", "Chapter 193 Basit tanƒ±mlayƒ±cƒ± istatistikler 193.1 summarytools 193.2 skimr 193.3 DataExplorer 193.4 Grafikler", " Chapter 193 Basit tanƒ±mlayƒ±cƒ± istatistikler summary() mean median min max sd table() {r, eval=FALSE, include=FALSE} library(readr) irisdata &lt;- read_csv( data/iris.csv ) jmv::descriptives( data = irisdata, vars = Sepal.Length , splitBy = Species , freq = TRUE, hist = TRUE, dens = TRUE, bar = TRUE, box = TRUE, violin = TRUE, dot = TRUE, mode = TRUE, sum = TRUE, sd = TRUE, variance = TRUE, range = TRUE, se = TRUE, skew = TRUE, kurt = TRUE, quart = TRUE, pcEqGr = TRUE) {r, echo=TRUE, include=FALSE} # install.packages( scatr ) scatr::scat( data = irisdata, x = Sepal.Length , y = Sepal.Width , group = Species , marg = dens , line = linear , se = TRUE) 193.1 summarytools https://cran.r-project.org/web/packages/summarytools/vignettes/Introduction.html {r eval=FALSE, include=FALSE, comment=NA, prompt=FALSE, cache=FALSE, echo=TRUE, results=&#39;asis&#39;} library(summarytools) summarytools::freq(iris$Species, style = rmarkdown ) {r, include=FALSE, comment=NA, prompt=FALSE, cache=FALSE, echo=TRUE, results=&#39;asis&#39;} summarytools::freq(iris$Species, report.nas = FALSE, style = rmarkdown , headings = TRUE) {r, include=FALSE, comment=NA, prompt=FALSE, cache=FALSE, echo=TRUE, results=&#39;asis&#39;} with(tobacco, print(ctable(smoker, diseased), method = &#39;render&#39;)) {r, include=FALSE, comment=NA, prompt=FALSE, cache=FALSE, echo=TRUE, results=&#39;asis&#39;} with(tobacco, print(ctable(smoker, diseased, prop = &#39;n&#39;, totals = FALSE), headings = TRUE, method = render )) {r, include=FALSE, comment=NA, prompt=FALSE, cache=FALSE, echo=TRUE, results=&#39;asis&#39;} summarytools::descr(iris, style = rmarkdown ) {r, include=FALSE, comment=NA, prompt=FALSE, cache=FALSE, echo=TRUE, results=&#39;asis&#39;} descr(iris, stats = c( mean , sd , min , med , max ), transpose = TRUE, headings = TRUE, style = rmarkdown ) {r, include=FALSE, comment=NA, prompt=FALSE, cache=FALSE, echo=TRUE, results=&#39;asis&#39;} # view(dfSummary(iris)) {r, include=FALSE, comment=NA, prompt=FALSE, cache=FALSE, echo=TRUE, results=&#39;asis&#39;} dfSummary(tobacco, plain.ascii = FALSE, style = grid ) {r, include=FALSE, comment=NA, prompt=FALSE, cache=FALSE, echo=TRUE, results=&#39;asis&#39;} # First save the results iris_stats_by_species &lt;- by(data = iris, INDICES = iris$Species, FUN = descr, stats = c( mean , sd , min , med , max ), transpose = TRUE) # Then use view(), like so: view(iris_stats_by_species, method = pander , style = rmarkdown ) {r, include=FALSE, comment=NA, prompt=FALSE, cache=FALSE, echo=TRUE, results=&#39;asis&#39;} # view(iris_stats_by_species) {r, include=FALSE, comment=NA, prompt=FALSE, cache=FALSE, echo=TRUE, results=&#39;asis&#39;} data(tobacco) # tobacco is an example dataframe included in the package BMI_by_age &lt;- with(tobacco, by(BMI, age.gr, descr, stats = c( mean , sd , min , med , max ))) view(BMI_by_age, pander , style = rmarkdown ) {r, include=FALSE, comment=NA, prompt=FALSE, cache=FALSE, echo=TRUE, results=&#39;asis&#39;} BMI_by_age &lt;- with(tobacco, by(BMI, age.gr, descr, transpose = TRUE, stats = c( mean , sd , min , med , max ))) view(BMI_by_age, pander , style = rmarkdown , headings = TRUE) {r, include=FALSE, comment=NA, prompt=FALSE, cache=FALSE, echo=TRUE, results=&#39;asis&#39;} tobacco_subset &lt;- tobacco[ ,c( gender , age.gr , smoker )] freq_tables &lt;- lapply(tobacco_subset, freq) # view(freq_tables, footnote = NA, file = &#39;freq-tables.html&#39;) {r, include=FALSE, comment=NA, prompt=FALSE, cache=FALSE, echo=TRUE, results=&#39;asis&#39;} what.is(iris) {r eval=FALSE, include=FALSE, echo=TRUE} freq(tobacco$gender, style = &#39;rmarkdown&#39;) {r eval=FALSE, include=FALSE, echo=TRUE} print(freq(tobacco$gender), method = &#39;render&#39;) 193.2 skimr library(skimr) skim(df) 193.3 DataExplorer library(DataExplorer) DataExplorer::create_report(df) 193.4 Grafikler {r eval=FALSE, include=FALSE, echo=TRUE} # library(ggplot2) # library(mosaic) # mPlot(irisdata) {r eval=FALSE, include=FALSE, echo=TRUE} ctable(tobacco$gender, tobacco$smoker, style = &#39;rmarkdown&#39;) {r eval=FALSE, include=FALSE, echo=TRUE} print(ctable(tobacco$gender, tobacco$smoker), method = &#39;render&#39;) descr(tobacco, style = &#39;rmarkdown&#39;) print(descr(tobacco), method = &#39;render&#39;, table.classes = &#39;st-small&#39;) dfSummary(tobacco, style = &#39;grid&#39;, plain.ascii = FALSE) print(dfSummary(tobacco, graph.magnif = 0.75), method = &#39;render&#39;) Here, building up a #ggplot2 as slowly as possible, #rstats. Incremental adjustments. #rstatsteachingideas pic.twitter.com/nUulQl8bPh ‚Äî Gina Reynolds ((???)) August 13, 2018 Dreaming of a fancy #Rstats #ggplot #dataviz but still scared of typing #code? (???) esquisse package has you covered https://t.co/1vIDXcVAAF pic.twitter.com/RlTkptnrNv ‚Äî Radoslaw Panczak ((???)) October 2, 2018 "],["rcmdr-1.html", "Chapter 194 Rcmdr", " Chapter 194 Rcmdr library(Rcmdr) Rcmdr::Commander() A Comparative Review of the R Commander GUI for R http://r4stats.com/articles/software-reviews/r-commander/ "],["jamovi-1.html", "Chapter 195 jamovi", " Chapter 195 jamovi https://www.jamovi.org/ https://blog.jamovi.org/2018/07/30/rj.html "],["sonraki-konular-1.html", "Chapter 196 Sonraki Konular", " Chapter 196 Sonraki Konular RStudio ile GitHub Hipotez testleri R Markdown ve R Notebook ile tekrarlanabilir rapor "],["diƒüer-kodlar-1.html", "Chapter 197 Diƒüer kodlar", " Chapter 197 Diƒüer kodlar Diƒüer kodlar i√ßin bakƒ±nƒ±z: https://sbalci.github.io/ "],["geri-bildirim-1.html", "Chapter 198 Geri Bildirim", " Chapter 198 Geri Bildirim Geri bildirim i√ßin tƒ±klayƒ±nƒ±z: Geri bildirim formu Please enable JavaScript to view the comments powered by Disqus. {r , echo=TRUE, cache=FALSE} library(knitr) library(rmdformats) ## Global options options(max.print= 75 ) opts_chunk$set(echo=TRUE, cache=TRUE, prompt=FALSE, tidy=TRUE, comment=NA, message=FALSE, warning=FALSE) opts_knit$set(width=75) R generation https://rss.onlinelibrary.wiley.com/doi/10.1111/j.1740-9713.2018.01169.x "],["r-y√ºkleme-2.html", "Chapter 199 R y√ºkleme 199.1 R-project 199.2 RStudio 199.3 X11 199.4 Java OS", " Chapter 199 R y√ºkleme http://www.youtube.com/watch?v=XcBLEVknqvY 199.1 R-project https://cran.r-project.org/ 199.2 RStudio https://www.rstudio.com/ https://www.rstudio.com/products/rstudio/download/ https://moderndive.com/2-getting-started.html 199.2.1 RStudio eklentileri Discover and install useful RStudio addins https://cran.r-project.org/web/packages/addinslist/README.html https://rstudio.github.io/rstudioaddins/ {r eval=FALSE, include=FALSE, echo=TRUE} # devtools::install_github( rstudio/addinexamples , type = source ) 199.3 X11 https://www.xquartz.org/ 199.4 Java OS https://support.apple.com/kb/dl1572 "],["r-zor-≈üeyler-i√ßin-kolay-kolay-≈üeyler-i√ßin-zor-2.html", "Chapter 200 R zor ≈üeyler i√ßin kolay, kolay ≈üeyler i√ßin zor", " Chapter 200 R zor ≈üeyler i√ßin kolay, kolay ≈üeyler i√ßin zor R makes easy things hard, and hard things easy Aynƒ± ≈üeyi √ßok fazla ≈üekilde yapmak m√ºmk√ºn R Syntax Comparison::CHEAT SHEET https://www.amelia.mn/Syntax-cheatsheet.pdf "],["r-paketleri-2.html", "Chapter 201 R paketleri 201.1 Neden paketler var 201.2 Paketleri nereden bulabiliriz 201.3 Kendi paket evrenini olu≈ütur 201.4 R i√ßin yardƒ±m bulma 201.5 R paket y√ºkleme", " Chapter 201 R paketleri 201.1 Neden paketler var I love the #rstats community.Someone is like, ‚Äúoh hey peeps, I saw a big need for this mundane but difficult task that I infrequently do, so I created a package that will literally scrape the last bits of peanut butter out of the jar for you. It's called pbplyr.‚ÄùWhat a tribe. ‚Äî Frank Elavsky   ≥ ((???)) July 3, 2018 https://blog.mitchelloharawild.com/blog/user-2018-feature-wall/ 201.2 Paketleri nereden bulabiliriz Available CRAN Packages By Name https://cran.r-project.org/web/packages/available_packages_by_name.html Bioconductor https://www.bioconductor.org RecommendR http://recommendr.info/ pkgsearch CRAN package search https://github.com/metacran/pkgsearch Awesome R https://awesome-r.com/ 201.3 Kendi paket evrenini olu≈ütur pkgverse: Build a Meta-Package Universe https://cran.r-project.org/web/packages/pkgverse/index.html 201.4 R i√ßin yardƒ±m bulma # ?mean # ??efetch # help(merge) # example(merge) Vignette RDocumentation https://www.rdocumentation.org R Package Documentation https://rdrr.io/ GitHub Stackoverflow https://stackoverflow.com/ Google uygun anahtar kelime How I use #rstats h/t (???) pic.twitter.com/erRnTG0Ujr ‚Äî Emily Bovee ((???)) August 10, 2018 Awesome Cheatsheet https://github.com/detailyang/awesome-cheatsheet http://cran.r-project.org/doc/contrib/Baggott-refcard-v2.pdf https://www.rstudio.com/resources/cheatsheets/ Awesome R https://github.com/qinwf/awesome-R#readme https://awesome-r.com/ Twitter https://twitter.com/hashtag/rstats?src=hash Reproducible Examples Got a question to ask on (???) or post on (???)? No time to read the long post on how to use reprex? Here is a 20-second gif for you to format your R codes nicely and for others to reproduce your problem. (An example from a talk given by (???)) #rstat pic.twitter.com/gpuGXpFIsX ‚Äî ZhiYang ((???)) October 18, 2018 201.5 R paket y√ºkleme install.packages( tidyverse , dependencies = TRUE) install.packages( jmv , dependencies = TRUE) install.packages( questionr , dependencies = TRUE) install.packages( Rcmdr , dependencies = TRUE) install.packages( summarytools ) {r} # install.packages( tidyverse , dependencies = TRUE) # install.packages( jmv , dependencies = TRUE) # install.packages( questionr , dependencies = TRUE) # install.packages( Rcmdr , dependencies = TRUE) # install.packages( summarytools ) {r, error=FALSE, message = FALSE, warning = FALSE, eval = TRUE, include = TRUE} # require(tidyverse) # require(jmv) # require(questionr) # library(summarytools) # library(gganimate) "],["r-studio-ile-proje-olu≈üturma-2.html", "Chapter 202 R studio ile proje olu≈üturma", " Chapter 202 R studio ile proje olu≈üturma https://support.rstudio.com/hc/en-us/articles/200526207-Using-Projects "],["rstudio-ile-veri-y√ºkleme-2.html", "Chapter 203 RStudio ile veri y√ºkleme 203.1 Excel 203.2 SPSS 203.3 csv", " Chapter 203 RStudio ile veri y√ºkleme https://support.rstudio.com/hc/en-us/articles/218611977-Importing-Data-with-RStudio 203.1 Excel 203.2 SPSS 203.3 csv "],["veriyi-g√∂r√ºnt√ºleme-2.html", "Chapter 204 Veriyi g√∂r√ºnt√ºleme", " Chapter 204 Veriyi g√∂r√ºnt√ºleme Spreadsheet users using #rstats: where's the data?#rstats users using spreadsheets: where's the code? ‚Äî Leonard Kiefer ((???)) July 7, 2018 {r, results= markup } # library(nycflights13) # summary(flights) View(data) data head tail glimpse str skimr::skim() "],["veriyi-deƒüi≈ütirme-2.html", "Chapter 205 Veriyi deƒüi≈ütirme 205.1 Veriyi kod ile deƒüi≈ütirelim 205.2 Veriyi eklentilerle deƒüi≈ütirme 205.3 RStudio aracƒ±lƒ±ƒüƒ±yla recode", " Chapter 205 Veriyi deƒüi≈ütirme 205.1 Veriyi kod ile deƒüi≈ütirelim 205.2 Veriyi eklentilerle deƒüi≈ütirme 205.3 RStudio aracƒ±lƒ±ƒüƒ±yla recode questionr paketi kullanƒ±lacak https://juba.github.io/questionr/articles/recoding_addins.html "],["basit-tanƒ±mlayƒ±cƒ±-istatistikler-2.html", "Chapter 206 Basit tanƒ±mlayƒ±cƒ± istatistikler 206.1 summarytools 206.2 skimr 206.3 DataExplorer 206.4 Grafikler", " Chapter 206 Basit tanƒ±mlayƒ±cƒ± istatistikler summary() mean median min max sd table() {r, echo=TRUE, include = TRUE} library(readr) irisdata &lt;- read_csv( data/iris.csv ) jmv::descriptives( data = irisdata, vars = Sepal.Length , splitBy = Species , freq = TRUE, hist = TRUE, dens = TRUE, bar = TRUE, box = TRUE, violin = TRUE, dot = TRUE, mode = TRUE, sum = TRUE, sd = TRUE, variance = TRUE, range = TRUE, se = TRUE, skew = TRUE, kurt = TRUE, quart = TRUE, pcEqGr = TRUE) {r, echo=TRUE, include=FALSE} # install.packages( scatr ) scatr::scat( data = irisdata, x = Sepal.Length , y = Sepal.Width , group = Species , marg = dens , line = linear , se = TRUE) 206.1 summarytools https://cran.r-project.org/web/packages/summarytools/vignettes/Introduction.html {r, include=FALSE, comment=NA, prompt=FALSE, cache=FALSE, echo=TRUE, results=&#39;asis&#39;} library(summarytools) summarytools::freq(iris$Species, style = rmarkdown ) {r, include=FALSE, comment=NA, prompt=FALSE, cache=FALSE, echo=TRUE, results=&#39;asis&#39;} summarytools::freq(iris$Species, report.nas = FALSE, style = rmarkdown , headings = TRUE) {r, include=FALSE, comment=NA, prompt=FALSE, cache=FALSE, echo=TRUE, results=&#39;asis&#39;} with(tobacco, print(ctable(smoker, diseased), method = &#39;render&#39;)) {r, include=FALSE, comment=NA, prompt=FALSE, cache=FALSE, echo=TRUE, results=&#39;asis&#39;} with(tobacco, print(ctable(smoker, diseased, prop = &#39;n&#39;, totals = FALSE), headings = TRUE, method = render )) {r, include=FALSE, comment=NA, prompt=FALSE, cache=FALSE, echo=TRUE, results=&#39;asis&#39;} summarytools::descr(iris, style = rmarkdown ) {r, include=FALSE, comment=NA, prompt=FALSE, cache=FALSE, echo=TRUE, results=&#39;asis&#39;} descr(iris, stats = c( mean , sd , min , med , max ), transpose = TRUE, headings = TRUE, style = rmarkdown ) {r, include=FALSE, comment=NA, prompt=FALSE, cache=FALSE, echo=TRUE, results=&#39;asis&#39;} # view(dfSummary(iris)) {r, include=FALSE, comment=NA, prompt=FALSE, cache=FALSE, echo=TRUE, results=&#39;asis&#39;} dfSummary(tobacco, plain.ascii = FALSE, style = grid ) {r, include=FALSE, comment=NA, prompt=FALSE, cache=FALSE, echo=TRUE, results=&#39;asis&#39;} # First save the results iris_stats_by_species &lt;- by(data = iris, INDICES = iris$Species, FUN = descr, stats = c( mean , sd , min , med , max ), transpose = TRUE) # Then use view(), like so: view(iris_stats_by_species, method = pander , style = rmarkdown ) {r, include=FALSE, comment=NA, prompt=FALSE, cache=FALSE, echo=TRUE, results=&#39;asis&#39;} # view(iris_stats_by_species) {r, include=FALSE, comment=NA, prompt=FALSE, cache=FALSE, echo=TRUE, results=&#39;asis&#39;} data(tobacco) # tobacco is an example dataframe included in the package BMI_by_age &lt;- with(tobacco, by(BMI, age.gr, descr, stats = c( mean , sd , min , med , max ))) view(BMI_by_age, pander , style = rmarkdown ) {r, include=FALSE, comment=NA, prompt=FALSE, cache=FALSE, echo=TRUE, results=&#39;asis&#39;} BMI_by_age &lt;- with(tobacco, by(BMI, age.gr, descr, transpose = TRUE, stats = c( mean , sd , min , med , max ))) view(BMI_by_age, pander , style = rmarkdown , headings = TRUE) {r, include=FALSE, comment=NA, prompt=FALSE, cache=FALSE, echo=TRUE, results=&#39;asis&#39;} tobacco_subset &lt;- tobacco[ ,c( gender , age.gr , smoker )] freq_tables &lt;- lapply(tobacco_subset, freq) # view(freq_tables, footnote = NA, file = &#39;freq-tables.html&#39;) {r, include=FALSE, comment=NA, prompt=FALSE, cache=FALSE, echo=TRUE, results=&#39;asis&#39;} what.is(iris) {r eval=FALSE, include=FALSE, echo=TRUE} freq(tobacco$gender, style = &#39;rmarkdown&#39;) {r eval=FALSE, include=FALSE, echo=TRUE} print(freq(tobacco$gender), method = &#39;render&#39;) 206.2 skimr library(skimr) skim(df) 206.3 DataExplorer library(DataExplorer) DataExplorer::create_report(df) 206.4 Grafikler {r eval=FALSE, include=FALSE, echo=TRUE} # library(ggplot2) # library(mosaic) # mPlot(irisdata) {r eval=FALSE, include=FALSE, echo=TRUE} ctable(tobacco$gender, tobacco$smoker, style = &#39;rmarkdown&#39;) {r eval=FALSE, include=FALSE, echo=TRUE} print(ctable(tobacco$gender, tobacco$smoker), method = &#39;render&#39;) descr(tobacco, style = &#39;rmarkdown&#39;) print(descr(tobacco), method = &#39;render&#39;, table.classes = &#39;st-small&#39;) dfSummary(tobacco, style = &#39;grid&#39;, plain.ascii = FALSE) print(dfSummary(tobacco, graph.magnif = 0.75), method = &#39;render&#39;) Here, building up a #ggplot2 as slowly as possible, #rstats. Incremental adjustments. #rstatsteachingideas pic.twitter.com/nUulQl8bPh ‚Äî Gina Reynolds ((???)) August 13, 2018 Dreaming of a fancy #Rstats #ggplot #dataviz but still scared of typing #code? (???) esquisse package has you covered https://t.co/1vIDXcVAAF pic.twitter.com/RlTkptnrNv ‚Äî Radoslaw Panczak ((???)) October 2, 2018 "],["rcmdr-2.html", "Chapter 207 Rcmdr", " Chapter 207 Rcmdr library(Rcmdr) Rcmdr::Commander() A Comparative Review of the R Commander GUI for R http://r4stats.com/articles/software-reviews/r-commander/ "],["jamovi-2.html", "Chapter 208 jamovi", " Chapter 208 jamovi https://www.jamovi.org/ https://blog.jamovi.org/2018/07/30/rj.html "],["sonraki-konular-2.html", "Chapter 209 Sonraki Konular", " Chapter 209 Sonraki Konular RStudio ile GitHub Hipotez testleri R Markdown ve R Notebook ile tekrarlanabilir rapor "],["diƒüer-kodlar-2.html", "Chapter 210 Diƒüer kodlar", " Chapter 210 Diƒüer kodlar Diƒüer kodlar i√ßin bakƒ±nƒ±z: https://sbalci.github.io/ "],["geri-bildirim-2.html", "Chapter 211 Geri Bildirim", " Chapter 211 Geri Bildirim Geri bildirim i√ßin tƒ±klayƒ±nƒ±z: Geri bildirim formu Please enable JavaScript to view the comments powered by Disqus. {r , echo = TRUE, warning = FALSE, message = FALSE} library(knitr) library(dplyr) library(huxtable) options(huxtable.knit_print_df = FALSE) is_latex &lt;- guess_knitr_output_format() == &#39;latex&#39; # is_latex &lt;- TRUE knitr::knit_hooks$set( barrier = function(before, options, envir) { if (! before &amp;&amp; is_latex) knitr::asis_output(&#39;\\\\FloatBarrier&#39;) } ) if (is_latex) knitr::opts_chunk$set(barrier = TRUE) {r, echo = TRUE} huxtable::hux_logo(latex = is_latex) {r, eval = FALSE, echo = TRUE} # PLAN # Make single document work in notebook format (maybe with minimal changes) # Installation # Dplyr examples # Examples with where and friends # Different kinds of output # Cookbook? # Limitations "],["introduction-1.html", "Chapter 212 Introduction 212.1 About this document 212.2 Huxtable 212.3 Installation 212.4 Getting started", " Chapter 212 Introduction 212.1 About this document This is the introductory vignette for the R package ‚Äòhuxtable‚Äô, version {r # packageVersion('huxtable'). A current version is available on the web in HTML or PDF format. 212.2 Huxtable Huxtable is a package for creating text tables. It is powerful, but easy to use. It is meant to be a replacement for packages like xtable, which is useful but not always very user-friendly. Huxtable‚Äôs features include: Export to LaTeX, HTML, Word and Markdown Easy integration with knitr and rmarkdown documents Multirow and multicolumn cells Fine-grained control over cell background, spacing, alignment, size and borders Control over text font, style, size, colour, alignment, number format and rotation Table manipulation using standard R subsetting, or dplyr functions like filter and select Easy conditional formatting based on table contents Quick table themes Automatic creation of regression output tables with the huxreg function We will cover all of these features below. 212.3 Installation If you haven‚Äôt already installed huxtable, you can do so from the R command line: {r, eval = FALSE} install.packages(&#39;huxtable&#39;) 212.4 Getting started A huxtable is a way of representing a table of text data in R. You already know that R can represent a table of data in a data frame. For example, if mydata is a data frame, then mydata[1, 2] represents the the data in row 1, column 2, and mydata$start_time is all the data in the column called start_time. A huxtable is just a data frame with some extra properties. So, if myhux is a huxtable, then myhux[1, 2] represents the data in row 1 column 2, as before. But this cell will also have some other properties - for example, the font size of the text, or the colour of the cell border. To create a table with huxtable, use the function huxtable, or hux for short. This works very much like data.frame. {r eval=FALSE, include=FALSE, echo=TRUE} library(huxtable) ht &lt;- hux( Employee = c(&#39;John Smith&#39;, &#39;Jane Doe&#39;, &#39;David Hugh-Jones&#39;), Salary = c(50000, 50000, 40000), add_colnames = TRUE ) If you already have your data in a data frame, you can convert it to a huxtable with as_hux. {r eval=FALSE, include=FALSE, echo=TRUE} data(mtcars) car_ht &lt;- as_hux(mtcars) If you look at a huxtable in R, it will print out a simple representation of the data. Notice that we‚Äôve added the column names to the data frame itself, using the add_colnames argument to hux. We‚Äôre going to print them out, so they need to be part of the actual table. NB: This means that row 1 of your data will be row 2 of the huxtable, and the column names of your data will be the new row 1. {r eval=FALSE, include=FALSE, results=&#39;markup&#39;} print_screen(ht) # on the R command line, you can just type ht To print a huxtable out using LaTeX or HTML, just call print_latex or print_html. In knitr documents, like this one, you can simply evaluate the hux. It will know what format to print itself in. {r eval=FALSE, include=FALSE, echo=TRUE} ht "],["changing-the-look-and-feel.html", "Chapter 213 Changing the look and feel 213.1 Huxtable properties 213.2 Tidyverse syntax 213.3 Getting properties", " Chapter 213 Changing the look and feel 213.1 Huxtable properties The default output is a very plain table. Let‚Äôs make it a bit smarter. We‚Äôll make the table headings bold, draw a line under the header row, and add some horizontal space to the cells. We also need to change that default number formatting to look less scientific. To do this, we need to set cell level properties. You set properties by assigning to the property name, just as you assign names(x) &lt;- new_names in base R. The following commands assign the value 10 to the {r # ight_padding and left_padding properties, for all cells in ht: {r eval=FALSE, include=FALSE, echo=TRUE} right_padding(ht) &lt;- 10 left_padding(ht) &lt;- 10 Similarly, we can set the number_format property to change how numbers are displayed in cells: {r eval=FALSE, include=FALSE, echo=TRUE} number_format(ht) &lt;- 2 # 2 decimal places To assign properties to just some cells, you use subsetting, just as in base R. So, to make the first row of the table bold and give it a bottom border, we do: {r eval=FALSE, include=FALSE, echo=TRUE} bold(ht)[1, ] &lt;- TRUE bottom_border(ht)[1, ] &lt;- 1 After these changes, our table looks smarter: {r eval=FALSE, include=FALSE, echo=TRUE} ht So far, all these properties have been set at cell level. Different cells can have different alignment, text formatting and so on. By contrast, caption is a table-level property. It only takes one value, which sets a table caption. {r eval=FALSE, include=FALSE, echo=TRUE} caption(ht) &lt;- &#39;Employee table&#39; ht As well as cell properties and table properties, there is also one row property, row heights, and one column property, column widths. The table below shows a complete list of properties. Most properties work the same for LaTeX and HTML, though there are some exceptions. {r, echo = TRUE} sides &lt;- c(&#39;left_&#39;, &#39;right_&#39;, &#39;top_&#39;, &#39;bottom_&#39;) props &lt;- list() props[[&#39;Cell_Text&#39;]] &lt;- sort(c(&#39;font&#39;, &#39;text_color&#39;, &#39;wrap&#39;, &#39;bold&#39;, &#39;italic&#39;, &#39;font&#39;, &#39;font_size&#39;, &#39;na_string&#39;, &#39;escape_contents&#39;, &#39;number_format&#39;, &#39;rotation&#39;)) props[[&#39;Cell&#39;]] &lt;- sort(c(&#39;align&#39;, &#39;valign&#39;, &#39;rowspan&#39;, &#39;colspan&#39;, &#39;background_color&#39;, paste0(sides, &#39;border&#39;), paste0(sides, &#39;border_color&#39;), paste0(sides, &#39;padding&#39;))) props[[&#39;Row&#39;]] &lt;- &#39;row_height&#39; props[[&#39;Column&#39;]] &lt;- &#39;col_width&#39; props[[&#39;Table&#39;]] &lt;- sort(c(&#39;width&#39;, &#39;height&#39;, &#39;position&#39;, &#39;caption&#39;, &#39;caption_pos&#39;, &#39;tabular_environment&#39;, &#39;label&#39;, &#39;latex_float&#39;)) maxl &lt;- max(sapply(props, length)) props &lt;- lapply(props, function(x) c(x, rep(&#39;&#39;, maxl - length(x)))) ss_font &lt;- if (guess_knitr_output_format() == &#39;latex&#39;) &#39;cmtt&#39; else &#39;courier&#39; prop_hux &lt;- hux(as.data.frame(props)) %&gt;% add_colnames %&gt;% {foo &lt;- .; foo[1,] &lt;- gsub(&#39;_&#39;, &#39; &#39;, foo[1,]); foo} %&gt;% set_font(-1, everywhere, ss_font) %&gt;% set_bold(1, everywhere, TRUE) %&gt;% set_width(0.9) %&gt;% set_background_color(everywhere, evens, grey(.9)) %&gt;% set_left_border(everywhere, 1, 1) %&gt;% set_right_border(everywhere, final(), 1) %&gt;% set_top_border(1, everywhere, 1) %&gt;% set_bottom_border(1, everywhere, 1) %&gt;% set_bottom_border(final(), everywhere, 1) %&gt;% set_top_padding(2) %&gt;% set_bottom_padding(4) %&gt;% set_caption(&#39;Huxtable properties&#39;) %&gt;% set_position(&#39;left&#39;) %&gt;% set_col_width(c(.2, .25, .15, .15, .25)) prop_hux 213.2 Tidyverse syntax If you prefer a tidyverse style of code, using the pipe operator %&gt;%, then you can use set_* functions. These have the same name as the property, with set_ prepended. For example, to set the bold property, you use the set_bold function. set_* functions return the modified huxtable, so you can chain them together like this: {r eval=FALSE, include=FALSE, echo=TRUE} library(dplyr) hux( Employee = c(&#39;John Smith&#39;, &#39;Jane Doe&#39;, &#39;David Hugh-Jones&#39;), Salary = c(50000, 50000, 40000), add_colnames = TRUE ) %&gt;% set_bold(1, 1:2, TRUE) %&gt;% set_bottom_border(1, 1:2, 1) %&gt;% set_align(1:4, 2, &#39;right&#39;) %&gt;% set_right_padding(10) %&gt;% set_left_padding(10) %&gt;% set_caption(&#39;Employee table&#39;) set_* functions for cell properties are called like this: set_xxx(ht, row, col, value) or like this: set_xxx(ht, value). If you use the second form, then the value is set for all cells. set_* functions for table properties are always called like set_xxx(ht, value). We‚Äôll learn more about this interface in a moment. There are also four useful convenience functions: set_all_borders sets left, right, top and bottom borders for selected cells; set_all_border_colors sets left, right, top and bottom border colors; set_all_padding sets left, right, top and bottom padding (the amount of space between the content and the border); set_outer_borders sets an outer border around a rectangle of cells. 213.3 Getting properties To get the current properties of a huxtable, just use the properties function without the left arrow: {r, results = &#39;markup&#39;, eval=FALSE} italic(ht) position(ht) As before, you can use subsetting to get particular rows or columns: {r, results = &#39;markup&#39;, eval=FALSE} bottom_border(ht)[1:2,] bold(ht)[,&#39;Salary&#39;] "],["editing-content.html", "Chapter 214 Editing content 214.1 Standard subsetting 214.2 Using dplyr with huxtable 214.3 Functions to insert rows, columns and footnotes", " Chapter 214 Editing content 214.1 Standard subsetting You can subset, sort and generally data-wrangle a huxtable just like a normal data frame. Cell and table properties will be carried over into subsets. {r eval=FALSE, include=FALSE, echo=TRUE} # Select columns by name: cars_mpg &lt;- car_ht[, c(&#39;mpg&#39;, &#39;cyl&#39;, &#39;am&#39;)] # Order by number of cylinders: cars_mpg &lt;- cars_mpg[order(cars_mpg$cyl),] cars_mpg &lt;- cars_mpg %&gt;% huxtable::add_rownames(colname = &#39;Car&#39;) %&gt;% huxtable::add_colnames() cars_mpg[1:5,] 214.2 Using dplyr with huxtable You can also use dplyr functions to edit a huxtable: {r eval=FALSE, include=FALSE, echo=TRUE} car_ht &lt;- car_ht %&gt;% huxtable::add_rownames(colname = &#39;Car&#39;) %&gt;% slice(1:10) %&gt;% select(Car, mpg, cyl, hp) %&gt;% arrange(hp) %&gt;% filter(cyl &gt; 4) %&gt;% rename(MPG = mpg, Cylinders = cyl, Horsepower = hp) %&gt;% mutate(kml = MPG/2.82) car_ht &lt;- car_ht %&gt;% set_number_format(1:7, &#39;kml&#39;, 2) %&gt;% set_col_width(c(.35, .15, .15, .15, .2)) %&gt;% set_width(.6) %&gt;% huxtable::add_colnames() car_ht In general it is a good idea to prepare your data first, before styling it. For example, it was easier to sort the cars_mpg data by cylinder, before adding column names to the data frame itself. 214.3 Functions to insert rows, columns and footnotes Huxtable has three convenience functions for adding a row or column to your table: insert_row, insert_column and add_footnote. insert_row and insert_column let you add a single row or column. The after parameter specifies where in the table to do the insertion, i.e. after what row or column number. add_footnote adds a single cell in a new row at the bottom. The cell spans the whole table row, and has a border above. {r eval=FALSE, include=FALSE, echo=TRUE} ht &lt;- insert_row(ht, &#39;Hadley Wickham&#39;, &#39;100000&#39;, after = 3) ht &lt;- add_footnote(ht, &#39;DHJ deserves a pay rise&#39;) ht "],["more-formatting.html", "Chapter 215 More formatting 215.1 Number format 215.2 Automatic formatting 215.3 Escaping HTML or LaTeX 215.4 Width and cell wrapping 215.5 Adding row and column names 215.6 Column and row spans 215.7 Quick themes", " Chapter 215 More formatting 215.1 Number format You can change how huxtable formats numbers using number_format. Set number_format to a number of decimal places (for more advanced options, see the help files). This affects all numbers, or number-like substrings within your cells. {r eval=FALSE, include=FALSE, echo=TRUE} pointy_ht &lt;- hux(c(&#39;Do not pad this.&#39;, 11.003, 300, 12.02, &#39;12.1 **&#39;)) %&gt;% set_all_borders(1) number_format(pointy_ht) &lt;- 3 pointy_ht You can also align columns by decimal places. If you want to do this for a cell, just set the align property to ‚Äò.‚Äô (or whatever you use for a decimal point). {r eval=FALSE, include=FALSE, echo=TRUE} align(pointy_ht)[2:5, ] &lt;- &#39;.&#39; # not the first row pointy_ht There is currently no true way to align cells by the decimal point in HTML, and only limited possibilities in TeX, so this works by right-padding cells with spaces. The output may look better if you use a fixed width font. 215.2 Automatic formatting By default, when you create a huxtable using huxtable or as_huxtable, the package will guess defaults for number formatting and alignment, based on the type of data in your columns. Numeric data will be right-aligned or aligned on the decimal point; character data will be left aligned; and the package will try to set sensible defaults for number formatting. If you want to, you can turn this off with autoformat = FALSE: {r eval=FALSE, include=FALSE, echo=TRUE} my_data &lt;- data.frame( Employee = c(&#39;John Smith&#39;, &#39;Jane Doe&#39;, &#39;David Hugh-Jones&#39;), Salary = c(50000L, 50000L, 40000L), Performance_rating = c(8.9, 9.2, 7.8) ) as_huxtable(my_data, add_colnames = TRUE) # with automatic formatting as_huxtable(my_data, add_colnames = TRUE, autoformat = FALSE) # no automatic formatting 215.3 Escaping HTML or LaTeX By default, HTML or LaTeX code will be escaped: {r eval=FALSE, include=FALSE, echo=TRUE} code_ht &lt;- if (is_latex) hux(c(&#39;Some maths&#39;, &#39;$a^b$&#39;)) else hux(c(&#39;Copyright symbol&#39;, &#39;&amp;copy;&#39;)) code_ht To avoid this, set the escape_contents property to FALSE. {r eval=FALSE, include=FALSE, echo=TRUE} escape_contents(code_ht)[2, 1] &lt;- FALSE code_ht 215.4 Width and cell wrapping You can set table widths using the width property, and column widths using the col_width property. If you use numbers for these, they will be interpreted as proportions of the table width (or for width, a proportion of the width of the surrounding text). If you use character vectors, they must be valid CSS or LaTeX widths. The only unit both systems have in common is pt for points. {r eval=FALSE, include=FALSE, echo=TRUE} width(ht) &lt;- 0.35 col_width(ht) &lt;- c(.7, .3) ht It is best to set table width explicitly, then set column widths as proportions. By default, if a cell contains long contents, it will be stretched. Use the wrap property to allow cell contents to wrap over multiple lines: {r eval=FALSE, include=FALSE, echo=TRUE} ht_wrapped &lt;- ht ht_wrapped[5, 1] &lt;- &#39;David Arthur Shrimpton Hugh-Jones&#39; wrap(ht_wrapped) &lt;- TRUE ht_wrapped 215.5 Adding row and column names Just like data frames, huxtables can have row and column names. Often, we want to add these to the final table. You can do this using either the add_colnames/add_rownames arguments to as_huxtable, or the add_colnames()/add_rownames() functions. (Note that earlier versions of dplyr used to have functions with the same name.) {r eval=FALSE, include=FALSE, echo=TRUE} as_hux(mtcars[1:4, 1:4]) %&gt;% huxtable::add_rownames(colname = &#39;Car name&#39;) %&gt;% huxtable::add_colnames() 215.6 Column and row spans Huxtable cells can span multiple rows or columns, using the colspan and {r # owspan properties. {r eval=FALSE, include=FALSE, echo=TRUE} cars_mpg &lt;- cbind(car_type = rep( , nrow(cars_mpg)), cars_mpg) cars_mpg$car_type[1] &lt;- &#39;Four cylinders&#39; cars_mpg$car_type[13] &lt;- &#39;Six cylinders&#39; cars_mpg$car_type[20] &lt;- &#39;Eight cylinders&#39; rowspan(cars_mpg)[1, 1] &lt;- 12 rowspan(cars_mpg)[13, 1] &lt;- 7 rowspan(cars_mpg)[20, 1] &lt;- 14 cars_mpg &lt;- rbind(c(&#39;&#39;, &#39;List of cars&#39;, &#39;&#39;, &#39;&#39;, &#39;&#39;), cars_mpg) colspan(cars_mpg)[1, 2] &lt;- 4 align(cars_mpg)[1, 2] &lt;- &#39;center&#39; # a little more formatting: cars_mpg &lt;- set_all_padding(cars_mpg, 2) cars_mpg &lt;- set_all_borders(cars_mpg, 1) valign(cars_mpg)[1,] &lt;- &#39;top&#39; col_width(cars_mpg) &lt;- c(.4 , .3 , .1, .1, .1) number_format(cars_mpg)[, 4:5] &lt;- 0 bold(cars_mpg)[1:2, ] &lt;- TRUE bold(cars_mpg)[, 1] &lt;- TRUE if (is_latex) font_size(cars_mpg) &lt;- 10 cars_mpg 215.7 Quick themes Huxtable comes with some predefined themes for formatting. {r eval=FALSE, include=FALSE, echo=TRUE} theme_striped(cars_mpg[14:20,], stripe = &#39;bisque1&#39;, header_col = FALSE, header_row = FALSE) "],["selecting-rows-columns-and-cells.html", "Chapter 216 Selecting rows, columns and cells 216.1 Row and column functions 216.2 Conditional formatting", " Chapter 216 Selecting rows, columns and cells 216.1 Row and column functions If you use the set_* style functions, huxtable has some convenience functions for selecting rows and columns. To select all rows, or all columns, use everywhere in the row or column specification. To select just even or odd-numbered rows or columns, use evens or odds. To select the last n rows or columns, use final(n). To select every nth row, use every(n) and to do this starting from row m use every(n, from = m). With these functions it is easy to add striped backgrounds to tables: {r eval=FALSE, include=FALSE, echo=TRUE} car_ht %&gt;% set_background_color(evens, everywhere, &#39;wheat&#39;) %&gt;% set_background_color(odds, everywhere, grey(.9)) %&gt;% set_bold(1, everywhere, TRUE) Of course you could also just do 1:nrow(car_ht), but, in the middle of a dplyr pipe, you may not know exactly how many rows or columns you have. Also, these functions make your code easy to read. You can also use dplyr functions like starts_with(), contains(), and matches() to specify columns by column name. For a full list of these functions, see ?select_helpers. {r eval=FALSE, include=FALSE, echo=TRUE} car_ht %&gt;% set_background_color(everywhere, starts_with(&#39;C&#39;), &#39;orange&#39;) car_ht %&gt;% set_italic(everywhere, matches(&#39;[aeiou]&#39;), TRUE) Note that unlike in dplyr‚Äôs select function, you have to specify rows as well as columns. Lastly, remember that you can set a property for every cell by simply omitting the {r # ow and col arguments, like this: set_background_color(ht, 'orange'). 216.2 Conditional formatting You may want to apply conditional formatting to cells, based on their contents. Suppose we want to display a table of correlations, and to highlight ones which are significant. We can use the where() function to select those cells. {r eval=FALSE, include=FALSE, echo=TRUE} library(psych) data(attitude) att_corr &lt;- corr.test(as.matrix(attitude)) att_hux &lt;- as_hux(att_corr$r) %&gt;% # selects cells with p &lt; 0.05: set_background_color(where(att_corr$p &lt; 0.05), &#39;yellow&#39;) %&gt;% # selects cells with p &lt; 0.01: set_background_color(where(att_corr$p &lt; 0.01), &#39;orange&#39;) %&gt;% set_text_color(where(row(att_corr$r) == col(att_corr$r)), &#39;grey&#39;) att_hux &lt;- att_hux %&gt;% huxtable::add_rownames() %&gt;% huxtable::add_colnames() %&gt;% set_caption(&#39;Correlations in attitudes among 30 departments&#39;) %&gt;% set_bold(1, everywhere, TRUE) %&gt;% set_bold(everywhere, 1, TRUE) %&gt;% set_all_borders(1) %&gt;% set_number_format(2) %&gt;% set_position(&#39;left&#39;) att_hux We have now seen three ways to call set_* functions in huxtable: With four arguments, like set_property(hux_object, rows, cols, value); With two arguments, like set_property(hux_object, value) to set a property everywhere; With three arguments, like set_property(hux_object, where(condition), value) to set a property for specific cells. The second argument of the three-argument version must return a 2-column matrix. Each row of the matrix gives one cell. where() does this for you: it takes a logical matrix argument and returns the rows and columns where a condition is TRUE. It‚Äôs easiest to show this with an example: {r eval=FALSE, include=FALSE, echo=TRUE} m &lt;- matrix(c(&#39;dog&#39;, &#39;cat&#39;, &#39;dog&#39;, &#39;dog&#39;, &#39;cat&#39;, &#39;cat&#39;, &#39;cat&#39;, &#39;dog&#39;), 4, 2) m where(m == &#39;dog&#39;) # m is equal to &#39;dog&#39; in cells (1, 1), (3, 1), (4, 1) and (4, 2): set_* functions have one more optional argument, the byrow argument, which is FALSE by default. If you set a single pattern for many cells, you may want the pattern to fill the matrix by column or by row. The default fills the pattern in going down columns. If you set byrow = TRUE, the pattern goes across rows instead. (This is a bit confusing: typically, byrow = TRUE means that the columns will all look the same. But it works the same way as the byrow argument to matrix().) {r eval=FALSE, include=FALSE, echo=TRUE} color_demo &lt;- matrix(&#39;text&#39;, 7, 7) rainbow &lt;- c(&#39;red&#39;, &#39;orange&#39;, &#39;yellow&#39;, &#39;green&#39;, &#39;blue&#39;, &#39;turquoise&#39;, &#39;violet&#39;) color_demo &lt;- as_hux(color_demo) %&gt;% set_text_color(rainbow) %&gt;% # text rainbow down columns set_background_color(rainbow, byrow = TRUE) %&gt;% # background color rainbow along rows set_all_borders(1) %&gt;% set_all_border_colors(&#39;white&#39;) color_demo "],["creating-a-regression-table.html", "Chapter 217 Creating a regression table", " Chapter 217 Creating a regression table A common task for scientists is to create a table of regressions. The function huxreg does this for you. Here‚Äôs a quick example: {r eval=FALSE, include=FALSE, echo=TRUE} data(diamonds, package = &#39;ggplot2&#39;) lm1 &lt;- lm(price ~ carat, diamonds) lm2 &lt;- lm(price ~ depth, diamonds) lm3 &lt;- lm(price ~ carat + depth, diamonds) huxreg(lm1, lm2, lm3) For more information see the huxreg vignette, available online in HTML or PDF or in R via vignette('huxreg'). "],["output-to-different-formats.html", "Chapter 218 Output to different formats 218.1 Automatic pretty-printing of data frames 218.2 Using huxtables in knitr and rmarkdown 218.3 Quick output commands", " Chapter 218 Output to different formats 218.1 Automatic pretty-printing of data frames If you load huxtable within a knitr document, it will automatically format data frames for you by installing a knit_print.data_frame command. {r, echo = TRUE, eval=FALSE} options(huxtable.knit_print_df = TRUE) {r eval=FALSE, include=FALSE, echo=TRUE} head(mtcars) If you don‚Äôt want this (e.g. if you want to use knitr::kable or the printr package, then you can turn it off like this: {r eval=FALSE, include=FALSE, echo=TRUE} options(huxtable.knit_print_df = FALSE) head(mtcars) # back to normal 218.2 Using huxtables in knitr and rmarkdown If you use knitr and rmarkdown in RStudio, huxtable objects should automatically display in the appropriate format (HTML or LaTeX). You need to have some LaTeX packages installed for huxtable to work. To find out what these are, you can call {r # eport_latex_dependencies(). This will print out and/or return a set of usepackage{...} statements. If you use Sweave or knitr without rmarkdown, you can use this function in your LaTeX preamble to load the packages you need. Rmarkdown exports to Word via Markdown. You can use huxtable to do this, but since Markdown tables are rather basic, a lot of formatting will be lost. If you want to create Word or Powerpoint documents directly, install the flextable package from CRAN. You can then convert your huxtable objects to flextable objects and include them in Word or Powerpoint documents. Almost all formatting should work. See the flextable and officer documentation and ?as_flextable for more details. Similarly, to create formatted reports in Excel, install the openxlsx package. You can then use as_Workbook to convert your huxtables to Workbook objects, and save them using openxlsx::saveWorkbook. Sometimes you may want to select how huxtable objects are printed by default. For example, in an RStudio notebook (a .Rmd document with output_format = html_notebook), huxtable can‚Äôt automatically work out what format to use, as of the time of writing. You can set it manually using options(huxtable.print = print_notebook) which prints out HTML in an appropriate format. You can print a huxtable on screen using print_screen (or just by typing its name at the command line.) Borders, column and row spans and cell alignment are shown. If the crayon package is installed, and your terminal or R IDE supports it, border, text and background colours are also displayed. {r, results = &#39;markup&#39;, eval=FALSE} print_screen(ht) If you need to output to another format, file an issue request on Github. 218.3 Quick output commands Sometimes you quickly want to get your data into a Word, HTML or PDF document. To do this you can use the quick_docx, quick_html, quick_pdf and quick_xlsx functions. These are called with one or more huxtable objects, or objects which can be turned into a huxtable such as data frames. A new document of the appropriate type will be created. By default the file will be in the current directory under the name e.g. huxtable-output.pdf. If the file already exists, you‚Äôll be asked to confirm the overwrite. For non-interactive use, you must specify a filename yourself explicitly ‚Äì this keeps you from accidentally trashing your files. {r, eval = FALSE} quick_pdf(mtcars) quick_pdf(mtcars, file = &#39;motorcars data.pdf&#39;) "],["end-matter.html", "Chapter 219 End matter", " Chapter 219 End matter For more information, see the website or github. "],["hypothesis-testing-1.html", "Chapter 220 Hypothesis Testing", " Chapter 220 Hypothesis Testing "],["glue.html", "Chapter 221 Glue", " Chapter 221 Glue Glue strings to data in R. Small, fast, dependency free interpreted string literals. https://glue.tidyverse.org/ "],["infer-package.html", "Chapter 222 infer package", " Chapter 222 infer package Statistical Inference: A Tidy Approach https://ismayc.github.io/talks/ness-infer/slide_deck.html https://infer.netlify.com/ https://moderndive.netlify.com/ https://cran.r-project.org/web/packages/infer/index.html "],["hypothesis-testing-2.html", "Chapter 223 Hypothesis Testing 223.1 Test Selection", " Chapter 223 Hypothesis Testing 223.1 Test Selection 223.1.1 Statkat https://statkat.com/ https://statkat.com/stattest_overview.php 223.1.1.1 Jamovi Statkat module https://blog.jamovi.org/2018/06/25/statkat.html {r eval=FALSE, include=FALSE, echo=TRUE} library(jmv) library(Statkat) {r eval=FALSE, include=FALSE, echo=TRUE} Statkat::correlational( data = data, dep = len , independents = supp ) "],["infer-2.html", "Chapter 224 infer", " Chapter 224 infer Full infer pipeline examples using nycflights13 flights data https://cran.r-project.org/web/packages/infer/vignettes/observed_stat_examples.html Full infer pipeline examples using nycflights13 flights data Chester Ismay Updated on 2018-06-14 Data preparation library(nycflights13) library(dplyr) library(ggplot2) library(stringr) library(infer) set.seed(2017) fli_small &lt;- flights %&gt;% na.omit() %&gt;% sample_n(size = 500) %&gt;% mutate(season = case_when( month %in% c(10:12, 1:3) ~ winter , month %in% c(4:9) ~ summer )) %&gt;% mutate(day_hour = case_when( between(hour, 1, 12) ~ morning , between(hour, 13, 24) ~ not morning )) %&gt;% select(arr_delay, dep_delay, season, day_hour, origin, carrier) Two numeric - arr_delay, dep_delay Two categories season ( winter , summer ), day_hour ( morning , not morning ) Three categories - origin ( EWR , JFK , LGA ) Sixteen categories - carrier Hypothesis tests One numerical variable (mean) Observed stat ( x_bar &lt;- fli_small %&gt;% specify(response = dep_delay) %&gt;% calculate(stat = mean ) ) stat 10.4 null_distn &lt;- fli_small %&gt;% specify(response = dep_delay) %&gt;% hypothesize(null = point , mu = 10) %&gt;% generate(reps = 1000) %&gt;% calculate(stat = mean ) ## Setting type = bootstrap in generate(). visualize(null_distn) + shade_p_value(obs_stat = x_bar, direction = two_sided ) null_distn %&gt;% get_p_value(obs_stat = x_bar, direction = two_sided ) p_value 0.794 One numerical variable (standardized mean t) Observed stat ( t_bar &lt;- fli_small %&gt;% specify(response = dep_delay) %&gt;% calculate(stat = t ) ) stat 6.93 null_distn &lt;- fli_small %&gt;% specify(response = dep_delay) %&gt;% hypothesize(null = point , mu = 8) %&gt;% generate(reps = 1000) %&gt;% calculate(stat = t ) ## Setting type = bootstrap in generate(). visualize(null_distn) + shade_p_value(obs_stat = t_bar, direction = two_sided ) null_distn %&gt;% get_p_value(obs_stat = t_bar, direction = two_sided ) p_value 0 One numerical variable (median) Observed stat ( x_tilde &lt;- fli_small %&gt;% specify(response = dep_delay) %&gt;% calculate(stat = median ) ) stat -2 null_distn &lt;- fli_small %&gt;% specify(response = dep_delay) %&gt;% hypothesize(null = point , med = -1) %&gt;% generate(reps = 1000) %&gt;% calculate(stat = median ) ## Setting type = bootstrap in generate(). visualize(null_distn) + shade_p_value(obs_stat = x_tilde, direction = two_sided ) null_distn %&gt;% get_p_value(obs_stat = x_tilde, direction = two_sided ) p_value 0.15 One categorical (one proportion) Observed stat ( p_hat &lt;- fli_small %&gt;% specify(response = day_hour, success = morning ) %&gt;% calculate(stat = prop ) ) stat 0.466 null_distn &lt;- fli_small %&gt;% specify(response = day_hour, success = morning ) %&gt;% hypothesize(null = point , p = .5) %&gt;% generate(reps = 1000) %&gt;% calculate(stat = prop ) ## Setting type = simulate in generate(). visualize(null_distn) + shade_p_value(obs_stat = p_hat, direction = two_sided ) null_distn %&gt;% get_p_value(obs_stat = p_hat, direction = two_sided ) p_value 0.11 Logical variables will be coerced to factors: null_distn &lt;- fli_small %&gt;% mutate(day_hour_logical = (day_hour == morning )) %&gt;% specify(response = day_hour_logical, success = TRUE ) %&gt;% hypothesize(null = point , p = .5) %&gt;% generate(reps = 1000) %&gt;% calculate(stat = prop ) ## Setting type = simulate in generate(). One categorical variable (standardized proportion z) Not yet implemented. Two categorical (2 level) variables Observed stat ( d_hat &lt;- fli_small %&gt;% specify(day_hour ~ season, success = morning ) %&gt;% calculate(stat = diff in props , order = c( winter , summer )) ) stat -0.0205 null_distn &lt;- fli_small %&gt;% specify(day_hour ~ season, success = morning ) %&gt;% hypothesize(null = independence ) %&gt;% generate(reps = 1000) %&gt;% calculate(stat = diff in props , order = c( winter , summer )) ## Setting type = permute in generate(). visualize(null_distn) + shade_p_value(obs_stat = d_hat, direction = two_sided ) null_distn %&gt;% get_p_value(obs_stat = d_hat, direction = two_sided ) p_value 0.708 Two categorical (2 level) variables (z) Standardized observed stat ( z_hat &lt;- fli_small %&gt;% specify(day_hour ~ season, success = morning ) %&gt;% calculate(stat = z , order = c( winter , summer )) ) stat -0.4605 null_distn &lt;- fli_small %&gt;% specify(day_hour ~ season, success = morning ) %&gt;% hypothesize(null = independence ) %&gt;% generate(reps = 1000) %&gt;% calculate(stat = z , order = c( winter , summer )) ## Setting type = permute in generate(). visualize(null_distn) + shade_p_value(obs_stat = z_hat, direction = two_sided ) null_distn %&gt;% get_p_value(obs_stat = z_hat, direction = two_sided ) p_value 0.684 Note the similarities in this plot and the previous one. One categorical (&gt;2 level) - GoF Observed stat Note the need to add in the hypothesized values here to compute the observed statistic. ( Chisq_hat &lt;- fli_small %&gt;% specify(response = origin) %&gt;% hypothesize(null = point , p = c( EWR = .33, JFK = .33, LGA = .34)) %&gt;% calculate(stat = Chisq ) ) stat 10.4 null_distn &lt;- fli_small %&gt;% specify(response = origin) %&gt;% hypothesize(null = point , p = c( EWR = .33, JFK = .33, LGA = .34)) %&gt;% generate(reps = 1000, type = simulate ) %&gt;% calculate(stat = Chisq ) visualize(null_distn) + shade_p_value(obs_stat = Chisq_hat, direction = greater ) null_distn %&gt;% get_p_value(obs_stat = Chisq_hat, direction = greater ) p_value 0.005 Two categorical (&gt;2 level) variables Observed stat ( Chisq_hat &lt;- fli_small %&gt;% specify(formula = day_hour ~ origin) %&gt;% calculate(stat = Chisq ) ) stat 9.027 null_distn &lt;- fli_small %&gt;% specify(day_hour ~ origin) %&gt;% hypothesize(null = independence ) %&gt;% generate(reps = 1000, type = permute ) %&gt;% calculate(stat = Chisq ) visualize(null_distn) + shade_p_value(obs_stat = Chisq_hat, direction = greater ) null_distn %&gt;% get_p_value(obs_stat = Chisq_hat, direction = greater ) p_value 0.007 One numerical variable, one categorical (2 levels) (diff in means) Observed stat ( d_hat &lt;- fli_small %&gt;% specify(dep_delay ~ season) %&gt;% calculate(stat = diff in means , order = c( summer , winter )) ) stat 2.266 null_distn &lt;- fli_small %&gt;% specify(dep_delay ~ season) %&gt;% hypothesize(null = independence ) %&gt;% generate(reps = 1000, type = permute ) %&gt;% calculate(stat = diff in means , order = c( summer , winter )) visualize(null_distn) + shade_p_value(obs_stat = d_hat, direction = two_sided ) null_distn %&gt;% get_p_value(obs_stat = d_hat, direction = two_sided ) p_value 0.488 One numerical variable, one categorical (2 levels) (t) Standardized observed stat ( t_hat &lt;- fli_small %&gt;% specify(dep_delay ~ season) %&gt;% calculate(stat = t , order = c( summer , winter )) ) stat 0.7542 null_distn &lt;- fli_small %&gt;% specify(dep_delay ~ season) %&gt;% hypothesize(null = independence ) %&gt;% generate(reps = 1000, type = permute ) %&gt;% calculate(stat = t , order = c( summer , winter )) visualize(null_distn) + shade_p_value(obs_stat = t_hat, direction = two_sided ) null_distn %&gt;% get_p_value(obs_stat = t_hat, direction = two_sided ) p_value 0.49 Note the similarities in this plot and the previous one. One numerical variable, one categorical (2 levels) (diff in medians) Observed stat ( d_hat &lt;- fli_small %&gt;% specify(dep_delay ~ season) %&gt;% calculate(stat = diff in medians , order = c( summer , winter )) ) stat 2 null_distn &lt;- fli_small %&gt;% specify(dep_delay ~ season) %&gt;% # alt: response = dep_delay, # explanatory = season hypothesize(null = independence ) %&gt;% generate(reps = 1000, type = permute ) %&gt;% calculate(stat = diff in medians , order = c( summer , winter )) visualize(null_distn) + shade_p_value(obs_stat = d_hat, direction = two_sided ) null_distn %&gt;% get_p_value(obs_stat = d_hat, direction = two_sided ) p_value 0.084 One numerical, one categorical (&gt;2 levels) - ANOVA Observed stat ( F_hat &lt;- fli_small %&gt;% specify(arr_delay ~ origin) %&gt;% calculate(stat = F ) ) stat 1.084 null_distn &lt;- fli_small %&gt;% specify(arr_delay ~ origin) %&gt;% hypothesize(null = independence ) %&gt;% generate(reps = 1000, type = permute ) %&gt;% calculate(stat = F ) visualize(null_distn) + shade_p_value(obs_stat = F_hat, direction = greater ) null_distn %&gt;% get_p_value(obs_stat = F_hat, direction = greater ) p_value 0.353 Two numerical vars - SLR Observed stat ( slope_hat &lt;- fli_small %&gt;% specify(arr_delay ~ dep_delay) %&gt;% calculate(stat = slope ) ) stat 1.017 null_distn &lt;- fli_small %&gt;% specify(arr_delay ~ dep_delay) %&gt;% hypothesize(null = independence ) %&gt;% generate(reps = 1000, type = permute ) %&gt;% calculate(stat = slope ) visualize(null_distn) + shade_p_value(obs_stat = slope_hat, direction = two_sided ) null_distn %&gt;% get_p_value(obs_stat = slope_hat, direction = two_sided ) p_value 0 Two numerical vars - correlation Observed stat ( correlation_hat &lt;- fli_small %&gt;% specify(arr_delay ~ dep_delay) %&gt;% calculate(stat = correlation ) ) stat 0.8943 null_distn &lt;- fli_small %&gt;% specify(arr_delay ~ dep_delay) %&gt;% hypothesize(null = independence ) %&gt;% generate(reps = 1000, type = permute ) %&gt;% calculate(stat = correlation ) visualize(null_distn) + shade_p_value(obs_stat = correlation_hat, direction = two_sided ) null_distn %&gt;% get_p_value(obs_stat = correlation_hat, direction = two_sided ) p_value 0 Two numerical vars - SLR (t) Not currently implemented since t could refer to standardized slope or standardized correlation. Confidence intervals One numerical (one mean) Point estimate ( x_bar &lt;- fli_small %&gt;% specify(response = arr_delay) %&gt;% calculate(stat = mean ) ) stat 4.572 boot &lt;- fli_small %&gt;% specify(response = arr_delay) %&gt;% generate(reps = 1000, type = bootstrap ) %&gt;% calculate(stat = mean ) ( percentile_ci &lt;- get_ci(boot) ) 2.5% 97.5% 1.436 7.819 visualize(boot) + shade_confidence_interval(endpoints = percentile_ci) ( standard_error_ci &lt;- get_ci(boot, type = se , point_estimate = x_bar) ) lower upper 1.267 7.877 visualize(boot) + shade_confidence_interval(endpoints = standard_error_ci) One numerical (one mean - standardized) Point estimate ( t_hat &lt;- fli_small %&gt;% specify(response = arr_delay) %&gt;% calculate(stat = t ) ) stat 2.679 boot &lt;- fli_small %&gt;% specify(response = arr_delay) %&gt;% generate(reps = 1000, type = bootstrap ) %&gt;% calculate(stat = t ) ( percentile_ci &lt;- get_ci(boot) ) 2.5% 97.5% 0.9338 4.362 visualize(boot) + shade_confidence_interval(endpoints = percentile_ci) ( standard_error_ci &lt;- get_ci(boot, type = se , point_estimate = t_hat) ) lower upper 0.9141 4.444 visualize(boot) + shade_confidence_interval(endpoints = standard_error_ci) One categorical (one proportion) Point estimate ( p_hat &lt;- fli_small %&gt;% specify(response = day_hour, success = morning ) %&gt;% calculate(stat = prop ) ) stat 0.466 boot &lt;- fli_small %&gt;% specify(response = day_hour, success = morning ) %&gt;% generate(reps = 1000, type = bootstrap ) %&gt;% calculate(stat = prop ) ( percentile_ci &lt;- get_ci(boot) ) 2.5% 97.5% 0.42 0.508 visualize(boot) + shade_confidence_interval(endpoints = percentile_ci) ( standard_error_ci &lt;- get_ci(boot, type = se , point_estimate = p_hat) ) lower upper 0.4218 0.5102 visualize(boot) + shade_confidence_interval(endpoints = standard_error_ci) One categorical variable (standardized proportion z) Not yet implemented. One numerical variable, one categorical (2 levels) (diff in means) Point estimate ( d_hat &lt;- fli_small %&gt;% specify(arr_delay ~ season) %&gt;% calculate(stat = diff in means , order = c( summer , winter )) ) stat -0.7452 boot &lt;- fli_small %&gt;% specify(arr_delay ~ season) %&gt;% generate(reps = 1000, type = bootstrap ) %&gt;% calculate(stat = diff in means , order = c( summer , winter )) ( percentile_ci &lt;- get_ci(boot) ) 2.5% 97.5% -7.167 6.079 visualize(boot) + shade_confidence_interval(endpoints = percentile_ci) ( standard_error_ci &lt;- get_ci(boot, type = se , point_estimate = d_hat) ) lower upper -7.296 5.806 visualize(boot) + shade_confidence_interval(endpoints = standard_error_ci) One numerical variable, one categorical (2 levels) (t) Standardized point estimate ( t_hat &lt;- fli_small %&gt;% specify(arr_delay ~ season) %&gt;% calculate(stat = t , order = c( summer , winter )) ) stat -0.2182 boot &lt;- fli_small %&gt;% specify(arr_delay ~ season) %&gt;% generate(reps = 1000, type = bootstrap ) %&gt;% calculate(stat = t , order = c( summer , winter )) ( percentile_ci &lt;- get_ci(boot) ) 2.5% 97.5% -2.236 1.718 visualize(boot) + shade_confidence_interval(endpoints = percentile_ci) ( standard_error_ci &lt;- get_ci(boot, type = se , point_estimate = t_hat) ) lower upper -2.183 1.746 visualize(boot) + shade_confidence_interval(endpoints = standard_error_ci) Two categorical variables (diff in proportions) Point estimate ( d_hat &lt;- fli_small %&gt;% specify(day_hour ~ season, success = morning ) %&gt;% calculate(stat = diff in props , order = c( summer , winter )) ) stat 0.0205 boot &lt;- fli_small %&gt;% specify(day_hour ~ season, success = morning ) %&gt;% generate(reps = 1000, type = bootstrap ) %&gt;% calculate(stat = diff in props , order = c( summer , winter )) ( percentile_ci &lt;- get_ci(boot) ) 2.5% 97.5% -0.0648 0.1083 visualize(boot) + shade_confidence_interval(endpoints = percentile_ci) ( standard_error_ci &lt;- get_ci(boot, type = se , point_estimate = d_hat) ) lower upper -0.0676 0.1087 visualize(boot) + shade_confidence_interval(endpoints = standard_error_ci) Two categorical variables (z) Standardized point estimate ( z_hat &lt;- fli_small %&gt;% specify(day_hour ~ season, success = morning ) %&gt;% calculate(stat = z , order = c( summer , winter )) ) stat 0.4605 boot &lt;- fli_small %&gt;% specify(day_hour ~ season, success = morning ) %&gt;% generate(reps = 1000, type = bootstrap ) %&gt;% calculate(stat = z , order = c( summer , winter )) ( percentile_ci &lt;- get_ci(boot) ) 2.5% 97.5% -1.479 2.501 visualize(boot) + shade_confidence_interval(endpoints = percentile_ci) ( standard_error_ci &lt;- get_ci(boot, type = se , point_estimate = z_hat) ) lower upper -1.522 2.443 visualize(boot) + shade_confidence_interval(endpoints = standard_error_ci) Two numerical vars - SLR Point estimate ( slope_hat &lt;- fli_small %&gt;% specify(arr_delay ~ dep_delay) %&gt;% calculate(stat = slope ) ) stat 1.017 boot &lt;- fli_small %&gt;% specify(arr_delay ~ dep_delay) %&gt;% generate(reps = 1000, type = bootstrap ) %&gt;% calculate(stat = slope ) ( percentile_ci &lt;- get_ci(boot) ) 2.5% 97.5% 0.9728 1.074 visualize(boot) + shade_confidence_interval(endpoints = percentile_ci) ( standard_error_ci &lt;- get_ci(boot, type = se , point_estimate = slope_hat) ) lower upper 0.9653 1.069 visualize(boot) + shade_confidence_interval(endpoints = standard_error_ci) Two numerical vars - correlation Point estimate ( correlation_hat &lt;- fli_small %&gt;% specify(arr_delay ~ dep_delay) %&gt;% calculate(stat = correlation ) ) stat 0.8943 boot &lt;- fli_small %&gt;% specify(arr_delay ~ dep_delay) %&gt;% generate(reps = 1000, type = bootstrap ) %&gt;% calculate(stat = correlation ) ( percentile_ci &lt;- get_ci(boot) ) 2.5% 97.5% 0.8502 0.9218 visualize(boot) + shade_confidence_interval(endpoints = percentile_ci) ( standard_error_ci &lt;- get_ci(boot, type = se , point_estimate = correlation_hat) ) lower upper 0.858 0.9306 visualize(boot) + shade_confidence_interval(endpoints = standard_error_ci) Two numerical vars - t Not currently implemented since t could refer to standardized slope or standardized correlation. "],["keras.html", "Chapter 225 Keras", " Chapter 225 Keras https://keras.io/ "],["k-means-clustering-1.html", "Chapter 226 K Means Clustering", " Chapter 226 K Means Clustering https://datascienceplus.com/k-means-clustering-in-r/ "],["lessr.html", "Chapter 227 lessR", " Chapter 227 lessR lessRstats.com/lessR.r tour of lessR functions for data analysis Purpose: Provide basic statistical computations for the analyses presented in intro stat texts and more Get R http://r-project.org one time only, to get the lessR functions onto your computer if asked to install into a personal library, say Yes {r eval=FALSE, include=FALSE, echo=TRUE} install.packages( lessR ) Begin each R session by loading the lessR functions {r eval=FALSE, include=FALSE, echo=TRUE} library(lessR) help {r eval=FALSE, include=FALSE, echo=TRUE} Help() # list the lessR help values by subject Help(lessR) # access to the full lessR manual and News items {r eval=FALSE, include=FALSE, echo=TRUE} Help(Read) read data from one of many different formats into a data table called mydata with the same Read statement: csv, tab-delimited text, Excel, SPSS, and SAS Categorical: Gender coded M, F; Dept coded ACCT, ADMN, FINC, MKTG, SALE Numeric: Salary and Years mydata &lt;- Read() or {r # d(), browse for text, Excel, SPSS, SAS, or R data file {r eval=FALSE, include=FALSE, echo=TRUE} head(mydata) # R function, see variable names and first 6 rows of data {r eval=FALSE, include=FALSE, echo=TRUE} mydata &lt;- Read( http://lessRstats.com/data/employee.csv ) # read data from web {r eval=FALSE, include=FALSE, echo=TRUE} mydata &lt;- Read( http://lessRstats.com/data/employee.csv , row.names=1) # row names data included as part of lessR {r eval=FALSE, include=FALSE, echo=TRUE} mydata &lt;- rd( Employee , format= lessR ) # this example includes variable labels {r eval=FALSE, include=FALSE, echo=TRUE} details() # provides full details of the data frame, defaults to mydata {r eval=FALSE, include=FALSE, echo=TRUE} db() brief version, same as provided by Read can also read a text file o fixed width formatted data example provided later distribution of a categorical variable all the same BarChart(Dept) or bc(Dept) bc(Dept) all function calls have a 2 or 3 digit abbreviation BarChart(Dept, data=mydata) mydata is the default data frame (table) bc(Dept, fill= colors ) specify a more colorful display bc(Dept, rotate.x=45, offset=1) value labels rotated and offset other summaries PieChart(Dept) or pc(Dept), provides a doughnut or ring chart PieChart(Dept, hole=0) full pie chart Plot(Dept) or Plot(Dept), 1-categorical variable bubble plot SummaryStats(Dept) or ss(Dept), no graphics read Mach IV data, integer Likert responses from 0 to 5 mydata &lt;- Read( Mach4 , format= lessR ) read a data file included with lessR form a Bubble Plot Frequency Matrix (BPFM) from a range of x-variables each line is a bubble plot of frequencies for a single variable Plot(c(m06,m07,m09,m10)) for each bubble, lighten fill color, make border black Plot(m06:m12, trans=.8, coloe= gray50 ) create BPFM for entire Mach IV scale and store as a pdf file LikertCats &lt;- c( Strongly Disagree , Disagree , Slightly Disagree , Slightly Agree , Agree , Strongly Agree ) Plot(m01:m20, value.labels=LikertCats, pdf.file= MachFreqs.pdf ) Plot(m06) Integrated Violin/Box/Scatterplot (VBS plot) back to the Employee data mydata &lt;- rd( Employee , format= lessR ) includes variable labels distribution of a numeric (continuous) variable Plot(Salary) Integrated Violin/Box/Scatterplot (VBS plot) Plot(Salary, by1=Gender) Trellis VBS plots for Gender Histogram(Salary) or hs(Salary) hs(Salary, bin.start=30000, bin.width=12000, bin.end=150000) common options Density(Salary) or dn(Salary) BoxPlot(Salary) or bx(Salary), a call to Plot with vbs.plot= b SummaryStats(Salary) or ss(Salary) ss.brief(Salary) brief output hs(Salary, Rmd= Salary ) generate an R markdown file of all the methods hs(Salry) ERROR, misspelled variable name to show lessR error messages for ordered values, such as by time (just for illustration here) LineChart(Salary) or lc(Salary) Plot(Salary, run=TRUE) Plot(Salary, run=TRUE, center.line= off , show.runs=TRUE) Plot(Salary, run=TRUE, lwd=0) points only Plot(Salary, run=TRUE, size=0) line only Plot(Salary, run=TRUE, area=TRUE) distributions for multiple variables (writes to pdf files) the corresponding standard R functions do not take multiple variables specify multiple variables with c function or : Histogram(c(Salary, Years)) c is R combine function BarChart(Gender:Satisfaction) : indicates a range of variables in R Histogram() a histogram for each numeric variable in mydata data table CountAll() or ca(), a bar chart or histogram for each variable in mydata relationship of two variables two numeric variables yields traditional scatter plot Plot(Years, Salary) or Plot(Years, Salary) Plot(Years, Salary, auto=TRUE) or Plot(Years, Salary) Plot(Years, Salary, ellipse=TRUE, fit= loess ) common options Plot(Years, Salary, ellipse=c(.50,.75,.90)) 3 data ellipses Plot(Years, Salary, size=3) bubbles classify by a 3rd variable, which is categorical Plot(Years, Salary, by=Gender) Plot(Years, Salary, by=Gender, color=c( green , brown )) Plot(Years, Salary, by=Gender, color=c( darkgreen , brown ), shape=c( F , M )) Plot(Years, Salary, by=Gender, color=c( darkgreen , brown ), fit= ls ) categorical with a numeric variable, shows scatter about each group mean Plot(Dept, Salary) Dept is categorical, Salary is numeric SummaryStats(Salary, by=Dept) stats, for each level of a grouping variable ss.brief(Salary, by=Dept) brief output two categorical variables (factors) BarChart(Gender, by=Dept) text from ss.brief bc(Gender, by=Dept, beside=TRUE, horiz=TRUE) common options Plot(Gender, Dept) scatter (bubble) plot in place of traditional bar chart SummaryStats(Gender, Dept) 4 cross-tab tables bc(Dept, by=Gender, beside=TRUE) the proportion of data values by fill variable within each group bc(Dept, by=Gender, proportion=TRUE) Plot(Dept, Salary) Plot refers to what is plotted in the scatter plot as the values Plot default for values is data Plot(Dept, Salary, values= mean ) Plot(c(Pre, Post), Salary) Plot(c(Pre, Post), Salary, fit= ls ) Cleveland dot plot test Plot(Salary, row.names) Plot(c(Pre, Post), row.names) Plot(row.names, Salary, means=FALSE, rotate.x=60, offset=1, xlab= ) Plot(Salary, row.names, fill= black , color= black , sort.y=TRUE) style(panel.fill= off , grid.color= off ) Plot(Salary, row.names, sort.y=TRUE, segments.y=TRUE) style() return to default style set system values color themes Help(theme) style( orange , sub.theme= black ) hs(Salary) Plot(Years, Salary, ellipse=TRUE, fit= loess ) common options style( darkred ) hs(Salary) style( gray ) hs(Salary) Plot(Years, Salary, ellipse=TRUE, fit= loess ) common options more system settings, can also be set by individual function calls style(brief=TRUE) partial text output style(quiet=TRUE) no text output style(n.cat=6) treat integer variables with &lt;=6 unique values as categorical style(brief=FALSE) back to default style(quiet=FALSE) style(n.cat=0) analysis of a mean ttest(Salary) or tt(Salary) for confidence interval tt.brief(Salary, mu0=70000) brief output, for CI and hypothesis test tt.brief(n=37, m=63795.557, s=21799.533) input summary stats instead of data ttestPower(n=20, s=5) compare 2 groups, with graphics ttest(Salary ~ Gender) or tt(Salary ~ Gender), compare means for two groups tt(Gender ~ Salary) ERROR, do it wrong to illustrate lessR error messages tt.brief(Salary ~ Gender) brief output tt(n1=18, n2=19, m1=71147.458, m2=56830.598, s1=23128.436, s2=18438.456) from stats ttestPower(n1=14, n2=27, s1=4, s2=6, msmd=.5) Pre and Post are variables that are two matched sets of data values ttest(Pre, Post) t-test to compare group means, data entered as two vectors ttest(Pre, Post, paired=TRUE) t-test of paired differences analysis of variance, compare 2 or more groups warpbreaks is a data set contained in R ANOVA(breaks ~ tension, data=warpbreaks) or av, one-way ANOVA two-factor between-groups ANOVA with replications and interaction ANOVA(breaks ~ wool * tension, data=warpbreaks) randomized blocks design with the second term the blocking factor ANOVA(breaks ~ wool + tension, data=warpbreaks) regression analysis single predictor variable regression reg(Salary ~ Years) or reg, or reg.brief, equivalent of Excel regression reg.brief(Salary ~ Years) reg.brief equivalent of Excel regression reg(Salary ~ 1) null model reg.brief(Salary ~ 1) null model multiple regression reg(Salary ~ Years + Pre + Post) multiple reg output to an object r &lt;- reg(Salary ~ Years + Pre + Post) multiple reg, save output to object r r see all the output names(r) see the output segments available for viewing or further analysis r$out_estimates see just the piece of the estimates create R markdown file that does full interpretation of the output r &lt;- reg(Salary ~ Years + Pre + Post, Rmd= MultReg ) R markdown nested models analysis Nest(Salary, c(Years), c(Pre, Post)) compare reduced to full model, with common data logit analysis, with a classification table Logit(Gender ~ Salary) transformations use Recode to reverse score four Likert variables: m01, m02, m03, m10 mydata &lt;- Read( Mach4 , format= lessR ) read a data file included with lessR mydata &lt;- Recode(c(m01:m03,m10), old=0:5, new=5:0) R has no recode function mydata &lt;- rd( Employee , format= lessR , quiet=TRUE) internal data, no console output mydata &lt;- Transform(Salary = Salary / 1000) transform by formula males &lt;- Subset(Gender== M , columns=c(Years, Salary)) only Males, two vars mydata &lt;- Sort(Salary) default from smallest to largest mydata &lt;- Sort(Salary, direction= - ) can also specify directiont mydata &lt;- Sort(row.names) unlike standard R, can sort by row name also Merge for vertical or horizontal merge of two data tables variable labels read data file w/o variable labels, and then separately read labels mydata &lt;- Read( http://lessRstats.com/data/employee.csv ) mylabels &lt;- VariableLabels( http://lessRstats.com/data/employee_lbl.csv ) read data file w/o variable labels, and then read labels from console mydata &lt;- Read( http://lessRstats.com/data/employee.csv ) lbl &lt;- Years, time of company employment Gender, Male or Female Dept, department employed Salary, annual salary Satisfaction, satisfaction with work environment HealthPlan, 1=GoodHealth 2=YellowCross 3=BestCare Pre, Test score on legal issues before instruction Post, Test score on legal issues after instruction mylabels &lt;- VariableLabels(lbl) modify/display a single variable label mylabels &lt;- VariableLabels(Salary, Annual Salary (USD) ) add/modify 1 label VariableLabels(Salary) list the contents of a single variable label display all variable labels db() also the variable names and sample values vl() write a data table to an external file Write( myfile ) write default mydata data table to myfile in csv format Write( myfile , row.names=FALSE) preferred if row names are only row numbers Write( myfile , format= Excel , row.names=FALSE) to Excel data table after reading original data and doing the transformations, save as is Write( myfile , format= R ) native R format, straight copy of mydata correlation matrices and factor analysis mydata &lt;- Read( Mach4 , format= lessR ) read a data file included with lessR calculate the correlations and store in mycor head(mydata) mydata &lt;- Recode(c(m03, m04, m06, m07, m09:m11, m14, m16, m17, m19), old=0:5, new=5:0) mycor &lt;- cr(m01:m20) efa(n.factors=4) confirmatory factor analysis of 4-factor solution of Mach IV scale Hunter, Gerbing and Boster (1982) MeasModel &lt;- Deceit =~ m07 + m06 + m10 + m09 Trust =~ m12 + m05 + m13 + m01 Cynicism =~ m11 + m16 + m04 Flattery =~ m15 + m02 c &lt;- cfa(MeasModel) view all the output c names of each output segment names(c) view just the scale reliabilities c$out_reliability correlation matrix operations mycor &lt;- cr(m01:m20, graphics=TRUE) mydata &lt;- corReorder() simple re-order algorithm by highest remaining cor prop() scree() read fixed width data, requires parameters: widths, color.names rep(1,20) standard R function, here a 1 listed 20 times to( m ,20) lessR, names sequential vars with same widths [unlike paste0( m , 1:20)] specify the width of each data column, then match with the column name the Mach4 scale is 6-pt Likert data, so each response is a single column mydata &lt;- Read( http://lessRstats.com/data/Mach4Plus.fwd , widths=c(4,2,1,5,1,rep(1,20)), col.names=c( ID , Age , Gender , Code , Form , to( m ,20))) "],["linear-regression-2.html", "Chapter 228 Linear Regression", " Chapter 228 Linear Regression https://www..com/community/tutorials/linear-regression-R {r, include=FALSE} library(readxl) ageandheight &lt;- read_excel( data/ageandheight.xls , sheet = Hoja2 ) lmHeight = lm(height~age, data = ageandheight) summary(lmHeight) {r eval=FALSE, include=FALSE, echo=TRUE} lmHeight2 = lm(height~age + no_siblings, data = ageandheight) summary(lmHeight2) {r eval=FALSE, include=FALSE, echo=TRUE} lmHeight2$coefficients {r eval=FALSE, include=FALSE, echo=TRUE} lmHeight2$residuals {r eval=FALSE, include=FALSE, echo=TRUE} pressure &lt;- read_excel( data/pressure.xlsx ) lmTemp &lt;- lm(Pressure~Temperature, data = pressure) summary(lmTemp) plot(pressure, pch = 16, col = blue ) abline(lmTemp) {r eval=FALSE, include=FALSE, echo=TRUE} plot(lmTemp$residuals, pch = 16, col = red ) {r eval=FALSE, include=FALSE, echo=TRUE} lmTemp2 = lm(Pressure~Temperature + I(Temperature^2), data = pressure) #Create a linear regression with a quadratic coefficient summary(lmTemp2) #Review the results {r eval=FALSE, include=FALSE, echo=TRUE} plot(lmTemp2$residuals, pch = 16, col = red ) {r eval=FALSE, include=FALSE, echo=TRUE} ageandheight[2, 2] = 7.7 head(ageandheight) {r eval=FALSE, include=FALSE, echo=TRUE} lmHeight3 = lm(height~age, data = ageandheight)#Create the linear regression summary(lmHeight3)#Review the results plot(cooks.distance(lmHeight3), pch = 16, col = blue ) #Plot the Cooks Distances. {r eval=FALSE, include=FALSE, echo=TRUE} plot(cooks.distance(lmHeight), pch = 16, col = blue ) "],["machine-learning.html", "Chapter 229 Machine Learning", " Chapter 229 Machine Learning "],["code-for-workshop-introduction-to-machine-learning-with-r.html", "Chapter 230 Code for Workshop: Introduction to Machine Learning with R", " Chapter 230 Code for Workshop: Introduction to Machine Learning with R https://shirinsplayground.netlify.com/2018/06/intro_to_ml_workshop_heidelberg/ title: R ile analize ba≈ülarken4 {r , echo=TRUE, cache=FALSE} library(knitr) library(rmdformats) ## Global options options(max.print= 75 ) opts_chunk$set(echo=TRUE, cache=TRUE, prompt=FALSE, tidy=TRUE, comment=NA, message=FALSE, warning=FALSE) opts_knit$set(width=75) Bu bir derlemedir, m√ºmk√ºn mertebe alƒ±ntƒ±lara referans vermeye √ßalƒ±≈ütƒ±m.‚Ü©Ô∏é "],["r-hakkƒ±nda.html", "Chapter 231 R hakkƒ±nda", " Chapter 231 R hakkƒ±nda R generation https://rss.onlinelibrary.wiley.com/doi/10.1111/j.1740-9713.2018.01169.x "],["r-y√ºkleme-3.html", "Chapter 232 R y√ºkleme 232.1 R-project 232.2 RStudio 232.3 X11 232.4 Java OS", " Chapter 232 R y√ºkleme http://www.youtube.com/watch?v=XcBLEVknqvY 232.1 R-project https://cran.r-project.org/ 232.2 RStudio https://www.rstudio.com/ https://www.rstudio.com/products/rstudio/download/ https://moderndive.com/2-getting-started.html 232.2.1 RStudio eklentileri Discover and install useful RStudio addins https://cran.r-project.org/web/packages/addinslist/README.html https://rstudio.github.io/rstudioaddins/ {r eval=FALSE, include=FALSE, echo=TRUE} # devtools::install_github( rstudio/addinexamples , type = source ) 232.3 X11 https://www.xquartz.org/ 232.4 Java OS https://support.apple.com/kb/dl1572 "],["r-zor-≈üeyler-i√ßin-kolay-kolay-≈üeyler-i√ßin-zor-3.html", "Chapter 233 R zor ≈üeyler i√ßin kolay, kolay ≈üeyler i√ßin zor", " Chapter 233 R zor ≈üeyler i√ßin kolay, kolay ≈üeyler i√ßin zor R makes easy things hard, and hard things easy Aynƒ± ≈üeyi √ßok fazla ≈üekilde yapmak m√ºmk√ºn R Syntax Comparison::CHEAT SHEET https://www.amelia.mn/Syntax-cheatsheet.pdf "],["r-paketleri-3.html", "Chapter 234 R paketleri 234.1 Neden paketler var 234.2 Paketleri nereden bulabiliriz 234.3 Kendi paket evrenini olu≈ütur 234.4 R i√ßin yardƒ±m bulma 234.5 R paket y√ºkleme", " Chapter 234 R paketleri 234.1 Neden paketler var I love the #rstats community.Someone is like, ‚Äúoh hey peeps, I saw a big need for this mundane but difficult task that I infrequently do, so I created a package that will literally scrape the last bits of peanut butter out of the jar for you. It's called pbplyr.‚ÄùWhat a tribe. ‚Äî Frank Elavsky   ≥ ((???)) July 3, 2018 https://blog.mitchelloharawild.com/blog/user-2018-feature-wall/ 234.2 Paketleri nereden bulabiliriz Available CRAN Packages By Name https://cran.r-project.org/web/packages/available_packages_by_name.html Bioconductor https://www.bioconductor.org RecommendR http://recommendr.info/ pkgsearch CRAN package search https://github.com/metacran/pkgsearch Awesome R https://awesome-r.com/ 234.3 Kendi paket evrenini olu≈ütur pkgverse: Build a Meta-Package Universe https://cran.r-project.org/web/packages/pkgverse/index.html 234.4 R i√ßin yardƒ±m bulma # ?mean # ??efetch # help(merge) # example(merge) Vignette RDocumentation https://www.rdocumentation.org R Package Documentation https://rdrr.io/ GitHub Stackoverflow https://stackoverflow.com/ Google uygun anahtar kelime How I use #rstats h/t (???) pic.twitter.com/erRnTG0Ujr ‚Äî Emily Bovee ((???)) August 10, 2018 Awesome Cheatsheet https://github.com/detailyang/awesome-cheatsheet http://cran.r-project.org/doc/contrib/Baggott-refcard-v2.pdf https://www.rstudio.com/resources/cheatsheets/ Awesome R https://github.com/qinwf/awesome-R#readme https://awesome-r.com/ Twitter https://twitter.com/hashtag/rstats?src=hash Reproducible Examples Got a question to ask on (???) or post on (???)? No time to read the long post on how to use reprex? Here is a 20-second gif for you to format your R codes nicely and for others to reproduce your problem. (An example from a talk given by (???)) #rstat pic.twitter.com/gpuGXpFIsX ‚Äî ZhiYang ((???)) October 18, 2018 234.5 R paket y√ºkleme install.packages( tidyverse , dependencies = TRUE) install.packages( jmv , dependencies = TRUE) install.packages( questionr , dependencies = TRUE) install.packages( Rcmdr , dependencies = TRUE) install.packages( summarytools ) {r} # install.packages( tidyverse , dependencies = TRUE) # install.packages( jmv , dependencies = TRUE) # install.packages( questionr , dependencies = TRUE) # install.packages( Rcmdr , dependencies = TRUE) # install.packages( summarytools ) {r, error=FALSE, message = FALSE, warning = FALSE, eval = TRUE, include = TRUE} # require(tidyverse) # require(jmv) # require(questionr) # library(summarytools) # library(gganimate) "],["r-studio-ile-proje-olu≈üturma-3.html", "Chapter 235 R studio ile proje olu≈üturma", " Chapter 235 R studio ile proje olu≈üturma https://support.rstudio.com/hc/en-us/articles/200526207-Using-Projects "],["rstudio-ile-veri-y√ºkleme-3.html", "Chapter 236 RStudio ile veri y√ºkleme 236.1 Excel 236.2 SPSS 236.3 csv", " Chapter 236 RStudio ile veri y√ºkleme https://support.rstudio.com/hc/en-us/articles/218611977-Importing-Data-with-RStudio 236.1 Excel 236.2 SPSS 236.3 csv "],["veriyi-g√∂r√ºnt√ºleme-3.html", "Chapter 237 Veriyi g√∂r√ºnt√ºleme", " Chapter 237 Veriyi g√∂r√ºnt√ºleme Spreadsheet users using #rstats: where's the data?#rstats users using spreadsheets: where's the code? ‚Äî Leonard Kiefer ((???)) July 7, 2018 {r, results= markup } # library(nycflights13) # summary(flights) View(data) data head tail glimpse str skimr::skim() "],["veriyi-deƒüi≈ütirme-3.html", "Chapter 238 Veriyi deƒüi≈ütirme 238.1 Veriyi kod ile deƒüi≈ütirelim 238.2 Veriyi eklentilerle deƒüi≈ütirme 238.3 RStudio aracƒ±lƒ±ƒüƒ±yla recode", " Chapter 238 Veriyi deƒüi≈ütirme 238.1 Veriyi kod ile deƒüi≈ütirelim 238.2 Veriyi eklentilerle deƒüi≈ütirme 238.3 RStudio aracƒ±lƒ±ƒüƒ±yla recode questionr paketi kullanƒ±lacak https://juba.github.io/questionr/articles/recoding_addins.html "],["basit-tanƒ±mlayƒ±cƒ±-istatistikler-3.html", "Chapter 239 Basit tanƒ±mlayƒ±cƒ± istatistikler 239.1 summarytools 239.2 skimr 239.3 DataExplorer 239.4 Grafikler", " Chapter 239 Basit tanƒ±mlayƒ±cƒ± istatistikler summary() mean median min max sd table() {r, echo=TRUE, include = TRUE} library(readr) irisdata &lt;- read_csv( data/iris.csv ) jmv::descriptives( data = irisdata, vars = Sepal.Length , splitBy = Species , freq = TRUE, hist = TRUE, dens = TRUE, bar = TRUE, box = TRUE, violin = TRUE, dot = TRUE, mode = TRUE, sum = TRUE, sd = TRUE, variance = TRUE, range = TRUE, se = TRUE, skew = TRUE, kurt = TRUE, quart = TRUE, pcEqGr = TRUE) {r, echo=TRUE, include=FALSE} # install.packages( scatr ) scatr::scat( data = irisdata, x = Sepal.Length , y = Sepal.Width , group = Species , marg = dens , line = linear , se = TRUE) 239.1 summarytools https://cran.r-project.org/web/packages/summarytools/vignettes/Introduction.html {r, include=FALSE, comment=NA, prompt=FALSE, cache=FALSE, echo=TRUE, results=&#39;asis&#39;} library(summarytools) summarytools::freq(iris$Species, style = rmarkdown ) {r, include=FALSE, comment=NA, prompt=FALSE, cache=FALSE, echo=TRUE, results=&#39;asis&#39;} summarytools::freq(iris$Species, report.nas = FALSE, style = rmarkdown , headings = TRUE) {r, include=FALSE, comment=NA, prompt=FALSE, cache=FALSE, echo=TRUE, results=&#39;asis&#39;} with(tobacco, print(ctable(smoker, diseased), method = &#39;render&#39;)) {r, include=FALSE, comment=NA, prompt=FALSE, cache=FALSE, echo=TRUE, results=&#39;asis&#39;} with(tobacco, print(ctable(smoker, diseased, prop = &#39;n&#39;, totals = FALSE), headings = TRUE, method = render )) {r, include=FALSE, comment=NA, prompt=FALSE, cache=FALSE, echo=TRUE, results=&#39;asis&#39;} summarytools::descr(iris, style = rmarkdown ) {r, include=FALSE, comment=NA, prompt=FALSE, cache=FALSE, echo=TRUE, results=&#39;asis&#39;} descr(iris, stats = c( mean , sd , min , med , max ), transpose = TRUE, headings = TRUE, style = rmarkdown ) {r, include=FALSE, comment=NA, prompt=FALSE, cache=FALSE, echo=TRUE, results=&#39;asis&#39;} # view(dfSummary(iris)) {r, include=FALSE, comment=NA, prompt=FALSE, cache=FALSE, echo=TRUE, results=&#39;asis&#39;} dfSummary(tobacco, plain.ascii = FALSE, style = grid ) {r, include=FALSE, comment=NA, prompt=FALSE, cache=FALSE, echo=TRUE, results=&#39;asis&#39;} # First save the results iris_stats_by_species &lt;- by(data = iris, INDICES = iris$Species, FUN = descr, stats = c( mean , sd , min , med , max ), transpose = TRUE) # Then use view(), like so: view(iris_stats_by_species, method = pander , style = rmarkdown ) {r, include=FALSE, comment=NA, prompt=FALSE, cache=FALSE, echo=TRUE, results=&#39;asis&#39;} # view(iris_stats_by_species) {r, include=FALSE, comment=NA, prompt=FALSE, cache=FALSE, echo=TRUE, results=&#39;asis&#39;} data(tobacco) # tobacco is an example dataframe included in the package BMI_by_age &lt;- with(tobacco, by(BMI, age.gr, descr, stats = c( mean , sd , min , med , max ))) view(BMI_by_age, pander , style = rmarkdown ) {r, include=FALSE, comment=NA, prompt=FALSE, cache=FALSE, echo=TRUE, results=&#39;asis&#39;} BMI_by_age &lt;- with(tobacco, by(BMI, age.gr, descr, transpose = TRUE, stats = c( mean , sd , min , med , max ))) view(BMI_by_age, pander , style = rmarkdown , headings = TRUE) {r, include=FALSE, comment=NA, prompt=FALSE, cache=FALSE, echo=TRUE, results=&#39;asis&#39;} tobacco_subset &lt;- tobacco[ ,c( gender , age.gr , smoker )] freq_tables &lt;- lapply(tobacco_subset, freq) # view(freq_tables, footnote = NA, file = &#39;freq-tables.html&#39;) {r, include=FALSE, comment=NA, prompt=FALSE, cache=FALSE, echo=TRUE, results=&#39;asis&#39;} what.is(iris) {r eval=FALSE, include=FALSE, echo=TRUE} freq(tobacco$gender, style = &#39;rmarkdown&#39;) {r eval=FALSE, include=FALSE, echo=TRUE} print(freq(tobacco$gender), method = &#39;render&#39;) 239.2 skimr library(skimr) skim(df) 239.3 DataExplorer library(DataExplorer) DataExplorer::create_report(df) 239.4 Grafikler {r eval=FALSE, include=FALSE, echo=TRUE} # library(ggplot2) # library(mosaic) # mPlot(irisdata) {r eval=FALSE, include=FALSE, echo=TRUE} ctable(tobacco$gender, tobacco$smoker, style = &#39;rmarkdown&#39;) {r eval=FALSE, include=FALSE, echo=TRUE} print(ctable(tobacco$gender, tobacco$smoker), method = &#39;render&#39;) descr(tobacco, style = &#39;rmarkdown&#39;) print(descr(tobacco), method = &#39;render&#39;, table.classes = &#39;st-small&#39;) dfSummary(tobacco, style = &#39;grid&#39;, plain.ascii = FALSE) print(dfSummary(tobacco, graph.magnif = 0.75), method = &#39;render&#39;) Here, building up a #ggplot2 as slowly as possible, #rstats. Incremental adjustments. #rstatsteachingideas pic.twitter.com/nUulQl8bPh ‚Äî Gina Reynolds ((???)) August 13, 2018 Dreaming of a fancy #Rstats #ggplot #dataviz but still scared of typing #code? (???) esquisse package has you covered https://t.co/1vIDXcVAAF pic.twitter.com/RlTkptnrNv ‚Äî Radoslaw Panczak ((???)) October 2, 2018 "],["rcmdr-3.html", "Chapter 240 Rcmdr", " Chapter 240 Rcmdr library(Rcmdr) Rcmdr::Commander() A Comparative Review of the R Commander GUI for R http://r4stats.com/articles/software-reviews/r-commander/ "],["jamovi-3.html", "Chapter 241 jamovi", " Chapter 241 jamovi https://www.jamovi.org/ https://blog.jamovi.org/2018/07/30/rj.html "],["sonraki-konular-3.html", "Chapter 242 Sonraki Konular", " Chapter 242 Sonraki Konular RStudio ile GitHub Hipotez testleri R Markdown ve R Notebook ile tekrarlanabilir rapor "],["diƒüer-kodlar-3.html", "Chapter 243 Diƒüer kodlar", " Chapter 243 Diƒüer kodlar Diƒüer kodlar i√ßin bakƒ±nƒ±z: https://sbalci.github.io/ "],["geri-bildirim-3.html", "Chapter 244 Geri Bildirim", " Chapter 244 Geri Bildirim Geri bildirim i√ßin tƒ±klayƒ±nƒ±z: Geri bildirim formu Please enable JavaScript to view the comments powered by Disqus. "],["recipes-for-mining-twitter-data-with-rtweet.html", "Chapter 245 21 Recipes for Mining Twitter Data with rtweet", " Chapter 245 21 Recipes for Mining Twitter Data with rtweet "],["recipes-for-mining-twitter-data-with-rtweet-1.html", "Chapter 246 21 Recipes for Mining Twitter Data with rtweet", " Chapter 246 21 Recipes for Mining Twitter Data with rtweet https://rud.is/books/21-recipes/ http://rtweet.info/index.html {r eval=FALSE, include=FALSE, echo=TRUE} # devtools::install_github( mkearney/rtweet ) library(rtweet) library(tidyverse) {r eval=FALSE, include=FALSE, echo=TRUE} (trends_avail &lt;- trends_available()) {r eval=FALSE, include=FALSE, echo=TRUE} (us &lt;- get_trends( united states )) {r eval=FALSE, include=FALSE, echo=TRUE} (tr &lt;- get_trends( turkey )) {r eval=FALSE, include=FALSE, echo=TRUE} library(DBI) library(RSQLite) library(rtweet) # mkearney/rtweet repeat { message( Retrieveing trends... ) # optional us &lt;- get_trends( united states ) db_con &lt;- dbConnect(RSQLite::SQLite(), data/us-trends.db ) dbWriteTable(db_con, us_trends , us, append=TRUE) # append=TRUE will update the table vs overwrite and also create it on first run if it does not exist dbDisconnect(db_con) Sys.sleep(10 * 60) # sleep for 10 minutes } {r eval=FALSE, include=FALSE, echo=TRUE} library(dplyr) trends_db &lt;- src_sqlite( data/us-trends.db ) us &lt;- tbl(trends_db, us_trends ) select(us, trend) {r eval=FALSE, include=FALSE, echo=TRUE} (rstats &lt;- search_tweets( #rstats , n=300)) # pull 300 tweets that used the #rstats hashtag {r eval=FALSE, include=FALSE, echo=TRUE} glimpse(rstats) {r eval=FALSE, include=FALSE, echo=TRUE} rstats %&gt;% select(hashtags) %&gt;% unnest() %&gt;% mutate(hashtags = tolower(hashtags)) %&gt;% count(hashtags, sort=TRUE) %&gt;% filter(hashtags != rstats ) %&gt;% top_n(10) {r eval=FALSE, include=FALSE, echo=TRUE} rstats &lt;- search_tweets( #rstats -filter:retweets ) %&gt;% select(text) {r eval=FALSE, include=FALSE, echo=TRUE} rstats &lt;- search_tweets( to:kearneymw ) %&gt;% select(text) {r eval=FALSE, include=FALSE, echo=TRUE} rstats &lt;- search_tweets( #rstats url:github -#python ) %&gt;% select(text) {r eval=FALSE, include=FALSE, echo=TRUE} library(rtweet) library(tidyverse) rstats &lt;- search_tweets( #rstats , n=500) glimpse(rstats) filter(rstats, retweet_count &gt; 0) %&gt;% select(text, mentions_screen_name, retweet_count) %&gt;% mutate(text = substr(text, 1, 30)) %&gt;% unnest() {r eval=FALSE, include=FALSE, echo=TRUE} # regex mod from https://stackoverflow.com/questions/655903/python-regular-expression-for-retweets filter(rstats, str_detect(text, (RT|via)((?:[[:blank:]:]\\\\W*@\\\\w+)+) )) %&gt;% select(text, mentions_screen_name, retweet_count) %&gt;% mutate(extracted = str_match(text, (RT|via)((?:[[:blank:]:]\\\\W*@\\\\w+)+) )[,3]) %&gt;% mutate(text = substr(text, 1, 30)) %&gt;% unnest() {r eval=FALSE, include=FALSE, echo=TRUE} library(rtweet) library(igraph) library(hrbrthemes) library(tidyverse) rstats &lt;- search_tweets( #rstats , n=1500) filter(rstats, retweet_count &gt; 0) %&gt;% select(screen_name, mentions_screen_name) %&gt;% unnest(mentions_screen_name) %&gt;% filter(!is.na(mentions_screen_name)) %&gt;% graph_from_data_frame() -&gt; rt_g summary(rt_g) {r eval=FALSE, include=FALSE, echo=TRUE} ggplot(data_frame(y=degree_distribution(rt_g), x=1:length(y))) + geom_segment(aes(x, y, xend=x, yend=0), color= slateblue ) + scale_y_continuous(expand=c(0,0), trans= sqrt ) + labs(x= Degree , y= Density (sqrt scale) , title= #rstats Retweet Degree Distribution ) + theme_ipsum_rc(grid= Y , axis= x ) {r eval=FALSE, include=FALSE, echo=TRUE} V(rt_g)$node_label &lt;- unname(ifelse(degree(rt_g)[V(rt_g)] &gt; 20, names(V(rt_g)), )) V(rt_g)$node_size &lt;- unname(ifelse(degree(rt_g)[V(rt_g)] &gt; 20, degree(rt_g), 0)) {r eval=FALSE, include=FALSE, echo=TRUE} library(ggraph) ggraph(rt_g, layout = &#39;linear&#39;, circular = TRUE) + geom_edge_arc(edge_width=0.125, aes(alpha=..index..)) + geom_node_label(aes(label=node_label, size=node_size), label.size=0, fill= #ffffff66 , segment.colour= springgreen , color= slateblue , repel=TRUE, family=font_rc, fontface= bold ) + coord_fixed() + scale_size_area(trans= sqrt ) + labs(title= Retweet Relationships , subtitle= Most retweeted screen names labeled. Darkers edges == more retweets. Node size == larger degree ) + theme_graph(base_family=font_rc) + theme(legend.position= none ) {r eval=FALSE, include=FALSE, echo=TRUE} stream_tweets( lookup_coords( usa ), # handy helper function in rtweet verbose = FALSE, timeout = (60 * 1), ) -&gt; usa {r eval=FALSE, include=FALSE, echo=TRUE} count(usa, place_full_name, sort=TRUE) {r eval=FALSE, include=FALSE, echo=TRUE} unnest(usa, hashtags) %&gt;% count(hashtags, sort=TRUE) %&gt;% filter(!is.na(hashtags)) {r eval=FALSE, include=FALSE, echo=TRUE} count(usa, source, sort=TRUE) {r eval=FALSE, include=FALSE, echo=TRUE} rtweet::write_as_csv() {r eval=FALSE, include=FALSE, echo=TRUE} library(rtweet) library(tidytext) library(magick) library(kumojars) # hrbrmstr/kumojars library(kumo) # hrbrmstr/kumo library(tidyverse) scifi &lt;- search_tweets( #NationalScienceFictionDay , n=1500) data_frame(txt=str_replace_all(scifi$text, #NationalScienceFictionDay , )) %&gt;% unnest_tokens(word, txt) %&gt;% anti_join(stop_words, word ) %&gt;% anti_join(rtweet::stopwordslangs, word ) %&gt;% anti_join(data_frame(word=c( https , t.co )), word ) %&gt;% # need to make a more technical stopwords list or clean up the text better filter(nchar(word)&gt;3) %&gt;% pull(word) %&gt;% paste0(collapse= ) -&gt; txt cloud_img &lt;- word_cloud(txt, width=800, height=500, min_font_size=10, max_font_size=60, scale= log ) image_write(cloud_img, data/wordcloud.png ) library(rtweet) library(LSAfun) library(jerichojars) # hrbrmstr/jerichojars library(jericho) # hrbrmstr/jericho library(tidyverse) stiles &lt;- get_timeline( stiles ) filter(stiles, str_detect(urls_expanded_url, nyti|reut|wapo|lat\\.ms|53ei )) %&gt;% # only get tweets with news links pull(urls_expanded_url) %&gt;% # extract the links flatten_chr() %&gt;% # mush them into a nice character vector head(3) %&gt;% # get the first 3 map_chr(~{ httr::GET(.x) %&gt;% # get the URL (I‚Äôm lazily calling fair use here vs check robots.txt since I‚Äôm suggesting you do this for your benefit vs profit) httr::content(as= text ) %&gt;% # extract the HTML jericho::html_to_text() %&gt;% # strip away extraneous HTML tags LSAfun::genericSummary(k=3) %&gt;% # summarise! paste0(collapse= ) # easier to see }) %&gt;% walk(cat) library(rtweet) library(tidyverse) (brooke_followers &lt;- rtweet::get_followers( gbwanderson )) (brooke_friends &lt;- rtweet::get_friends( gbwanderson )) {r eval=FALSE, include=FALSE, echo=TRUE} library(rtweet) library(tidyverse) my_followers &lt;- rtweet::get_followers( serdarbalci ) my_friends &lt;- rtweet::get_friends( serdarbalci ) {r eval=FALSE, include=FALSE, echo=TRUE} length(intersect(my_followers$user_id, my_friends$user_id)) {r eval=FALSE, include=FALSE, echo=TRUE} length(setdiff(my_followers$user_id, my_friends$user_id)) {r eval=FALSE, include=FALSE, echo=TRUE} length(setdiff(my_friends$user_id, my_followers$user_id)) {r eval=FALSE, include=FALSE, echo=TRUE} rtweet::lookup_users(my_friends$user_id) {r eval=FALSE, include=FALSE, echo=TRUE} library(rtweet) library(hrbrthemes) library(tidyverse) influence_snapshot &lt;- function(user, trans=c( log10 , identity )) { user &lt;- user[1] trans &lt;- match.arg(tolower(trimws(trans[1])), c( log10 , identity )) user_info &lt;- lookup_users(user) user_followers &lt;- get_followers(user_info$user_id) uf_details &lt;- lookup_users(user_followers$user_id) primary_influence &lt;- scales::comma(sum(c(uf_details$followers_count, user_info$followers_count))) filter(uf_details, followers_count &gt; 0) %&gt;% ggplot(aes(followers_count)) + geom_density(aes(y=..count..), color= lightslategray , fill= lightslategray , alpha=2/3, size=1) + scale_x_continuous(expand=c(0,0), trans= log10 , labels=scales::comma) + scale_y_comma() + labs( x= Number of Followers of Followers (log scale) , y= Number of Followers , title=sprintf( Follower chain distribution of %s (@%s) , user_info$name, user_info$screen_name), subtitle=sprintf( Follower count: %s; Primary influence/reach: %s , scales::comma(user_info$followers_count), scales::comma(primary_influence)) ) + theme_ipsum_rc(grid= XY ) -&gt; gg print(gg) return(invisible(list(user_info=user_info, follower_details=uf_details))) } {r eval=FALSE, include=FALSE, echo=TRUE} juliasilge &lt;- influence_snapshot( juliasilge ) {r eval=FALSE, include=FALSE, echo=TRUE} library(rtweet) library(broom) library(eechidna) library(cartogram) # chxy/cartogram library(hrbrthemes) library(tidyverse) # search twitter for tweets rstats_us &lt;- search_tweets( #rstats , 3000, geocode = 2.877742,-97.380979,3000mi ) # geocode request isn&#39;t perfect but helps narrow down # lookup each user (uniquely) so we can grab location information user_info &lt;- lookup_users(unique(rstats_us$user_id)) {r eval=FALSE, include=FALSE, echo=TRUE} discard(user_info$location, `==`, ) %&gt;% # ignore blank data str_match(sprintf( (%s) , paste0(state.abb, collapse= | ))) %&gt;% # try to match U.S. state abbreviations .[,2] %&gt;% # the previous step creates a matrix with column 2 being the extracted information (if any) discard(is.na) %&gt;% # if no state match was found the value is NA so discard this one table() %&gt;% # some habits are hard to break broom::tidy() %&gt;% # but we can tidy them! set_names(c( state , n )) %&gt;% # these are more representative names tbl_df() %&gt;% # not really necessary but I was printing this when testing arrange(desc(n)) %&gt;% # same as ^^ left_join( as_tibble(maps::state.carto.center) %&gt;% # join state cartographic center data mutate(state=state.abb) ) -&gt; for_dor # %&gt;% # the GitHub-only cartogram package nas a data structure which holds state adjacency information # by specifying that here, it will help make the force-directed cartogram circle positioning more precise (and pretty) # filter(state %in% names(cartogram::statenbrs)) -&gt; for_dor glimpse(for_dor) {r eval=FALSE, include=FALSE, echo=TRUE} par(family=font_rc, col= white ) eechidna:::dorling( for_dor$state, for_dor$x, for_dor$y, sqrt(for_dor$n), # nbr=cartogram::statenbrs, animation = FALSE, nbredge = TRUE, iteration=100, name.text=TRUE, dist.ratio=1.2, main= Dorling Cartogram of U.S. #rstats , xlab=&#39;&#39;, ylab=&#39;&#39;, col= lightslategray , frame=FALSE, asp=1, family=font_rc, cex.main=1.75, adj=0 ) -&gt; dor par(family=font_rc, col= black ) {r eval=FALSE, include=FALSE, echo=TRUE} library(rtweet) library(ggmap) library(tidyverse) rstats_us &lt;- search_tweets( #rstats , 300) user_info &lt;- lookup_users(unique(rstats_us$user_id)) discard(user_info$location, `==`, ) %&gt;% ggmap::geocode() -&gt; coded coded$location &lt;- discard(user_info$location, `==`, ) user_info &lt;- left_join(user_info, coded, location ) {r eval=FALSE, include=FALSE, echo=TRUE} library(rtweet) library(tidyverse) library(UpSetR) {r eval=FALSE, include=FALSE, echo=TRUE} # get a list of twitter handles you want to compare rstaters &lt;- c( dataandme , serdarbalci , JennyBryan , hrbrmstr , xieyihui # drob , # juliasilge , # thomasp85 ) # scrape the user_id of all followers for each handle in the list and bind into 1 dataframe followers &lt;- rstaters %&gt;% map_df(~ get_followers(.x, n = 200, retryonratelimit = TRUE) %&gt;% mutate(account = .x)) head(followers) tail(followers) {r eval=FALSE, include=FALSE, echo=TRUE} # get a de-duplicated list of all followers aRdent_followers &lt;- unique(followers$user_id) # for each follower, get a binary indicator of whether they follow each tweeter or not and bind to one dataframe binaries &lt;- rstaters %&gt;% map_dfc(~ ifelse(aRdent_followers %in% filter(followers, account == .x)$user_id, 1, 0) %&gt;% as.data.frame) # UpSetR doesn&#39;t like tibbles # set column names names(binaries) &lt;- rstaters # have a look at the data glimpse(binaries) {r eval=FALSE, include=FALSE, echo=TRUE} # plot the sets with UpSetR upset(binaries, nsets = 7, main.bar.color = SteelBlue , sets.bar.color = DarkCyan , sets.x.label = Follower Count , text.scale = c(rep(1.4, 5), 1), order.by = freq ) "],["multiple-pages.html", "Chapter 247 Multiple Pages", " Chapter 247 Multiple Pages output: flexdashboard::flex_dashboard "],["page-1.html", "Chapter 248 Page 1 248.1 Column 248.2 Column", " Chapter 248 Page 1 248.1 Column 248.1.1 Chart 1 {r eval=FALSE, include=FALSE, echo=TRUE} 248.2 Column 248.2.1 Chart 2 {r eval=FALSE, include=FALSE, echo=TRUE} 248.2.2 Chart 3 {r eval=FALSE, include=FALSE, echo=TRUE} "],["page-2.html", "Chapter 249 Page 2 249.1 Row 249.2 Row", " Chapter 249 Page 2 249.1 Row 249.1.1 Chart 1 {r eval=FALSE, include=FALSE, echo=TRUE} 249.2 Row 249.2.1 Chart 2 {r eval=FALSE, include=FALSE, echo=TRUE} 249.2.2 Chart 3 {r eval=FALSE, include=FALSE, echo=TRUE} "],["mxnet.html", "Chapter 250 mxnet", " Chapter 250 mxnet https://mxnet.apache.org/versions/master/install/index.html?platform=MacOS&amp;language=R&amp;processor=CPU {r eval=FALSE, include=FALSE, echo=TRUE} cran &lt;- getOption( repos ) cran[ dmlc ] &lt;- https://apache-mxnet.s3-accelerate.dualstack.amazonaws.com/R/CRAN/ options(repos = cran) install.packages( mxnet ) https://mxnet.apache.org/versions/master/api/r/index.html https://mxnet.apache.org/versions/master/tutorials/index.html R Tutorials Getting Started Basic Classification Using a pre-trained model for Image Classification Models MNIST Handwritten Digit Classification with Convolutional Network Shakespeare generation with Character-level RNN API Guides NDArray API Symbol API Callbacks Custom Data Iterators Custom Loss Functions "],["mygist.html", "Chapter 251 MyGIST", " Chapter 251 MyGIST https://gist.github.com/sbalci https://docs.ropensci.org/gistr/ https://docs.ropensci.org/gistr/articles/gistr.html {r eval=FALSE, include=FALSE, echo=TRUE} # install.packages( gistr ) # devtools::install_github( ropensci/gistr ) # renv::install( gistr ) library( gistr ) {r eval=FALSE, include=FALSE, echo=TRUE} (mygists &lt;- gistr::gists(&#39;minepublic&#39;)) {r eval=FALSE, include=FALSE, echo=TRUE} gistembedcodes &lt;- purrr::map(.x = mygists, .f = gistr::embed) gistembedcodes &lt;- unlist(gistembedcodes) gistembedcodes &lt;- cat(gistembedcodes, sep = &#39;\\n\\n\\n&#39;) `{r # gistembedcodes` {r # gistembedcodes {r eval=FALSE, include=FALSE, echo=TRUE} citation(package = &#39;gistr&#39;) "],["news-1.html", "Chapter 252 News", " Chapter 252 News Text Analysis of Newspaper News on Electoral Integrity and Electoral Violence in Turkey http://rpubs.com/emretoros/dievt "],["presentation-ninja.html", "Chapter 253 Presentation Ninja", " Chapter 253 Presentation Ninja sub# &lt;br/&gt;with xaringan author: Yihui Xie date: 2016/12/12 (updated: ) output: xaringan::moon_reader: lib_dir: libs nature: highlightStyle: github highlightLines: true countIncrementalSlides: false {r eval=FALSE, include=FALSE, echo=TRUE} options(htmltools.dir.version = FALSE) ??? Image credit: Wikimedia Commons class: center, middle "],["xaringan.html", "Chapter 254 xaringan", " Chapter 254 xaringan 254.0.1 / É√¶.‚Äôri≈ã.…°√¶n/ class: inverse, center, middle "],["get-started.html", "Chapter 255 Get Started", " Chapter 255 Get Started "],["hello-world.html", "Chapter 256 Hello World", " Chapter 256 Hello World Install the xaringan package from Github: {r eval=FALSE, tidy=FALSE} devtools::install_github( yihui/xaringan ) ‚Äì You are recommended to use the RStudio IDE, but you do not have to. Create a new R Markdown document from the menu File -&gt; New File -&gt; R Markdown -&gt; From Template -&gt; Ninja Presentation;1 ‚Äì Click the Knit button to compile it; ‚Äì or use the RStudio Addin2 Infinite Moon Reader to live preview the slides (every time you update and save the Rmd document, the slides will be automatically reloaded in RStudio Viewer. .footnote[ [1] ‰∏≠ÊñáÁî®Êà∑ËØ∑ÁúãËøô‰ªΩÊïôÁ®ã [2] See #2 if you do not see the template or addin in RStudio. ] background-image: url({r # xaringan:::karl) background-position: 50% 50% class: center, bottom, inverse "],["you-only-live-once.html", "Chapter 257 You only live once!", " Chapter 257 You only live once! "],["hello-ninja.html", "Chapter 258 Hello Ninja", " Chapter 258 Hello Ninja As a presentation ninja, you certainly should not be satisfied by the Hello World example. You need to understand more about two things: The remark.js library; The xaringan package; Basically xaringan injected the chakra of R Markdown (minus Pandoc) into remark.js. The slides are rendered by remark.js in the web browser, and the Markdown source needed by remark.js is generated from R Markdown (knitr). "],["remark-js.html", "Chapter 259 remark.js", " Chapter 259 remark.js You can see an introduction of remark.js from its homepage. You should read the remark.js Wiki at least once to know how to create a new slide (Markdown syntax* and slide properties); format a slide (e.g. text alignment); configure the slideshow; and use the presentation (keyboard shortcuts). It is important to be familiar with remark.js before you can understand the options in xaringan. .footnote[[*] It is different with Pandoc‚Äôs Markdown! It is limited but should be enough for presentation purposes. Come on‚Ä¶ You do not need a slide for the Table of Contents! Well, the Markdown support in remark.js may be improved in the future.] background-image: url({r # xaringan:::karl) background-size: cover class: center, bottom, inverse "],["i-was-so-happy-to-have-discovered-remark-js.html", "Chapter 260 I was so happy to have discovered remark.js!", " Chapter 260 I was so happy to have discovered remark.js! class: inverse, middle, center "],["using-xaringan.html", "Chapter 261 Using xaringan", " Chapter 261 Using xaringan "],["xaringan-1.html", "Chapter 262 xaringan", " Chapter 262 xaringan Provides an R Markdown output format xaringan::moon_reader as a wrapper for remark.js, and you can use it in the YAML metadata, e.g. # A Cool Presentation output: xaringan::moon_reader: yolo: true nature: autoplay: 30000 See the help page ?xaringan::moon_reader for all possible options that you can use. "],["remark-js-vs-xaringan.html", "Chapter 263 remark.js vs xaringan", " Chapter 263 remark.js vs xaringan Some differences between using remark.js (left) and using xaringan (right): .pull-left[ 1. Start with a boilerplate HTML file; Plain Markdown; Write JavaScript to autoplay slides; Manually configure MathJax; Highlight code with *; Edit Markdown source and refresh browser to see updated slides; ] .pull-right[ 1. Start with an R Markdown document; R Markdown (can embed R/other code chunks); Provide an option autoplay; MathJax just works;* Highlight code with {{}}; The RStudio addin Infinite Moon Reader automatically refreshes slides on changes; ] .footnote[[*] Not really. See next page.] "],["math-expressions.html", "Chapter 264 Math Expressions", " Chapter 264 Math Expressions You can write LaTeX math expressions inside a pair of dollar signs, e.g. $+$ renders \\(\\alpha+\\beta\\). You can use the display style with double dollar signs: $$\\bar{X}=\\frac{1}{n}\\sum_{i=1}^nX_i$$ \\[\\bar{X}=\\frac{1}{n}\\sum_{i=1}^nX_i\\] Limitations: The source code of a LaTeX math expression must be in one line, unless it is inside a pair of double dollar signs, in which case the starting $$ must appear in the very beginning of a line, followed immediately by a non-space character, and the ending $$ must be at the end of a line, led by a non-space character; There should not be spaces after the opening $ or before the closing $. Math does not work on the title slide (see #61 for a workaround). "],["r-code.html", "Chapter 265 R Code", " Chapter 265 R Code {r comment=&#39;#&#39;} # a boring regression fit = lm(dist ~ 1 + speed, data = cars) coef(summary(fit)) dojutsu = c(&#39;Âú∞ÁàÜÂ§©Êòü&#39;, &#39;Â§©ÁÖß&#39;, &#39;Âä†ÂÖ∑ÂúüÂëΩ&#39;, &#39;Á•ûÂ®Å&#39;, &#39;È†à‰ΩêËÉΩ‰πé&#39;, &#39;ÁÑ°ÈôêÊúàË™≠&#39;) grep(&#39;Â§©&#39;, dojutsu, value = TRUE) "],["r-plots.html", "Chapter 266 R Plots", " Chapter 266 R Plots {r , fig.height=4, dev=&#39;svg&#39;} par(mar = c(4, 4, 1, .1)) plot(cars, pch = 19, col = &#39;darkgray&#39;, las = 1) abline(fit, lwd = 2) "],["tables-1.html", "Chapter 267 Tables", " Chapter 267 Tables If you want to generate a table, make sure it is in the HTML format (instead of Markdown or other formats), e.g., {r eval=FALSE, include=FALSE, echo=TRUE} knitr::kable(head(iris), format = &#39;html&#39;) "],["html-widgets.html", "Chapter 268 HTML Widgets", " Chapter 268 HTML Widgets I have not thoroughly tested HTML widgets against xaringan. Some may work well, and some may not. It is a little tricky. Similarly, the Shiny mode ({r # untime: shiny) does not work. I might get these issues fixed in the future, but these are not of high priority to me. I never turn my presentation into a Shiny app. When I need to demonstrate more complicated examples, I just launch them separately. It is convenient to share slides with other people when they are plain HTML/JS applications. See the next page for two HTML widgets. {r out.width=&#39;100%&#39;, fig.height=6, eval=require(&#39;leaflet&#39;)} library(leaflet) leaflet() %&gt;% addTiles() %&gt;% setView(-93.65, 42.0285, zoom = 17) {r eval=require(&#39;DT&#39;), tidy=FALSE} DT::datatable( head(iris, 10), fillContainer = FALSE, options = list(pageLength = 8) ) "],["some-tips.html", "Chapter 269 Some Tips", " Chapter 269 Some Tips When you use the Infinite Moon Reader addin in RStudio, your R session will be blocked by default. You can click the red button on the right of the console to stop serving the slides, or use the daemonized mode so that it does not block your R session. To do the latter, you can set the option {r # options(servr.daemon = TRUE) in your current R session, or in ~/.Rprofile so that it is applied to all future R sessions. I do the latter by myself. To know more about the web server, see the servr package. ‚Äì Do not forget to try the yolo option of xaringan::moon_reader. output: xaringan::moon_reader: yolo: true "],["some-tips-1.html", "Chapter 270 Some Tips", " Chapter 270 Some Tips Slides can be automatically played if you set the autoplay option under nature, e.g. go to the next slide every 30 seconds in a lightning talk: output: xaringan::moon_reader: nature: autoplay: 30000 ‚Äì A countdown timer can be added to every page of the slides using the countdown option under nature, e.g. if you want to spend one minute on every page when you give the talk, you can set: output: xaringan::moon_reader: nature: countdown: 60000 Then you will see a timer counting down from 01:00, to 00:59, 00:58, ‚Ä¶ When the time is out, the timer will continue but the time turns red. "],["some-tips-2.html", "Chapter 271 Some Tips", " Chapter 271 Some Tips The title slide is created automatically by xaringan, but it is just another remark.js slide added before your other slides. The title slide is set to class: center, middle, inverse, title-slide by default. You can change the classes applied to the title slide with the titleSlideClass option of nature (title-slide is always applied). output: xaringan::moon_reader: nature: titleSlideClass: [top, left, inverse] ‚Äì If you‚Äôd like to create your own title slide, disable xaringan‚Äôs title slide with the seal = FALSE option of moon_reader. output: xaringan::moon_reader: seal: false "],["some-tips-3.html", "Chapter 272 Some Tips", " Chapter 272 Some Tips There are several ways to build incremental slides. See this presentation for examples. The option highlightLines: true of nature will highlight code lines that start with *, or are wrapped in {{ }}, or have trailing comments #&lt;&lt;; output: xaringan::moon_reader: nature: highlightLines: true See examples on the next page. "],["some-tips-4.html", "Chapter 273 Some Tips", " Chapter 273 Some Tips .pull-left[ An example using a leading *: ``` {r # if (TRUE) { ** message( Very important! ) } Output: {r # if (TRUE) { * message( Very important! ) } This is invalid R code, so it is a plain fenced code block that is not executed. ] .pull-right[ An example using `{{}}`: `{r # &#39;&#39;```` {r tidy=FALSE} if (TRUE) { *{{ message( Very important! ) }} } ``` Output: {r tidy=FALSE} if (TRUE) { {{ message( Very important! ) }} } It is valid R code so you can run it. Note that `{{}}` can wrap an R expression of multiple lines. ] # Some Tips An example of using the trailing comment `#&lt;&lt;` to highlight lines: ````markdown `{r # &#39;&#39;```` {r tidy=FALSE} library(ggplot2) ggplot(mtcars) + aes(mpg, disp) + geom_point() + #&lt;&lt; geom_smooth() #&lt;&lt; Output: ``` {r tidy=FALSE, eval=FALSE} library(ggplot2) ggplot(mtcars) + aes(mpg, disp) + geom_point() + #&lt;&lt; geom_smooth() #&lt;&lt; ``` # Some Tips When you enable line-highlighting, you can also use the chunk option `highlight.output` to highlight specific lines of the text output from a code chunk. For example, `highlight.output = TRUE` means highlighting all lines, and `highlight.output = c(1, 3)` means highlighting the first and third line. ````md `{r # &#39;&#39;```` {r, highlight.output=c(1, 3)} head(iris) ``` {r, highlight.output=c(1, 3), echo=TRUE} head(iris) Question: what does highlight.output = c(TRUE, FALSE) mean? (Hint: think about R‚Äôs recycling of vectors) "],["some-tips-5.html", "Chapter 274 Some Tips", " Chapter 274 Some Tips To make slides work offline, you need to download a copy of remark.js in advance, because xaringan uses the online version by default (see the help page ?xaringan::moon_reader). You can use xaringan::summon_remark() to download the latest or a specified version of remark.js. By default, it is downloaded to libs/remark-latest.min.js. Then change the chakra option in YAML to point to this file, e.g. output: xaringan::moon_reader: chakra: libs/remark-latest.min.js If you used Google fonts in slides (the default theme uses Yanone Kaffeesatz, Droid Serif, and Source Code Pro), they won‚Äôt work offline unless you download or install them locally. The Heroku app google-webfonts-helper can help you download fonts and generate the necessary CSS. "],["macros.html", "Chapter 275 Macros", " Chapter 275 Macros remark.js allows users to define custom macros (JS functions) that can be applied to Markdown text using the syntax ![:macroName arg1, arg2, ...] or ![:macroName arg1, arg2, ...](this). For example, before remark.js initializes the slides, you can define a macro named scale: remark.macros.scale = function (percentage) { var url = this; return &#39;&lt;img src= &#39; + url + &#39; style= width: &#39; + percentage + &#39; /&gt;&#39;; }; Then the Markdown text ![:scale 50%](image.jpg) will be translated to &lt;img src= image.jpg style= width: 50% /&gt; "],["macros-continued.html", "Chapter 276 Macros (continued)", " Chapter 276 Macros (continued) To insert macros in xaringan slides, you can use the option beforeInit under the option nature, e.g., output: xaringan::moon_reader: nature: beforeInit: macros.js You save your remark.js macros in the file macros.js. The beforeInit option can be used to insert arbitrary JS code before {r # emark.create(). Inserting macros is just one of its possible applications. "],["css.html", "Chapter 277 CSS", " Chapter 277 CSS Among all options in xaringan::moon_reader, the most challenging but perhaps also the most rewarding one is css, because it allows you to customize the appearance of your slides using any CSS rules or hacks you know. You can see the default CSS file here. You can completely replace it with your own CSS files, or define new rules to override the default. See the help page ?xaringan::moon_reader for more information. "],["css-1.html", "Chapter 278 CSS", " Chapter 278 CSS For example, suppose you want to change the font for code from the default Source Code Pro to Ubuntu Mono . You can create a CSS file named, say, ubuntu-mono.css: @import url(https://fonts.googleapis.com/css?family=Ubuntu+Mono:400,700,400italic); .remark-code, .remark-inline-code { font-family: &#39;Ubuntu Mono&#39;; } Then set the css option in the YAML metadata: output: xaringan::moon_reader: css: [ default , ubuntu-mono.css ] Here I assume ubuntu-mono.css is under the same directory as your Rmd. See yihui/xaringan#83 for an example of using the Fira Code font, which supports ligatures in program code. "],["themes.html", "Chapter 279 Themes", " Chapter 279 Themes Don‚Äôt want to learn CSS? Okay, you can use some user-contributed themes. A theme typically consists of two CSS files foo.css and foo-fonts.css, where foo is the theme name. Below are some existing themes: {r eval=FALSE, include=FALSE, echo=TRUE} names(xaringan:::list_css()) "],["themes-1.html", "Chapter 280 Themes", " Chapter 280 Themes To use a theme, you can specify the css option as an array of CSS filenames (without the .css extensions), e.g., output: xaringan::moon_reader: css: [default, metropolis, metropolis-fonts] If you want to contribute a theme to xaringan, please read this blog post. class: inverse, middle, center background-image: url(https://upload.wikimedia.org/wikipedia/commons/3/39/Naruto_Shiki_Fujin.svg) background-size: contain "],["naruto.html", "Chapter 281 Naruto", " Chapter 281 Naruto background-image: url(https://upload.wikimedia.org/wikipedia/commons/b/be/Sharingan_triple.svg) background-size: 100px background-position: 90% 8% "],["sharingan.html", "Chapter 282 Sharingan", " Chapter 282 Sharingan The R package name xaringan was derived1 from Sharingan, a d≈çjutsu in the Japanese anime Naruto with two abilities: the Eye of Insight the Eye of Hypnotism I think a presentation is basically a way to communicate insights to the audience, and a great presentation may even hypnotize the audience.2,3 .footnote[ [1] In Chinese, the pronounciation of X is Sh / É/ (as in shrimp). Now you should have a better idea of how to pronounce my last name Xie. [2] By comparison, bad presentations only put the audience to sleep. [3] Personally I find that setting background images for slides is a killer feature of remark.js. It is an effective way to bring visual impact into your presentations. ] "],["naruto-terminology.html", "Chapter 283 Naruto terminology", " Chapter 283 Naruto terminology The xaringan package borrowed a few terms from Naruto, such as Sharingan (ÂÜôËº™Áúº; the package name) The moon reader (ÊúàË™≠; an attractive R Markdown output format) Chakra (Êü•ÂÖãÊãâ; the path to the remark.js library, which is the power to drive the presentation) Nature transformation (ÊÄßË≥™Â§âÂåñ; transform the chakra by setting different options) The infinite moon reader (ÁÑ°ÈôêÊúàË™≠; start a local web server to continuously serve your slides) The summoning technique (download remark.js from the web) You can click the links to know more about them if you want. The jutsu Moon Reader may seem a little evil, but that does not mean your slides are evil. class: center "],["hand-seals-.html", "Chapter 284 Hand seals (Âç∞)", " Chapter 284 Hand seals (Âç∞) Press h or ? to see the possible ninjutsu you can use in remark.js. class: center, middle "],["thanks.html", "Chapter 285 Thanks!", " Chapter 285 Thanks! Slides created via the R package xaringan. The chakra comes from remark.js, knitr, and R Markdown. "],["opencpu.html", "Chapter 286 OpenCPU", " Chapter 286 OpenCPU {r eval=FALSE, include=FALSE, echo=TRUE} # Install OpenCPU # install.packages( opencpu ) # Run Apps directly from Github library(opencpu) ocpu_start_app( rwebapps/nabel ) ocpu_start_app( rwebapps/markdownapp ) ocpu_start_app( rwebapps/stockapp ) # Install / remove apps # remove_apps( rwebapps/stockapp ) "],["paper.html", "Chapter 287 papeR", " Chapter 287 papeR copied from: https://cran.r-project.org/web/packages/papeR/vignettes/papeR_introduction.html The package is intended to ease reporting of standard data analysis tasks such as descriptive statistics, simple test results, plots and to prettify the output of various statistical models. https://github.com/hofnerb/papeR {r eval=FALSE, include=FALSE, echo=TRUE} # install.packages( papeR ) # devtools::install_github( hofnerb/papeR ) library( papeR ) {r eval=FALSE, include=FALSE, echo=TRUE} # install.packages( nlme ) data(Orthodont, package = nlme ) Orthodont_orig &lt;- Orthodont Orthodont {r eval=FALSE, include=FALSE, echo=TRUE} is.ldf(Orthodont) {r eval=FALSE, include=FALSE, echo=TRUE} labels(Orthodont) {r eval=FALSE, include=FALSE, echo=TRUE} labels(Orthodont) &lt;- c( fissure distance (mm) , age (years) , Subject , Sex ) Orthodont {r eval=FALSE, include=FALSE, echo=TRUE} is.ldf(Orthodont) {r eval=FALSE, include=FALSE, echo=TRUE} class(Orthodont) {r eval=FALSE, include=FALSE, echo=TRUE} labels(Orthodont) {r eval=FALSE, include=FALSE, echo=TRUE} labels(Orthodont, which = c( distance , age )) &lt;- c( Fissure distance (mm) , Age (years) ) {r eval=FALSE, include=FALSE, echo=TRUE} labels(Orthodont, which = age ) {r eval=FALSE, include=FALSE, echo=TRUE} labels(Orthodont, which = 1:2) 287.0.0.1 Conversion to labeled data frames Instead of manually setting labels, we can simply convert a data frame to a labeled data frame, either with the function as.ldf() or with convert.labels(). Actually, both calls reference the same function (for an object of class data.frame). While as.ldf() can be seen as the classical counterpart of is.ldf(), the function name convert.labels() is inspired by the fact that these functions either convert the variable names to labels or convert other variable labels to papeR-type variable labels. Hence, these functions can, for example, be used to convert labels from data sets which are imported via the function {r # ead.spss() to papeR-type variable labels. If no variable labels are specified, the original variable names are used. {r eval=FALSE, include=FALSE, echo=TRUE} Orthodont2 &lt;- convert.labels(Orthodont_orig) class(Orthodont2) labels(Orthodont2) 287.0.1 Plotting labeled data frames For data frames of class 'ldf', there exist special plotting functions: {r plot_labeled_dataframe, eval=FALSE, include=FALSE} par(mfrow = c(2, 2)) plot(Orthodont) As one can see, the plot type is automatically determined based on the data type and the axis label is defined by the labels(). To obtain group comparisons, we can use grouped plots. To plot all variable in the groups of Sex one can use {r grouped_plot, eval=FALSE, include=FALSE} par(mfrow = c(1, 3)) plot(Orthodont, by = Sex ) We can as well plot everything against the metrical variable distance {r with_x, eval=FALSE, include=FALSE} par(mfrow = c(1, 3)) plot(Orthodont, with = distance ) To plot only a subset of the data, say all but Subject, against distance and suppress the regression line we can use {r univariate_no_regressionline, eval=FALSE, include=FALSE} par(mfrow = c(1, 2)) plot(Orthodont, variables = -3, with = distance , regression.line = FALSE) Note that again we can use either variable names or indices to specify the variables which are to be plotted. 287.0.2 Summary tables One can use the command summarize() to automatically produce summary tables for either numerical variables (i.e., variables where is.numeric() is TRUE) or categorical variables (where is.factor() is TRUE). We now extract a summary table for numerical variables of the Orthodont data set: {r eval=FALSE, include=FALSE, echo=TRUE} data(Orthodont, package = nlme ) summarize(Orthodont, type = numeric ) Similarly, we can extract summaries for all factor variables. As one of the factors is the Subject which has {r # nlevels(Orthodont$Subject) levels, each with {r # unique(table(Orthodont$Subject)) observations, we exclude this from the summary table and only have a look at Sex {r eval=FALSE, include=FALSE, echo=TRUE} summarize(Orthodont, type = factor , variables = Sex ) Again, as for the plots, one can specify groups to obtain grouped statistics: {r eval=FALSE, include=FALSE, echo=TRUE} summarize(Orthodont, type = numeric , group = Sex , test = FALSE) Per default, one also gets tests for group differences: {r eval=FALSE, include=FALSE, echo=TRUE} summarize(Orthodont, type = numeric , group = Sex ) 287.0.3 Converting summaries to PDF So far, we only got standard R output. Yet, any of these summary tables can be easily converted to LaTeX code using the package xtable. In papeR two special functions xtable.summary() and print.xtable.summary() are defined for easy and pretty conversion. In Sweave we can use {r # &lt;&lt;echo = TRUE, results = tex&gt;&gt;= xtable(summarize(Orthodont, type = numeric )) xtable(summarize(Orthodont, type = factor , variables = Sex )) xtable(summarize(Orthodont, type = numeric , group = Sex )) @ and in knitr we can use {r # &lt;&lt;echo = TRUE, results = &#39;asis&#39;&gt;&gt;= xtable(summarize(Orthodont, type = numeric )) xtable(summarize(Orthodont, type = factor , variables = Sex )) xtable(summarize(Orthodont, type = numeric , group = Sex )) @ to get the following PDF output LaTeX Output Note that per default, booktabs is set to TRUE in print.xtable.summary, and thus \\usepackage{booktabs} is needed in the header of the LaTeX report. For details on LaTeX summary tables see the dedicated vignette, which can be obtained, e.g., via vignette( papeR\\_with\\_latex , package = papeR ). See also there for more details on summary tables in general. 287.0.4 Converting summaries to Markdown To obtain markdown output we can use, for example, the function kable() from package knitr on the summary objects: {r , eval=FALSE, echo = TRUE, results = &#39;asis&#39;} library( knitr ) kable(summarize(Orthodont, type = numeric )) kable(summarize(Orthodont, type = factor , variables = Sex , cumulative = TRUE)) kable(summarize(Orthodont, type = numeric , group = Sex , test = FALSE)) which gives the following results {r, eval=FALSE, echo = TRUE, results = &#39;asis&#39;} library( knitr ) kable(summarize(Orthodont, type = numeric )) kable(summarize(Orthodont, type = factor , variables = Sex , cumulative = TRUE)) kable(summarize(Orthodont, type = numeric , group = Sex )) 287.0.5 Prettify model output To prettify the output of a linear model, one can use the function prettify(). This function adds confidence intervals, properly prints p-values, adds significance stars to the output (if desired) and additionally adds pretty formatting for factors. {r eval=FALSE, include=FALSE, echo=TRUE} linmod &lt;- lm(distance ~ age + Sex, data = Orthodont) ## Extract pretty summary (pretty_lm &lt;- prettify(summary(linmod))) The resulting table can now be formatted for printing using packages like xtable for LaTeX which can be used in .Rnw files with the option {r # esults='asis' (in knitr) or {r # esults = tex (in Sweave) {r, results=&#39;hide&#39;} xtable(pretty_lm) In markdown files (.Rmd) one can instead use the function kable() with the chunk option {r # esults='asis'. The result looks as follows: {r, results=&#39;asis&#39;} kable(pretty_lm) 287.0.5.1 Supported objects The function prettify is currently implemented for objects of the following classes: lm (linear models) glm (generalized linear models) coxph (Cox proportional hazards models) lme (linear mixed models; implemented in package nlme) mer (linear mixed models; implemented in package lme4, version &lt; 1.0) merMod (linear mixed models; implemented in package lme4, version &gt;= 1.0) anova (anova objects) "],["power-analysis-1.html", "Chapter 288 Power Analysis", " Chapter 288 Power Analysis How to calculate statistical power for your meta-analysis https://towardsdatascience.com/how-to-calculate-statistical-power-for-your-meta-analysis-e108ee586ae8 "],["r-scripts.html", "Chapter 289 R scripts", " Chapter 289 R scripts author: Marcello Gallucci, Giulio Constantini &amp; Marco Perugini {r eval=FALSE, include=FALSE, echo=TRUE} knitr::opts_chunk$set(echo = TRUE) if(!require( pacman )) install.packages( pacman ) pacman::p_load( ggplot2 , pwr , easypower , powerMediation , bmem ) "],["introduction-2.html", "Chapter 290 Introduction", " Chapter 290 Introduction In this page one can found all the R functions discussed in Perugini, Gallucci, Constantini (2018), A practical primer to power analysis for simple experimental designs. International Review of Social Psychology. rewf here. Examples are taken from the papers. In all the examples that require multiple lines of code the user needs to update only the variables listed before ### end of input ### comment. The results are computed automatically for the required N. Small changes in the code are required to compute other power parameters. "],["t-test.html", "Chapter 291 t-test 291.1 Independent Samples 291.2 Sensitivity analysis 291.3 Paired Samples", " Chapter 291 t-test 291.1 Independent Samples A very simple function for independent samples t-test. To use with Cohen‚Äôs d, set sd=1 and put delta=d, where d is your effect size. {r include=FALSE, eval=FALSE, echo = TRUE} power.t.test(power=.80, sig.level=.05, delta=.5, sd=1) Required n for each group should be rounded up to the first whole number. Exactly one of n, delta, sd, power, and sig.level must be NULL. The NULL parameter is estimated by the function. Thus, post-hoc analysis is done omitting power and including n. {r eval=FALSE, include=FALSE, echo=TRUE} power.t.test(n=64, sig.level=.05, delta=.5, sd=1) ?power.t.test for all the other options. 291.2 Sensitivity analysis In the paper we suggested to look at the relationship between total sample size and effect size for a given power level (.80 in the example) and alpha (.05 in the example). In R this can be done as follows: {r include=FALSE, eval=FALSE, echo = TRUE} library( ggplot2 ) N&lt;-seq(20,140,by=20) ## end of input ## dat&lt;-data.frame(n=N) for (i in seq(N)) { one.power&lt;-power.t.test(power=.80, sig.level=.05, sd=1, n=N[i]) dat$d[i]&lt;-one.power$delta } plt&lt;-ggplot(dat,aes(y=d,x=n, label = round(d,digits = 2))) +stat_smooth(method = loess ,se=FALSE) plt&lt;-plt+labs(x= Totale sample size ,y= Effect size d ) plt&lt;-plt+scale_x_continuous(breaks=seq(min(N),max(N),by=20)) plt&lt;-plt+geom_label() plt 291.3 Paired Samples Same function with the option type = paired {r include=FALSE, eval=FALSE, echo = TRUE} power.t.test(power=.80, sig.level=.05, delta=0.527, sd=1,type = paired ) "],["one-way-analysis-of-variance.html", "Chapter 292 One way Analysis of Variance", " Chapter 292 One way Analysis of Variance For all F-test related power analysis, it is generally better to use pwr.f2.test() function from pwr package. The function uses \\(f^2\\) as effect size, which is the square of the \\(f\\) used by GPower ANOVA: Fixed effects, omnibus and one-way . Here we assume one has the \\(\\eta_p^2\\), which in one-way ANOVA is the \\(R^2\\). For required N (total sample size) one needs to input also \\(k\\), the number of groups defined by the independent variable. {r include=FALSE, eval=FALSE, echo = TRUE} library( pwr ) ## input ## etap&lt;-.326 k&lt;-8 # number of groups in the design ## end of input ## f2=etap/(1-etap) dfn&lt;-k-1 # numerator degrees of freedom (res&lt;-pwr.f2.test(u = dfn,f2 = f2,sig.level = .05,power = .80)) ## required N ## ceiling(res$v)+res$u+1 "],["factorial-designs.html", "Chapter 293 Factorial Designs 293.1 method 1 293.2 Method 2 293.3 Power analysis for contrasts 293.4 Guessing the interaction effect size from one-way designs 293.5 Means based method", " Chapter 293 Factorial Designs Assume that in a 3 X 2 factorial design the researcher expects the interaction to explain around 10% of the variance. If the researcher expects no main effect, the proportion of residual variance is 1-.10=.90, so the pŒ∑2=.10. 293.1 method 1 Here we keep using pwr.f2.test because it can be used for any F-test in the linear model. However, for prospective power (required total N) the function returns the required error degrees of freedom, so the required N must be approximate adding the effects degrees of freedom to the error degrees of freedom. The following code does it automatically. {r include=FALSE, eval=FALSE, echo = TRUE} library( pwr ) ## input for interaction## etap&lt;-.10 k1&lt;-3 # levels of the first factor k2&lt;-2 # levels of the second factor ## end of input ## (f2=etap/(1-etap)) dfn&lt;-(k1-1)*(k2-1) # df for the interaction (res&lt;-pwr.f2.test(u = dfn,f2 = f2,sig.level = .05,power = .80)) ## required N ## ceiling(res$v)+(k1*k2)-1 ## input for main effects## etap&lt;-.10 k1&lt;-3 # levels of the first factor k2&lt;-2 # levels of the second factor ## end of input ## (f2=etap/(1-etap)) dfn&lt;-(k1-1) # df for the main effect 1 (res&lt;-pwr.f2.test(u = dfn,f2 = f2,sig.level = .05,power = .80)) ## required N ## ceiling(res$v)+(k1*k2)-1 dfn&lt;-(k2-1) # df for the main effect 2 (res&lt;-pwr.f2.test(u = dfn,f2 = f2,sig.level = .05,power = .80)) ## required N ## ceiling(res$v)+(k1*k2)-1 293.2 Method 2 The package easypower provides short cuts for factorial ANOVA power analysis. It provides a dedicated function n.multiway() which simplifies computation of required N for main effects and interaction. It does not provide estimates for post-hoc power and other applications of power analysis. {r include=FALSE, eval=FALSE, echo = TRUE} library( easypower ) ## input ## etap&lt;-.10 k1&lt;-3 k2&lt;-2 ## end of input ## main.eff1 &lt;- list(name = A , levels = k1, eta.sq = etap) main.eff2 &lt;- list(name = C , levels = k2, eta.sq = etap) int.eff1 &lt;- list(name = A*C , eta.sq = etap) n.multiway(iv1 = main.eff1, iv2 = main.eff2, int1 = int.eff1) Results are the same than method 1 a part from rounding. 293.3 Power analysis for contrasts 293.3.1 Generic Contrasts Main effects and interaction for example in Table 2. They are all equivalent, so we go for the interaction. We compute the \\(f\\) as in the paper, but use \\(f^2\\) as required by `pwr.f2. {r include=FALSE, eval=FALSE, echo = TRUE} ## input ## means&lt;-c(10,0,0,0) sd=5 contInt&lt;-c(1,-1,-1,1) ## end input ## ### contrast value cvInt&lt;-contInt%*%means ## Effect sizes ## fInt&lt;-cvInt/sqrt(length(means)*sum(contInt^2*sd^2)) #### expected N res&lt;-pwr.f2.test(u=1,power=.80,sig.level = .05, f2=fInt^2) ceiling(res$v)+length(means) 293.4 Guessing the interaction effect size from one-way designs Consider the case in which the pattern of means in Table 3, case 1, in the paper is taken from a one-way design in which A1 and A2 show means equal to 5 and 2, respectively, and the same within groups variability (say equal to 1). Basically, the researcher observes in the literature a one-way design with only A as a factor and wishes to test the moderating effect of B in a 2 x 2 design. The B factor has two levels, that for simplicity we name ‚Äúreplicated‚Äù and ‚Äúmoderated‚Äù. The problem is to determine the power parameters of the expected interaction effect. 293.5 Means based method {r include=FALSE, eval=FALSE, echo = TRUE} # observed means # A1&lt;-c(5,2) # expected means # A2&lt;-c(0,0) sd=1 contInt&lt;-c(1,-1,-1,1) ## end input ## means&lt;-c(A1,A2) cvInt&lt;-contInt%*%means ## Effect sizes ## fInt&lt;-cvInt/sqrt(length(means)*sum(contInt^2*sd^2)) #### expected N res&lt;-pwr.f2.test(u=1,power=.80,sig.level = .05, f2=fInt^2) ceiling(res$v)+length(means) 293.5.1 Percentage of moderation Researcher should estimate the percentage of moderation, where 100% means full suppression of the effec, 200% full reversing of the effect. {r include=FALSE, eval=FALSE, echo = TRUE} # observed means # A1&lt;-c(5,2) cont1&lt;-c(1,-1) l&lt;-2 #levels of moderators sd=1 # percentage of moderation pm&lt;-100 ## end of input ## k1&lt;-length(A1) k2&lt;-length(A1)*l f0&lt;-(A1%*%cont1)/(sqrt(length(A1)*sum(cont1^2)*sd^2)) f&lt;-(pm/100)*f0*sqrt(length(cont1)/(length(cont1)*l^2)) #### expected N (res&lt;-pwr.f2.test(u=1,power=.80,sig.level = .05, f2=f^2)) ceiling(res$v)+length(cont1)*l 293.5.2 Paper example An example of a more complex design Consider a researcher who wishes to design a moderation study based on an one-way design with four conditions implementing an increasing intensity of a stimulus, such that the observed pattern of mean shows a linear trend. In particular, the observed linear trend contrast has an p=.184, corresponding to a f=0.475. {r include=FALSE, eval=FALSE, echo = TRUE} ### starting from eta-squared ### # percentage of moderation pm&lt;-125 # observed parameters # etap&lt;-.184 cont&lt;-c(-3,-1,1,3) l&lt;-2 # levels of moderator ### end of input ### (f0&lt;-sqrt(etap/(1-etap))) (f&lt;-(pm/100)*f0*sqrt(length(cont1)/(length(cont1)*l^2))) #### expected N (total sample) (res&lt;-pwr.f2.test(u=1,power=.80,sig.level = .05, f2=f^2)) ceiling(res$v)+(length(cont)*l) The same results can be obtained if one anticipates all expected means and pooled standard deviation (tedious method) {r include=FALSE, eval=FALSE, echo = TRUE} Replicated&lt;-c(5,9,16,20) sd&lt;-12.239 Expected&lt;-c(5,3,2,1) cont&lt;-c(-3,-1,1,3) ## end of input ## contInt&lt;-c(cont,-cont) means&lt;-c(Replicated,Expected) (f&lt;-(contInt%*%means)/sqrt(length(contInt)*sum(contInt^2)*sd^2)) #### expected N res&lt;-pwr.f2.test(u=1,power=.80,sig.level = .05, f2=f^2) ceiling(res$v)+length(cont1)*2 "],["regression-analysis.html", "Chapter 294 Regression Analysis", " Chapter 294 Regression Analysis Any effect in simple and the multiple regression for which the \\(\\eta_p^2\\) is available can be evaluated witht he following code. here is an example for a regression with three predictors and a small effect size, according to Cohen‚Äôs (Cohen, 1988) classification. {r include=FALSE, eval=FALSE, echo = TRUE} ## input ## etap&lt;-0.02 k&lt;-3 # number of predictors ## end of input ### (f2&lt;-etap/(1-etap)) res&lt;-pwr.f2.test(u=1,power=.80,sig.level = .05, f2=f^2) ceiling(res$v)+k "],["moderated-regression.html", "Chapter 295 Moderated regression 295.1 Interaction between continuous and dichotomous predictors. 295.2 Interaction between continuous predictors", " Chapter 295 Moderated regression 295.1 Interaction between continuous and dichotomous predictors. The researcher needs to estimate the expected correlation between the continuous independent variable and the dependent variable for the two groups defined by the moderator. {r include=FALSE, eval=FALSE, echo = TRUE} ## input ## ra&lt;-.0 rb&lt;-.50 k&lt;-3 # number of coefficients in the regression (not considering the intercept) ## end of input ## (bint&lt;-abs(ra-rb)) (f2&lt;-bint^2/(2*(2-ra^2-rb^2))) ## expected N ### res&lt;-pwr.f2.test(u=1,power=.80,sig.level = .05, f2=f2) ceiling(res$v)+k+1 295.2 Interaction between continuous predictors The researcher needs to estimate the expected correlation between the continuous independent variables and the dependent variable, and the change of correlation from the average value of the moderator to one standard deviation above average of the moderator. We assume here that there {r include=FALSE, eval=FALSE, echo = TRUE} ## input ## r_yx&lt;-.35 r_ym&lt;-.25 r_change&lt;-.175 k&lt;-3 # number of coefficients in the regression (not considering the intercept) ## end of input ## (f2&lt;-r_change^2/(1-r_yx^2-r_ym^2)) ## expected N ### res&lt;-pwr.f2.test(u=1,power=.80,sig.level = .05, f2=f2) ceiling(res$v)+k+1 "],["mediation.html", "Chapter 296 Mediation", " Chapter 296 Mediation 296.0.1 method based on Sobel test {r eval=FALSE, include=FALSE, echo=TRUE} library( powerMediation ) ### input ### # a, b, are the coefficients as in standard mediational path diagram a&lt;-.8186 b&lt;-.4039 c&lt;- .4334 ### end of input ### # compute sigma.epsilon sigma.epsilon&lt;-sqrt(1-(b^2+c^2+2*a*b*c)) ## results: expected N res&lt;-ssMediation.Sobel(power = .80, theta.1a = .8186, lambda.a = .4039, sigma.x = 1, sigma.m = 1, sigma.epsilon = .6020) ceiling(res$n) 296.0.2 method based on bootstrap analysis The code below allows computing the power achieved with a certain sample size, that must be indicated as nobs below. The achieved power for the Indirect/Mediation effect (ab) using a sample of size nobs can be read under column Power . The following code is based on boostrap and it takes a lot of time (several hours) to run. Be extremely patient (or leave it running at night). If you have a multi-core processor, function power.boot implements parallel processing to speed-up computations. See ?power.boot for details. {r eval=F} library( bmem ) ### input ### # a, b, are the coefficients as in standard mediational path diagram, n is the sample size- a&lt;-.8186 b&lt;-.4039 c&lt;- .4334 nobs &lt;- 100 ### end of input ### model &lt;- paste(c( &#39;M ~ a*X + start(&#39;,a,&#39;)*X&#39;, &#39;Y ~ b*M + c*X + start(&#39;,b,&#39;) * M + start(.4334)*X&#39;, &#39;X ~~ start(1)*X&#39;, &#39;M ~~ start(1)*M&#39;, &#39;Y ~~ start(1)*Y&#39;), collapse = \\n ) set.seed(1234) power.result &lt;- power.boot(model, indirect = &#39;ab := a*b&#39;, nobs = nobs) summary(power.result) "],["power-analysis-2.html", "Chapter 297 Power Analysis", " Chapter 297 Power Analysis A Practical Primer To Power Analysis for Simple Experimental Designs. International Review of Social Psychology, 31(1), 20. DOI: http://doi.org/10.5334/irsp.181 https://www.rips-irsp.com/article/10.5334/irsp.181/ https://github.com/mcfanda/primerPowerIRSP "],["my-r-codes-for-data-analysis-7.html", "Chapter 298 My R Codes For Data Analysis", " Chapter 298 My R Codes For Data Analysis "],["prepare-data-for-analysis-veriyi-analiz-i√ßin-hazƒ±rlamak-1.html", "Chapter 299 Prepare Data for Analysis / Veriyi Analiz i√ßin hazƒ±rlamak 299.1 The Quartz guide to bad data 299.2 K√∂t√º veri kƒ±lavuzu 299.3 data organization organizing data in spreadsheets 299.4 Tidy Data 299.5 The Ten Commandments for a well-formatted database 299.6 Software specific problems 299.7 Data reorganization", " Chapter 299 Prepare Data for Analysis / Veriyi Analiz i√ßin hazƒ±rlamak 299.1 The Quartz guide to bad data An exhaustive reference to problems seen in real-world data along with suggestions on how to resolve them. https://github.com/Quartz/bad-data-guide 299.2 K√∂t√º veri kƒ±lavuzu K√∂t√º veri kƒ±lavuzu https://sbalci.github.io/Kotu-Veri-Kilavuzu/index.html 299.3 data organization organizing data in spreadsheets https://kbroman.org/dataorg/ Daniel Kaplan. (2018) Teaching Stats for Data Science. The American Statistician 72:1, pages 89-96. https://doi.org/10.1080/00031305.2017.1375989 299.4 Tidy Data Hadley Wickham. Tidy data. The Journal of Statistical Software, vol. 59, 2014. http://vita.had.co.nz/papers/tidy-data.html https://www.jstatsoft.org/article/view/v059i10 http://dx.doi.org/10.18637/jss.v059.i10 299.5 The Ten Commandments for a well-formatted database The Ten Commandments for a well-formatted database https://rtask.thinkr.fr/blog/the-ten-commandments-for-a-well-formatted-database/ 299.6 Software specific problems 299.6.1 Keep SPSS labels {r eval=FALSE, include=FALSE, echo=TRUE} library(foreign) # foreign paketi y√ºkleniyor read.spss komutu ile deƒüer etiketlerini almasƒ±nƒ± ve bunu liste olarak deƒüil de data.frame olarak kaydetmesini istiyoruz {r eval=FALSE, include=FALSE, echo=TRUE} mydata &lt;- read.spss( mydata.sav , use.value.labels = TRUE, to.data.frame = TRUE) aktardƒ±ƒüƒ±mƒ±z data.frame‚Äôin √∂zellikleri (attr) i√ßinde deƒüi≈ükenlerin etiketleri var, bunlarƒ± dƒ±≈üarƒ± √ßƒ±kartƒ±yoruz {r eval=FALSE, include=FALSE, echo=TRUE} VariableLabels &lt;- as.data.frame(attr(mydata, variable.labels )) elde ettiƒüimiz data.frame‚Äôdeki satƒ±r isimleri deƒüi≈ükenlerin isimleri oluyor, kar≈üƒ±larƒ±nda da deƒüi≈üken etiketleri var satƒ±r isimlerini de dƒ±≈üarƒ± √ßƒ±kartƒ±yoruz {r eval=FALSE, include=FALSE, echo=TRUE} VariableLabels$original &lt;- rownames(VariableLabels) Deƒüi≈üken etiketi olanlarƒ± etiketleri ile diƒüerlerini olduƒüu gibi saklƒ±yoruz {r eval=FALSE, include=FALSE, echo=TRUE} VariableLabels$label[VariableLabels$label == ] &lt;- NA VariableLabels$colname &lt;- VariableLabels$original VariableLabels$colname[!is.na(VariableLabels$label)] &lt;- as.vector(VariableLabels$label[!is.na(VariableLabels$label)]) son olarak da data.frame‚Äôdeki s√ºtun isimlerini deƒüi≈ütiriyoruz {r eval=FALSE, include=FALSE, echo=TRUE} names(mydata) &lt;- VariableLabels$colname 299.6.2 Make both computer and human readible variable names turkce karakter donusumu {r turkce karakter donusumu, eval=FALSE, include=FALSE} # https://suatatan.wordpress.com/2017/10/07/bulk-replacing-turkish-characters-in-r/ to.plain &lt;- function(s) { # 1 character substitutions old1 &lt;- √ßƒü≈üƒ±√º√∂√áƒû≈ûƒ∞√ñ√ú new1 &lt;- cgsiuocgsiou s1 &lt;- chartr(old1, new1, s) # 2 character substitutions old2 &lt;- c( ≈ì , √ü , √¶ , √∏ ) new2 &lt;- c( oe , ss , ae , oe ) s2 &lt;- s1 for(i in seq_along(old2)) s2 &lt;- gsub(old2[i], new2[i], s2, fixed = TRUE) s2 } names(df) &lt;- make.names(to.plain(tolower(names(df)))) names(df) &lt;- names(df) %&gt;% tolower() %&gt;% to.plain() %&gt;% make.names() df$source=as.vector(sapply(df$source,to.plain)) make.names(tolower(names(df))) to.plain(names(df)) purrr::map(df, to.plain) 299.6.3 Anonimisation 299.6.4 Add subject ID to the data / veriye ID ekleme {r eval=FALSE, include=FALSE, echo=TRUE} df &lt;- tibble::rowid_to_column(df, subject ) 299.7 Data reorganization {r eval=FALSE, include=FALSE, echo=TRUE} scabies &lt;- read.csv(file = http://datacompass.lshtm.ac.uk/607/2/S1-Dataset_CSV.csv , header = TRUE, sep = , ) scabies$gender == male scabies$age[scabies$gender == male ] mean(scabies$age[scabies$gender == male ]) scabies %&gt;% filter(gender == male ) %&gt;% summarise_at( age ,mean) {r eval=FALSE, include=FALSE, echo=TRUE} scabies$agegroups &lt;- as.factor(cut(scabies$age, c(0,10,20,Inf), labels = c( 0-10 , 11-20 , 21+ ), include.lowest = TRUE)) scabies$house_cat &lt;- as.factor(cut(scabies$house_inhabitants, c(0,5,10,Inf), labels = c( 0-5 , 6-10 , 10+ ), include.lowest = TRUE)) table(scabies$house_cat, scabies$house_inhabitants) {r eval=FALSE, include=FALSE, echo=TRUE} ebola$status &lt;- as.numeric(ebola$status) {r eval=FALSE, include=FALSE, echo=TRUE} ebola$transmission &lt;- recode(ebola$transmission, syringe = needle ) {r eval=FALSE, include=FALSE, echo=TRUE} scabies$house_cat &lt;- relevel(scabies$house_cat, ref = 0-5 ) #Make 0-5 household size the baseline group {r eval=FALSE, include=FALSE, echo=TRUE} df &lt;- data.frame(month=rep(1:3,2), student=rep(c( Amy , Bob ), each=3), A=c(9, 7, 6, 8, 6, 9), B=c(6, 7, 8, 5, 6, 7)) #Here we have a where each student is on a different row for each month #The students took two tests A and B. #For each student/month combination we have a value for A and a value for B df2 &lt;- gather(df,test,score,A:B) #Make a new datatable df2 #Have a column called test . #This will have the value either A or B as these are the names of the columns we specified. #Have a column called score . #This will have the value previously in column A or B respectively for each row df2 {r eval=FALSE, include=FALSE, echo=TRUE} #Now we have a single row for each combination of month/student/test #Their score is in the score column df3 &lt;- spread(df2,test,score) #Make a new datatable df3 #Make a column for each unique value in the test variable. #Name each of these columns based on that unique value #Under each column put the corresponding value that was in the score column df3 {r eval=FALSE, include=FALSE, echo=TRUE} dt3 &lt;- expandRows(dt, 2) #Expand the original datatable. Replicate each row by the value in column 2. dt3 https://stackoverflow.com/questions/22353633/filter-for-complete-cases-in-data-frame-using-dplyr-case-wise-deletion df %&gt;% na.omit or this: df %&gt;% filter(complete.cases(.)) or this: library(tidyr) df %&gt;% drop_na If you want to filter based on one variable‚Äôs missingness, use a conditional: df %&gt;% filter(!is.na(x1)) or df %&gt;% drop_na(x1) Other answers indicate that of the solutions above na.omit is much slower but that has to be balanced against the fact that it returns and row indices of the omitted rows as the na.action attribute whereas the other solutions above do not. str(df %&gt;% na.omit) ‚Äòdata.frame‚Äô: 2 obs. of 2 variables: $ x1: num 1 2 $ x2: num 1 2 - attr(, na.action )= ‚Äòomit‚Äô Named int 3 4 ..- attr(, names )= chr 3 4 ozp &lt;- veri %&gt;% select(RaporNo, Hasta, cinsiyet, Yas, ozp_parca, ozp_kaset, ozp_cap, ozp_tani, ozp_kod) %&gt;% filter(!is.na(ozp_parca) | !is.na(ozp_kaset) | !is.na(ozp_cap) | !is.na(ozp_tani) | !is.na(ozp_kod) ) ozp2 &lt;- veri %&gt;% select(RaporNo, Hasta, cinsiyet, Yas, ozp_parca, ozp_kaset, ozp_cap, ozp_tani, ozp_kod) %&gt;% filter(complete.cases(ozp_parca, ozp_kaset, ozp_cap, ozp_tani, ozp_kod) ) ozp3 &lt;- veri %&gt;% select(RaporNo, Hasta, cinsiyet, Yas, ozp_parca, ozp_kaset, ozp_cap, ozp_tani, ozp_kod) %&gt;% na.omit(ozp_parca, ozp_kaset, ozp_cap, ozp_tani, ozp_kod) "],["xray.html", "Chapter 300 xray", " Chapter 300 xray The R Package to Have X Ray Vision on your Datasets https://blog.datascienceheroes.com/x-ray-vision-on-your-datasets/ {r eval=FALSE, include=FALSE, echo=TRUE} # install.packages( devtools ) devtools::install_github( sicarul/xray ) {r eval=FALSE, include=FALSE, echo=TRUE} data(longley) badLongley &lt;- longley badLongley$GNP &lt;- NA xray::anomalies(badLongley) {r eval=FALSE, include=FALSE, echo=TRUE} distrLongley &lt;- longley distrLongley$testCategorical &lt;- c(rep(&#39;One&#39;,7), rep(&#39;Two&#39;, 9)) xray::distributions(distrLongley) {r eval=FALSE, include=FALSE, echo=TRUE} dateLongley &lt;- longley dateLongley$Year &lt;- as.Date(paste0(dateLongley$Year,&#39;-01-01&#39;)) dateLongley$Data &lt;- &#39;Original&#39; ndateLongley &lt;- dateLongley ndateLongley$GNP &lt;- dateLongley$GNP+10 ndateLongley$Data &lt;- &#39;Offseted&#39; xray::timebased(rbind(dateLongley, ndateLongley), &#39;Year&#39;) "],["convert-all-data-frame-into-character.html", "Chapter 301 convert all data.frame into character", " Chapter 301 convert all data.frame into character df &lt;- purrr::map_df(df, as.character) "],["rcase4base-reshape-data-with-base-r.html", "Chapter 302 R:case4base - reshape data with base R", " Chapter 302 R:case4base - reshape data with base R https://jozefhajnala.gitlab.io/r/r002-data-manipulation/ "],["rcase4base-data-aggregation-with-base-r.html", "Chapter 303 R:case4base - data aggregation with base R", " Chapter 303 R:case4base - data aggregation with base R https://jozefhajnala.gitlab.io/r/r003-aggregation/ "],["tidyr.html", "Chapter 304 tidyr", " Chapter 304 tidyr This week I'm going to be looking at some #tidyr functions! üí° First is uncount() which might come in handy if you want to transform a summary table to individual rows. #rstats pic.twitter.com/UZE0wUcEHC ‚Äî Nic Crane ((???)) November 26, 2018 df &lt;- data.frame( V1 = c(0, 0, 0, 0, 1, 1, 1, 1, 0, 1), V2 = c(1, 1, 1, 0, 0, 0, 0, 0, 0, 1), V3 = c(0, 0, 0, 1, 1, 1, 1, 1, 0, 1) ) df$V1_rec &lt;- grepl(pattern = 0 , x = df$V1) df$V2_rec &lt;- grepl(pattern = 0 , x = df$V2) df$V3_rec &lt;- grepl(pattern = 0 , x = df$V3) df %&gt;% mutate(toplam = select(., V1_rec:V3_rec) %&gt;% rowSums(na.rm = TRUE) ) "],["python-pandas.html", "Chapter 305 Python Pandas", " Chapter 305 Python Pandas http://nbviewer.jupyter.org/github/justmarkham/pandas-videos/blob/master/pandas.ipynb https://www.youtube.com/watch?v=5_QXMwezPJE&amp;list=PL5-da3qGB5ICCsgW1MxlZ0Hq8LL5U3u9y&amp;index=2 https://developers.google.com/edu/python/ "],["text-data.html", "Chapter 306 Text Data", " Chapter 306 Text Data "],["quanteda-quantitative-analysis-of-textual-data.html", "Chapter 307 quanteda: Quantitative Analysis of Textual Data", " Chapter 307 quanteda: Quantitative Analysis of Textual Data An R package for the Quantitative Analysis of Textual Data https://quanteda.io https://github.com/quanteda/quanteda "],["quick-start-guide.html", "Chapter 308 Quick Start Guide", " Chapter 308 Quick Start Guide https://quanteda.io/articles/pkgdown/quickstart.html {r download package} # devtools package required to install quanteda from Github # devtools::install_github( quanteda/quanteda ) # install.packages( quanteda ) # install.packages( readtext ) "],["readtext-import-and-handling-for-plain-and-formatted-text-files.html", "Chapter 309 readtext: Import and handling for plain and formatted text files", " Chapter 309 readtext: Import and handling for plain and formatted text files https://readtext.quanteda.io https://github.com/quanteda/readtext "],["httpsgithub-comquantedareadtext.html", "Chapter 310 https://github.com/quanteda/readtext", " Chapter 310 https://github.com/quanteda/readtext "],["httpsgithub-comquantedastopwords.html", "Chapter 311 https://github.com/quanteda/stopwords", " Chapter 311 https://github.com/quanteda/stopwords "],["httpsgithub-comquantedaspacyr.html", "Chapter 312 https://github.com/quanteda/spacyr", " Chapter 312 https://github.com/quanteda/spacyr "],["httpsgithub-comquantedaquanteda-corpora.html", "Chapter 313 https://github.com/quanteda/quanteda.corpora", " Chapter 313 https://github.com/quanteda/quanteda.corpora "],["httpsgithub-comkbenoitquanteda-dictionaries.html", "Chapter 314 https://github.com/kbenoit/quanteda.dictionaries", " Chapter 314 https://github.com/kbenoit/quanteda.dictionaries "],["httpsquanteda-ioarticlesquickstart-html.html", "Chapter 315 https://quanteda.io/articles/quickstart.html", " Chapter 315 https://quanteda.io/articles/quickstart.html "],["httpsquanteda-ioarticlespkgdowncomparison-html.html", "Chapter 316 https://quanteda.io/articles/pkgdown/comparison.html", " Chapter 316 https://quanteda.io/articles/pkgdown/comparison.html "],["httpsquanteda-ioarticlespkgdowndesign-html.html", "Chapter 317 https://quanteda.io/articles/pkgdown/design.html", " Chapter 317 https://quanteda.io/articles/pkgdown/design.html "],["httpsquanteda-ioarticlespkgdownexamplesphrase-html.html", "Chapter 318 https://quanteda.io/articles/pkgdown/examples/phrase.html", " Chapter 318 https://quanteda.io/articles/pkgdown/examples/phrase.html "],["httpsquanteda-ioarticlespkgdownexamplesplotting-html.html", "Chapter 319 https://quanteda.io/articles/pkgdown/examples/plotting.html", " Chapter 319 https://quanteda.io/articles/pkgdown/examples/plotting.html "],["httpsquanteda-ioarticlespkgdownexampleslsa-html.html", "Chapter 320 https://quanteda.io/articles/pkgdown/examples/lsa.html", " Chapter 320 https://quanteda.io/articles/pkgdown/examples/lsa.html "],["httpsquanteda-ioarticlespkgdownexamplestwitter-html.html", "Chapter 321 https://quanteda.io/articles/pkgdown/examples/twitter.html", " Chapter 321 https://quanteda.io/articles/pkgdown/examples/twitter.html "],["httpsquanteda-ioarticlespkgdownreplicationdigital-humanities-html.html", "Chapter 322 https://quanteda.io/articles/pkgdown/replication/digital-humanities.html", " Chapter 322 https://quanteda.io/articles/pkgdown/replication/digital-humanities.html "],["httpsquanteda-ioarticlespkgdownreplicationtext2vec-html.html", "Chapter 323 https://quanteda.io/articles/pkgdown/replication/text2vec.html", " Chapter 323 https://quanteda.io/articles/pkgdown/replication/text2vec.html "],["httpsquanteda-ioarticlespkgdownreplicationqss-html.html", "Chapter 324 https://quanteda.io/articles/pkgdown/replication/qss.html", " Chapter 324 https://quanteda.io/articles/pkgdown/replication/qss.html "],["footer.html", "Chapter 325 footer", " Chapter 325 footer {r citation 2} citation(package = quanteda ) title: R Aray√ºzler5 Bu bir derlemedir, m√ºmk√ºn mertebe alƒ±ntƒ±lara referans vermeye √ßalƒ±≈ütƒ±m.‚Ü©Ô∏é "],["r-i√ßin-aray√ºzler.html", "Chapter 326 R i√ßin aray√ºzler 326.1 Analiz i√ßin genel GUI 326.2 Raporlama i√ßin 326.3 Grafikler", " Chapter 326 R i√ßin aray√ºzler 326.1 Analiz i√ßin genel GUI 326.1.1 R Commander https://socialsciences.mcmaster.ca/jfox/Misc/Rcmdr/ #### Obtain names of all packages on CRAN names.available.packages &lt;- rownames(available.packages()) #### Extract packages names that contain Rcmdr Rcmdr.related.packages &lt;- names.available.packages[grep( Rcmdr , names.available.packages)] Rcmdr.related.packages #### Install these packages install.packages(pkgs = Rcmdr.related.packages) # load library &amp; run Rcmdr library(Rcmdr) # run Rcmdr Rcmdr::Commander() 326.1.1.1 EZR An extension for R Commander http://www.jichi.ac.jp/saitama-sct/SaitamaHP.files/statmedEN.html Investigation of the freely available easy-to-use software ‚ÄòEZR‚Äô for medical statistics 326.1.2 RKWard https://rkward.kde.org/Main_Page 326.1.3 Rattle https://rattle.togaware.com/ 326.1.4 Jamovi https://www.jamovi.org/ 326.1.5 JASP https://jasp-stats.org/ 326.1.6 Blue Sky Statistics https://www.blueskystatistics.com/ 326.1.7 R AnalyticFlow https://r.analyticflow.com/en/ 326.1.8 Deducer http://www.deducer.org/pmwiki/pmwiki.php?n=Main.DeducerManual https://r4stats.com/2018/06/13/deducer/ install.packages(c( JGR , Deducer , DeducerExtras )) library( JGR ) JGR() 326.1.9 Radiant https://radiant-rstats.github.io/docs/tutorials.html 326.1.10 lessR https://cran.r-project.org/web/packages/lessR/index.html http://www.lessrstats.com/ http://www.lessrstats.com/videos.html 326.1.11 Renjin http://www.renjin.org/ 326.1.12 FastR https://github.com/oracle/fastr http://www.graalvm.org/docs/reference-manual/languages/r/ 326.2 Raporlama i√ßin 326.2.1 Stencila 326.2.2 Datazar 326.3 Grafikler 326.3.1 esquisse https://github.com/dreamRs/esquisse 326.3.2 ggExtra https://github.com/daattali/ggExtra 326.3.3 ggplotAssist https://github.com/cardiomoon/ggplotAssist 326.3.4 ggraptR https://github.com/cargomoose/ggraptR "],["eclipse-an-alternative-to-rstudio.html", "Chapter 327 Eclipse ‚Äì an alternative to RStudio", " Chapter 327 Eclipse ‚Äì an alternative to RStudio https://datascienceplus.com/eclipse-an-alternative-to-rstudio-part-1/ "],["diƒüer-kodlar-4.html", "Chapter 328 Diƒüer kodlar", " Chapter 328 Diƒüer kodlar Diƒüer kodlar i√ßin bakƒ±nƒ±z: https://sbalci.github.io/ "],["geri-bildirim-4.html", "Chapter 329 Geri Bildirim", " Chapter 329 Geri Bildirim Geri bildirim i√ßin tƒ±klayƒ±nƒ±z: Geri bildirim formu Please enable JavaScript to view the comments powered by Disqus. "],["r-drake.html", "Chapter 330 R drake", " Chapter 330 R drake Reproducible workflows at scale with drake https://ropensci.org/commcalls/2019-09-24/ title: R ile analize ba≈ülarken6 {r setup global chunk settings, include=FALSE} knitr::opts_chunk$set( message = FALSE, warning = FALSE, comment = NA, include = FALSE, tidy = TRUE ) Bu bir derlemedir, m√ºmk√ºn mertebe alƒ±ntƒ±lara referans vermeye √ßalƒ±≈ütƒ±m.‚Ü©Ô∏é "],["r-nerede-kullanƒ±lƒ±r.html", "Chapter 331 R nerede kullanƒ±lƒ±r", " Chapter 331 R nerede kullanƒ±lƒ±r Veri d√ºzenleme ƒ∞statistik analiz Web sayfasƒ± hazƒ±rlama (Statik/Dinamik) https://sbalci.github.io/ https://kevinrue.shinyapps.io/isee-shiny-contest/ Sunum hazƒ±rlama (bu sunum) Programlama https://serdarbalci.netlify.com/pathtweets/ Otomatik, periodik ve tekrarlanabilir rapor hazƒ±rlama https://sbalci.github.io/AutoJournalWatch/GallbladderRecent.html pdf, html, ppt olu≈üturma tez yazma kitap yazma CV olu≈üturma https://rpubs.com/sbalci/CV poster hazƒ±rlama rapor ≈üablonu olu≈üturma Robot uygulamalarƒ± (Arudino kodu) ‚Ä¶ "],["r-generation.html", "Chapter 332 R generation", " Chapter 332 R generation R yƒ±llar i√ßinde √ßok fazla deƒüi≈üim g√∂sterdi https://rss.onlinelibrary.wiley.com/doi/10.1111/j.1740-9713.2018.01169.x :scale 30% {r eval=FALSE, include=FALSE, echo=TRUE} knitr::include_graphics(path = https://wol-prod-cdn.literatumonline.com/cms/attachment/1a49c07b-b56f-4327-8e92-7827ef51a7bb/sign1169-gra-0002-m.jpg ) "],["r-y√ºkleme-4.html", "Chapter 333 R y√ºkleme 333.1 R-project 333.2 RStudio 333.3 MacOS i√ßin", " Chapter 333 R y√ºkleme http://www.youtube.com/watch?v=XcBLEVknqvY What is R? 333.1 R-project https://cran.r-project.org/ 333.2 RStudio 333.2.1 RStudio https://www.rstudio.com/ https://www.rstudio.com/products/rstudio/download/ https://moderndive.com/2-getting-started.html 333.2.2 RStudio 333.2.3 RStudio eklentileri Discover and install useful RStudio addins https://cran.r-project.org/web/packages/addinslist/README.html https://rstudio.github.io/rstudioaddins/ devtools::install_github( rstudio/addinexamples , type = source ) 333.3 MacOS i√ßin 333.3.1 X11 https://www.xquartz.org/ 333.3.2 Java OS https://support.apple.com/kb/dl1572 "],["r-zor-≈üeyler-i√ßin-kolay-kolay-≈üeyler-i√ßin-zor-4.html", "Chapter 334 R zor ≈üeyler i√ßin kolay, kolay ≈üeyler i√ßin zor", " Chapter 334 R zor ≈üeyler i√ßin kolay, kolay ≈üeyler i√ßin zor R makes easy things hard, and hard things easy Aynƒ± ≈üeyi √ßok fazla ≈üekilde yapmak m√ºmk√ºn R Syntax Comparison::CHEAT SHEET https://www.amelia.mn/Syntax-cheatsheet.pdf #RStats ‚Äî There are always several ways to do the same thing‚Ä¶ nice example on with the identity matrix by (???) https://t.co/O3GXdPiM32 ‚Äî Colin Fay ü§ò ((???)) April 1, 2019 "],["r-paketleri-4.html", "Chapter 335 R paketleri 335.1 Neden paketler var 335.2 Paketleri nereden bulabiliriz 335.3 Kendi paket evrenini olu≈ütur 335.4 R paket y√ºkleme 335.5 Paket √ßaƒüƒ±rma", " Chapter 335 R paketleri 335.1 Neden paketler var I love the #rstats community.Someone is like, ‚Äúoh hey peeps, I saw a big need for this mundane but difficult task that I infrequently do, so I created a package that will literally scrape the last bits of peanut butter out of the jar for you. It's called pbplyr.‚ÄùWhat a tribe. ‚Äî Frank Elavsky   ≥ ((???)) July 3, 2018 335.2 Paketleri nereden bulabiliriz Available CRAN Packages By Name https://cran.r-project.org/web/packages/available_packages_by_name.html CRAN Task Views https://cran.r-project.org/web/views/ Bioconductor https://www.bioconductor.org RecommendR http://recommendr.info/ pkgsearch CRAN package search https://github.com/metacran/pkgsearch CRANsearcher https://github.com/RhoInc/CRANsearcher Awesome R https://awesome-r.com/ 335.3 Kendi paket evrenini olu≈ütur pkgverse: Build a Meta-Package Universe https://cran.r-project.org/web/packages/pkgverse/index.html 335.4 R paket y√ºkleme {r # install.packages( tidyverse , dependencies = TRUE) install.packages( jmv , dependencies = TRUE) install.packages( questionr , dependencies = TRUE) install.packages( Rcmdr , dependencies = TRUE) install.packages( summarytools ) 335.5 Paket √ßaƒüƒ±rma {r # require(tidyverse) require(jmv) require(questionr) library(summarytools) library(gganimate) "],["r-i√ßin-yardƒ±m-bulma-4.html", "Chapter 336 R i√ßin yardƒ±m bulma", " Chapter 336 R i√ßin yardƒ±m bulma {r # ?mean ??efetch help(merge) example(merge) RSiteSearch( shiny ) Vignette :scale 80% RDocumentation https://www.rdocumentation.org R Package Documentation https://rdrr.io/ GitHub Stackoverflow https://stackoverflow.com/ Google uygun anahtar kelime How I use #rstats h/t (???) pic.twitter.com/erRnTG0Ujr ‚Äî Emily Bovee ((???)) August 10, 2018 Google‚Äôda ararken [R] yazmak da i≈üe yarayabiliyor. searcher package üì¶ Awesome Cheatsheet https://github.com/detailyang/awesome-cheatsheet http://cran.r-project.org/doc/contrib/Baggott-refcard-v2.pdf https://www.rstudio.com/resources/cheatsheets/ Awesome R https://github.com/qinwf/awesome-R#readme https://awesome-r.com/ Twitter https://twitter.com/hashtag/rstats?src=hash Use Reproducible Examples When Asking Got a question to ask on (???) or post on (???)? No time to read the long post on how to use reprex? Here is a 20-second gif for you to format your R codes nicely and for others to reproduce your problem. (An example from a talk given by (???)) #rstat pic.twitter.com/gpuGXpFIsX ‚Äî ZhiYang ((???)) October 18, 2018 Keeping up to date with R news https://masalmon.eu/2019/01/25/uptodate/ "],["rstudio-ile-proje-olu≈üturma.html", "Chapter 337 Rstudio ile proje olu≈üturma", " Chapter 337 Rstudio ile proje olu≈üturma https://support.rstudio.com/hc/en-us/articles/200526207-Using-Projects "],["rstudio-ile-veri-y√ºkleme-4.html", "Chapter 338 RStudio ile veri y√ºkleme 338.1 Excel 338.2 SPSS 338.3 CSV", " Chapter 338 RStudio ile veri y√ºkleme https://support.rstudio.com/hc/en-us/articles/218611977-Importing-Data-with-RStudio 338.1 Excel 338.2 SPSS 338.3 CSV "],["veriyi-g√∂r√ºnt√ºleme-4.html", "Chapter 339 Veriyi g√∂r√ºnt√ºleme", " Chapter 339 Veriyi g√∂r√ºnt√ºleme Spreadsheet users using #rstats: where's the data?#rstats users using spreadsheets: where's the code? ‚Äî Leonard Kiefer ((???)) July 7, 2018 "],["veriyi-g√∂r√ºnt√ºleme-5.html", "Chapter 340 Veriyi g√∂r√ºnt√ºleme", " Chapter 340 Veriyi g√∂r√ºnt√ºleme {r # library(nycflights13) summary(flights) {r # View(data) {r # data() {r # head(data, n = 10) {r # tail(data) {r # glimpse(data) {r # str(data) {r # skimr::skim() "],["veriyi-deƒüi≈ütirme-4.html", "Chapter 341 Veriyi deƒüi≈ütirme 341.1 Veriyi kod ile deƒüi≈ütirelim 341.2 Veriyi eklentilerle deƒüi≈ütirme 341.3 RStudio aracƒ±lƒ±ƒüƒ±yla recode", " Chapter 341 Veriyi deƒüi≈ütirme 341.1 Veriyi kod ile deƒüi≈ütirelim 341.2 Veriyi eklentilerle deƒüi≈ütirme :scale 30% 341.3 RStudio aracƒ±lƒ±ƒüƒ±yla recode questionr paketi kullanƒ±lacak :scale 30% https://juba.github.io/questionr/articles/recoding_addins.html "],["basit-tanƒ±mlayƒ±cƒ±-istatistikler-4.html", "Chapter 342 Basit tanƒ±mlayƒ±cƒ± istatistikler 342.1 summarytools 342.2 DataExplorer 342.3 inspectdf 342.4 Grafikler", " Chapter 342 Basit tanƒ±mlayƒ±cƒ± istatistikler summary() mean median min max sd table() {r, echo=TRUE, include = TRUE} library(readr) irisdata &lt;- read_csv( data/iris.csv ) jmv::descriptives( data = irisdata, vars = Sepal.Length , splitBy = Species , freq = TRUE, hist = TRUE, dens = TRUE, bar = TRUE, box = TRUE, violin = TRUE, dot = TRUE, mode = TRUE, sum = TRUE, sd = TRUE, variance = TRUE, range = TRUE, se = TRUE, skew = TRUE, kurt = TRUE, quart = TRUE, pcEqGr = TRUE) {r, echo=TRUE, include=FALSE} # install.packages( scatr ) scatr::scat( data = irisdata, x = Sepal.Length , y = Sepal.Width , group = Species , marg = dens , line = linear , se = TRUE) 342.1 summarytools https://cran.r-project.org/web/packages/summarytools/vignettes/Introduction.html {r, include=FALSE, comment=NA, prompt=FALSE, cache=FALSE, echo=TRUE, results=&#39;asis&#39;} library(summarytools) summarytools::freq(iris$Species, style = rmarkdown ) {r, include=FALSE, comment=NA, prompt=FALSE, cache=FALSE, echo=TRUE, results=&#39;asis&#39;} summarytools::freq(iris$Species, report.nas = FALSE, style = rmarkdown , headings = FALSE) {r, include=FALSE, comment=NA, prompt=FALSE, cache=FALSE, echo=TRUE, results=&#39;asis&#39;} with(tobacco, print(ctable(smoker, diseased), method = &#39;render&#39;)) {r, include=FALSE, comment=NA, prompt=FALSE, cache=FALSE, echo=TRUE, results=&#39;asis&#39;} with(tobacco, print(ctable(smoker, diseased, prop = &#39;n&#39;, totals = FALSE), headings = TRUE, method = render )) {r, include=FALSE, comment=NA, prompt=FALSE, cache=FALSE, echo=TRUE, results=&#39;asis&#39;} summarytools::descr(iris, style = rmarkdown ) {r, include=FALSE, comment=NA, prompt=FALSE, cache=FALSE, echo=TRUE, results=&#39;asis&#39;} descr(iris, stats = c( mean , sd , min , med , max ), transpose = TRUE, headings = FALSE, style = rmarkdown ) {r, include=FALSE, comment=NA, prompt=FALSE, cache=FALSE, echo=TRUE, results=&#39;asis&#39;} # view(dfSummary(iris)) {r, include=FALSE, comment=NA, prompt=FALSE, cache=FALSE, echo=TRUE, results=&#39;asis&#39;} # First save the results iris_stats_by_species &lt;- by(data = iris, INDICES = iris$Species, FUN = descr, stats = c( mean , sd , min , med , max ), transpose = TRUE) # Then use view(), like so: view(iris_stats_by_species, method = pander , style = rmarkdown ) {r, include=FALSE, comment=NA, prompt=FALSE, cache=FALSE, echo=TRUE, results=&#39;asis&#39;} # view(iris_stats_by_species) {r, include=FALSE, comment=NA, prompt=FALSE, cache=FALSE, echo=TRUE, results=&#39;asis&#39;} data(tobacco) # tobacco is an example dataframe included in the package BMI_by_age &lt;- with(tobacco, by(BMI, age.gr, descr, stats = c( mean , sd , min , med , max ))) view(BMI_by_age, pander , style = rmarkdown ) {r, include=FALSE, comment=NA, prompt=FALSE, cache=FALSE, echo=TRUE, results=&#39;asis&#39;} BMI_by_age &lt;- with(tobacco, by(BMI, age.gr, descr, transpose = TRUE, stats = c( mean , sd , min , med , max ))) view(BMI_by_age, pander , style = rmarkdown , headings = TRUE) {r, include=FALSE, comment=NA, prompt=FALSE, cache=FALSE, echo=TRUE, results=&#39;asis&#39;} what.is(iris) 342.2 DataExplorer library(DataExplorer) DataExplorer::create_report(df) 342.3 inspectdf https://github.com/alastairrushworth/inspectdf 342.4 Grafikler {r echo=TRUE} # library(ggplot2) # library(mosaic) # mPlot(irisdata) descr(tobacco, style = &#39;rmarkdown&#39;) print(descr(tobacco), method = &#39;render&#39;, table.classes = &#39;st-small&#39;) dfSummary(tobacco, style = &#39;grid&#39;, plain.ascii = FALSE) print(dfSummary(tobacco, graph.magnif = 0.75), method = &#39;render&#39;) A beginner kit for #rstats The Landscape of R Packages for Automated Exploratory Data Analysis https://journal.r-project.org/archive/2019/RJ-2019-033/ (???){RJ-2019-033, author = {Mateusz Staniak and Przemys≈Çaw Biecek}, title = {{The Landscape of R Packages for Automated Exploratory Data Analysis}}, year = {2019}, journal = {{The R Journal}}, doi = {10.32614/RJ-2019-033}, url = {https://journal.r-project.org/archive/2019/RJ-2019-033/index.html} } Here, building up a #ggplot2 as slowly as possible, #rstats. Incremental adjustments. #rstatsteachingideas pic.twitter.com/nUulQl8bPh ‚Äî Gina Reynolds ((???)) August 13, 2018 Dreaming of a fancy #Rstats #ggplot #dataviz but still scared of typing #code? (???) esquisse package has you covered https://t.co/1vIDXcVAAF pic.twitter.com/RlTkptnrNv ‚Äî Radoslaw Panczak ((???)) October 2, 2018 "],["bazƒ±-aray√ºzler.html", "Chapter 343 Bazƒ± aray√ºzler 343.1 Rcmdr 343.2 jamovi", " Chapter 343 Bazƒ± aray√ºzler https://sbalci.github.io/MyRCodesForDataAnalysis/R-Arayuzler.nb.html 343.1 Rcmdr library(Rcmdr) Rcmdr::Commander() A Comparative Review of the R Commander GUI for R http://r4stats.com/articles/software-reviews/r-commander/ 343.2 jamovi https://www.jamovi.org/ https://blog.jamovi.org/2018/07/30/rj.html "],["r-nereden-√∂ƒürenilir.html", "Chapter 344 R nereden √∂ƒürenilir", " Chapter 344 R nereden √∂ƒürenilir https://sbalci.github.io/MyRCodesForDataAnalysis/WhereToLearnR.nb.html "],["sonraki-konular-4.html", "Chapter 345 Sonraki Konular", " Chapter 345 Sonraki Konular RStudio ile GitHub kullanƒ±mƒ± R Markdown ve R Notebook ile tekrarlanabilir rapor Hipotez testleri ‚Äì "],["geri-bildirim-5.html", "Chapter 346 Geri Bildirim", " Chapter 346 Geri Bildirim Geri bildirim i√ßin tƒ±klayƒ±nƒ±z: Geri bildirim formu Please enable JavaScript to view the comments powered by Disqus. "],["libraries-used.html", "Chapter 347 Libraries Used", " Chapter 347 Libraries Used {r eval=FALSE, include=FALSE, echo=TRUE} citation() citation( tidyverse ) citation( foreign ) citation( tidylog ) citation( janitor ) citation( jmv ) citation( tangram ) citation( finalfit ) citation( summarytools ) citation( ggstatplot ) citation( readxl ) {r eval=FALSE, include=FALSE, echo=TRUE} citation( tidyverse ) citation( foreign ) citation( tidylog ) citation( janitor ) citation( jmv ) citation( tangram ) citation( finalfit ) citation( summarytools ) citation( ggstatplot ) citation( readxl ) {r eval=FALSE, include=FALSE, echo=TRUE} sessionInfo() "],["ileti≈üim.html", "Chapter 348 ƒ∞leti≈üim", " Chapter 348 ƒ∞leti≈üim Completed on {r # Sys.time(). Serdar Balci, MD, Pathologist drserdarbalci@gmail.com https://rpubs.com/sbalci/CV https://sbalci.github.io/ https://github.com/sbalci https://twitter.com/serdarbalci {r eval=FALSE, include=FALSE, echo=TRUE} CommitMessage &lt;- paste( updated on , Sys.time(), sep = ) wd &lt;- getwd() gitCommand &lt;- paste( cd , wd, \\n git add . \\n git commit --message &#39; , CommitMessage, &#39; \\n git push origin master \\n , sep = ) system(command = gitCommand, intern = TRUE ) title: R, RStudio ve RMarkdown ile Tekrarlanabilir Rapor7 author: [Serdar Balcƒ±, MD, Pathologist](https://sbalci.github.io/) institute: [serdarbalci.com](https://www.serdarbalci.com) date: `{r # format(Sys.Date())` output: revealjs::revealjs_presentation: incremental: true theme: sky highlight: pygments center: false smart: true transition: fade self_contained: true ig_width: 7 fig_height: 6 fig_caption: true reveal_options: slideNumber: true previewLinks: true rmdshower::shower_presentation: xaringan::moon_reader: lib_dir: libs nature: beforeInit: [ macros.js , https://platform.twitter.com/widgets.js ] highlightStyle: github highlightLines: true countIncrementalSlides: false self_contained: true html_notebook: fig_caption: yes highlight: kate number_sections: yes theme: flatly toc: yes toc_depth: 5 toc_float: yes prettydoc::html_pretty: theme: leonids highlight: github pdf_document: toc: yes toc_depth: &#39;5&#39; html_document: fig_caption: yes keep_md: yes toc: yes toc_depth: 5 toc_float: yes editor_options: chunk_output_type: inline {r eval=FALSE, include=FALSE, echo=TRUE} knitr::opts_chunk$set(fig.width = 12, fig.height = 8, fig.path = &#39;Figs/&#39;, echo = TRUE, warning = FALSE, message = FALSE, error = FALSE, eval = TRUE, tidy = TRUE, comment = NA, cache = TRUE) {r eval=FALSE, message=FALSE, warning=FALSE, include=FALSE} # xaringan::inf_mr() # servr::daemon_stop(1) Bu bir derlemedir, m√ºmk√ºn mertebe alƒ±ntƒ±lara linklerle referans vermeye √ßalƒ±≈ütƒ±m.‚Ü©Ô∏é "],["tekrarlanabilir-analiz-ve-rapor.html", "Chapter 349 Tekrarlanabilir Analiz ve Rapor 349.1 Replication Crisis 349.2 Replication Crisis Excel Version", " Chapter 349 Tekrarlanabilir Analiz ve Rapor 349.1 Replication Crisis https://en.wikipedia.org/wiki/Replication_crisis 349.2 Replication Crisis Excel Version "],["rstudio-ile-proje-olu≈ütur.html", "Chapter 350 RStudio ile proje olu≈ütur", " Chapter 350 RStudio ile proje olu≈ütur "],["r-notebook-3.html", "Chapter 351 R Notebook 351.1 R Notebook d√∂k√ºmanƒ± olu≈üturma 351.2 R Notebook‚Äôtan html, pdf ve word olu≈üturma 351.3 RNotebook vs RMarkdown", " Chapter 351 R Notebook 351.1 R Notebook d√∂k√ºmanƒ± olu≈üturma 351.2 R Notebook‚Äôtan html, pdf ve word olu≈üturma 351.3 RNotebook vs RMarkdown &lt;iframe width= 560 height= 315 src= https://www.youtube.com/embed/zNzZ1PfUDNk frameborder= 0 allow= accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture allowfullscreen&gt; https://youtu.be/zNzZ1PfUDNk "],["r-markdown.html", "Chapter 352 R Markdown 352.1 Hem kendi kodlarƒ± hem de html kodlarƒ± yazƒ±labilir 352.2 R Markdown: The Definitive Guide 352.3 R Markdown syntax 352.4 Remedy Package 352.5 R Markdown paket ve ≈üablonlarƒ± 352.6 Render Markdown via code 352.7 pandoc Rstudio integration", " Chapter 352 R Markdown 352.1 Hem kendi kodlarƒ± hem de html kodlarƒ± yazƒ±labilir https://rmarkdown.rstudio.com What is R Markdown? from RStudio, Inc. on Vimeo. 352.2 R Markdown: The Definitive Guide https://bookdown.org/yihui/rmarkdown/ 352.3 R Markdown syntax https://gist.github.com/MinhasKamal/7fdebb7c424d23149140 352.4 Remedy Package 352.4.1 Remedy 352.5 R Markdown paket ve ≈üablonlarƒ± https://bookdown.org/yihui/rmarkdown/document-templates.html 352.6 Render Markdown via code inside R markdown::markdownToHTML(&#39;markdown_example.md&#39;, &#39;markdown_example.html&#39;) command line R -e markdown::markdownToHTML(&#39;markdown_example.md&#39;, &#39;markdown_example.html&#39;) 352.7 pandoc Rstudio integration command line export PATH=$PATH:/Applications/RStudio.app/Contents/MacOS/pandoc R -e rmarkdown::render(&#39;markdown_example.md&#39;) "],["rmarkdown-chunk-i√ßinde-r-kodlarƒ±nƒ±-√ßalƒ±≈ütƒ±rma.html", "Chapter 353 RMarkdown chunk i√ßinde R kodlarƒ±nƒ± √ßalƒ±≈ütƒ±rma", " Chapter 353 RMarkdown chunk i√ßinde R kodlarƒ±nƒ± √ßalƒ±≈ütƒ±rma {r, results=&#39;asis&#39;} iris %&gt;% tibble::as_tibble() %&gt;% details::details(summary = &#39;tibble&#39;) "],["metin-arasƒ±nda-r-kodlarƒ±nƒ±-√ßalƒ±≈ütƒ±rma.html", "Chapter 354 Metin arasƒ±nda R kodlarƒ±nƒ± √ßalƒ±≈ütƒ±rma", " Chapter 354 Metin arasƒ±nda R kodlarƒ±nƒ± √ßalƒ±≈ütƒ±rma "],["chunk-options.html", "Chapter 355 Chunk Options 355.1 Global Options 355.2 Other Code Languages", " Chapter 355 Chunk Options 355.1 Global Options {r , include=FALSE} knitr::opts_chunk$set(fig.width = 12, fig.height = 8, fig.path = &#39;Figs/&#39;, echo = TRUE, warning = FALSE, message = FALSE, error = FALSE, eval = TRUE, tidy = TRUE, comment = NA) 355.2 Other Code Languages "],["r-markdown-kod-√∂rneƒüi.html", "Chapter 356 R Markdown kod √∂rneƒüi", " Chapter 356 R Markdown kod √∂rneƒüi {r} data( cancer ) cancer foreign::write.foreign(df = cancer, datafile = data/cancer.sav , codefile = data/cancer.spo , package = SPSS ) "],["r-markdown-paket-√ßaƒüƒ±rma.html", "Chapter 357 R Markdown Paket √áaƒüƒ±rma üì¶ 357.1 Sƒ±k kullandƒ±ƒüƒ±m paketler üì¶", " Chapter 357 R Markdown Paket √áaƒüƒ±rma üì¶ {r} suppressPackageStartupMessages(library( tidyverse )) suppressPackageStartupMessages(library( survival )) 357.1 Sƒ±k kullandƒ±ƒüƒ±m paketler üì¶ {tidyverse} {tidylog} {lubridate} {janitor} {readxl} {foreign} {summarytools} {ggstatsplot} {tangram} {finalfit} {psycho} {jmv} {survival} {survminer} {report} {kableExtra} "],["r-markdown-veri-y√ºkleme-spss.html", "Chapter 358 R Markdown Veri Y√ºkleme SPSS", " Chapter 358 R Markdown Veri Y√ºkleme SPSS "],["r-markdown-veri-y√ºkleme-excel.html", "Chapter 359 R Markdown Veri Y√ºkleme Excel", " Chapter 359 R Markdown Veri Y√ºkleme Excel "],["veri-g√∂r√ºnt√ºleme.html", "Chapter 360 Veri G√∂r√ºnt√ºleme", " Chapter 360 Veri G√∂r√ºnt√ºleme {r} View(mydata) glimpse(mydata) "],["veri-d√ºzenleme.html", "Chapter 361 Veri D√ºzenleme", " Chapter 361 Veri D√ºzenleme {r} mydata &lt;- janitor::clean_names(mydata) {r} mydata$sontarih &lt;- janitor::excel_numeric_to_date( as.numeric(mydata$olum_tarihi) ) "],["recode.html", "Chapter 362 Recode", " Chapter 362 Recode {r} mydata$Outcome &lt;- Dead mydata$Outcome[mydata$olum_tarihi == yok ] &lt;- Alive {r} ## Recoding mydata$cinsiyet into mydata$Cinsiyet mydata$Cinsiyet &lt;- recode(mydata$cinsiyet, K = Kadin , E = Erkek ) mydata$Cinsiyet &lt;- factor(mydata$Cinsiyet) "],["recode-regular-expression.html", "Chapter 363 Recode regular expression", " Chapter 363 Recode regular expression {r recode TNM stage} #pT2N0Mx -&gt; 2 mydata$Tstage &lt;- stringr::str_match( mydata$patolojik_evre, paste(&#39;(.+)&#39;, N , sep=&#39;&#39;))[,2] ) "],["recode-regular-expression-case-when.html", "Chapter 364 Recode regular expression case_when", " Chapter 364 Recode regular expression case_when {r recode TNM2} mydata &lt;- mydata %&gt;% mutate( T_stage = case_when( grepl(pattern = T1 , x = .$Tstage) == TRUE ~ T1 , grepl(pattern = T2 , x = .$Tstage) == TRUE ~ T2 , grepl(pattern = T3 , x = .$Tstage) == TRUE ~ T3 , grepl(pattern = T4 , x = .$Tstage) == TRUE ~ T4 , TRUE ~ Tx ) ) "],["recode-regular-expression-case-when-1.html", "Chapter 365 Recode regular expression case_when", " Chapter 365 Recode regular expression case_when {r} mydata &lt;- mydata %&gt;% mutate( TumorPDL1gr1 = case_when( t_pdl1 &lt; 1 ~ kucuk1 , t_pdl1 &gt;= 1 ~ buyukesit1 ) ) "],["r-markdown-tanƒ±mlayƒ±cƒ±-istatistikler.html", "Chapter 366 R Markdown Tanƒ±mlayƒ±cƒ± ƒ∞statistikler 366.1 Table One 366.2 The Grammar of Tables 366.3 Kategorik Veriler 366.4 Kategorik Veriler i√ßin Grafikler 366.5 Continious Variables", " Chapter 366 R Markdown Tanƒ±mlayƒ±cƒ± ƒ∞statistikler {r} library(summarytools) view(dfSummary(colon_s)) A beginner kit for #rstats The Landscape of R Packages for Automated Exploratory Data Analysis https://journal.r-project.org/archive/2019/RJ-2019-033/ (???){RJ-2019-033, author = {Mateusz Staniak and Przemys≈Çaw Biecek}, title = {{The Landscape of R Packages for Automated Exploratory Data Analysis}}, year = {2019}, journal = {{The R Journal}}, doi = {10.32614/RJ-2019-033}, url = {https://journal.r-project.org/archive/2019/RJ-2019-033/index.html} } 366.1 Table One {r, results=&#39;asis&#39;} # cat(names(mydata), sep = + \\n ) library(arsenal) tab1 &lt;- tableby(~ Cinsiyet + Yas + TumorYerlesimi , data = mydata) summary(tab1) 366.2 The Grammar of Tables tangram: The Grammar of Tables A grammar of tables Grammar of Tables? Easily generate information-rich, publication-quality tables from R 366.3 Kategorik Veriler {r} mydata %&gt;% janitor::tabyl(Categorical) %&gt;% adorn_pct_formatting(rounding = &#39;half up&#39;, digits = 1) %&gt;% knitr::kable() {r crosstable} mydata %&gt;% summary_factorlist(dependent = dependent, explanatory = explanatory, total_col = TRUE, p = TRUE, add_dependent_label = TRUE) -&gt; table knitr::kable(table, row.names = FALSE, align = c(&#39;l&#39;, &#39;l&#39;, &#39;r&#39;, &#39;r&#39;, &#39;r&#39;)) 366.4 Kategorik Veriler i√ßin Grafikler {r ggstatplot, layout=&#39;l-page&#39;} mydata %&gt;% ggstatsplot::ggbarstats(data = ., main = Categorical_variable, condition = dependent_variable ) 366.5 Continious Variables {r} mydata %&gt;% jmv::descriptives( data = ., vars = c(yas), hist = TRUE, dens = TRUE, box = TRUE, violin = TRUE, dot = TRUE, mode = TRUE, sd = TRUE, variance = TRUE, skew = TRUE, kurt = TRUE, quart = TRUE) "],["r-markdown-√∂rneƒüi-√ßapraz-tablolar.html", "Chapter 367 R Markdown √∂rneƒüi √áapraz Tablolar", " Chapter 367 R Markdown √∂rneƒüi √áapraz Tablolar {r crosstable} library(finalfit) mydata %&gt;% summary_factorlist(dependent = dependent, explanatory = explanatory, column = TRUE, total_col = TRUE, p = TRUE, add_dependent_label = TRUE, na_include=FALSE # catTest = catTestfisher ) -&gt; table knitr::kable(table, row.names = FALSE, align = c(&#39;l&#39;, &#39;l&#39;, &#39;r&#39;, &#39;r&#39;, &#39;r&#39;)) "],["r-markdown-√∂rneƒüi-saƒükalƒ±m.html", "Chapter 368 R Markdown √∂rneƒüi Saƒükalƒ±m 368.1 Saƒükalƒ±m i√ßin veriyi d√ºzenleme 368.2 Kaplan-Meier 368.3 Saƒükalƒ±m Tablolarƒ± 368.4 Pairwise comparison 368.5 Multivariate Analysis Survival", " Chapter 368 R Markdown √∂rneƒüi Saƒükalƒ±m Drawing Survival Curves Using ggplot2 https://rpkgs.datanovia.com/survminer/reference/ggsurvplot.html 368.1 Saƒükalƒ±m i√ßin veriyi d√ºzenleme {r define survival time} mydata$int &lt;- lubridate::interval( lubridate::ymd(mydata$CerrahiTarih), lubridate::ymd(mydata$SonTarih) ) mydata$OverallTime &lt;- lubridate::time_length(mydata$int, month ) mydata$OverallTime &lt;- round(mydata$OverallTime, digits = 1) {r} ## Recoding mydata$Outcome into mydata$Outcome2 mydata$Outcome2 &lt;- recode(mydata$Outcome, Alive = 0 , Dead = 1 ) mydata$Outcome2 &lt;- as.numeric(mydata$Outcome2) 368.2 Kaplan-Meier {r Kaplan-Meier} mydata %&gt;% finalfit::surv_plot(dependent, explanatory, xlab=&#39;Time (months)&#39;, pval=TRUE, legend = &#39;none&#39;, break.time.by = 12, xlim = c(0,60), legend.labs = c(&#39;a&#39;,&#39;b&#39;) ) 368.3 Saƒükalƒ±m Tablolarƒ± {r} km_fit &lt;- survfit(dependent ~ explanatory, data = mydata) km_fit {r, eval=FALSE, include=FALSE, echo=TRUE} library(survival) km &lt;- with(mydata, Surv(OverallTime, Outcome2)) # head(km,80) # plot(km) {r 1-3-5-yr} summary(km_fit, times = c(12,36,60)) 368.4 Pairwise comparison {r} survminer::pairwise_survdiff(formula = Surv(time, Outcome) ~ Group, data = mydata, p.adjust.method = BH ) 368.5 Multivariate Analysis Survival {r Multivariate Analysis, eval=FALSE, include=FALSE, echo=TRUE} library(finalfit) library(survival) explanatoryMultivariate &lt;- explanatoryKM dependentMultivariate &lt;- dependentKM mydata %&gt;% finalfit(dependentMultivariate, explanatoryMultivariate) -&gt; tMultivariate knitr::kable(tMultivariate, row.names=FALSE, align=c( l , l , r , r , r , r )) "],["jamovi-6.html", "Chapter 369 jamovi 369.1 jamovi ve R entegrasyonu 369.2 {jmv} paket kodlarƒ±", " Chapter 369 jamovi 369.1 jamovi ve R entegrasyonu Rj Editor ‚Äì Analyse your data with R in jamovi 369.2 {jmv} paket kodlarƒ± jamovi syntax mode "],["g√ºncellemeler-olunca-kodlar-√ßalƒ±≈üacak-mƒ±.html", "Chapter 370 G√ºncellemeler olunca kodlar √ßalƒ±≈üacak mƒ±? 370.1 Paket K√ºt√ºphaneleri 370.2 Docker 370.3 Yeni R s√ºr√ºmleri", " Chapter 370 G√ºncellemeler olunca kodlar √ßalƒ±≈üacak mƒ±? 370.1 Paket K√ºt√ºphaneleri packrat / renv https://environments.rstudio.com 370.2 Docker docker 370.2.1 The Rocker Project Docker Containers for the R Environment docker run --rm -ti rocker/r-base Or get started with an RStudio¬Æ instance: docker run -e PASSWORD=yourpassword --rm -p 8787:8787 rocker/rstudio and point your browser to localhost:8787 Log in with user/password rstudio/yourpassword Managing containers 370.3 Yeni R s√ºr√ºmleri RSwitch https://rud.is/rswitch/ Using RSwitch https://rud.is/rswitch/guide/ : scale 30% "],["yedeklemeyi-nasƒ±l-yapacaƒüƒ±z.html", "Chapter 371 Yedeklemeyi nasƒ±l yapacaƒüƒ±z 371.1 Projeyi d√ºzg√ºn organize edin 371.2 Save Final Data 371.3 GitHub 371.4 GitHub Yedekleme", " Chapter 371 Yedeklemeyi nasƒ±l yapacaƒüƒ±z 371.1 Projeyi d√ºzg√ºn organize edin pdf R images bib {r load library} source(file = here::here( R , loadLibrary.R )) 371.2 Save Final Data {r} saved data after analysis to `mydata.xlsx`. save.image(file = here::here( data , mydata_work_space.RData )) readr::write_rds(x = mydata, path = here::here( data , mydata_afteranalysis.rds )) saveRDS(object = mydata, file = here::here( data , mydata.rds )) writexl::write_xlsx(mydata, here::here( data , mydata.xlsx )) paste0(rownames(file.info(here::here( data , mydata.xlsx ))), : , file.info(here::here( data , mydata.xlsx ))$ctime) 371.3 GitHub {r github push} CommitMessage &lt;- paste( updated on , Sys.time(), sep = ) wd &lt;- getwd() gitCommand &lt;- paste( cd , wd, \\n git add . \\n git commit --message &#39; , CommitMessage, &#39; \\n git push origin master \\n , sep = ) system(command = gitCommand, intern = TRUE ) 371.4 GitHub Yedekleme {r github push, echo=TRUE} CommitMessage &lt;- paste( updated on , Sys.time(), sep = ) wd &lt;- getwd() gitCommand &lt;- paste( cd , wd, \\n git add . \\n git commit --message &#39; , CommitMessage, &#39; \\n git push origin master \\n , sep = ) system(command = gitCommand, intern = TRUE ) "],["her-d√∂k√ºmanƒ±n-sonuna-kullandƒ±ƒüƒ±nƒ±z-k√ºt√ºphaneler-i√ßin-atƒ±f-yazdƒ±rabilirsiniz.html", "Chapter 372 Her d√∂k√ºmanƒ±n sonuna kullandƒ±ƒüƒ±nƒ±z k√ºt√ºphaneler i√ßin atƒ±f yazdƒ±rabilirsiniz 372.1 Libraries Used 372.2 Bu oturuma spesifik kullanƒ±lan paketler 372.3 Tek tek paket atƒ±flarƒ± 372.4 Jamovi ve R i√ßin atƒ±f √∂rneƒüi", " Chapter 372 Her d√∂k√ºmanƒ±n sonuna kullandƒ±ƒüƒ±nƒ±z k√ºt√ºphaneler i√ßin atƒ±f yazdƒ±rabilirsiniz {r} citation() 372.1 Libraries Used {r citation, echo=TRUE} citation() 372.2 Bu oturuma spesifik kullanƒ±lan paketler {r citation as report, echo=TRUE, results=&#39;asis&#39;} # report::cite_packages(session = sessionInfo()) 372.3 Tek tek paket atƒ±flarƒ± {r citations} citation( tidyverse ) citation( readxl ) citation( janitor ) citation( report ) citation( finalfit ) citation( ggstatplot ) 372.4 Jamovi ve R i√ßin atƒ±f √∂rneƒüi The jamovi project (2019). jamovi. (Version 0.9) [Computer Software]. Retrieved from https://www.jamovi.org. R Core Team (2018). R: A Language and envionment for statistical computing. [Computer software]. Retrieved from https://cran.r-project.org/. Fox, J., &amp; Weisberg, S. (2018). car: Companion to Applied Regression. [R package]. Retrieved from https://cran.r-project.org/package=car. "],["her-d√∂k√ºmanƒ±n-sonuna-oturum-detaylarƒ±nƒ±zƒ±-yazdƒ±rabilirsiniz.html", "Chapter 373 Her d√∂k√ºmanƒ±n sonuna oturum detaylarƒ±nƒ±zƒ± yazdƒ±rabilirsiniz 373.1 Session Info", " Chapter 373 Her d√∂k√ºmanƒ±n sonuna oturum detaylarƒ±nƒ±zƒ± yazdƒ±rabilirsiniz {r session info, echo=TRUE} sessionInfo() 373.1 Session Info {r session info, echo=TRUE} sessionInfo() "],["sonraki-konular-5.html", "Chapter 374 Sonraki Konular", " Chapter 374 Sonraki Konular RStudio ile GitHub kullanƒ±mƒ± ‚Ä¶ "],["√∂nerilen-kaynaklar.html", "Chapter 375 √ñnerilen Kaynaklar", " Chapter 375 √ñnerilen Kaynaklar Reproducible Templates for Analysis and Dissemination Happy Git and GitHub for the useR "],["sunum-linkleri.html", "Chapter 376 Sunum Linkleri", " Chapter 376 Sunum Linkleri https://sbalci.github.io/MyRCodesForDataAnalysis/R-Markdown.nb.html https://sbalci.github.io/MyRCodesForDataAnalysis/R-Markdown.html https://forms.gle/UqGJBiAjB8uLPRon8 "],["geri-bildirim-6.html", "Chapter 377 Geri Bildirim", " Chapter 377 Geri Bildirim Geri bildirim i√ßin tƒ±klayƒ±nƒ±z: Geri bildirim formu Please enable JavaScript to view the comments powered by Disqus. "],["ileti≈üim-1.html", "Chapter 378 ƒ∞leti≈üim", " Chapter 378 ƒ∞leti≈üim Completed on . Serdar Balci, MD, Pathologist drserdarbalci@gmail.com https://rpubs.com/sbalci/CV https://sbalci.github.io/ https://github.com/sbalci https://twitter.com/serdarbalci "],["other-links.html", "Chapter 379 Other Links", " Chapter 379 Other Links https://andrewbtran.github.io/NICAR/2018/workflow/docs/02-rmarkdown.html Troubleshooting in R Markdown https://smithcollege-sds.github.io/sds-public/rmarkdown_problems.html http://kbroman.org/knitr_knutshell/pages/Rmarkdown.html https://kbroman.org/knitr_knutshell/pages/overview.html https://kbroman.org/knitr_knutshell/pages/Rmarkdown.html https://kbroman.org/knitr_knutshell/pages/markdown.html https://onp4.com/ csv {headers: true, # **Drawing Tables In Markdown** } Name, Surname, Known As, Age Marcelo, David, coldzera, 22 Oleksandr, Kostyliev, s1mple, 19 Nikola, Kovaƒç, NiKo, 20 Richard, Papillon, shox, 25 Nicolai, Reedtz, dev1ce, 21 {pgn} [Event Bled-Zagreb-Belgrade Candidates ] [Site Bled, Zagreb &amp; Belgrade YUG ] [Date 1959.10.11 ] [Round 20 ] [Result 1-0 ] [White Mikhail Tal ] [Black Robert James Fischer ] 1. d4 Nf6 2. c4 g6 3. Nc3 Bg7 4. e4 d6 5. Be2 O-O 6. Nf3 e5 7. d5 Nbd7 8. Bg5 h6 9. Bh4 a6 10. O-O Qe8 11. Nd2 Nh7 12. b4 Bf6 13. Bxf6 Nhxf6 14. Nb3 Qe7 15. Qd2 Kh7 16. Qe3 Ng8 17. c5 f5 18. exf5 gxf5 19. f4 exf4 20. Qxf4 dxc5 21. Bd3 cxb4 22. Rae1 Qf6 23. Re6 Qxc3 24. Bxf5+ Rxf5 25. Qxf5+ Kh8 26. Rf3 Qb2 27. Re8 Nf6 28. Qxf6+ Qxf6 29. Rxf6 Kg7 30. Rff8 Ne7 31. Na5 h5 32. h4 Rb8 33. Nc4 b5 34. Ne5 1-0 Keeping Credentials Secret with Keyrings in R https://ras44.github.io/blog/2019/01/19/keeping-credentials-secret-with-keyrings-in-r.html How to build a website with Blogdown in R http://www.storybench.org/how-to-build-a-website-with-blogdown-in-r/ "],["r-tipps.html", "Chapter 380 R-Tipps", " Chapter 380 R-Tipps "],["readclipboard.html", "Chapter 381 readClipboard", " Chapter 381 readClipboard setwd(readClipboard()) #Rstats: When using setwd(), R expects forward-slashes in the directory name. But on Windows, copying the directory from folder gives back-slashes. To ‚Äòde-windowsify‚Äô the path name, copy the windows directory &amp; use:setwd(readClipboard())#phdchat #phd pic.twitter.com/cMbdmhNVuf ‚Äî Guy Prochilo üè≥Ô∏è üåà ((???)) January 9, 2019 "],["separate.html", "Chapter 382 separate", " Chapter 382 separate One form of messy data is a table in which multiple variables are stored in a single column. The #tidyverse separate() function will transform this type of data into one column per variable by simply specifying the separator! #rstats #tidytuesday #animation #dataviz pic.twitter.com/jjVvqWToIg ‚Äî Omni Analytics Group ((???)) January 22, 2019 "],["glue-1.html", "Chapter 383 glue", " Chapter 383 glue  glue:: finally clicked for me this AM! ü§ìI work with monthly data files and need to update filenames often. My go-to is paste0(). Gave glue() another try. Way less fiddling with commas &amp; quote marks, and it's easier to see the filename. #Rstats #tidyverse pic.twitter.com/EdpozMxrtM ‚Äî Adam Stone ((???)) January 23, 2019 #RStats ‚Äî {dplyr} debugging tip: put a browser() somewhere inside your mutate() call to have access to the intermediate elements and to the columns: pic.twitter.com/KN53YJIMOe ‚Äî Colin Fay ü§ò ((???)) January 23, 2019 Accidentally discovered if you copy &amp; paste a file into your #R script you get the filepath including filename with / (not ) and you just delete \"file:///\"; off the start and don't have to change to / (???) demanded I tweet this #rstats ‚Äî Faye Jackson ((???)) January 18, 2019 "],["radiant-1.html", "Chapter 384 radiant", " Chapter 384 radiant https://radiant-rstats.github.io/docs/ "],["rchess.html", "Chapter 385 rchess", " Chapter 385 rchess http://jkunst.com/rchess/ https://github.com/jbkunst/rchess {r eval=FALSE, include=FALSE, echo=TRUE} install.packages( rchess ) {r eval=FALSE, include=FALSE, echo=TRUE} devtools::install_github( jbkunst/rchess ) "],["rcookbook.html", "Chapter 386 RCookbook", " Chapter 386 RCookbook https://rc2e.com/index.html {r eval=FALSE, include=FALSE, echo=TRUE} data(cars) library(purrr) map_dbl(cars, mean) {r eval=FALSE, include=FALSE, echo=TRUE} map_dbl(cars, sd) {r eval=FALSE, include=FALSE, echo=TRUE} map_dbl(cars, median) {r eval=FALSE, include=FALSE, echo=TRUE} var(cars) {r eval=FALSE, include=FALSE, echo=TRUE} cor(cars) {r eval=FALSE, include=FALSE, echo=TRUE} cov(cars) {r eval=FALSE, include=FALSE, echo=TRUE} v &lt;- c(3, pi, 4) any(v == pi) # Return TRUE if any element of v equals pi all(v == 0) # Return TRUE if all elements of v are zero {r , echo=TRUE, cache=FALSE} library(knitr) library(rmdformats) ## Global options options(max.print= 75 ) opts_chunk$set(echo=TRUE, cache=TRUE, prompt=FALSE, tidy=TRUE, comment=NA, message=FALSE, warning=FALSE) opts_knit$set(width=75) R generation https://rss.onlinelibrary.wiley.com/doi/10.1111/j.1740-9713.2018.01169.x "],["r-y√ºkleme-5.html", "Chapter 387 R y√ºkleme 387.1 R-project 387.2 RStudio 387.3 X11 387.4 Java OS", " Chapter 387 R y√ºkleme http://www.youtube.com/watch?v=XcBLEVknqvY 387.1 R-project https://cran.r-project.org/ 387.2 RStudio https://www.rstudio.com/ https://www.rstudio.com/products/rstudio/download/ https://moderndive.com/2-getting-started.html 387.2.1 RStudio eklentileri Discover and install useful RStudio addins https://cran.r-project.org/web/packages/addinslist/README.html https://rstudio.github.io/rstudioaddins/ {r eval=FALSE, include=FALSE, echo=TRUE} # devtools::install_github( rstudio/addinexamples , type = source ) 387.3 X11 https://www.xquartz.org/ 387.4 Java OS https://support.apple.com/kb/dl1572 "],["r-zor-≈üeyler-i√ßin-kolay-kolay-≈üeyler-i√ßin-zor-5.html", "Chapter 388 R zor ≈üeyler i√ßin kolay, kolay ≈üeyler i√ßin zor", " Chapter 388 R zor ≈üeyler i√ßin kolay, kolay ≈üeyler i√ßin zor R makes easy things hard, and hard things easy Aynƒ± ≈üeyi √ßok fazla ≈üekilde yapmak m√ºmk√ºn R Syntax Comparison::CHEAT SHEET https://www.amelia.mn/Syntax-cheatsheet.pdf "],["r-paketleri-5.html", "Chapter 389 R paketleri 389.1 Neden paketler var 389.2 Paketleri nereden bulabiliriz 389.3 Kendi paket evrenini olu≈ütur 389.4 R i√ßin yardƒ±m bulma 389.5 R paket y√ºkleme", " Chapter 389 R paketleri 389.1 Neden paketler var I love the #rstats community.Someone is like, ‚Äúoh hey peeps, I saw a big need for this mundane but difficult task that I infrequently do, so I created a package that will literally scrape the last bits of peanut butter out of the jar for you. It's called pbplyr.‚ÄùWhat a tribe. ‚Äî Frank Elavsky   ≥ ((???)) July 3, 2018 https://blog.mitchelloharawild.com/blog/user-2018-feature-wall/ 389.2 Paketleri nereden bulabiliriz Available CRAN Packages By Name https://cran.r-project.org/web/packages/available_packages_by_name.html Bioconductor https://www.bioconductor.org RecommendR http://recommendr.info/ pkgsearch CRAN package search https://github.com/metacran/pkgsearch Awesome R https://awesome-r.com/ 389.3 Kendi paket evrenini olu≈ütur pkgverse: Build a Meta-Package Universe https://cran.r-project.org/web/packages/pkgverse/index.html 389.4 R i√ßin yardƒ±m bulma # ?mean # ??efetch # help(merge) # example(merge) Vignette RDocumentation https://www.rdocumentation.org R Package Documentation https://rdrr.io/ GitHub Stackoverflow https://stackoverflow.com/ Google uygun anahtar kelime How I use #rstats h/t (???) pic.twitter.com/erRnTG0Ujr ‚Äî Emily Bovee ((???)) August 10, 2018 Awesome Cheatsheet https://github.com/detailyang/awesome-cheatsheet http://cran.r-project.org/doc/contrib/Baggott-refcard-v2.pdf https://www.rstudio.com/resources/cheatsheets/ Awesome R https://github.com/qinwf/awesome-R#readme https://awesome-r.com/ Twitter https://twitter.com/hashtag/rstats?src=hash Reproducible Examples Got a question to ask on (???) or post on (???)? No time to read the long post on how to use reprex? Here is a 20-second gif for you to format your R codes nicely and for others to reproduce your problem. (An example from a talk given by (???)) #rstat pic.twitter.com/gpuGXpFIsX ‚Äî ZhiYang ((???)) October 18, 2018 389.5 R paket y√ºkleme install.packages( tidyverse , dependencies = TRUE) install.packages( jmv , dependencies = TRUE) install.packages( questionr , dependencies = TRUE) install.packages( Rcmdr , dependencies = TRUE) install.packages( summarytools ) {r} # install.packages( tidyverse , dependencies = TRUE) # install.packages( jmv , dependencies = TRUE) # install.packages( questionr , dependencies = TRUE) # install.packages( Rcmdr , dependencies = TRUE) # install.packages( summarytools ) {r, error=FALSE, message = FALSE, warning = FALSE, eval = TRUE, include = TRUE} # require(tidyverse) # require(jmv) # require(questionr) # library(summarytools) # library(gganimate) "],["r-studio-ile-proje-olu≈üturma-4.html", "Chapter 390 R studio ile proje olu≈üturma", " Chapter 390 R studio ile proje olu≈üturma https://support.rstudio.com/hc/en-us/articles/200526207-Using-Projects "],["rstudio-ile-veri-y√ºkleme-5.html", "Chapter 391 RStudio ile veri y√ºkleme 391.1 Excel 391.2 SPSS 391.3 csv", " Chapter 391 RStudio ile veri y√ºkleme https://support.rstudio.com/hc/en-us/articles/218611977-Importing-Data-with-RStudio 391.1 Excel 391.2 SPSS 391.3 csv "],["veriyi-g√∂r√ºnt√ºleme-6.html", "Chapter 392 Veriyi g√∂r√ºnt√ºleme", " Chapter 392 Veriyi g√∂r√ºnt√ºleme Spreadsheet users using #rstats: where's the data?#rstats users using spreadsheets: where's the code? ‚Äî Leonard Kiefer ((???)) July 7, 2018 {r, results= markup } # library(nycflights13) # summary(flights) View(data) data head tail glimpse str skimr::skim() "],["veriyi-deƒüi≈ütirme-5.html", "Chapter 393 Veriyi deƒüi≈ütirme 393.1 Veriyi kod ile deƒüi≈ütirelim 393.2 Veriyi eklentilerle deƒüi≈ütirme 393.3 RStudio aracƒ±lƒ±ƒüƒ±yla recode", " Chapter 393 Veriyi deƒüi≈ütirme 393.1 Veriyi kod ile deƒüi≈ütirelim 393.2 Veriyi eklentilerle deƒüi≈ütirme 393.3 RStudio aracƒ±lƒ±ƒüƒ±yla recode questionr paketi kullanƒ±lacak https://juba.github.io/questionr/articles/recoding_addins.html "],["basit-tanƒ±mlayƒ±cƒ±-istatistikler-5.html", "Chapter 394 Basit tanƒ±mlayƒ±cƒ± istatistikler 394.1 summarytools 394.2 skimr 394.3 DataExplorer 394.4 Grafikler", " Chapter 394 Basit tanƒ±mlayƒ±cƒ± istatistikler summary() mean median min max sd table() {r, echo=TRUE, include = TRUE} library(readr) irisdata &lt;- read_csv( data/iris.csv ) jmv::descriptives( data = irisdata, vars = Sepal.Length , splitBy = Species , freq = TRUE, hist = TRUE, dens = TRUE, bar = TRUE, box = TRUE, violin = TRUE, dot = TRUE, mode = TRUE, sum = TRUE, sd = TRUE, variance = TRUE, range = TRUE, se = TRUE, skew = TRUE, kurt = TRUE, quart = TRUE, pcEqGr = TRUE) {r, echo=TRUE, include=FALSE} # install.packages( scatr ) scatr::scat( data = irisdata, x = Sepal.Length , y = Sepal.Width , group = Species , marg = dens , line = linear , se = TRUE) 394.1 summarytools https://cran.r-project.org/web/packages/summarytools/vignettes/Introduction.html {r, include=FALSE, comment=NA, prompt=FALSE, cache=FALSE, echo=TRUE, results=&#39;asis&#39;} library(summarytools) summarytools::freq(iris$Species, style = rmarkdown ) {r, include=FALSE, comment=NA, prompt=FALSE, cache=FALSE, echo=TRUE, results=&#39;asis&#39;} summarytools::freq(iris$Species, report.nas = FALSE, style = rmarkdown , headings = TRUE) {r, include=FALSE, comment=NA, prompt=FALSE, cache=FALSE, echo=TRUE, results=&#39;asis&#39;} with(tobacco, print(ctable(smoker, diseased), method = &#39;render&#39;)) {r, include=FALSE, comment=NA, prompt=FALSE, cache=FALSE, echo=TRUE, results=&#39;asis&#39;} with(tobacco, print(ctable(smoker, diseased, prop = &#39;n&#39;, totals = FALSE), headings = TRUE, method = render )) {r, include=FALSE, comment=NA, prompt=FALSE, cache=FALSE, echo=TRUE, results=&#39;asis&#39;} summarytools::descr(iris, style = rmarkdown ) {r, include=FALSE, comment=NA, prompt=FALSE, cache=FALSE, echo=TRUE, results=&#39;asis&#39;} descr(iris, stats = c( mean , sd , min , med , max ), transpose = TRUE, headings = TRUE, style = rmarkdown ) {r, include=FALSE, comment=NA, prompt=FALSE, cache=FALSE, echo=TRUE, results=&#39;asis&#39;} # view(dfSummary(iris)) {r, include=FALSE, comment=NA, prompt=FALSE, cache=FALSE, echo=TRUE, results=&#39;asis&#39;} dfSummary(tobacco, plain.ascii = FALSE, style = grid ) {r, include=FALSE, comment=NA, prompt=FALSE, cache=FALSE, echo=TRUE, results=&#39;asis&#39;} # First save the results iris_stats_by_species &lt;- by(data = iris, INDICES = iris$Species, FUN = descr, stats = c( mean , sd , min , med , max ), transpose = TRUE) # Then use view(), like so: view(iris_stats_by_species, method = pander , style = rmarkdown ) {r, include=FALSE, comment=NA, prompt=FALSE, cache=FALSE, echo=TRUE, results=&#39;asis&#39;} # view(iris_stats_by_species) {r, include=FALSE, comment=NA, prompt=FALSE, cache=FALSE, echo=TRUE, results=&#39;asis&#39;} data(tobacco) # tobacco is an example dataframe included in the package BMI_by_age &lt;- with(tobacco, by(BMI, age.gr, descr, stats = c( mean , sd , min , med , max ))) view(BMI_by_age, pander , style = rmarkdown ) {r, include=FALSE, comment=NA, prompt=FALSE, cache=FALSE, echo=TRUE, results=&#39;asis&#39;} BMI_by_age &lt;- with(tobacco, by(BMI, age.gr, descr, transpose = TRUE, stats = c( mean , sd , min , med , max ))) view(BMI_by_age, pander , style = rmarkdown , headings = TRUE) {r, include=FALSE, comment=NA, prompt=FALSE, cache=FALSE, echo=TRUE, results=&#39;asis&#39;} tobacco_subset &lt;- tobacco[ ,c( gender , age.gr , smoker )] freq_tables &lt;- lapply(tobacco_subset, freq) # view(freq_tables, footnote = NA, file = &#39;freq-tables.html&#39;) {r, include=FALSE, comment=NA, prompt=FALSE, cache=FALSE, echo=TRUE, results=&#39;asis&#39;} what.is(iris) {r eval=FALSE, include=FALSE, echo=TRUE} freq(tobacco$gender, style = &#39;rmarkdown&#39;) {r eval=FALSE, include=FALSE, echo=TRUE} print(freq(tobacco$gender), method = &#39;render&#39;) 394.2 skimr library(skimr) skim(df) 394.3 DataExplorer library(DataExplorer) DataExplorer::create_report(df) 394.4 Grafikler {r eval=FALSE, include=FALSE, echo=TRUE} # library(ggplot2) # library(mosaic) # mPlot(irisdata) {r eval=FALSE, include=FALSE, echo=TRUE} ctable(tobacco$gender, tobacco$smoker, style = &#39;rmarkdown&#39;) {r eval=FALSE, include=FALSE, echo=TRUE} print(ctable(tobacco$gender, tobacco$smoker), method = &#39;render&#39;) descr(tobacco, style = &#39;rmarkdown&#39;) print(descr(tobacco), method = &#39;render&#39;, table.classes = &#39;st-small&#39;) dfSummary(tobacco, style = &#39;grid&#39;, plain.ascii = FALSE) print(dfSummary(tobacco, graph.magnif = 0.75), method = &#39;render&#39;) Here, building up a #ggplot2 as slowly as possible, #rstats. Incremental adjustments. #rstatsteachingideas pic.twitter.com/nUulQl8bPh ‚Äî Gina Reynolds ((???)) August 13, 2018 Dreaming of a fancy #Rstats #ggplot #dataviz but still scared of typing #code? (???) esquisse package has you covered https://t.co/1vIDXcVAAF pic.twitter.com/RlTkptnrNv ‚Äî Radoslaw Panczak ((???)) October 2, 2018 "],["rcmdr-5.html", "Chapter 395 Rcmdr", " Chapter 395 Rcmdr library(Rcmdr) Rcmdr::Commander() A Comparative Review of the R Commander GUI for R http://r4stats.com/articles/software-reviews/r-commander/ "],["jamovi-7.html", "Chapter 396 jamovi", " Chapter 396 jamovi https://www.jamovi.org/ https://blog.jamovi.org/2018/07/30/rj.html "],["sonraki-konular-6.html", "Chapter 397 Sonraki Konular 397.1 HTML Rendering 397.2 Rmarkdown Style 397.3 HTML Rendering", " Chapter 397 Sonraki Konular RStudio ile GitHub Hipotez testleri R Markdown ve R Notebook ile tekrarlanabilir rapor output: rmarkdown::html_vignette: css: - !expr system.file( rmarkdown/templates/html_vignette/resources/vignette.css , package = rmarkdown ) vignette: &gt; % % % {r summarytool options 1, include=FALSE} library(knitr) opts_chunk$set(comment=NA, prompt=FALSE, cache=FALSE, echo=TRUE, results=&#39;asis&#39;) {r summarytool options 2, echo=TRUE} library(summarytools) st_css() {r summarytool options 3} st_options(bootstrap.css = FALSE, # Already part of the theme so no need for it plain.ascii = FALSE, # One of the essential settings style = rmarkdown , # Idem. dfSummary.silent = TRUE, # Suppresses messages about temporary files footnote = NA, # Keeping the results minimalistic subtitle.emphasis = FALSE) # For the vignette theme, this gives # much better results. Your mileage may vary. {r summarytools freq Rmarkdown Style} freq(tobacco$gender, style = &#39;rmarkdown&#39;) 397.1 HTML Rendering {r eval=FALSE, include=FALSE, echo=TRUE} print(freq(tobacco$gender), method = &#39;render&#39;) If you find the table too large, you can use table.classes = 'st-small' - an example is provided further below. ‚Äì Back to top 397.2 Rmarkdown Style Tables with heading spanning over 2 rows are not fully supported in markdown (yet), but the result is getting close to acceptable. This, however, is not true for all themes. That‚Äôs why the rendering method is preferred. {r eval=FALSE, include=FALSE, echo=TRUE} ctable(tobacco$gender, tobacco$smoker, style = &#39;rmarkdown&#39;) 397.3 HTML Rendering For best results, use this method. {r include=FALSE, eval=FALSE, echo = TRUE} print(ctable(tobacco$gender, tobacco$smoker), method = &#39;render&#39;) ‚Äì Back to top "],["descr2.html", "Chapter 398 descr() 2 398.1 Rmarkdown Style 398.2 HTML Rendering", " Chapter 398 descr() 2 descr() is also best used with style = 'rmarkdown', and HTML rendering is also supported. 398.1 Rmarkdown Style {r eval=FALSE, include=FALSE, echo=TRUE} descr(tobacco, style = &#39;rmarkdown&#39;) 398.2 HTML Rendering We‚Äôll use table.classes = ‚Äòst-small‚Äô to show how it affects the table‚Äôs size (compare to the freq() table rendered earlier). {r eval=FALSE, include=FALSE, echo=TRUE} print(descr(tobacco), method = &#39;render&#39;, table.classes = &#39;st-small&#39;) ‚Äì Back to top "],["dfsummary1.html", "Chapter 399 dfSummary() 1 399.1 Grid Style", " Chapter 399 dfSummary() 1 399.1 Grid Style This style gives good results, and since v0.9, the graphs are shown as true images. Don‚Äôt forget to specify plain.ascii = FALSE (or set it as a global option with st_options(plain.ascii = FALSE)), or you won‚Äôt get good results. {r , eval=FALSE} dfSummary(tobacco, style = &#39;grid&#39;, graph.magnif = 0.75, tmp.img.dir = /tmp ) {r eval=FALSE, include=FALSE, echo=TRUE} print(dfSummary(tobacco, graph.magnif = 0.75), method = &#39;render&#39;) "],["regression-1.html", "Chapter 400 Regression", " Chapter 400 Regression Logistic Regression in R Tutorial https://www..com/community/tutorials/logistic-regression-R "],["reprex.html", "Chapter 401 reprex", " Chapter 401 reprex author: Serdar Balcƒ±, MD, Pathologist date: 21 09 2018 output: html_document https://www.youtube.com/watch?v=MmTPhGQWPUo https://reprex.tidyverse.org/index.html {r eval=FALSE, include=FALSE, echo=TRUE} # install.packages( reprex ) # library(reprex) (y &lt;- 1:4) #&gt; [1] 1 2 3 4 mean(y) #&gt; [1] 2.5 Created on 2018-09-21 by the reprex package (v0.2.1) "],["reproducible-research-1.html", "Chapter 402 Reproducible Research", " Chapter 402 Reproducible Research karthik/binder-test Example repo for testing holepunch https://github.com/karthik/binder-test Hole punch https://karthik.github.io/holepunch/ How to set up a My Binder for your R project rstudio2019/binder-notes.md https://github.com/karthik/rstudio2019/blob/master/binder-notes.md reproducibility guidelines https://kbroman.org/blog/2019/04/01/reproducibility-guidelines/ Disease risk modelling and visualization using R http://manio.org/2015/06/22/my-approach-to-reproducible-research.html initial steps toward reproducible research https://kbroman.org/steps2rr/ Comments on reproducibility https://kbroman.org/knitr_knutshell/pages/reproducible.html Reproducible science in R https://grunwaldlab.github.io/Reproducible-science-in-R/index.html The Practice of Reproducible Research Case Studies and Lessons from the Data-Intensive Sciences https://www.practicereproducibleresearch.org/ papaja papaja (Preparing APA Journal Articles) is an R package that provides document formats and helper functions to produce complete APA manscripts from RMarkdown-files (PDF and Word documents). https://crsh.github.io/papaja/ Stencila An open source office suite for reproducible research http://stenci.la/ redoc - reversible R Markdown/MS Word documents. https://noamross.github.io/redoc/ output: redoc::rdocx_reversible: keep_md: TRUE highlight_outputs: TRUE redoc::undoc( reversible2.docx ) redoc::undoc(file.choose()) redoc::redoc_extract_rmd( reversible.docx ) "],["docker-1.html", "Chapter 403 docker", " Chapter 403 docker An Introduction to Docker for R Users https://colinfay.me/docker-r-reproducibility/ "],["naming-files.html", "Chapter 404 Naming Files", " Chapter 404 Naming Files How to name files https://speakerdeck.com/jennybc/how-to-name-files "],["example-articles.html", "Chapter 405 Example Articles", " Chapter 405 Example Articles Increasing the Transparency of Research Papers withExplorable Multiverse Analyses https://hal.inria.fr/hal-01976951/document Replication Study: Transcriptional amplification in tumor cells with elevated c-Myc https://repro.elifesciences.org/example.html Introducing eLife‚Äôs first computationally reproducible article https://elifesciences.org/labs/ad58f08d/introducing-elife-s-first-computationally-reproducible-article Re-Evaluating the Efficiency of Physical Visualizations: A Simple Multiverse Analysis https://explorablemultiverse.github.io/examples/frequentist/ R Online Try R Run a piece of code against a specific version of R, from 3.6.0 to 3.1.0. https://srv.colinfay.me:1001/#about https://github.com/ColinFay/ronline Sys.setenv( DSN = database_name , UID = User ID , PASS = Password ) db &lt;- DBI::dbConnect( drv = odbc::odbc(), dsn = Sys.getenv( DSN ), uid = Sys.getenv( UID ), pwd = Sys.getenv( PASS ) ) "],["r-in-pathology-research.html", "Chapter 406 R in Pathology Research", " Chapter 406 R in Pathology Research Progesterone Receptor Status Predicts Response to Progestin Therapy in Endometriosis https://www.ncbi.nlm.nih.gov/pubmed/30357380 irr: Various Coefficients of Interrater Reliability and Agreement https://cran.r-project.org/web/packages/irr/index.html "],["r-notebook-4.html", "Chapter 407 R Notebook", " Chapter 407 R Notebook library(RISmed) library(ggplot2) query &lt;- (exome OR whole OR deep OR high-throughput OR (next AND generation) OR (massively AND parallel)) AND sequencing ngs_search &lt;- EUtilsSummary(query, type= esearch ,db = pubmed ,mindate=1980, maxdate=2013, retmax=30000) QueryCount(ngs_search) ngs_records &lt;- EUtilsGet(ngs_search) years &lt;- Year(ngs_records) ngs_pubs_count &lt;- as.data.frame(table(years)) total &lt;- NULL for (i in 1980:2013){ peryear &lt;- EUtilsSummary( , type= esearch , db= pubmed , mindate=i, maxdate=i) total[i] &lt;- QueryCount(peryear) } year &lt;- 1980:2013 total_pubs_count&lt;- as.data.frame(cbind(year,total[year])) names(total_pubs_count) &lt;- c( year , Total_publications ) names(ngs_pubs_count) &lt;- c( year , NGS_publications ) pubs_year &lt;- merge(ngs_pubs_count,total_pubs_count,by= year ) pubs_year\\(NGS_publications_normalized &lt;- pubs_year\\)NGS_publications *100000 / pubs_year$Total_publications write.table(pubs_year, NGS_publications_per_year.txt ,quote=F,sep= row.names=F) journal &lt;- MedlineTA(ngs_records) ngs_journal_count &lt;- as.data.frame(table(journal)) ngs_journal_count_top25 &lt;- ngs_journal_count[order(-ngs_journal_count[,2]),][1:25,] journal_names &lt;- paste(ngs_journal_count_top25$journal, [jo] ,sep= ) total_journal &lt;- NULL for (i in journal_names){ perjournal &lt;- EUtilsSummary(i, type=‚Äòesearch‚Äô, db=‚Äòpubmed‚Äô,mindate=1980, maxdate=2013) total_journal[i] &lt;- QueryCount(perjournal) } journal_ngs_total &lt;- cbind(ngs_journal_count_top25,total_journal) names(journal_ngs_total) &lt;- c( journal , NGS_publications , Total_publications ) journal_ngs_total\\(NGS_publications_normalized &lt;- journal_ngs_total\\)NGS_publications / journal_ngs_total$Total_publications write.table(journal_ngs_total, NGS_publications_per_journal.txt ,quote=F,sep= row.names=F) pubs_per_year &lt;- read.table( NGS_publications_per_year.txt ,header = T,sep= pubs_per_journal &lt;- read.table( NGS_publications_per_journal.txt ,header = T,sep= ggplot(pubs_per_year,aes(year, NGS_publications_normalized)) + geom_line (colour= blue ,size=2) + xlab( Year ) + ylab( NGS/100000 articles )+ ggtitle( NGS PubMed articles ) ggplot(pubs_per_journal,aes(journal, NGS_publications,fill=journal)) + geom_bar(stat= identity )+ coord_flip()+ theme(legend.position= none ) ggplot(pubs_per_journal ,aes(journal, NGS_publications_normalized,fill=journal)) + geom_bar(stat= identity )+ coord_flip()+ theme(legend.position= none ) "],["creating-websites-in-r-1.html", "Chapter 408 Creating websites in R 408.1 Types of websites 408.2 R Markdown website basics 408.3 GitHub 408.4 Personal websites 408.5 Package websites 408.6 Project websites 408.7 Blogs 408.8 Additional resources 408.9 Demo", " Chapter 408 Creating websites in R author: Emily C. Zabor output: html_document: toc: TRUE toc_float: TRUE http://www.emilyzabor.com/tutorials/rmarkdown_websites_tutorial.html This tutorial provides an introduction to creating websites using R, R Markdown and GitHub pages. This tutorial was originally presented at the Memorial Sloan Kettering Cancer Center Department of Epidemiology and Biostatistics R User Group meeting on January 23, 2018. The current version was updated and presented at the R Ladies NYC Meetup on February 15, 2018. 408.1 Types of websites The main types of websites you may want to create include: Personal websites Package websites Project websites Blogs 408.2 R Markdown website basics The minimum requirements for an R Markdown website are: index.Rmd: contains the content for the website homepage _site.yml: contains metadata for the website A basic example of a _site.yml file for a website with two pages: {r eval = FALSE} name: my-website navbar: # My Website left: - text: Home href: index.html - text: About href: about.html And a basic index.Rmd to create the Home page: {r eval = FALSE} # My Website Hello, Website! Welcome to the world. You can find an overview of R Markdown website basics here. 408.3 GitHub This tutorial will focus on hosting websites through GitHub pages. Hosting websites on GitHub pages is free. If you don‚Äôt have a GitHub account already, sign up for one at https://github.com/join?source=header-home with username YOUR_GH_NAME. I‚Äôll be referring to this username, YOUR_GH_NAME, as your GitHub username throughout this tutorial. There are other free sites for website hosting, and another popular choice is Netlify. 408.4 Personal websites An example from the homepage of my personal website: &lt;img src= img/personal.png style= border: #A9A9A9 1px solid; width:75% &gt; There are two main steps for creating a personal website that will be hosted on GitHub: GitHub setup Local setup 408.4.1 GitHub setup Create a GitHub repository ( repo ) named YOUR_GH_NAME.github.io, where YOUR_GH_NAME is your GitHub username. Initialize it with a README For the GitHub inexperienced: this can ease the process of cloning the repo by initializing the remote repo with a master branch 408.4.2 Local setup Clone this remote repository to a local directory with the same name, YOUR_GH_NAME.github.io Add an R Project to this directory Create a _site.yml and index.Rmd file in your new directory 408.4.3 Why do I need an R Project? The R Project is useful because RStudio will recognize your project as a website, and provide appropriate build tools. Note: After creating the R Project and initial files, you may need to close the project and reopen it before R will recognize it as a website and show the appropriate build tools. 408.4.4 Create content Edit the _site.yml file to change the metadata, layout, and theme of your website. Preview Jekyll themes here and play around with different options. Themes are easy to change even after you have added content. For example, the _site.yml for my personal website looks like this: {r eval = FALSE} name: Emily C. Zabor output_dir: . navbar: # Emily C. Zabor left: - text: Writing href: research.html - text: Speaking href: talks.html - text: Programming href: software.html - text: Teaching href: teaching.html right: - icon: fa-envelope fa-lg href: contact.html - icon: fa-github fa-lg href: http://github.com/zabore - icon: fa-twitter fa-lg href: https://twitter.com/zabormetrics - icon: fa-linkedin fa-lg href: https://www.linkedin.com/in/emily-zabor-59b902b7/ output: html_document: theme: paper css: &#39;styles.css&#39; Edit and create .Rmd files that contain your website content, which will produce the html pages of your website when you knit them. For example, the index.Rmd file for my personal website homepage looks like this: {r eval = FALSE} &lt;link rel= stylesheet href= styles.css type= text/css &gt; &lt;img src= images/emily_2.jpg style= width:25%; border:10px solid; margin-right: 20px align= left &gt; I like to analyze data to answer research questions and test hypotheses. Currently I investigate questions related to breast cancer through my work as a Research Biostatistician at [Memorial Sloan Kettering Cancer Center](https://www.mskcc.org/departments/epidemiology-biostatistics) in the department of Epidemiology &amp; Biostatistics. I graduated from the [University of Minnesota](http://www.sph.umn.edu/academics/divisions/biostatistics/) with a MS in biostatistics in 2010. In 2012 I began working toward my DrPH in biostatistics as a part-time student at [Columbia University](https://www.mailman.columbia.edu/become-student/departments/biostatistics), where I am investigating statistical methods for the study of etiologic heterogeneity in epidemiologic studies under the advisement of [Dr. Shuang Wang](https://www.mailman.columbia.edu/people/our-faculty/sw2206) at Columbia University and [Dr. Colin Begg](https://www.mskcc.org/profile/colin-begg) at Memorial Sloan Kettering Cancer Center. I expect to graduate by the end of 2018. I am a well-known R enthusiast, including serving on the board and being an active member of [R Ladies NYC](http://www.rladiesnyc.org/). My full CV is available [here](files/Zabor_CV_2017_Q4.pdf). Once you have your content written and the layout setup, on the Build tab in RStudio, select Build Website : &lt;img src= img/build.png style= border: #A9A9A9 1px solid; width:75% &gt; Now your local directory contains all of the files needed for your website: &lt;img src= img/directory.png style= border: #A9A9A9 1px solid; width:75% &gt; 408.4.5 Deploy website Basic approach: Select Upload files from the main page of your GitHub repo: &lt;img src= img/uploadbutton.png style= border: #A9A9A9 1px solid; width:75% &gt; And simply drag or select the files from the local repository: &lt;img src= img/upload.png style= border: #A9A9A9 1px solid; width:75% &gt; Advanced approach (recommended): use Git from the shell, from a Git client, or from within RStudio (another great reason to use an R Project!) &lt;img src= img/rstudiogit.png style= border: #A9A9A9 1px solid; width:75% &gt; But this is not a Git/GitHub tutorial. If you want to learn more about Git/GitHub, which I encourage you to do, here‚Äôs a great resource to get you started: http://happygitwithr.com/ 408.4.6 Custom domains The default is for your site to be hosted at http://YOUR_GH_NAME.github.io, but you can add a custom domain name as well. There are two steps: In your GitHub repository YOUR_GH_NAME.github.io, go to Settings &gt; GitHub pages. Type your domain name in the box under Custom domain and hit Save. &lt;img src= img/domain.png style= border: #A9A9A9 1px solid; width:75% &gt; Add a CNAME file to your GitHub repsitory YOUR_GH_NAME.github.io. It will appear like this in your repository: &lt;img src= img/cname1.png style= border: #A9A9A9 1px solid; width:75% &gt; And inside the file you will simply have your domain name: &lt;img src= img/cname2.png style= border: #A9A9A9 1px solid; width:75% &gt; 408.5 Package websites An example from the website for my package ezfun: &lt;img src= img/package.png style= border: #A9A9A9 1px solid; width:75% &gt; Use Hadley Wickham‚Äôs great package pkgdown to easily build a website from your package that is hosted on GitHub. Details of pkgdown can be found on the pkgdown website, which was also created using pkgdown. This assumes you already have an R package with a local directory and a GitHub repository. From within your package directory run: {r eval = FALSE} devtools::install_github( hadley/pkgdown ) pkgdown::build_site() This will add a folder called docs to the local directory for your package Upload/push these changes to the GitHub repository for your package In the GitHub repository for your package go to Settings &gt; GitHub pages. Select master branch/docs folder as the source and hit Save &lt;img src= img/ghsource.png style= border: #A9A9A9 1px solid; width:75% &gt; The page will be added as to your personal website as YOUR_GH_NAME.github.io/repo_name The Home page of the site will be pulled from the README file on your package repository The Reference page of the site lists the included functions with their description Each function can be clicked through to see the help page, if any Would also build pages for any available vignettes And you‚Äôre done, it‚Äôs that easy. 408.6 Project websites You can create a website for a non-package repository as well. For example, I have a page on my website linking to the repository in which this tutorial is stored. &lt;img src= img/project.png style= border: #A9A9A9 1px solid; width:75% &gt; 408.6.1 Local setup From within the local directory of the project of interest: Create a _site.yml and index.Rmd file in your new directory Edit these files to create content and manage layout, as before for personal websites 408.6.2 GitHub setup Upload/push these new files to the GitHub repository for your project Enable GitHub pages for the repository by going to Settings &gt; GitHub Pages, where you‚Äôll select the master branch folder and hit Save &lt;img src= img/ghpages.png style= border: #A9A9A9 1px solid; width:75% &gt; 408.7 Blogs R Markdown websites are simple to create and deploy, but can become cumbersome if you make frequent updates or additions to the website, as in the case of a blog. Luckily, the R package blogdown exists just for this purpose. blogdown is an R package that allows you to create static websites, which means that the deployed version of the website only consists of JavaScript, HTML, CSS, and images. Luckily the blogdown package makes it so that you don‚Äôt have to know any of those things to create a beautiful website for your blog, powered by Hugo. For a complete resource on using the blogdown website, checkout this short blogdown book. I don‚Äôt have a personal blog, so let‚Äôs look at the website I built to feature the events and blog of the R-Ladies NYC organization as an example. &lt;img src= img/rladiesnychome.png style= border: #A9A9A9 1px solid; width:75% &gt; 408.7.1 Setup The first three steps are similar to those from creating a basic R Markdown website: Create a GitHub repository named YOUR_GH_NAME.github.io, where YOUR_GH_NAME is your GitHub username, initialized with a README file Clone the GitHub repo to a local directory with the same name Add an R Project to the local directoroy Next we get started with blogdown. Install blogdown and Hugo {r eval = FALSE} install.packages( blogdown ) blogdown::install_hugo() Choose a theme and find the link to the theme‚Äôs GitHub repository. In this case themes aren‚Äôt quite as easy to change as with basic R Markdown websites, so choose carefully. Within your project session, generate a new site. The option theme_example = TRUE will obtain the files for an example site that you can then customize for your needs. Below user/repo refers to the GitHub username and GitHub repository for your selected theme. {r eval = FALSE} blogdown::new_site(theme = user/repo , theme_example = TRUE) This will generate all of the file structure for your new blog. &lt;img src= img/blogdirectory.png style= border: #A9A9A9 1px solid; width:75% &gt; After this is complete, you should quit and then reopen the project. Upon reopening, RStudio will recognize the project as a website. 408.7.2 Customizing the appearance Make changes to the config.toml file (equivalent to the _site.yml from basic R Markdown websites) to change the layout and appearance of your website. The available features of the config.toml file will differ depending on your theme, and most theme examples come with a well annotated config.toml that you can use as a template. Once you have customized your website features, click on the RStudio addin Serve Site to preview the site locally. &lt;img src= img/servesite.png style= border: #A9A9A9 1px solid; width:75% &gt; 408.7.3 Writing a new blog post There are several ways to create a new post for your site, but the easiest is using the RStudio addin New Post : &lt;img src= img/newpost.png style= border: #A9A9A9 1px solid; width:75% &gt; This opens a pop-up where you can enter the meta-data for a new post: &lt;img src= img/newpostbox.png style= border: #A9A9A9 1px solid; width:75% &gt; In addition to setting the Title, Author and Date of the post, you can additionally create categories, which will organize your posts in folders, and can add tags to posts, which can make them searchable within your site‚Äôs content. Be aware that the functioning of these features will vary by theme. Dates can be in the future to allow future release of a post. Notice at the bottom that you can select whether to use a regular markdown (.md) or R markdown (.Rmd) file. .Rmd files will have to be rendered before generating html pages so it is best practice to limit their use to cases where R code is included. A file name and slug will automatically be generated based on the other metadata. The slug is a URL friendly title of your post. &lt;img src= img/newpostboxfilled.png style= border: #A9A9A9 1px solid; width:75% &gt; 408.7.4 Hosting A blogdown site is a bit more cumbersome both to build and to host on GitHub as compared to a regular R Markdown website, and as compared to what I described above. Problem 1: Because it is a static site, upon building, the files needed to generate the site online are automatically created in a separate subdirectory called public within your local directory. However this will cause problems with GitHub hosting since the files to host need to be in the local YOUR_GH_NAME.github.io directory My solution: Maintain separate directories for the source files (I named this directory source ) and for the static files (the directory YOUR_GH_NAME.github.io) that will be generated on build. The source folder is where your R project and config.toml files will live. &lt;img src= img/blogfolders.png style= border: #A9A9A9 1px solid; width:75% &gt; In your config.toml use the option publishDir = to customize blogdown to publish to the YOUR_GH_NAME.github.io folder, rather than to the default local location &lt;img src= img/publishdir.png style= border: #A9A9A9 1px solid; width:75% &gt; Problem 2: GitHub defaults to using Jekyll with website content, and this needs to be disabled since blogdown sites are built with Hugo To get around this, you need to include an empty file named .nojekyll in your GitHub repo YOUR_GH_NAME.github.io, prior to publishing. &lt;img src= img/nojekyll.png style= border: #A9A9A9 1px solid; width:75% &gt; 408.8 Additional resources A compiled list of the additional resources/links presented throughout this tutorial: http://rmarkdown.rstudio.com/rmarkdown_websites.html: an overview of R Markdown website basics http://jekyllthemes.org/: Jekyll themes for use with your R Markdown website http://happygitwithr.com/: an introduction to Git/GitHub http://pkgdown.r-lib.org/: Hadley Wickham‚Äôs pkgdown website https://bookdown.org/yihui/blogdown/: Yihui Xie‚Äôs blogdown book https://themes.gohugo.io/: Hugo themes for use with your blogdown website 408.9 Demo "],["creating-websites-in-r-2.html", "Chapter 409 Creating websites in R 409.1 Types of websites 409.2 R Markdown website basics 409.3 GitHub 409.4 Personal websites 409.5 Package websites 409.6 Project websites 409.7 Blogs 409.8 Additional resources 409.9 Demo", " Chapter 409 Creating websites in R author: Emily C. Zabor output: html_document: toc: TRUE toc_float: TRUE This tutorial provides an introduction to creating websites using R, R Markdown and GitHub pages. This tutorial was originally presented at the Memorial Sloan Kettering Cancer Center Department of Epidemiology and Biostatistics R User Group meeting on January 23, 2018. The current version was updated and presented at the R Ladies NYC Meetup on February 15, 2018. 409.1 Types of websites The main types of websites you may want to create include: Personal websites Package websites Project websites Blogs 409.2 R Markdown website basics The minimum requirements for an R Markdown website are: index.Rmd: contains the content for the website homepage _site.yml: contains metadata for the website A basic example of a _site.yml file for a website with two pages: {r eval = FALSE} name: my-website navbar: # My Website left: - text: Home href: index.html - text: About href: about.html And a basic index.Rmd to create the Home page: {r eval = FALSE} # My Website Hello, Website! Welcome to the world. You can find an overview of R Markdown website basics here. 409.3 GitHub This tutorial will focus on hosting websites through GitHub pages. Hosting websites on GitHub pages is free. If you don‚Äôt have a GitHub account already, sign up for one at https://github.com/join?source=header-home with username YOUR_GH_NAME. I‚Äôll be referring to this username, YOUR_GH_NAME, as your GitHub username throughout this tutorial. There are other free sites for website hosting, and another popular choice is Netlify. 409.4 Personal websites An example from the homepage of my personal website: &lt;img src= img/personal.png style= border: #A9A9A9 1px solid; width:75% &gt; There are two main steps for creating a personal website that will be hosted on GitHub: GitHub setup Local setup 409.4.1 GitHub setup Create a GitHub repository ( repo ) named YOUR_GH_NAME.github.io, where YOUR_GH_NAME is your GitHub username. Initialize it with a README For the GitHub inexperienced: this can ease the process of cloning the repo by initializing the remote repo with a master branch 409.4.2 Local setup Clone this remote repository to a local directory with the same name, YOUR_GH_NAME.github.io Add an R Project to this directory Create a _site.yml and index.Rmd file in your new directory 409.4.3 Why do I need an R Project? The R Project is useful because RStudio will recognize your project as a website, and provide appropriate build tools. Note: After creating the R Project and initial files, you may need to close the project and reopen it before R will recognize it as a website and show the appropriate build tools. 409.4.4 Create content Edit the _site.yml file to change the metadata, layout, and theme of your website. Preview Jekyll themes here and play around with different options. Themes are easy to change even after you have added content. For example, the _site.yml for my personal website looks like this: {r eval = FALSE} name: Emily C. Zabor output_dir: . navbar: # Emily C. Zabor left: - text: Writing href: research.html - text: Speaking href: talks.html - text: Programming href: software.html - text: Teaching href: teaching.html right: - icon: fa-envelope fa-lg href: contact.html - icon: fa-github fa-lg href: http://github.com/zabore - icon: fa-twitter fa-lg href: https://twitter.com/zabormetrics - icon: fa-linkedin fa-lg href: https://www.linkedin.com/in/emily-zabor-59b902b7/ output: html_document: theme: paper css: &#39;styles.css&#39; Edit and create .Rmd files that contain your website content, which will produce the html pages of your website when you knit them. For example, the index.Rmd file for my personal website homepage looks like this: {r eval = FALSE} &lt;link rel= stylesheet href= styles.css type= text/css &gt; &lt;img src= images/emily_2.jpg style= width:25%; border:10px solid; margin-right: 20px align= left &gt; I like to analyze data to answer research questions and test hypotheses. Currently I investigate questions related to breast cancer through my work as a Research Biostatistician at [Memorial Sloan Kettering Cancer Center](https://www.mskcc.org/departments/epidemiology-biostatistics) in the department of Epidemiology &amp; Biostatistics. I graduated from the [University of Minnesota](http://www.sph.umn.edu/academics/divisions/biostatistics/) with a MS in biostatistics in 2010. In 2012 I began working toward my DrPH in biostatistics as a part-time student at [Columbia University](https://www.mailman.columbia.edu/become-student/departments/biostatistics), where I am investigating statistical methods for the study of etiologic heterogeneity in epidemiologic studies under the advisement of [Dr. Shuang Wang](https://www.mailman.columbia.edu/people/our-faculty/sw2206) at Columbia University and [Dr. Colin Begg](https://www.mskcc.org/profile/colin-begg) at Memorial Sloan Kettering Cancer Center. I expect to graduate by the end of 2018. I am a well-known R enthusiast, including serving on the board and being an active member of [R Ladies NYC](http://www.rladiesnyc.org/). My full CV is available [here](files/Zabor_CV_2017_Q4.pdf). Once you have your content written and the layout setup, on the Build tab in RStudio, select Build Website : &lt;img src= img/build.png style= border: #A9A9A9 1px solid; width:75% &gt; Now your local directory contains all of the files needed for your website: &lt;img src= img/directory.png style= border: #A9A9A9 1px solid; width:75% &gt; 409.4.5 Deploy website Basic approach: Select Upload files from the main page of your GitHub repo: &lt;img src= img/uploadbutton.png style= border: #A9A9A9 1px solid; width:75% &gt; And simply drag or select the files from the local repository: &lt;img src= img/upload.png style= border: #A9A9A9 1px solid; width:75% &gt; Advanced approach (recommended): use Git from the shell, from a Git client, or from within RStudio (another great reason to use an R Project!) &lt;img src= img/rstudiogit.png style= border: #A9A9A9 1px solid; width:75% &gt; But this is not a Git/GitHub tutorial. If you want to learn more about Git/GitHub, which I encourage you to do, here‚Äôs a great resource to get you started: http://happygitwithr.com/ 409.4.6 Custom domains The default is for your site to be hosted at http://YOUR_GH_NAME.github.io, but you can add a custom domain name as well. There are two steps: In your GitHub repository YOUR_GH_NAME.github.io, go to Settings &gt; GitHub pages. Type your domain name in the box under Custom domain and hit Save. &lt;img src= img/domain.png style= border: #A9A9A9 1px solid; width:75% &gt; Add a CNAME file to your GitHub repsitory YOUR_GH_NAME.github.io. It will appear like this in your repository: &lt;img src= img/cname1.png style= border: #A9A9A9 1px solid; width:75% &gt; And inside the file you will simply have your domain name: &lt;img src= img/cname2.png style= border: #A9A9A9 1px solid; width:75% &gt; 409.5 Package websites An example from the website for my package ezfun: &lt;img src= img/package.png style= border: #A9A9A9 1px solid; width:75% &gt; Use Hadley Wickham‚Äôs great package pkgdown to easily build a website from your package that is hosted on GitHub. Details of pkgdown can be found on the pkgdown website, which was also created using pkgdown. This assumes you already have an R package with a local directory and a GitHub repository. From within your package directory run: {r eval = FALSE} devtools::install_github( hadley/pkgdown ) pkgdown::build_site() This will add a folder called docs to the local directory for your package Upload/push these changes to the GitHub repository for your package In the GitHub repository for your package go to Settings &gt; GitHub pages. Select master branch/docs folder as the source and hit Save &lt;img src= img/ghsource.png style= border: #A9A9A9 1px solid; width:75% &gt; The page will be added as to your personal website as YOUR_GH_NAME.github.io/repo_name The Home page of the site will be pulled from the README file on your package repository The Reference page of the site lists the included functions with their description Each function can be clicked through to see the help page, if any Would also build pages for any available vignettes And you‚Äôre done, it‚Äôs that easy. 409.6 Project websites You can create a website for a non-package repository as well. For example, I have a page on my website linking to the repository in which this tutorial is stored. &lt;img src= img/project.png style= border: #A9A9A9 1px solid; width:75% &gt; 409.6.1 Local setup From within the local directory of the project of interest: Create a _site.yml and index.Rmd file in your new directory Edit these files to create content and manage layout, as before for personal websites 409.6.2 GitHub setup Upload/push these new files to the GitHub repository for your project Enable GitHub pages for the repository by going to Settings &gt; GitHub Pages, where you‚Äôll select the master branch folder and hit Save &lt;img src= img/ghpages.png style= border: #A9A9A9 1px solid; width:75% &gt; 409.7 Blogs R Markdown websites are simple to create and deploy, but can become cumbersome if you make frequent updates or additions to the website, as in the case of a blog. Luckily, the R package blogdown exists just for this purpose. blogdown is an R package that allows you to create static websites, which means that the deployed version of the website only consists of JavaScript, HTML, CSS, and images. Luckily the blogdown package makes it so that you don‚Äôt have to know any of those things to create a beautiful website for your blog, powered by Hugo. For a complete resource on using the blogdown website, checkout this short blogdown book. I don‚Äôt have a personal blog, so let‚Äôs look at the website I built to feature the events and blog of the R-Ladies NYC organization as an example. &lt;img src= img/rladiesnychome.png style= border: #A9A9A9 1px solid; width:75% &gt; 409.7.1 Setup The first three steps are similar to those from creating a basic R Markdown website: Create a GitHub repository named YOUR_GH_NAME.github.io, where YOUR_GH_NAME is your GitHub username, initialized with a README file Clone the GitHub repo to a local directory with the same name Add an R Project to the local directoroy Next we get started with blogdown. Install blogdown and Hugo {r eval = FALSE} install.packages( blogdown ) blogdown::install_hugo() Choose a theme and find the link to the theme‚Äôs GitHub repository. In this case themes aren‚Äôt quite as easy to change as with basic R Markdown websites, so choose carefully. Within your project session, generate a new site. The option theme_example = TRUE will obtain the files for an example site that you can then customize for your needs. Below user/repo refers to the GitHub username and GitHub repository for your selected theme. {r eval = FALSE} blogdown::new_site(theme = user/repo , theme_example = TRUE) This will generate all of the file structure for your new blog. &lt;img src= img/blogdirectory.png style= border: #A9A9A9 1px solid; width:75% &gt; After this is complete, you should quit and then reopen the project. Upon reopening, RStudio will recognize the project as a website. 409.7.2 Customizing the appearance Make changes to the config.toml file (equivalent to the _site.yml from basic R Markdown websites) to change the layout and appearance of your website. The available features of the config.toml file will differ depending on your theme, and most theme examples come with a well annotated config.toml that you can use as a template. Once you have customized your website features, click on the RStudio addin Serve Site to preview the site locally. &lt;img src= img/servesite.png style= border: #A9A9A9 1px solid; width:75% &gt; 409.7.3 Writing a new blog post There are several ways to create a new post for your site, but the easiest is using the RStudio addin New Post : &lt;img src= img/newpost.png style= border: #A9A9A9 1px solid; width:75% &gt; This opens a pop-up where you can enter the meta-data for a new post: &lt;img src= img/newpostbox.png style= border: #A9A9A9 1px solid; width:75% &gt; In addition to setting the Title, Author and Date of the post, you can additionally create categories, which will organize your posts in folders, and can add tags to posts, which can make them searchable within your site‚Äôs content. Be aware that the functioning of these features will vary by theme. Dates can be in the future to allow future release of a post. Notice at the bottom that you can select whether to use a regular markdown (.md) or R markdown (.Rmd) file. .Rmd files will have to be rendered before generating html pages so it is best practice to limit their use to cases where R code is included. A file name and slug will automatically be generated based on the other metadata. The slug is a URL friendly title of your post. &lt;img src= img/newpostboxfilled.png style= border: #A9A9A9 1px solid; width:75% &gt; 409.7.4 Hosting A blogdown site is a bit more cumbersome both to build and to host on GitHub as compared to a regular R Markdown website, and as compared to what I described above. Problem 1: Because it is a static site, upon building, the files needed to generate the site online are automatically created in a separate subdirectory called public within your local directory. However this will cause problems with GitHub hosting since the files to host need to be in the local YOUR_GH_NAME.github.io directory My solution: Maintain separate directories for the source files (I named this directory source ) and for the static files (the directory YOUR_GH_NAME.github.io) that will be generated on build. The source folder is where your R project and config.toml files will live. &lt;img src= img/blogfolders.png style= border: #A9A9A9 1px solid; width:75% &gt; In your config.toml use the option publishDir = to customize blogdown to publish to the YOUR_GH_NAME.github.io folder, rather than to the default local location &lt;img src= img/publishdir.png style= border: #A9A9A9 1px solid; width:75% &gt; Problem 2: GitHub defaults to using Jekyll with website content, and this needs to be disabled since blogdown sites are built with Hugo To get around this, you need to include an empty file named .nojekyll in your GitHub repo YOUR_GH_NAME.github.io, prior to publishing. &lt;img src= img/nojekyll.png style= border: #A9A9A9 1px solid; width:75% &gt; 409.8 Additional resources A compiled list of the additional resources/links presented throughout this tutorial: http://rmarkdown.rstudio.com/rmarkdown_websites.html: an overview of R Markdown website basics http://jekyllthemes.org/: Jekyll themes for use with your R Markdown website http://happygitwithr.com/: an introduction to Git/GitHub http://pkgdown.r-lib.org/: Hadley Wickham‚Äôs pkgdown website https://bookdown.org/yihui/blogdown/: Yihui Xie‚Äôs blogdown book https://themes.gohugo.io/: Hugo themes for use with your blogdown website 409.9 Demo "],["roc.html", "Chapter 410 ROC", " Chapter 410 ROC "],["tidyroc.html", "Chapter 411 tidyroc", " Chapter 411 tidyroc https://github.com/dariyasydykova/tidyroc "],["section.html", "Chapter 412 ", " Chapter 412 "],["orcid-1.html", "Chapter 413 ORCID", " Chapter 413 ORCID Introduction to ORCID Researcher Identifiers in R with rorcid https://www.pauloldham.net/introduction-to-orcid-with-rorcid/ "],["r-packages-used.html", "Chapter 414 R Packages Used", " Chapter 414 R Packages Used "],["r-package-development.html", "Chapter 415 R package development", " Chapter 415 R package development Analyses as Packages http://rmflight.github.io/posts/2014/07/analyses_as_packages.html Creating an analysis as a package and vignette http://rmflight.github.io/posts/2014/07/vignetteAnalysis.html pkgdown https://pkgdown.r-lib.org/index.html Writing an R package from scratch https://hilaryparker.com/2014/04/29/writing-an-r-package-from-scratch/ R packages http://r-pkgs.had.co.nz/intro.html https://r-pkgs.org devtools https://github.com/r-lib/devtools "],["purrr.html", "Chapter 416 purrr", " Chapter 416 purrr Learning Functional Programming &amp; purrr https://paulvanderlaken.com/2018/12/05/learning-functional-programming-purrr/ "],["diagrammer.html", "Chapter 417 DiagrammeR", " Chapter 417 DiagrammeR http://rich-iannone.github.io/DiagrammeR/ "],["gggenes.html", "Chapter 418 gggenes", " Chapter 418 gggenes https://cran.r-project.org/web/packages/gggenes/vignettes/introduction-to-gggenes.html "],["textminer.html", "Chapter 419 textmineR", " Chapter 419 textmineR https://cran.r-project.org/web/packages/textmineR/vignettes/a_start_here.html https://cran.r-project.org/web/packages/textmineR/vignettes/b_document_clustering.html https://cran.r-project.org/web/packages/textmineR/vignettes/c_topic_modeling.html https://cran.r-project.org/web/packages/textmineR/vignettes/d_text_embeddings.html https://cran.r-project.org/web/packages/textmineR/vignettes/e_doc_summarization.html "],["tidyshiny.html", "Chapter 420 tidyshiny", " Chapter 420 tidyshiny https://github.com/MangoTheCat/tidyshiny/ "],["rio-1.html", "Chapter 421 rio", " Chapter 421 rio https://cran.r-project.org/web/packages/rio/README.html install.packages( rio ) install_formats() "],["addinplots.html", "Chapter 422 addinplots", " Chapter 422 addinplots RStudio Addins for plotting https://github.com/homerhanumat/addinplots/ "],["citr.html", "Chapter 423 citr", " Chapter 423 citr citr: RStudio Addin to Insert Markdown Citations https://github.com/crsh/citr "],["ggplotassist-1.html", "Chapter 424 ggplotAssist", " Chapter 424 ggplotAssist https://github.com/cardiomoon/ggplotAssist/blob/master/README.md "],["radiant-2.html", "Chapter 425 Radiant", " Chapter 425 Radiant Radiant ‚Äì Business analytics using R and Shiny https://radiant-rstats.github.io/docs/tutorials.html "],["rspivot.html", "Chapter 426 rspivot", " Chapter 426 rspivot https://ryantimpe.github.io/rspivot/index.html "],["finalfit-3.html", "Chapter 427 finalfit", " Chapter 427 finalfit http://www.datasurg.net/2018/05/22/finalfit-knitr-and-r-markdown-for-quick-results/ "],["ggraptr-1.html", "Chapter 428 ggraptR", " Chapter 428 ggraptR https://github.com/cargomoose/ggraptR "],["bookdownthemeeditor.html", "Chapter 429 bookdownThemeEditor", " Chapter 429 bookdownThemeEditor https://github.com/hebrewseniorlife/bookdownThemeEditor "],["esquisse-1.html", "Chapter 430 esquisse", " Chapter 430 esquisse https://github.com/dreamRs/esquisse "],["ggtextures.html", "Chapter 431 ggtextures", " Chapter 431 ggtextures https://github.com/clauswilke/ggtextures "],["ggstatsplot.html", "Chapter 432 ggstatsplot 432.1 ggcoefstats", " Chapter 432 ggstatsplot ggplot2 Based Plots with Statistical Details https://indrajeetpatil.github.io/ggstatsplot/ 432.1 ggcoefstats https://indrajeetpatil.github.io/ggstatsplot/articles/ggcoefstats.html "],["choroplethr.html", "Chapter 433 Choroplethr", " Chapter 433 Choroplethr https://arilamstein.com/open-source/ "],["purrr-1.html", "Chapter 434 purrr", " Chapter 434 purrr https://colinfay.me/happy-dev-purrr/ "],["knitcitations-1.html", "Chapter 435 knitcitations", " Chapter 435 knitcitations http://www.carlboettiger.info/2012/05/30/knitcitations.html "],["infer-3.html", "Chapter 436 infer 436.1 Packages to be studied", " Chapter 436 infer Tidy Statistical Inference https://cran.r-project.org/web/packages/infer/index.html https://kbroman.org/pkg_primer/ 436.1 Packages to be studied {r eval=FALSE, include=FALSE, echo=TRUE} packagefinder::findPackage(c( pubmed , bibliography )) https://cran.rstudio.com/web/packages/sys/ "],["scales.html", "Chapter 437 scales", " Chapter 437 scales Scale Functions for Visualization https://cran.r-project.org/web/packages/scales/index.html {r eval=FALSE, include=FALSE, echo=TRUE} paste0(round(.20394*100, 1), % ) scales::percent(.20394) "],["animation.html", "Chapter 438 animation", " Chapter 438 animation A Gallery of Animations in Statistics and Utilities to Create Animations https://cran.r-project.org/web/packages/animation/ "],["gganimate-2.html", "Chapter 439 gganimate", " Chapter 439 gganimate A Grammar of Animated Graphics https://github.com/thomasp85/gganimate https://cran.r-project.org/web/packages/naniar/index.html https://hughjonesd.github.io/anim.plots/anim.plots.html You Can Design a Good Chart with R https://towardsdatascience.com/you-can-design-a-good-chart-with-r-5d00ed7dd18e Bubble-grid-maps https://jschoeley.github.io/2018/06/30/bubble-grid-map.html Taking control of animations in R and demystifying them in the process https://www.data-imaginist.com/2017/animating-the-logo/?utm_content=bufferd418f&amp;utm_medium=social&amp;utm_source=twitter.com&amp;utm_campaign=buffer https://cran.r-project.org/web/packages/rticles/rticles.pdf https://colinfay.me/build-api-wrapper-package-r/ https://www..com/community/tutorials/setup-data-science-environment https://cran.r-project.org/web/packages/kableExtra/vignettes/awesome_table_in_html.html####installation https://cran.r-project.org/web/packages/expss/vignettes/tables-with-labels.html https://cran.r-project.org/web/packages/tabulizer/vignettes/tabulizer.html https://github.com/benjaminrich/table1 https://cran.r-project.org/web/packages/tableone/vignettes/introduction.html https://twitter.com/mitchoharawild/status/1007297976711110659?s=12 https://twitter.com/mf_viz/status/1004954297962917891?s=12 https://github.com/cmap/morpheus.R https://github.com/talgalili/heatmaply/ https://github.com/rstudio/d3heatmap http://worksmarter.pl/en/post/webscraping-case-study-1/ http://worksmarter.pl/en/post/job-automation-r-1/ http://www.brodrigues.co/blog/2018-06-10-scraping_pdfs/ https://www.r-bloggers.com/how-to-plot-with-ggiraph/ https://www.r-bloggers.com/understanding-pca-using-stack-overflow-data/ https://www.r-bloggers.com/beautiful-and-powerful-correlation-tables-in-r/ http://journals.plos.org/plosone/article?id=10.1371%2Fjournal.pone.0188299 https://github.com/trinker/reports Animated Directional Chord Diagrams https://guyabel.com/post/animated-directional-chord-diagrams/ modelDown: a website generator for your predictive models http://smarterpoland.pl/index.php/2018/06/modeldown-a-website-generator-for-your-predictive-models/ "],["drake.html", "Chapter 440 drake 440.1 chromoMap 440.2 tabplot 440.3 treemap 440.4 VIM 440.5 survey 440.6 tidytext 440.7 rapport an R templating system 440.8 ztable 440.9 texreg 440.10 SortableHTMLTables 440.11 hwriter 440.12 HTMLUtils 440.13 htmlTable 440.14 formattable 440.15 DT 440.16 stargazer 440.17 conflicted 440.18 psycho 440.19 styler 440.20 tidyverse 440.21 splitstackshape 440.22 epiR 440.23 lmtest 440.24 popEpi 440.25 Extra tools to go with ‚ÄòEpi‚Äô and make nice rate tables 440.26 survival 440.27 survminer 440.28 Random-Effects Modeks 440.29 Multiple Imputation 440.30 Really nice summmary tables 440.31 Complex survey data analysis 440.32 For handling complex survey data 440.33 Alternative methods for ICC calculation from survey data 440.34 Meta-analysis 440.35 abind 440.36 acepack 440.37 AlgDesign 440.38 AnnotationDbi 440.39 aplpack 440.40 arm 440.41 assertthat 440.42 backports 440.43 base 440.44 base64enc 440.45 BCA 440.46 BH 440.47 BiasedUrn 440.48 bindr 440.49 bindrcpp 440.50 Biobase 440.51 BiocGenerics 440.52 BiocInstaller 440.53 bit 440.54 bit64 440.55 bitops 440.56 blob 440.57 boot 440.58 broom 440.59 BsMD 440.60 ca 440.61 car 440.62 caTools 440.63 cellranger 440.64 checkmate 440.65 class 440.66 cluster 440.67 clv 440.68 coda 440.69 codetools 440.70 coin 440.71 colorspace 440.72 combinat 440.73 compiler 440.74 conf.design 440.75 curl 440.76 data.table 440.77 datasets 440.78 date 440.79 DBI 440.80 deldir 440.81 depthTools 440.82 DiceDesign 440.83 dichromat 440.84 digest 440.85 DoE.base 440.86 DoE.wrapper 440.87 doParallel 440.88 dplyr 440.89 e1071 440.90 effects 440.91 ENmisc 440.92 epiR 440.93 estimability 440.94 evaluate 440.95 ez 440.96 flexclust 440.97 forcats 440.98 foreach 440.99 foreign 440.100 formatR 440.101 Formula 440.102 fracdiff 440.103 FrF2 440.104 gdata 440.105 ggplot2 440.106 ggthemes 440.107 ggvis 440.108 glue 440.109 goftest 440.110 graphics 440.111 grDevices 440.112 grid 440.113 gridBase 440.114 gridExtra 440.115 gtable 440.116 gtools 440.117 haven 440.118 highr 440.119 Hmisc 440.120 hms 440.121 htmlTable 440.122 htmltools 440.123 htmlwidgets 440.124 httpuv 440.125 httr 440.126 igraph 440.127 IRanges 440.128 irlba 440.129 iterators 440.130 jsonlite 440.131 KernSmooth 440.132 knitr 440.133 labeling 440.134 lattice 440.135 latticeExtra 440.136 lazyeval 440.137 leaps 440.138 lhs 440.139 lme4 440.140 lmtest 440.141 lsmeans 440.142 lubridate 440.143 magrittr 440.144 markdown 440.145 MASS 440.146 Matrix 440.147 matrixcalc 440.148 MatrixModels 440.149 memoise 440.150 methods 440.151 mgcv 440.152 mi 440.153 mime 440.154 minqa 440.155 mnormt 440.156 modelr 440.157 modeltools 440.158 multcomp 440.159 munsell 440.160 mvtnorm 440.161 nlme 440.162 nloptr 440.163 NLP 440.164 NMF 440.165 nnet 440.166 nortest 440.167 openssl 440.168 ordinal 440.169 orloca 440.170 orloca.es 440.171 parallel 440.172 pbkrtest 440.173 pkgconfig 440.174 pkgmaker 440.175 plogr 440.176 plyr 440.177 polyclip 440.178 psych 440.179 purrr 440.180 quadprog 440.181 quantmod 440.182 quantreg 440.183 R2HTML 440.184 R6 440.185 randtests 440.186 Rcmdr 440.187 RcmdrMisc 440.188 RcmdrPlugin.BCA 440.189 RcmdrPlugin.coin 440.190 RcmdrPlugin.depthTools 440.191 RcmdrPlugin.DoE 440.192 RcmdrPlugin.doex 440.193 RcmdrPlugin.EACSPIR 440.194 RcmdrPlugin.EBM 440.195 RcmdrPlugin.epack 440.196 RcmdrPlugin.EZR 440.197 RcmdrPlugin.KMggplot2 440.198 RcmdrPlugin.mosaic 440.199 RcmdrPlugin.MPAStats 440.200 RcmdrPlugin.orloca 440.201 RcmdrPlugin.plotByGroup 440.202 RcmdrPlugin.qual 440.203 RcmdrPlugin.SCDA 440.204 RcmdrPlugin.sampling 440.205 RcmdrPlugin.seeg 440.206 RcmdrPlugin.SLC 440.207 RcmdrPlugin.SM 440.208 RcmdrPlugin.steepness 440.209 RcmdrPlugin.survival 440.210 RcmdrPlugin.TeachingDemos 440.211 RcmdrPlugin.temis 440.212 RcmdrPlugin.UCA 440.213 RColorBrewer 440.214 Rcpp 440.215 RcppArmadillo 440.216 RcppEigen 440.217 readr 440.218 readxl 440.219 registry 440.220 relimp 440.221 rematch 440.222 reshape 440.223 reshape2 440.224 rgl 440.225 RISmed 440.226 rJava 440.227 rlang 440.228 rmarkdown 440.229 RMySQL 440.230 rngtools 440.231 rpart 440.232 rpart.plot 440.233 rprojroot 440.234 rsm 440.235 rticles tufte 440.236 RSQLite 440.237 rvest 440.238 S4Vectors 440.239 sandwich 440.240 scales 440.241 scatterplot3d 440.242 SCMA 440.243 SCRT 440.244 SCVA 440.245 seeg 440.246 selectr 440.247 sem 440.248 sfsmisc 440.249 sgeostat 440.250 shiny 440.251 shinythemes 440.252 slam 440.253 SLC 440.254 sourcetools 440.255 SparseM 440.256 spatial 440.257 spatstat 440.258 spatstat.utils 440.259 splines 440.260 stats 440.261 stats4 440.262 steepness 440.263 stringi 440.264 stringr 440.265 survival 440.266 tcltk 440.267 tcltk2 440.268 TeachingDemos 440.269 tensor 440.270 TH.data 440.271 tibble 440.272 tidyr 440.273 tidyverse 440.274 timeDate 440.275 tkrplot 440.276 tm 440.277 tools 440.278 tseries 440.279 TTR 440.280 ucminf 440.281 utils 440.282 vcd 440.283 viridis 440.284 viridisLite 440.285 XLConnect 440.286 XLConnectJars 440.287 xml2 440.288 xtable 440.289 xts 440.290 yaml 440.291 zoo 440.292 Rcmdr 440.293 Schedule R Script 440.294 flatxml 440.295 roomba 440.296 tufterhandout 440.297 papeR 440.298 tm", " Chapter 440 drake https://ropensci.github.io/drake/ 440.1 chromoMap An R package for Interactive visualization and mapping of human chromosomes https://cran.r-project.org/web/packages/chromoMap/vignettes/chromoMap.html 440.2 tabplot Tableplot, a Visualization of Large Datasets https://cran.r-project.org/web/packages/tabplot/index.html https://cran.r-project.org/web/packages/tabplot/vignettes/tabplot-vignette.html https://cran.r-project.org/web/packages/tabplot/vignettes/tabplot-timings.html 440.3 treemap Treemap Visualization https://cran.r-project.org/web/packages/treemap/index.html 440.4 VIM Visualization and Imputation of Missing Values https://cran.r-project.org/web/packages/VIM/index.html 440.5 survey https://cran.r-project.org/web/packages/survey/index.html 440.6 tidytext https://cran.r-project.org/web/packages/tidytext/index.html https://cran.r-project.org/web/packages/tidytext/vignettes/tf_idf.html https://cran.r-project.org/web/packages/tidytext/vignettes/tidying_casting.html https://cran.r-project.org/web/packages/tidytext/vignettes/tidytext.html https://cran.r-project.org/web/packages/tidytext/vignettes/topic_modeling.html 440.7 rapport an R templating system http://rapport-package.info/ 440.8 ztable https://github.com/cardiomoon/ztable 440.9 texreg https://cran.r-project.org/web/packages/texreg/vignettes/texreg.pdf 440.10 SortableHTMLTables https://cran.r-project.org/web/packages/SortableHTMLTables/index.html 440.11 hwriter https://cran.r-project.org/web/packages/hwriter/vignettes/hwriter.pdf 440.12 HTMLUtils https://cran.r-project.org/web/packages/HTMLUtils/index.html 440.13 htmlTable https://cran.r-project.org/web/packages/htmlTable/index.html 440.14 formattable https://cran.r-project.org/web/packages/formattable/index.html 440.15 DT DT: An R interface to the DataTables library https://rstudio.github.io/DT/ 440.16 stargazer Well-Formatted Regression and Summary Statistics Tables https://cran.r-project.org/web/packages/stargazer/index.html 440.17 conflicted 440.18 psycho {r, eval=FALSE, include=FALSE} install.packages( devtools ) library( devtools ) install_github( neuropsychology/psycho.R ) library( psycho ) 440.19 styler 440.20 tidyverse 440.21 splitstackshape The ‚Äúsplitstackshape‚Äù package for R https://www.r-bloggers.com/the-splitstackshape-package-for-r/ 440.22 epiR Basic Epi install.packages( epiR ) 440.23 lmtest Some useful epitools especially 2*2 tables and stratified MH Odds install.packages( lmtest ) 440.24 popEpi install.packages( popEpi ) 440.25 Extra tools to go with ‚ÄòEpi‚Äô and make nice rate tables 440.26 survival install.packages( survival ) for survial analysis, kaplan-meier plots and cox regression 440.27 survminer install.packages( survminer ) Really nice KM plots! 440.28 Random-Effects Modeks install.packages( lme4 ) install.packages( sjstats ) 440.29 Multiple Imputation install.packages( mice ) install.packages( arsenal ) 440.30 Really nice summmary tables 440.31 Complex survey data analysis install.packages( survey ) 440.32 For handling complex survey data install.packages( ICC ) 440.33 Alternative methods for ICC calculation from survey data 440.34 Meta-analysis install.packages( meta ) install.packages( metafor ) install.packages( workflowr ) install.packages( DiagrammeR ) install.packages( tangram ) devtools::install_github( lbusett/insert_table ) {r eval=FALSE, include=FALSE, echo=TRUE} #### Obtain names of all packages on CRAN names.available.packages &lt;- rownames(available.packages()) #### Extract packages names that contain Rcmdr Rcmdr.related.packages &lt;- names.available.packages[grep( Rcmdr , names.available.packages)] Rcmdr.related.packages #### Install these packages install.packages(pkgs = Rcmdr.related.packages) {r eval=FALSE, include=FALSE, echo=TRUE} usePackage &lt;- function(p) { if (!is.element(p, installed.packages()[, 1])) { install.packages(p, dep = TRUE) } require(p, character.only = TRUE) } {r eval=FALSE, include=FALSE, echo=TRUE} file.edit( ~/Desktop/foo/.Rprofile ) ## This opens up a script window, within which you can enter in your library commands library(ggplot2) library(scales) library(plyr) library(reshape2) 440.35 abind 440.36 acepack 440.37 AlgDesign 440.38 AnnotationDbi 440.39 aplpack 440.40 arm 440.41 assertthat 440.42 backports 440.43 base 440.44 base64enc 440.45 BCA 440.46 BH 440.47 BiasedUrn 440.48 bindr 440.49 bindrcpp 440.50 Biobase 440.51 BiocGenerics 440.52 BiocInstaller 440.53 bit 440.54 bit64 440.55 bitops 440.56 blob 440.57 boot 440.58 broom 440.59 BsMD 440.60 ca 440.61 car 440.62 caTools 440.63 cellranger 440.64 checkmate 440.65 class 440.66 cluster 440.67 clv 440.68 coda 440.69 codetools 440.70 coin 440.71 colorspace 440.72 combinat 440.73 compiler 440.74 conf.design 440.75 curl 440.76 data.table 440.77 datasets 440.78 date 440.79 DBI 440.80 deldir 440.81 depthTools 440.82 DiceDesign 440.83 dichromat 440.84 digest 440.85 DoE.base 440.86 DoE.wrapper 440.87 doParallel 440.88 dplyr 440.89 e1071 440.90 effects 440.91 ENmisc 440.92 epiR 440.93 estimability 440.94 evaluate 440.95 ez 440.96 flexclust 440.97 forcats 440.98 foreach 440.99 foreign 440.100 formatR 440.101 Formula 440.102 fracdiff 440.103 FrF2 440.104 gdata 440.105 ggplot2 http://www.cookbook-r.com/Graphs/ 440.105.1 ggplot2 extensions http://www.ggplot2-exts.org/gallery/ http://corybrunson.github.io/ggalluvial/ http://www.sthda.com/english/rpkgs/survminer/ 440.106 ggthemes 440.107 ggvis 440.108 glue 440.109 goftest 440.110 graphics 440.111 grDevices 440.112 grid 440.113 gridBase 440.114 gridExtra 440.115 gtable 440.116 gtools 440.117 haven 440.118 highr 440.119 Hmisc 440.120 hms 440.121 htmlTable 440.122 htmltools 440.123 htmlwidgets 440.124 httpuv 440.125 httr 440.126 igraph 440.127 IRanges 440.128 irlba 440.129 iterators 440.130 jsonlite 440.131 KernSmooth 440.132 knitr 440.133 labeling 440.134 lattice 440.135 latticeExtra 440.136 lazyeval 440.137 leaps 440.138 lhs 440.139 lme4 440.140 lmtest 440.141 lsmeans 440.142 lubridate 440.143 magrittr 440.144 markdown 440.145 MASS 440.146 Matrix 440.147 matrixcalc 440.148 MatrixModels 440.149 memoise 440.150 methods 440.151 mgcv 440.152 mi 440.153 mime 440.154 minqa 440.155 mnormt 440.156 modelr 440.157 modeltools 440.158 multcomp 440.159 munsell 440.160 mvtnorm 440.161 nlme 440.162 nloptr 440.163 NLP 440.164 NMF 440.165 nnet 440.166 nortest 440.167 openssl 440.168 ordinal 440.169 orloca 440.170 orloca.es 440.171 parallel 440.172 pbkrtest 440.173 pkgconfig 440.174 pkgmaker 440.175 plogr 440.176 plyr 440.177 polyclip 440.178 psych 440.179 purrr https://colinfay.me/happy-dev-purrr/ 440.180 quadprog 440.181 quantmod 440.182 quantreg 440.183 R2HTML 440.184 R6 440.185 randtests 440.186 Rcmdr 440.187 RcmdrMisc 440.188 RcmdrPlugin.BCA 440.189 RcmdrPlugin.coin 440.190 RcmdrPlugin.depthTools 440.191 RcmdrPlugin.DoE 440.192 RcmdrPlugin.doex 440.193 RcmdrPlugin.EACSPIR 440.194 RcmdrPlugin.EBM 440.195 RcmdrPlugin.epack 440.196 RcmdrPlugin.EZR 440.197 RcmdrPlugin.KMggplot2 440.198 RcmdrPlugin.mosaic 440.199 RcmdrPlugin.MPAStats 440.200 RcmdrPlugin.orloca 440.201 RcmdrPlugin.plotByGroup 440.202 RcmdrPlugin.qual 440.203 RcmdrPlugin.SCDA 440.204 RcmdrPlugin.sampling Tools for sampling in Official Statistical Surveys https://cran.r-project.org/web/packages/RcmdrPlugin.sampling/index.html 440.205 RcmdrPlugin.seeg 440.206 RcmdrPlugin.SLC 440.207 RcmdrPlugin.SM 440.208 RcmdrPlugin.steepness 440.209 RcmdrPlugin.survival 440.210 RcmdrPlugin.TeachingDemos 440.211 RcmdrPlugin.temis 440.212 RcmdrPlugin.UCA 440.213 RColorBrewer 440.214 Rcpp 440.215 RcppArmadillo 440.216 RcppEigen 440.217 readr 440.218 readxl 440.219 registry 440.220 relimp 440.221 rematch 440.222 reshape 440.223 reshape2 440.224 rgl 440.225 RISmed 440.226 rJava 440.227 rlang 440.228 rmarkdown 440.229 RMySQL 440.230 rngtools 440.231 rpart 440.232 rpart.plot 440.233 rprojroot 440.234 rsm 440.235 rticles tufte https://github.com/rstudio/rticles https://www.coursera.org/learn/reproducible-templates-analysis/lecture/QaDy7/building-a-document-template-part-2 https://www.coursera.org/learn/reproducible-templates-analysis/supplement/GQz0G/lecture-prep-code-file https://github.com/rstudio/tufte 440.236 RSQLite 440.237 rvest 440.238 S4Vectors 440.239 sandwich 440.240 scales 440.241 scatterplot3d 440.242 SCMA 440.243 SCRT 440.244 SCVA 440.245 seeg 440.246 selectr 440.247 sem 440.248 sfsmisc 440.249 sgeostat 440.250 shiny 440.251 shinythemes 440.252 slam 440.253 SLC 440.254 sourcetools 440.255 SparseM 440.256 spatial 440.257 spatstat 440.258 spatstat.utils 440.259 splines 440.260 stats 440.261 stats4 440.262 steepness 440.263 stringi 440.264 stringr 440.265 survival 440.266 tcltk 440.267 tcltk2 440.268 TeachingDemos 440.269 tensor 440.270 TH.data 440.271 tibble 440.272 tidyr 440.273 tidyverse 440.274 timeDate 440.275 tkrplot 440.276 tm 440.277 tools 440.278 tseries 440.279 TTR 440.280 ucminf 440.281 utils 440.282 vcd 440.283 viridis 440.284 viridisLite 440.285 XLConnect 440.286 XLConnectJars 440.287 xml2 440.288 xtable 440.289 xts 440.290 yaml 440.291 zoo install.packages( stargazer ) install.packages( pander ) install.packages( tables ) install.packages( ascii ) install.packages( xtable ) devtools::install_github( hadley/dplyr ) install.packages( gapminder ) install.packages( alluvial ) vignette( databases , package = dplyr ) install.packages( robustbase ) install.packages( insuranceData ) install.packages( Lahman ) install.packages( tidyverse ) install.packages( qdap ) install.packages( openxlsx ) devtools::install_github( kassambara/r2excel ) install.packages( WriteXLS ) install.packages( XLConnect ) library(XLConnect) {r eval=FALSE, include=FALSE, echo=TRUE} if (!require(wordcloud)) { install.packages( wordcloud ) library(wordcloud) } {r eval=FALSE, include=FALSE, echo=TRUE} installed &lt;- as.data.frame(installed.packages()) write.csv(installed, installed_previously.csv ) installedPreviously &lt;- read.csv( installed_previously.csv ) baseR &lt;- as.data.frame(installed.packages()) toInstall &lt;- setdiff(installedPreviously, baseR) install.packages(toInstall) packagelist &lt;- installed.packages() class(packagelist) packagelist[, 1] 440.292 Rcmdr {r eval=FALSE, include=FALSE, echo=TRUE} install.packages( Rcmdr ) install.packages( RcmdrPlugin.BCA ) install.packages( RcmdrPlugin.coin ) install.packages( RcmdrPlugin.depthTools ) install.packages( RcmdrPlugin.doBy ) install.packages( RcmdrPlugin.DoE ) install.packages( RcmdrPlugin.doex ) install.packages( RcmdrPlugin.EACSPIR ) install.packages( RcmdrPlugin.EBM ) install.packages( RcmdrPlugin.epack ) install.packages( RcmdrPlugin.EZR ) install.packages( RcmdrPlugin.HH ) install.packages( RcmdrPlugin.IPSUR ) install.packages( RcmdrPlugin.KMggplot2 ) install.packages( RcmdrPlugin.mosaic ) install.packages( RcmdrPlugin.MPAStats ) install.packages( RcmdrPlugin.orloca ) install.packages( RcmdrPlugin.plotByGroup ) install.packages( RcmdrPlugin.qcc ) install.packages( RcmdrPlugin.qual ) install.packages( RcmdrPlugin.SCDA ) install.packages( RcmdrPlugin.seeg ) install.packages( RcmdrPlugin.SLC ) install.packages( RcmdrPlugin.SM ) install.packages( RcmdrPlugin.StatisticalURV ) install.packages( RcmdrPlugin.steepness ) install.packages( RcmdrPlugin.survival ) install.packages( RcmdrPlugin.TeachingDemos ) install.packages( RcmdrPlugin.temis ) install.packages( RcmdrPlugin.UCA ) {r eval=FALSE, include=FALSE, echo=TRUE} source( https://bioconductor.org/biocLite.R ) biocLite() install.packages( tidyverse ) install.packages( devtools ) devtools::install_github( hadley/colformat ) devtools::install_github( ropenscilabs/skimr ) install.packages( hflights ) install.packages( dbplyr ) install.packages( xlsx ) require(devtools) install_github( ProjectMOSAIC/mosaic ) install.packages( jmv ) install.packages( wordcloud ) install.packages( https://cran.r-project.org/src/contrib/Archive/tm/tm_0.6.tar.gz , repos = NULL) install.packages( SnowballC ) install.packages( RColorBrewer ) install.packages( tidytext ) install.packages( packrat ) install.packages( testthat ) install.packages( prettydoc ) install.packages( rmdformats ) install.packages(c( fivethirtyeight , tidyverse , knitr , kableExtra , ggthemes )) library(devtools) install_github( SPSStoR , username = lebebr01 ) library(SPSStoR) {r eval=FALSE, include=FALSE, echo=TRUE} ip &lt;- installed.packages() pkgs.to.remove &lt;- ip[!(ip[, Priority ] %in% c( base , recommended )), 1] sapply(pkgs.to.remove, remove.packages) sapply(pkgs.to.remove, install.packages) {r eval=FALSE, include=FALSE, echo=TRUE} install.packages( devtools ) devtools::install_github( AndreaCirilloAC/updateR ) library(updateR) updateR(admin_password = ) ## Where PASSWORD stands for your system password survival survminer install.packages( tutorial ) devtools::install_github( romainfrancois/highlight ) install.packages( highr , repos = http://rforge.net , type = source ) install.packages( xplain ) {r remove all user installed packages in R, eval=FALSE, include=FALSE} ## How to remove all user installed packages in R ## https://www.r-bloggers.com/how-to-remove-all-user-installed-packages-in-r/ ## create a list of all installed packages ip &lt;- as.data.frame(installed.packages()) head(ip) ## if you use MRO, make sure that no packages in this library will be removed ip &lt;- subset(ip, !grepl( MRO , ip$LibPath)) ## we don&#39;t want to remove base or recommended packages either\\ ip &lt;- ip[!(ip[, Priority ] %in% c( base , recommended )),] ## determine the library where the packages are installed path.lib &lt;- unique(ip$LibPath) ## create a vector with all the names of the packages you want to remove pkgs.to.remove &lt;- ip[,1] head(pkgs.to.remove) ## remove the packages sapply(pkgs.to.remove, remove.packages, lib = path.lib) 440.293 Schedule R Script library(knitr) library(markdown) knit( SchedulePubMedAnalysis.Rmd ) markdownToHTML( SchedulePubMedAnalysis.md , docs/SchedulePubMedAnalysis.html ) setwd( ~ ) file.choose() library(rmarkdown) Preperation Packages Schedule R Script https://github.com/jkeirstead/scholar http://rogiersbart.blogspot.com.tr/2015/05/put-google-scholar-citations-on-your.html http://tuxette.nathalievilla.org/?p=1682 https://github.com/sunitj/scholar https://tgmstat.wordpress.com/2013/09/11/schedule-rscript-with-cron/ https://github.com/ropensci/git2r library(knitr) library(markdown) install.packages(‚Äòdata.table‚Äô) install.packages(‚Äòknitr‚Äô) install.packages(‚ÄòminiUI‚Äô) install.packages(‚Äòshiny‚Äô) install.packages(‚ÄòshinyFiles‚Äô) install.packages( taskscheduleR , repos = http://www.datatailor.be/rcube , type = source ) devtools::install_github( bnosac/cronR ) install.packages(‚Äòscholar‚Äô) install.packages( git2r ) 440.294 flatxml http://www.zuckarelli.de/flatxml/articles/example/example.html {r eval=FALSE, include=FALSE, echo=TRUE} xmldata &lt;- flatxml::fxml_importXMLFlat( data/PathologyTurkey.xml ) 440.295 roomba General purpose API response tidier https://github.com/ropenscilabs/roomba/ {r eval=FALSE, include=FALSE, echo=TRUE} roomba::shiny_roomba() 440.296 tufterhandout Tufte-style html document format for rmarkdown https://cran.r-project.org/web/packages/tufterhandout/index.html {r eval=FALSE, include=FALSE, echo=TRUE} ## install.packages( tufterhandout ) library(tufterhandout) 440.297 papeR https://sbalci.github.io/MyRCodesForDataAnalysis/papeR.nb.html 440.298 tm install.packages( tm ) https://cran.r-project.org/web/packages/tm/vignettes/tm.pdf https://atom.io/packages/hydrogen R Package Documentation https://rdrr.io/ R Packages Search and Statistics https://www.rpackages.io/ ReporteRs is an R package for creating Microsoft Word and Powerpoint documents. https://davidgohel.github.io/ReporteRs/ officer Manipulation of Microsoft Word and PowerPoint Documents https://cran.r-project.org/web/packages/officer/ pubh https://cran.rstudio.com/web/packages/pubh/vignettes/introduction.html What does visdat do? Initially inspired by csv-fingerprint, vis_dat helps you visualise a dataframe and ‚Äúget a look at the data‚Äù by displaying the variable classes in a dataframe as a plot with vis_dat, and getting a brief look into missing data patterns using vis_miss. https://www.tidyverse.org/ https://cran.r-project.org/web/packages/shiny/index.html https://rmarkdown.rstudio.com/ http://ggplot2.org/ https://yihui.name/knitr/ http://vita.had.co.nz/papers/tidy-data.html https://blog.rstudio.com/2015/04/09/readr-0-1-0/ http://readxl.tidyverse.org/ https://cran.r-project.org/web/packages/lubridate/vignettes/lubridate.html https://github.com/r-lib/devtools https://cran.r-project.org/web/packages/magrittr/index.html http://rstudio.github.io/packrat/ https://cran.r-project.org/web/packages/stringr/vignettes/stringr.html https://github.com/tidyverse/dplyr https://cran.r-project.org/web/packages/broom/vignettes/broom.html https://cran.r-project.org/web/packages/roxygen2/vignettes/roxygen2.html http://tibble.tidyverse.org/ https://blog.rstudio.com/2016/03/29/feather/ https://blog.rstudio.com/2016/08/31/forcats-0-1-0/ https://github.com/tidyverse/hms https://github.com/trestletech/plumber https://github.com/r-lib/testthat http://purrr.tidyverse.org/ "],["broom-1.html", "Chapter 441 broom 441.1 abind 441.2 acepack 441.3 addinexamples 441.4 additivityTests 441.5 ade4 441.6 AER 441.7 afex 441.8 AlgDesign 441.9 alluvial 441.10 alr4 441.11 animation 441.12 ANTsR 441.13 ANTsRCore 441.14 anytime 441.15 apaTables 441.16 aplpack 441.17 arm 441.18 arsenal 441.19 ash 441.20 assertthat 441.21 aws.s3 441.22 aws.signature 441.23 backports 441.24 base 441.25 base64enc 441.26 BayesFactor 441.27 bayesplot 441.28 BCA 441.29 BcDiag 441.30 BDgraph 441.31 bdsmatrix 441.32 BH 441.33 BiasedUrn 441.34 BiBitR 441.35 bibtex 441.36 biclust 441.37 bindr 441.38 bindrcpp 441.39 BiocGenerics 441.40 BiocInstaller 441.41 biostatmethods 441.42 bit 441.43 bit64 441.44 bitops 441.45 blob 441.46 blogdown 441.47 bookdown 441.48 boot 441.49 brew 441.50 bridgesampling 441.51 brms 441.52 Brobdingnag 441.53 broom 441.54 BsMD 441.55 ca 441.56 cairoDevice 441.57 callr 441.58 car 441.59 carData 441.60 carbonate 441.61 caTools 441.62 cellranger 441.63 checkmate 441.64 class 441.65 classInt 441.66 cli 441.67 clipr 441.68 clisymbols 441.69 clue 441.70 cluster 441.71 clv 441.72 cmaker 441.73 cmprsk 441.74 coauthornetwork 441.75 cobs 441.76 coda 441.77 CodeDepends 441.78 codetools 441.79 coin 441.80 colorspace 441.81 colourpicker 441.82 combinat 441.83 commonmark 441.84 compiler 441.85 conf.design 441.86 config 441.87 conflicted 441.88 contfrac 441.89 corpcor 441.90 covr 441.91 cowplot 441.92 coxme 441.93 cramer 441.94 crayon 441.95 cronR 441.96 crosstalk 441.97 csvy 441.98 curl 441.99 d3Network 441.100 data.table 441.101 data.tree 441.102 datapasta 441.103 datasets 441.104 date 441.105 DBI 441.106 dbplyr 441.107 debugme 441.108 Deducer 441.109 DeducerExtras 441.110 demography 441.111 dendextend 441.112 DEoptimR 441.113 depthTools 441.114 desc 441.115 descr1 441.116 DescTools 441.117 deSolve 441.118 devtools 441.119 DiagrammeR 441.120 DiceDesign 441.121 dichromat 441.122 digest 441.123 diptest 441.124 DoE.base 441.125 DoE.wrapper 441.126 doParallel 441.127 downloader 441.128 dplyr 441.129 DT 441.130 dtplyr 441.131 dunn.test 441.132 dygraphs 441.133 e1071 441.134 EcoVirtual 441.135 effects 441.136 effsize 441.137 ellipse 441.138 elliptic 441.139 emmeans 441.140 enc 441.141 ENmisc 441.142 Epi 441.143 epibasix 441.144 epiDisplay 441.145 epiR 441.146 epitools 441.147 errorist 441.148 estimability 441.149 etm 441.150 evaluate 441.151 exact2x2 441.152 exactci 441.153 exactRankTests 441.154 experiment 441.155 expm 441.156 extrantsr 441.157 ez 441.158 FactoMineR 441.159 Fahrmeir 441.160 fansi 441.161 fastmatch 441.162 fBasics 441.163 fda 441.164 fdrtool 441.165 feather 441.166 ff 441.167 ffbase 441.168 finalfit 441.169 fit.models 441.170 fivethirtyeight 441.171 flashClust 441.172 flatxml 441.173 flexclust 441.174 flexmix 441.175 flextable 441.176 FNN 441.177 forcats 441.178 foreach 441.179 forecast 441.180 forecastHybrid 441.181 foreign 441.182 formatR 441.183 formattable 441.184 Formula 441.185 fpc 441.186 fracdiff 441.187 FrF2 441.188 fslr 441.189 fst 441.190 ftsa 441.191 gdata 441.192 gdtools 441.193 gee 441.194 getPass 441.195 GGally 441.196 ggcorrplot 441.197 ggdendro 441.198 ggExtra 441.199 ggforce 441.200 ggformula 441.201 ggfortify 441.202 ggm 441.203 ggplot2 441.204 ggplot2movies 441.205 ggpubr 441.206 ggraph 441.207 ggrepel 441.208 ggridges 441.209 ggsci 441.210 ggsignif 441.211 ggstance 441.212 ggstatsplot 441.213 ggthemes 441.214 ggvis 441.215 gh 441.216 git2r 441.217 glasso 441.218 glmmTMB 441.219 glue 441.220 gmailr 441.221 gmodels 441.222 gmp 441.223 gnm 441.224 googledrive 441.225 markdrive 441.226 googleVis 441.227 GPArotation 441.228 gplots 441.229 graph 441.230 graphics 441.231 graphTweets 441.232 grDevices 441.233 grid 441.234 gridExtra 441.235 gsl 441.236 gss 441.237 gtable 441.238 gtools 441.239 gWidgets 441.240 gWidgetstcltk 441.241 GWRM 441.242 h2o 441.243 hash 441.244 haven 441.245 hdrcde 441.246 hexbin 441.247 HH 441.248 highlight 441.249 highr 441.250 Hmisc 441.251 hms 441.252 htmlTable 441.253 htmltools 441.254 htmlwidgets 441.255 httpuv 441.256 httr 441.257 huge 441.258 hunspell 441.259 hypergeo 441.260 ICC 441.261 IRdisplay 441.262 IRkernel 441.263 ISLR 441.264 ISwR 441.265 ITKR 441.266 igraph 441.267 influenceR 441.268 ini 441.269 inline 441.270 ipred 441.271 irlba 441.272 irr 441.273 iterators 441.274 JavaGD 441.275 JGR 441.276 jmv 441.277 jmvcore 441.278 jmvconnect 441.279 jomo 441.280 jose 441.281 jpeg 441.282 jsonlite 441.283 kableExtra 441.284 kernlab 441.285 KernSmooth 441.286 klaR 441.287 km.ci 441.288 KMsurv 441.289 knitr 441.290 kohonen 441.291 ks 441.292 labeling 441.293 labelled 441.294 Lahman 441.295 later 441.296 lattice 441.297 latticeExtra 441.298 lava 441.299 lavaan 441.300 lazyeval 441.301 leaps 441.302 lfstat 441.303 lhs 441.304 lintr 441.305 lisrelToR 441.306 lme4 441.307 lmerTest 441.308 lmom 441.309 lmomRFA 441.310 lmtest 441.311 locfit 441.312 loo 441.313 lpSolve 441.314 lubridate 441.315 MAd 441.316 magic 441.317 magicfor 441.318 magrittr 441.319 manipulate 441.320 manipulateWidget 441.321 mapproj 441.322 maps 441.323 maptools 441.324 markdown 441.325 MASS 441.326 Matrix 441.327 matrixcalc 441.328 MatrixModels 441.329 matrixStats 441.330 maxstat 441.331 MBESS 441.332 mc2d 441.333 mclust 441.334 memisc 441.335 memoise 441.336 meta 441.337 metafor 441.338 methods 441.339 mgcv 441.340 mi 441.341 mice 441.342 microbenchmark 441.343 mime 441.344 miniUI 441.345 minpack.lm 441.346 minqa 441.347 misc3d 441.348 mitml 441.349 mixlm 441.350 mixOmics 441.351 mlbench 441.352 mnormt 441.353 modelr 441.354 modeltools 441.355 mosaic 441.356 mosaicCore 441.357 mosaicData 441.358 multcomp 441.359 multcompView 441.360 multicool 441.361 MuMIn 441.362 munsell 441.363 mvnormtest 441.364 mvtnorm 441.365 network 441.366 networkD3 441.367 neurobase 441.368 neuroim 441.369 nFactors 441.370 nleqslv 441.371 nlme 441.372 nloptr 441.373 NLP 441.374 nnet 441.375 nortest 441.376 nparLD 441.377 numbers 441.378 numDeriv 441.379 nycflights13 441.380 OceanView 441.381 officer 441.382 OpenMx 441.383 openssl 441.384 openxlsx 441.385 OptimClassifier 441.386 ordinal 441.387 orloca 441.388 orloca.es 441.389 oro.dicom 441.390 oro.nifti 441.391 packagefinder 441.392 packrat 441.393 pacman 441.394 pan 441.395 pander 441.396 papeR 441.397 parallel 441.398 party 441.399 pbapply 441.400 pbdZMQ 441.401 pbivnorm 441.402 pbkrtest 441.403 pcaPP 441.404 permute 441.405 phia 441.406 pillar 441.407 pkgbuild 441.408 pkgconfig 441.409 pkgload 441.410 PKI 441.411 plogr 441.412 plot3D 441.413 plot3Drgl 441.414 plotly 441.415 pls 441.416 plyr 441.417 PMCMR 441.418 png 441.419 popEpi 441.420 ppcor 441.421 prabclus 441.422 pracma 441.423 praise 441.424 prediction 441.425 prettydoc 441.426 prettyR 441.427 prettyunits 441.428 pROC 441.429 processx 441.430 prodlim 441.431 progress 441.432 promises 441.433 pspearman 441.434 psych 441.435 psycho 441.436 pubh 441.437 purrr 441.438 purrrlyr 441.439 pwr 441.440 qgraph 441.441 quadprog 441.442 quantmod 441.443 quantreg 441.444 questionr 441.445 qvcalc 441.446 R.cache 441.447 R.matlab 441.448 R.methodsS3 441.449 R.oo 441.450 R.utils 441.451 R2HTML 441.452 R6 441.453 rainbow 441.454 rainfreq 441.455 randomcoloR 441.456 randtests 441.457 ranger 441.458 RApiDatetime 441.459 rappdirs 441.460 rARPACK 441.461 raster 441.462 rattle 441.463 rCharts 441.464 rcmdcheck 441.465 Rcmdr 441.466 RcmdrMisc 441.467 RcmdrPlugin.aRnova 441.468 RcmdrPlugin.BCA 441.469 RcmdrPlugin.BiclustGUI 441.470 RcmdrPlugin.coin 441.471 RcmdrPlugin.depthTools 441.472 RcmdrPlugin.DoE 441.473 RcmdrPlugin.doex 441.474 RcmdrPlugin.EACSPIR 441.475 RcmdrPlugin.EBM 441.476 RcmdrPlugin.EcoVirtual 441.477 RcmdrPlugin.epack 441.478 RcmdrPlugin.Export 441.479 RcmdrPlugin.EZR 441.480 RcmdrPlugin.FactoMineR 441.481 RcmdrPlugin.FuzzyClust 441.482 RcmdrPlugin.GWRM 441.483 RcmdrPlugin.HH 441.484 RcmdrPlugin.IPSUR 441.485 RcmdrPlugin.KMggplot2 441.486 RcmdrPlugin.lfstat 441.487 RcmdrPlugin.MA 441.488 RcmdrPlugin.mosaic 441.489 RcmdrPlugin.MPAStats 441.490 RcmdrPlugin.NMBU 441.491 RcmdrPlugin.OptimClassifier 441.492 RcmdrPlugin.orloca 441.493 RcmdrPlugin.PcaRobust 441.494 RcmdrPlugin.plotByGroup 441.495 RcmdrPlugin.pointG 441.496 RcmdrPlugin.qual 441.497 RcmdrPlugin.RiskDemo 441.498 RcmdrPlugin.RMTCJags 441.499 RcmdrPlugin.ROC 441.500 RcmdrPlugin.sampling 441.501 RcmdrPlugin.SCDA 441.502 RcmdrPlugin.SLC 441.503 RcmdrPlugin.SM 441.504 RcmdrPlugin.sos 441.505 RcmdrPlugin.steepness 441.506 RcmdrPlugin.survival 441.507 RcmdrPlugin.sutteForecastR 441.508 RcmdrPlugin.TeachingDemos 441.509 RcmdrPlugin.temis 441.510 RcmdrPlugin.UCA 441.511 RColorBrewer 441.512 Rcpp 441.513 RcppArmadillo 441.514 RcppEigen 441.515 RCurl 441.516 readODS 441.517 readr 441.518 readstata13 441.519 readxl 441.520 RefManageR 441.521 relimp 441.522 rematch 441.523 rematch2 441.524 remotes 441.525 repr 441.526 reprex 441.527 reshape 441.528 reshape2 441.529 ResourceSelection 441.530 rex 441.531 rgexf 441.532 rgl 441.533 RGtk2 441.534 rhandsontable 441.535 RISmed 441.536 rio 441.537 rjags 441.538 rJava 441.539 rjson 441.540 RJSONIO 441.541 rlang 441.542 rmarkdown 441.543 rmatio 441.544 rmdformats 441.545 rmeta 441.546 Rmpfr 441.547 RMySQL 441.548 RNifti 441.549 robets 441.550 robust 441.551 robustbase 441.552 rockchalk 441.553 ROCR 441.554 Rook 441.555 roomba 441.556 roxygen2 441.557 rpart 441.558 rpart.plot 441.559 rpf 441.560 RPostgreSQL 441.561 rprojroot 441.562 rrcov 441.563 rsconnect 441.564 rsm 441.565 RSpectra 441.566 RSQLite 441.567 rstan 441.568 rstanarm 441.569 rstantools 441.570 rstudioapi 441.571 RStudioConsoleRender 441.572 rsvd 441.573 Rtsne 441.574 rtweet 441.575 runjags 441.576 RVAideMemoire 441.577 rversions 441.578 rvest 441.579 rvg 441.580 s4vd 441.581 sampling 441.582 sandwich 441.583 scales 441.584 scatr 441.585 scatterplot3d 441.586 scholar 441.587 SCMA 441.588 SCRT 441.589 SCVA 441.590 sde 441.591 SDMTools 441.592 selectr 441.593 sem 441.594 SemiCompRisks 441.595 semPlot 441.596 semTools 441.597 sendmailR 441.598 servr 441.599 sessioninfo 441.600 sf 441.601 sfsmisc 441.602 shape 441.603 shiny 441.604 shinyFiles 441.605 shinyjs 441.606 shinystan 441.607 shinythemes 441.608 sjlabelled 441.609 sjmisc 441.610 sjstats 441.611 skimr 441.612 slam 441.613 SLC 441.614 Sleuth2 441.615 smcure 441.616 sna 441.617 snakecase 441.618 som 441.619 sos 441.620 sourcetools 441.621 sp 441.622 sparklyr 441.623 SparseM 441.624 spatial 441.625 spData 441.626 splines 441.627 splitstackshape 441.628 SQUAREM 441.629 ssanv 441.630 ssgraph 441.631 stabledist 441.632 StanHeaders 441.633 stapler 441.634 stargazer 441.635 statisticalModeling 441.636 statnet.common 441.637 stats 441.638 stats4 441.639 steepness 441.640 stencila 441.641 stringdist 441.642 stringi 441.643 stringr 441.644 strucchange 441.645 styler 441.646 superbiclust 441.647 SuppDists 441.648 survey 441.649 survival 441.650 survminer 441.651 survMisc 441.652 sutteForecastR 441.653 svglite 441.654 tableone 441.655 tables 441.656 tabplot 441.657 tangram 441.658 tcltk 441.659 tcltk2 441.660 TeachingDemos 441.661 testit 441.662 testthat 441.663 TH.data 441.664 threejs 441.665 tibble 441.666 tidybayes 441.667 tidygraph 441.668 tidyr 441.669 tidyselect 441.670 tidyverse 441.671 timeDate 441.672 timeSeries 441.673 tinytex 441.674 tkrplot 441.675 tm 441.676 TMB 441.677 tools 441.678 tree 441.679 triebeard 441.680 trimcluster 441.681 tseries 441.682 TTR 441.683 tufterhandout 441.684 tutorial 441.685 tweenr 441.686 ucminf 441.687 units 441.688 updateR 441.689 urca 441.690 urltools 441.691 uroot 441.692 userfriendlyscience 441.693 usethis 441.694 utf8 441.695 utils 441.696 uuid 441.697 V8 441.698 vcd 441.699 vcdExtra 441.700 vegan 441.701 VGAM 441.702 ViewPipeSteps 441.703 viridis 441.704 viridisLite 441.705 visdat 441.706 visNetwork 441.707 visreg 441.708 wbstats 441.709 webshot 441.710 whisker 441.711 WhiteStripe 441.712 withr 441.713 wordcloud 441.714 workflowr 441.715 writexl 441.716 WRS2 441.717 xfun 441.718 XLConnect 441.719 XLConnectJars 441.720 xlsx 441.721 xlsxjars 441.722 XML 441.723 xml2 441.724 xplain 441.725 xray 441.726 xtable 441.727 xts 441.728 yaImpute 441.729 yaml 441.730 zip 441.731 zoo 441.732 Effsize 441.733 keyring 441.734 testthat 441.735 cronR", " Chapter 441 broom summarizes key information about models https://broom.tidyverse.org/index.html 441.1 abind 441.2 acepack 441.3 addinexamples 441.4 additivityTests 441.5 ade4 441.6 AER 441.7 afex 441.8 AlgDesign 441.9 alluvial 441.10 alr4 441.11 animation 441.12 ANTsR 441.13 ANTsRCore 441.14 anytime 441.15 apaTables 441.16 aplpack 441.17 arm 441.18 arsenal 441.19 ash 441.20 assertthat 441.21 aws.s3 441.22 aws.signature 441.23 backports 441.24 base 441.25 base64enc 441.26 BayesFactor 441.27 bayesplot 441.28 BCA 441.29 BcDiag 441.30 BDgraph 441.31 bdsmatrix 441.32 BH 441.33 BiasedUrn 441.34 BiBitR 441.35 bibtex 441.36 biclust 441.37 bindr 441.38 bindrcpp 441.39 BiocGenerics 441.40 BiocInstaller 441.41 biostatmethods 441.42 bit 441.43 bit64 441.44 bitops 441.45 blob 441.46 blogdown 441.47 bookdown 441.48 boot 441.49 brew 441.50 bridgesampling 441.51 brms 441.52 Brobdingnag 441.53 broom 441.54 BsMD 441.55 ca 441.56 cairoDevice 441.57 callr 441.58 car 441.59 carData 441.60 carbonate https://yonicd.github.io/carbonate/ https://carbon.now.sh/ 441.61 caTools 441.62 cellranger 441.63 checkmate 441.64 class 441.65 classInt 441.66 cli 441.67 clipr 441.68 clisymbols 441.69 clue 441.70 cluster 441.71 clv 441.72 cmaker 441.73 cmprsk 441.74 coauthornetwork 441.75 cobs 441.76 coda 441.77 CodeDepends 441.78 codetools 441.79 coin 441.80 colorspace 441.81 colourpicker 441.82 combinat 441.83 commonmark 441.84 compiler 441.85 conf.design 441.86 config 441.87 conflicted 441.88 contfrac 441.89 corpcor 441.90 covr 441.91 cowplot 441.92 coxme 441.93 cramer 441.94 crayon 441.95 cronR 441.96 crosstalk 441.97 csvy 441.98 curl 441.99 d3Network 441.100 data.table 441.101 data.tree 441.102 datapasta https://github.com/MilesMcBain/datapasta 441.103 datasets 441.104 date 441.105 DBI 441.106 dbplyr 441.107 debugme 441.108 Deducer 441.109 DeducerExtras 441.110 demography 441.111 dendextend 441.112 DEoptimR 441.113 depthTools 441.114 desc 441.115 descr1 441.116 DescTools 441.117 deSolve 441.118 devtools 441.119 DiagrammeR 441.120 DiceDesign 441.121 dichromat 441.122 digest 441.123 diptest 441.124 DoE.base 441.125 DoE.wrapper 441.126 doParallel 441.127 downloader 441.128 dplyr 441.129 DT 441.130 dtplyr 441.131 dunn.test 441.132 dygraphs 441.133 e1071 441.134 EcoVirtual 441.135 effects 441.136 effsize 441.137 ellipse 441.138 elliptic 441.139 emmeans 441.140 enc 441.141 ENmisc 441.142 Epi 441.143 epibasix 441.144 epiDisplay 441.145 epiR 441.146 epitools 441.147 errorist Automatic Error and Warning Search https://github.com/coatless/errorist 441.148 estimability 441.149 etm 441.150 evaluate 441.151 exact2x2 441.152 exactci 441.153 exactRankTests 441.154 experiment 441.155 expm 441.156 extrantsr 441.157 ez 441.158 FactoMineR 441.159 Fahrmeir 441.160 fansi 441.161 fastmatch 441.162 fBasics 441.163 fda 441.164 fdrtool 441.165 feather 441.166 ff 441.167 ffbase 441.168 finalfit 441.169 fit.models 441.170 fivethirtyeight 441.171 flashClust 441.172 flatxml 441.173 flexclust 441.174 flexmix 441.175 flextable 441.176 FNN 441.177 forcats 441.178 foreach 441.179 forecast 441.180 forecastHybrid 441.181 foreign 441.182 formatR 441.183 formattable 441.184 Formula 441.185 fpc 441.186 fracdiff 441.187 FrF2 441.188 fslr 441.189 fst 441.190 ftsa 441.191 gdata 441.192 gdtools 441.193 gee 441.194 getPass 441.195 GGally 441.196 ggcorrplot 441.197 ggdendro 441.198 ggExtra 441.199 ggforce 441.200 ggformula 441.201 ggfortify 441.202 ggm 441.203 ggplot2 441.204 ggplot2movies 441.205 ggpubr 441.206 ggraph 441.207 ggrepel 441.208 ggridges 441.209 ggsci 441.210 ggsignif 441.211 ggstance 441.212 ggstatsplot 441.213 ggthemes 441.214 ggvis 441.215 gh 441.216 git2r 441.217 glasso 441.218 glmmTMB 441.219 glue 441.220 gmailr 441.221 gmodels 441.222 gmp 441.223 gnm 441.224 googledrive https://googledrive.tidyverse.org/ 441.225 markdrive https://github.com/MilesMcBain/markdrive 441.226 googleVis 441.227 GPArotation 441.228 gplots 441.229 graph 441.230 graphics 441.231 graphTweets 441.232 grDevices 441.233 grid 441.234 gridExtra 441.235 gsl 441.236 gss 441.237 gtable 441.238 gtools 441.239 gWidgets 441.240 gWidgetstcltk 441.241 GWRM 441.242 h2o 441.243 hash 441.244 haven 441.245 hdrcde 441.246 hexbin 441.247 HH 441.248 highlight 441.249 highr 441.250 Hmisc 441.251 hms 441.252 htmlTable 441.253 htmltools 441.254 htmlwidgets 441.255 httpuv 441.256 httr 441.257 huge 441.258 hunspell 441.259 hypergeo 441.260 ICC 441.261 IRdisplay 441.262 IRkernel 441.263 ISLR 441.264 ISwR 441.265 ITKR 441.266 igraph 441.267 influenceR 441.268 ini 441.269 inline 441.270 ipred 441.271 irlba 441.272 irr 441.273 iterators 441.274 JavaGD 441.275 JGR 441.276 jmv 441.277 jmvcore 441.278 jmvconnect https://cran.r-project.org/package=jmvconnect 441.279 jomo 441.280 jose 441.281 jpeg 441.282 jsonlite 441.283 kableExtra 441.284 kernlab 441.285 KernSmooth 441.286 klaR 441.287 km.ci 441.288 KMsurv 441.289 knitr 441.290 kohonen 441.291 ks 441.292 labeling 441.293 labelled 441.294 Lahman 441.295 later 441.296 lattice 441.297 latticeExtra 441.298 lava 441.299 lavaan 441.300 lazyeval 441.301 leaps 441.302 lfstat 441.303 lhs 441.304 lintr 441.305 lisrelToR 441.306 lme4 441.307 lmerTest 441.308 lmom 441.309 lmomRFA 441.310 lmtest 441.311 locfit 441.312 loo 441.313 lpSolve 441.314 lubridate 441.315 MAd 441.316 magic 441.317 magicfor https://cran.r-project.org/web/packages/magicfor/vignettes/magicfor.html {r eval=FALSE, include=FALSE, echo=TRUE} magicfor::magic_for(silent = TRUE) for (i in 1:3) { squared &lt;- i ^ 2 cubed &lt;- i ^ 3 put(squared, cubed) } magicfor::magic_result_as_dataframe() 441.318 magrittr 441.319 manipulate 441.320 manipulateWidget 441.321 mapproj 441.322 maps 441.323 maptools 441.324 markdown 441.325 MASS 441.326 Matrix 441.327 matrixcalc 441.328 MatrixModels 441.329 matrixStats 441.330 maxstat 441.331 MBESS 441.332 mc2d 441.333 mclust 441.334 memisc 441.335 memoise 441.336 meta 441.337 metafor 441.338 methods 441.339 mgcv 441.340 mi 441.341 mice http://www.stefvanbuuren.nl/mi/FIMD.html Flexible Imputation of Missing Data https://stefvanbuuren.name/fimd/ 441.342 microbenchmark 441.343 mime 441.344 miniUI 441.345 minpack.lm 441.346 minqa 441.347 misc3d 441.348 mitml 441.349 mixlm 441.350 mixOmics 441.351 mlbench 441.352 mnormt 441.353 modelr 441.354 modeltools 441.355 mosaic https://github.com/ProjectMOSAIC/mosaic/blob/master/vignettes/mosaic-resources.Rmd http://mosaic-web.org/ 441.356 mosaicCore 441.357 mosaicData 441.358 multcomp 441.359 multcompView 441.360 multicool 441.361 MuMIn 441.362 munsell 441.363 mvnormtest 441.364 mvtnorm 441.365 network 441.366 networkD3 441.367 neurobase 441.368 neuroim 441.369 nFactors 441.370 nleqslv 441.371 nlme 441.372 nloptr 441.373 NLP 441.374 nnet 441.375 nortest 441.376 nparLD 441.377 numbers 441.378 numDeriv 441.379 nycflights13 441.380 OceanView 441.381 officer 441.382 OpenMx 441.383 openssl 441.384 openxlsx 441.385 OptimClassifier 441.386 ordinal 441.387 orloca 441.388 orloca.es 441.389 oro.dicom 441.390 oro.nifti 441.391 packagefinder 441.392 packrat 441.393 pacman 441.394 pan 441.395 pander 441.396 papeR 441.397 parallel 441.398 party 441.399 pbapply 441.400 pbdZMQ 441.401 pbivnorm 441.402 pbkrtest 441.403 pcaPP 441.404 permute 441.405 phia 441.406 pillar 441.407 pkgbuild 441.408 pkgconfig 441.409 pkgload 441.410 PKI 441.411 plogr 441.412 plot3D 441.413 plot3Drgl 441.414 plotly 441.415 pls 441.416 plyr 441.417 PMCMR 441.418 png 441.419 popEpi 441.420 ppcor 441.421 prabclus 441.422 pracma 441.423 praise 441.424 prediction 441.425 prettydoc 441.426 prettyR 441.427 prettyunits 441.428 pROC 441.429 processx 441.430 prodlim 441.431 progress 441.432 promises 441.433 pspearman 441.434 psych 441.435 psycho 441.436 pubh 441.437 purrr 441.438 purrrlyr 441.439 pwr 441.440 qgraph 441.441 quadprog 441.442 quantmod 441.443 quantreg 441.444 questionr 441.445 qvcalc 441.446 R.cache 441.447 R.matlab 441.448 R.methodsS3 441.449 R.oo 441.450 R.utils 441.451 R2HTML 441.452 R6 441.453 rainbow 441.454 rainfreq 441.455 randomcoloR 441.456 randtests 441.457 ranger 441.458 RApiDatetime 441.459 rappdirs 441.460 rARPACK 441.461 raster 441.462 rattle 441.463 rCharts 441.464 rcmdcheck 441.465 Rcmdr 441.466 RcmdrMisc 441.467 RcmdrPlugin.aRnova 441.468 RcmdrPlugin.BCA 441.469 RcmdrPlugin.BiclustGUI 441.470 RcmdrPlugin.coin 441.471 RcmdrPlugin.depthTools 441.472 RcmdrPlugin.DoE 441.473 RcmdrPlugin.doex 441.474 RcmdrPlugin.EACSPIR 441.475 RcmdrPlugin.EBM 441.476 RcmdrPlugin.EcoVirtual 441.477 RcmdrPlugin.epack 441.478 RcmdrPlugin.Export 441.479 RcmdrPlugin.EZR 441.480 RcmdrPlugin.FactoMineR 441.481 RcmdrPlugin.FuzzyClust 441.482 RcmdrPlugin.GWRM 441.483 RcmdrPlugin.HH 441.484 RcmdrPlugin.IPSUR 441.485 RcmdrPlugin.KMggplot2 441.486 RcmdrPlugin.lfstat 441.487 RcmdrPlugin.MA 441.488 RcmdrPlugin.mosaic 441.489 RcmdrPlugin.MPAStats 441.490 RcmdrPlugin.NMBU 441.491 RcmdrPlugin.OptimClassifier 441.492 RcmdrPlugin.orloca 441.493 RcmdrPlugin.PcaRobust 441.494 RcmdrPlugin.plotByGroup 441.495 RcmdrPlugin.pointG 441.496 RcmdrPlugin.qual 441.497 RcmdrPlugin.RiskDemo 441.498 RcmdrPlugin.RMTCJags 441.499 RcmdrPlugin.ROC 441.500 RcmdrPlugin.sampling 441.501 RcmdrPlugin.SCDA 441.502 RcmdrPlugin.SLC 441.503 RcmdrPlugin.SM 441.504 RcmdrPlugin.sos 441.505 RcmdrPlugin.steepness 441.506 RcmdrPlugin.survival 441.507 RcmdrPlugin.sutteForecastR 441.508 RcmdrPlugin.TeachingDemos 441.509 RcmdrPlugin.temis 441.510 RcmdrPlugin.UCA 441.511 RColorBrewer 441.512 Rcpp 441.513 RcppArmadillo 441.514 RcppEigen 441.515 RCurl 441.516 readODS 441.517 readr 441.518 readstata13 441.519 readxl 441.520 RefManageR 441.521 relimp 441.522 rematch 441.523 rematch2 441.524 remotes 441.525 repr 441.526 reprex 441.527 reshape 441.528 reshape2 441.529 ResourceSelection 441.530 rex 441.531 rgexf 441.532 rgl 441.533 RGtk2 441.534 rhandsontable 441.535 RISmed 441.536 rio 441.537 rjags 441.538 rJava 441.539 rjson 441.540 RJSONIO 441.541 rlang 441.542 rmarkdown 441.543 rmatio 441.544 rmdformats 441.545 rmeta 441.546 Rmpfr 441.547 RMySQL 441.548 RNifti 441.549 robets 441.550 robust 441.551 robustbase 441.552 rockchalk 441.553 ROCR 441.554 Rook 441.555 roomba 441.556 roxygen2 441.557 rpart 441.558 rpart.plot 441.559 rpf 441.560 RPostgreSQL 441.561 rprojroot 441.562 rrcov 441.563 rsconnect 441.564 rsm 441.565 RSpectra 441.566 RSQLite 441.567 rstan 441.568 rstanarm 441.569 rstantools 441.570 rstudioapi 441.571 RStudioConsoleRender 441.572 rsvd 441.573 Rtsne 441.574 rtweet 441.575 runjags 441.576 RVAideMemoire 441.577 rversions 441.578 rvest 441.579 rvg https://github.com/davidgohel/rvg 441.580 s4vd 441.581 sampling 441.582 sandwich 441.583 scales 441.584 scatr 441.585 scatterplot3d 441.586 scholar 441.587 SCMA 441.588 SCRT 441.589 SCVA 441.590 sde 441.591 SDMTools 441.592 selectr 441.593 sem 441.594 SemiCompRisks 441.595 semPlot 441.596 semTools 441.597 sendmailR 441.598 servr 441.599 sessioninfo 441.600 sf 441.601 sfsmisc 441.602 shape 441.603 shiny 441.604 shinyFiles 441.605 shinyjs 441.606 shinystan 441.607 shinythemes 441.608 sjlabelled 441.609 sjmisc 441.610 sjstats 441.611 skimr 441.612 slam 441.613 SLC 441.614 Sleuth2 441.615 smcure 441.616 sna 441.617 snakecase 441.618 som 441.619 sos 441.620 sourcetools 441.621 sp 441.622 sparklyr 441.623 SparseM 441.624 spatial 441.625 spData 441.626 splines 441.627 splitstackshape 441.628 SQUAREM 441.629 ssanv 441.630 ssgraph 441.631 stabledist 441.632 StanHeaders 441.633 stapler 441.634 stargazer 441.635 statisticalModeling 441.636 statnet.common 441.637 stats 441.638 stats4 441.639 steepness 441.640 stencila 441.641 stringdist 441.642 stringi 441.643 stringr 441.644 strucchange 441.645 styler 441.646 superbiclust 441.647 SuppDists 441.648 survey 441.649 survival 441.650 survminer 441.651 survMisc 441.652 sutteForecastR 441.653 svglite 441.654 tableone https://cran.r-project.org/web/packages/tableone/vignettes/introduction.html 441.655 tables 441.656 tabplot 441.657 tangram https://github.com/spgarbet/tangram https://cran.r-project.org/web/packages/tangram/index.html https://cran.r-project.org/web/packages/tangram/vignettes/summary-example.html Global Style for Rmd Example \\``` {r, results='asis'} cat(custom_css( lancet.css )) ``` .figbody thead { background: #aaffff !important; } .figbody tbody .odd { background: #aaffff !important; } https://cran.r-project.org/web/packages/tangram/vignettes/fda-example.html https://cran.r-project.org/web/packages/tangram/vignettes/example.html https://cran.r-project.org/web/packages/tangram/vignettes/single-style.html 441.658 tcltk 441.659 tcltk2 441.660 TeachingDemos 441.661 testit 441.662 testthat 441.663 TH.data 441.664 threejs 441.665 tibble 441.666 tidybayes Bayesian analysis + tidy data + geoms http://mjskay.github.io/tidybayes/ 441.667 tidygraph 441.668 tidyr 441.669 tidyselect 441.670 tidyverse 441.671 timeDate 441.672 timeSeries 441.673 tinytex 441.674 tkrplot 441.675 tm 441.676 TMB 441.677 tools 441.678 tree 441.679 triebeard 441.680 trimcluster 441.681 tseries 441.682 TTR 441.683 tufterhandout 441.684 tutorial 441.685 tweenr 441.686 ucminf 441.687 units 441.688 updateR 441.689 urca 441.690 urltools 441.691 uroot 441.692 userfriendlyscience 441.693 usethis 441.694 utf8 441.695 utils 441.696 uuid 441.697 V8 441.698 vcd 441.699 vcdExtra 441.700 vegan 441.701 VGAM 441.702 ViewPipeSteps https://github.com/daranzolin/ViewPipeSteps 441.703 viridis 441.704 viridisLite 441.705 visdat 441.706 visNetwork 441.707 visreg 441.708 wbstats 441.709 webshot 441.710 whisker 441.711 WhiteStripe 441.712 withr 441.713 wordcloud 441.714 workflowr 441.715 writexl 441.716 WRS2 441.717 xfun 441.718 XLConnect 441.719 XLConnectJars 441.720 xlsx 441.721 xlsxjars 441.722 XML 441.723 xml2 441.724 xplain 441.725 xray 441.726 xtable 441.727 xts 441.728 yaImpute 441.729 yaml 441.730 zip 441.731 zoo 441.732 Effsize a package for efficient effect size computation https://github.com/mtorchiano/effsize 441.733 keyring https://www.infoworld.com/video/91987/r-tip-keep-passwords-and-tokens-secure-with-the-keyring-package 441.734 testthat R tip: Test your code with testthat https://www.infoworld.com/video/87735/r-tip-test-your-code-with-testthat 441.735 cronR Schedule R scripts on a Mac https://www.infoworld.com/video/90629/r-tip-schedule-r-scripts-on-a-mac "],["sankey-diagrams-1.html", "Chapter 442 Sankey Diagrams", " Chapter 442 Sankey Diagrams https://www.r-bloggers.com/creating-custom-sankey-diagrams-using-r/ {r eval=FALSE, include=FALSE, echo=TRUE} library(networkD3) nodes = data.frame( name = c( Node A , # Node 0 Node B , # Node 1 Node C , # Node 2 Node D ))# Node 3 links = as.data.frame(matrix(c( 0, 1, 10, # Each row represents a link. The first number 0, 2, 20, # represents the node being conntected from. 1, 3, 30, # the second number represents the node connected to. 2, 3, 40),# The third number is the value of the node byrow = TRUE, ncol = 3)) names(links) = c( source , target , value ) sankeyNetwork(Links = links, Nodes = nodes, Source = source , Target = target , Value = value , NodeID = name , fontSize= 12, nodeWidth = 30) https://plot.ly/r/sankey-diagram/ https://cran.r-project.org/web/packages/riverplot/riverplot.pdf "],["sensitivity-specificity.html", "Chapter 443 Sensitivity Specificity", " Chapter 443 Sensitivity Specificity "],["classification-and-regression-training.html", "Chapter 444 Classification And REgression Training 444.1 Calculate Sensitivity, Specificity And Predictive Values", " Chapter 444 Classification And REgression Training http://topepo.github.io/caret/index.html 444.1 Calculate Sensitivity, Specificity And Predictive Values https://www.rdocumentation.org/packages/caret/versions/3.51/topics/sensitivity sensitivity(data, reference, positive = levels(reference)[1]) specificity(data, reference, negative = levels(reference)[2]) posPredValue(data, reference, positive = levels(reference)[1]) negPredValue(data, reference, negative = levels(reference)[2]) "],["shiny-3.html", "Chapter 445 Shiny", " Chapter 445 Shiny Some notes on my first shiny app https://ggvy.cl/post/some-notes-on-my-first-shiny-app/ "],["shiny-4.html", "Chapter 446 Shiny", " Chapter 446 Shiny "],["shiny-online-documentation.html", "Chapter 447 Shiny Online Documentation", " Chapter 447 Shiny Online Documentation https://shiny.rstudio.com "],["mastering-shiny.html", "Chapter 448 Mastering Shiny", " Chapter 448 Mastering Shiny https://mastering-shiny.org/ "],["shinycodes.html", "Chapter 449 ShinyCodes", " Chapter 449 ShinyCodes {r eval=FALSE, include=FALSE, echo=TRUE} # from CRAN # install.packages( bs4Dash ) # latest devel version # devtools::install_github( RinteRface/bs4Dash ) {r eval=FALSE, include=FALSE, echo=TRUE} library(bs4Dash) # classic theme # bs4DashGallery() # old_school theme # bs4DashGallery(theme = old_school ) {r eval=FALSE, include=FALSE, echo=TRUE} # install.packages( argonR ) # devtools::install_github( RinteRface/argonDash ) library(argonR) {r eval=FALSE, include=FALSE, echo=TRUE} "],["snahelper.html", "Chapter 450 snahelper", " Chapter 450 snahelper {r eval=FALSE, include=FALSE, echo=TRUE} # devtools::install_github( schochastics/snahelper ) # devtools::install_github( schochastics/smglr ) http://blog.schochastics.net/post/an-rstudio-addin-for-network-analysis-and-visualization/ "],["introduction-to-summarytools.html", "Chapter 451 Introduction to summarytools 451.1 summarytool‚Äôs Core Functions 451.2 1 - freq() : Frequency Tables 451.3 2 - ctable() : Cross-Tabulations 451.4 3 - descr() : Descriptive Univariate Stats 451.5 4 - dfSummary() : Data Frame Summaries 451.6 The print() and view() Functions 451.7 Using by() to Show Results By Groups 451.8 Using lapply() to Show Several freq() tables at once 451.9 Using summarytools in Rmarkdown documents 451.10 Writing Output to Files 451.11 Global options 451.12 Overriding formatting attributes 451.13 Order of Priority for Options / Parameters 451.14 Customizing looks with CSS 451.15 Working with shiny apps 451.16 Getting Most Properties of an Object With what.is() 451.17 Limitations 451.18 Stay Up-to-date", " Chapter 451 Introduction to summarytools author: Dominic Comtois date: output: rmarkdown::html_vignette: css: - !expr system.file( rmarkdown/templates/html_vignette/resources/vignette.css , package = rmarkdown ) - !expr system.file( includes/stylesheets/summarytools.css , package = summarytools ) vignette: &gt; % % % {r eval=FALSE, include=FALSE, echo=TRUE} library(knitr) opts_chunk$set(comment=NA, prompt=FALSE, cache=FALSE, results=&#39;asis&#39;) library(summarytools) st_options(&#39;footnote&#39;, NA) summarytools is an R package providing tools to neatly and quickly summarize data. It can also make R a little easier to learn and use. Four functions are at the core of the package: freq() : frequency tables with proportions, cumulative proportions and missing data information. ctable() : cross-tabulations between two factors or any discrete data, with total, rows or columns proportions, as well as marginal totals. descr() : descriptive (univariate) statistics for numerical data. dfSummary() : Extensive data frame summaries that facilitate data cleaning and firsthand evaluation. An emphasis has been put on both what and how results are presented, so that the package can serve both as a data exploration and reporting tool, which can be used either on its own for minimal reports, or along with larger sets of tools such as RStudio‚Äôs for rmarkdown, and knitr. Building on the strengths of pander and htmltools, the outputs produced by summarytools can be: Displayed in plain text in the R console (default behaviour) Used in Rmardown documents and knitted along with other text and R output Written to html files that fire up in RStudio‚Äôs Viewer pane or in your system‚Äôs default browser Written to plain text files / Rmarkdown text files 451.0.1 Latest Improvements Version 0.8.3 brings several improvements to summarytools, notably: Introduction of global settings (customizable defaults) Options to make content fit more naturally in shiny apps or Rmarkdown documents A better handling of split-group statistics with by() A more thorough documentation 451.1 summarytool‚Äôs Core Functions 451.2 1 - freq() : Frequency Tables The freq() function generates a table of frequencies with counts and proportions. Since this page use markdown rendering, we‚Äôll set style = 'rmarkdown' to take advantage of it. {r eval=FALSE, include=FALSE, echo=TRUE} library(summarytools) freq(iris$Species, style = rmarkdown ) If we do not worry about missing data, we can set {r # eport.nas = FALSE: {r eval=FALSE, include=FALSE, echo=TRUE} freq(iris$Species, report.nas = FALSE, style = rmarkdown , headings = TRUE) We could simplify further and omit the Totals row by setting totals = FALSE. To get familiar with the output styles, try different values for style= and see how results look in the console. 451.3 2 - ctable() : Cross-Tabulations We‚Äôll now use a sample data frame called tobacco, which is included in the package. We want to cross-tabulate the two categorical variables smoker and diseased. By default, ctable() gives row proportions, but we‚Äôll include the full syntax anyway. Since markdown has not support (yet) for multi-line headings, we‚Äôll show an image of the resulting html table. {r eval=FALSE, include=FALSE, echo=TRUE} with(tobacco, print(ctable(smoker, diseased), method = &#39;render&#39;)) Notice that instead of ctable(tobacco$smoker, tobacco$diseased, ...), we used the with() function, making the syntax less redundant. It is possible to display column, total, or no proportions at all. We can also omit the marginal totals to have a simple ‚Äú2 x 2‚Äù table. {r eval=FALSE, include=FALSE, echo=TRUE} with(tobacco, print(ctable(smoker, diseased, prop = &#39;n&#39;, totals = FALSE), headings = TRUE, method = render )) 451.4 3 - descr() : Descriptive Univariate Stats The descr() function generates common central tendency statistics and measures of dispersion for numerical data. It can handle single vectors as well as data frames, in which case it just ignores non-numerical columns (and displays a message to that effect). {r eval=FALSE, include=FALSE, echo=TRUE} descr(iris, style = rmarkdown ) 451.4.1 Transposing and selecting only the stats you need If your eyes/brain prefer seeing things the other way around, just use transpose = TRUE. Here, we also select only the statistics we wish to see, and specify headings = TRUE to avoid reprinting the same information as above. {r eval=FALSE, include=FALSE, echo=TRUE} descr(iris, stats = c( mean , sd , min , med , max ), transpose = TRUE, headings = TRUE, style = rmarkdown ) 451.5 4 - dfSummary() : Data Frame Summaries dfSummary() collects information about all variables in a data frame and displays it in a singe, legible table. To generate a summary report and have it displayed in RStudio‚Äôs Viewer pane (or in your default Web Browser if working with another interface), we simply do like this: {r, eval=FALSE} view(dfSummary(iris)) It is also possible to use dfSummary() in Rmarkdown documents. In this next example, note that due to rmarkdown compatibility issues, histograms are not shown. We‚Äôre working on this. Further down, we‚Äôll see how tu use html rendering to go around this problem. {r eval=FALSE, include=FALSE, echo=TRUE} dfSummary(tobacco, plain.ascii = FALSE, style = grid ) 451.6 The print() and view() Functions summarytools has a generic print method, print.summarytools(). By default, its method argument is set to 'pander'. One of the ways in which view() is useful is that we can use it to easily display html outputs in RStudio‚Äôs Viewer. In this case, the view() function simply acts as a wrapper around the generic print() function, specifying the method = 'viewer' for us. When used outside RStudio, the method falls back on 'browser' and the report is fired up in the system‚Äôs default browser. 451.7 Using by() to Show Results By Groups With freq() and descr(), you can use R‚Äôs base function by() to show statistics split by a ventilation / categorical variable. R‚Äôs by() function returns a list containing as many summarytools objects as there are categories in our ventilation variable. To propertly display the content present in that list, we use the view() function. Using print(), while technically possible, will not give as much satisfactory results. 451.7.0.1 Example Using the iris data frame, we will display descriptive statistics broken down by Species. {r eval=FALSE, include=FALSE, echo=TRUE} # First save the results iris_stats_by_species &lt;- by(data = iris, INDICES = iris$Species, FUN = descr, stats = c( mean , sd , min , med , max ), transpose = TRUE) # Then use view(), like so: view(iris_stats_by_species, method = pander , style = rmarkdown ) To see an html version of these results, we‚Äôd simply do this (results not shown): {r, eval=FALSE} view(iris_stats_by_species) 451.7.1 Special Case - Using descr() With by() For A Single Variable Instead of showing several tables having only one column each, the view() function will assemble the results into a single table: {r eval=FALSE, include=FALSE, echo=TRUE} data(tobacco) # tobacco is an example dataframe included in the package BMI_by_age &lt;- with(tobacco, by(BMI, age.gr, descr, stats = c( mean , sd , min , med , max ))) view(BMI_by_age, pander , style = rmarkdown ) The transposed version looks like this: {r eval=FALSE, include=FALSE, echo=TRUE} BMI_by_age &lt;- with(tobacco, by(BMI, age.gr, descr, transpose = TRUE, stats = c( mean , sd , min , med , max ))) view(BMI_by_age, pander , style = rmarkdown , headings = TRUE) 451.8 Using lapply() to Show Several freq() tables at once As is the case for by(), the view() function is essential for making results nice and tidy. {r, eval=FALSE} tobacco_subset &lt;- tobacco[ ,c( gender , age.gr , smoker )] freq_tables &lt;- lapply(tobacco_subset, freq) view(freq_tables, footnote = NA, file = &#39;freq-tables.html&#39;) 451.9 Using summarytools in Rmarkdown documents As we have seen, summarytools can generate both text (including rmarkdown) and html results. Both can be used in Rmarkdown, according to your preferences. There is a vignette dedicated to this, which gives several examples, but if you‚Äôre in a hurry, here are a few tips to get started: Always set the knitr chunk option {r # esults = 'asis'. You can do this on a chunk-by-chunk basis, but here is how to do it globally: {r, eval=FALSE, tidy=FALSE} knitr::opts_chunk$set(echo = TRUE, results = &#39;asis&#39;) Refer to this page for more on knitr‚Äôs options. To get better results when using html (with method = 'render'), set up your .Rmd document so it includes summarytool‚Äôs css. 451.9.0.1 Example # # # RMarkdown using summarytools # output: # html_document: # css: C:/R/win-library/3.4/summarytools/includes/stylesheets/summarytools.css # For more details on using summarytools in Rmarkdown documents, please refer to the corresponding vignette. 451.10 Writing Output to Files The console will always tell you the location of the temporary html file that is created in the process. However, you can specify the name and location of that file explicitly if you need to reuse it later on: {r, eval=FALSE} view(iris_stats_by_species, file = ~/iris_stats_by_species.html ) Based on the file extension you provide (.html vs others), summarytools will use the appropriate method; there is no need to specify the method argument. 451.10.1 Appending output files There is also an append = logical argument for adding content to existing reports, both text/Rmarkdown and html. This is useful if you want to quickly include several statistical tables in a single file. It is fast alternative to creating an .Rmd document if you don‚Äôt need the extra content that the latter allows. 451.11 Global options Version 0.8.3 introduced the following set of global options: {r # ound.digits = 2 plain.ascii = TRUE headings = FALSE (if using in a markdown document or a shiny app, setting this to TRUE might be preferable footnote = 'default' (set to empty string or NA to omit footnote) display.labels = TRUE freq.totals = TRUE freq.display.nas = TRUE ctable.totals = TRUE ctable.prop = 'r' (display row proportions by default) descr.stats = 'all' descr.transpose = FALSE bootstrap.css = TRUE (if using in a markdown document or a shiny app, setting this to FALSE might be preferable custom.css = NA escape.pipe = FALSE 451.11.0.1 Examples {r, eval=FALSE} st_options() # display all global options&#39; values st_options(&#39;round.digits&#39;) # display only one option st_options(&#39;headings&#39;, TRUE) # change an option&#39;s value st_options(&#39;footnote&#39;, NA) # Turn off the footnote on all outputs. # This option was used prior to generating # the present document. 451.12 Overriding formatting attributes When a summarytools object is stored, its formatting attributes are stored with it. However, you can override most of them when using the print() and view() functions. 451.12.0.1 Example {r eval=FALSE, include=FALSE, echo=TRUE} age_stats &lt;- freq(tobacco$age.gr) # age_stats contains a regular output for freq # including headings, NA counts, and Totals print(age_stats, style = rmarkdown , report.nas = FALSE, totals = FALSE, headings = TRUE) Note that the omitted attributes are stil part of the age_stats object. 451.13 Order of Priority for Options / Parameters Options over-ridden explicitly with print() or view() have precendence options specified as explicit arguments to freq() / ctable() / descr() / dfSummary() come second Global options, which can be set with st_options, come third 451.14 Customizing looks with CSS Version 0.8 of summarytools uses RStudio‚Äôs htmltools package and version 4 of Bootstrap‚Äôs cascading stylesheets. It is possible to include your own css if you wish to customize the look of the output tables. See the documentation for the package‚Äôs print.summarytools() function for details, but here is a quick example to give you the gist of it. 451.14.0.1 Example Say you need to make the font size really small, smaller than by using the st-small class as seen in a previous example. For this, you would create a CSS file - let‚Äôs call it ‚Äúcustom.css‚Äù - containing a class such as the following: .table-condensed { font-size: 8px; } Then, to apply it to a summarytools object and display it in your browser: {r, eval=FALSE} view(dfSummary(tobacco), custom.css = &#39;path/to/custom.css&#39;, table.classes = &#39;table-condensed&#39;) 451.15 Working with shiny apps To include summarytools functions into shiny apps, it is recommended that you: set bootstrap.css = FASE to avoid interacting with the app‚Äôs layout adjust the size of the graphs in dfSummary() set headings to TRUE 451.15.0.1 Example {r # print(dfSummary(somedata, graph.magnif = 0.8), method = &#39;render&#39;, headings = TRUE, bootstrap.css = FALSE) 451.16 Getting Most Properties of an Object With what.is() When developing, we often use a number of functions to obtain an object‚Äôs properties. what.is() proposes to lump together the results of most of these functions (class(), typeof(), attributes() and others). {r what_is, warning=FALSE, results=&#39;markup&#39;} what.is(iris) 451.17 Limitations The text histograms in dfSummary are not yet supported in {r # markdown. Better use the ‚Äòrender‚Äô method. It is not possible to control the heading levels of individual items. You can however choose to headings altogether. 451.18 Stay Up-to-date Check out the project‚Äôs page - from there you can see the latest updates and also submit feature requests. To install the version of summarytools that is on CRAN, but that might have benefited from quick fixes: {r # install.packages(&#39;devtools&#39;) library(devtools) install_github(&#39;dcomtois/summarytools&#39;) To install the package in its development version, use {r # install_github(&#39;dcomtois/summarytools&#39;, ref=&#39;dev-current&#39;) "],["final-notes.html", "Chapter 452 Final notes", " Chapter 452 Final notes The package comes with no guarantees. It is a work in progress and feedback / feature requests are welcome. Just send me an email (dominic.comtois (at) gmail.com), or open an Issue on GitHub if you find a bug. "],["recommendations-for-using-summarytools-with-rmarkdown.html", "Chapter 453 Recommendations for Using summarytools With Rmarkdown 453.1 Important Note 453.2 Using method = ‚Äòrender‚Äô", " Chapter 453 Recommendations for Using summarytools With Rmarkdown author: Dominic Comtois date: output: rmarkdown::html_vignette: css: - !expr system.file( rmarkdown/templates/html_vignette/resources/vignette.css , package = rmarkdown ) - !expr system.file( includes/stylesheets/summarytools.css , package = summarytools ) vignette: &gt; % % % {r, include=FALSE} library(knitr) opts_chunk$set(comment=NA, prompt=FALSE, cache=FALSE, echo=TRUE, results=&#39;asis&#39;) library(summarytools) This document uses theme {r # markdown::html_vignette. Below are examples using recommended styles for Rmarkdown rendering. Available styles in summarytools are the same as pander‚Äôs: simple (default) rmarkdown grid multiline For freq(), descr() (and ctable(), although with caveats), rmarkdown style is recommended. For dfSummary(), grid is recommended. 453.1 Important Note knitr option {r # esults = 'asis' must be specified to get good results. This can be done globally via opts_chunk$set(results='asis'), or in the individual chunks. The following summarytools global options have been set: {r eval=FALSE, include=FALSE, echo=TRUE} #st_options(&#39;headings&#39;, TRUE) st_options(&#39;bootstrap.css&#39;, FALSE) st_options(&#39;footnote&#39;, NA) 453.2 Using method = ‚Äòrender‚Äô To generate tables using summarytool‚Äôs own html rendering, the .Rmd document‚Äôs configuration part (yaml) must point to the package‚Äôs summarytools.css file. This can be achieved in several ways; the current vignette uses this configuration: output: rmarkdown::html_vignette: css: - !expr system.file( rmarkdown/templates/html_vignette/resources/vignette.css , package = rmarkdown ) - !expr system.file( includes/stylesheets/summarytools.css , package = summarytools ) An alternative is to point to the directory on your system containing summarytools.css: # RMarkdown using summarytools output: html_document: css: C:/R/win-library/3.4/summarytools/includes/stylesheets/summarytools.css Starting with freq(), we‚Äôll review the recommended methods and styles to get going with summarytools in Rmarkdown documents. Jump to‚Ä¶ ctable() descr() dfSummary() ‚Äì "],["freq.html", "Chapter 454 freq() 454.1 Rmarkdown Style 454.2 HTML Rendering", " Chapter 454 freq() freq() is best used with `style = ‚Äòrmarkdown‚Äô; html rendering is also possible. 454.1 Rmarkdown Style {r eval=FALSE, include=FALSE, echo=TRUE} freq(tobacco$gender, style = &#39;rmarkdown&#39;) 454.2 HTML Rendering {r eval=FALSE, include=FALSE, echo=TRUE} print(freq(tobacco$gender), method = &#39;render&#39;) If you find the table too large, you can use table.classes = 'st-small' - an example is provided further below. ‚Äì Back to top "],["ctable.html", "Chapter 455 ctable() 455.1 Rmarkdown Style 455.2 HTML Rendering", " Chapter 455 ctable() 455.1 Rmarkdown Style Tables with heading spanning over 2 rows are not fully supported in markdown (yet), but the result is getting close to acceptable. {r eval=FALSE, include=FALSE, echo=TRUE} ctable(tobacco$gender, tobacco$smoker, style = &#39;rmarkdown&#39;) 455.2 HTML Rendering For best results, use this method. {r include=FALSE, eval=FALSE, echo = TRUE} print(ctable(tobacco$gender, tobacco$smoker), method = &#39;render&#39;) ‚Äì Back to top "],["descr3.html", "Chapter 456 descr() 3 456.1 Rmarkdown Style 456.2 HTML Rendering", " Chapter 456 descr() 3 descr() is also best used with style = 'rmarkdown', and HTML rendering is also supported. 456.1 Rmarkdown Style {r eval=FALSE, include=FALSE, echo=TRUE} descr(tobacco, style = &#39;rmarkdown&#39;) 456.2 HTML Rendering We‚Äôll use table.classes = ‚Äòst-small‚Äô to show how it affects the table‚Äôs size (compare to the freq() table rendered earlier). {r eval=FALSE, include=FALSE, echo=TRUE} print(descr(tobacco), method = &#39;render&#39;, table.classes = &#39;st-small&#39;) ‚Äì Back to top "],["dfsummary.html", "Chapter 457 dfSummary() 457.1 Grid Style 457.2 HTML Rendering", " Chapter 457 dfSummary() 457.1 Grid Style This gives good results, although the histograms are not shown. This has to do with an unresolved issue, but we‚Äôre working hard to figure out a solution. Don‚Äôt forget to specify plain.ascii = FALSE, or you won‚Äôt get good results. {r , results=&#39;asis&#39;} dfSummary(tobacco, style = &#39;grid&#39;, plain.ascii = FALSE) 457.2 HTML Rendering Although the results are not as neat as they are when simply generating an html report from the R interpreter ‚Äì the transparency of the graphs is lost in translation ‚Äì, this is the best method still. {r eval=FALSE, include=FALSE, echo=TRUE} print(dfSummary(tobacco, graph.magnif = 0.75), method = &#39;render&#39;) Back to top "],["r-project-giri≈ü.html", "Chapter 458 R-project giri≈ü", " Chapter 458 R-project giri≈ü background-image: url(https://upload.wikimedia.org/wikipedia/commons/b/be/Sharingan_triple.svg) {r eval=FALSE, include=FALSE, echo=TRUE} options(htmltools.dir.version = FALSE) ??? Image credit: Wikimedia Commons class: center, middle "],["xaringan-2.html", "Chapter 459 xaringan", " Chapter 459 xaringan 459.0.1 / ÉaÀê.‚Äôri≈ã.…°an/ class: inverse, center, middle "],["get-started-1.html", "Chapter 460 Get Started", " Chapter 460 Get Started "],["hello-world-1.html", "Chapter 461 Hello World", " Chapter 461 Hello World Install the xaringan package from Github: {r eval=FALSE, tidy=FALSE} devtools::install_github( yihui/xaringan ) ‚Äì You are recommended to use the RStudio IDE, but you do not have to. Create a new R Markdown document from the menu File -&gt; New File -&gt; R Markdown -&gt; From Template -&gt; Ninja Presentation;1 ‚Äì Click the Knit button to compile it; ‚Äì or use the RStudio Addin2 Infinite Moon Reader to live preview the slides (every time you update and save the Rmd document, the slides will be automatically reloaded in RStudio Viewer. .footnote[ [1] ‰∏≠ÊñáÁî®Êà∑ËØ∑ÁúãËøô‰ªΩÊïôÁ®ã [2] See #2 if you do not see the template or addin in RStudio. ] background-image: url({r # xaringan:::karl) background-position: 50% 50% class: center, bottom, inverse "],["you-only-live-once-1.html", "Chapter 462 You only live once!", " Chapter 462 You only live once! "],["hello-ninja-1.html", "Chapter 463 Hello Ninja", " Chapter 463 Hello Ninja As a presentation ninja, you certainly should not be satisfied by the Hello World example. You need to understand more about two things: The remark.js library; The xaringan package; Basically xaringan injected the chakra of R Markdown (minus Pandoc) into remark.js. The slides are rendered by remark.js in the web browser, and the Markdown source needed by remark.js is generated from R Markdown (knitr). "],["remark-js-1.html", "Chapter 464 remark.js", " Chapter 464 remark.js You can see an introduction of remark.js from its homepage. You should read the remark.js Wiki at least once to know how to create a new slide (Markdown syntax* and slide properties); format a slide (e.g. text alignment); configure the slideshow; and use the presentation (keyboard shortcuts). It is important to be familiar with remark.js before you can understand the options in xaringan. .footnote[[*] It is different with Pandoc‚Äôs Markdown! It is limited but should be enough for presentation purposes. Come on‚Ä¶ You do not need a slide for the Table of Contents! Well, the Markdown support in remark.js may be improved in the future.] background-image: url({r # xaringan:::karl) background-size: cover class: center, bottom, inverse "],["i-was-so-happy-to-have-discovered-remark-js-1.html", "Chapter 465 I was so happy to have discovered remark.js!", " Chapter 465 I was so happy to have discovered remark.js! class: inverse, middle, center "],["using-xaringan-1.html", "Chapter 466 Using xaringan", " Chapter 466 Using xaringan "],["xaringan-3.html", "Chapter 467 xaringan", " Chapter 467 xaringan Provides an R Markdown output format xaringan::moon_reader as a wrapper for remark.js, and you can use it in the YAML metadata, e.g. # A Cool Presentation output: xaringan::moon_reader: yolo: true nature: autoplay: 30000 See the help page ?xaringan::moon_reader for all possible options that you can use. "],["remark-js-vs-xaringan-1.html", "Chapter 468 remark.js vs xaringan", " Chapter 468 remark.js vs xaringan Some differences between using remark.js (left) and using xaringan (right): .pull-left[ 1. Start with a boilerplate HTML file; Plain Markdown; Write JavaScript to autoplay slides; Manually configure MathJax; Highlight code with *; Edit Markdown source and refresh browser to see updated slides; ] .pull-right[ 1. Start with an R Markdown document; R Markdown (can embed R/other code chunks); Provide an option autoplay; MathJax just works;* Highlight code with {{}}; The RStudio addin Infinite Moon Reader automatically refreshes slides on changes; ] .footnote[[*] Not really. See next page.] "],["math-expressions-1.html", "Chapter 469 Math Expressions", " Chapter 469 Math Expressions "],["r-code-1.html", "Chapter 470 R Code", " Chapter 470 R Code {r comment=&#39;#&#39;} # a boring regression fit = lm(dist ~ 1 + speed, data = cars) coef(summary(fit)) dojutsu = c(&#39;Âú∞ÁàÜÂ§©Êòü&#39;, &#39;Â§©ÁÖß&#39;, &#39;Âä†ÂÖ∑ÂúüÂëΩ&#39;, &#39;Á•ûÂ®Å&#39;, &#39;È†à‰ΩêËÉΩ‰πé&#39;, &#39;ÁÑ°ÈôêÊúàË™≠&#39;) grep(&#39;Â§©&#39;, dojutsu, value = TRUE) "],["r-plots-1.html", "Chapter 471 R Plots", " Chapter 471 R Plots {r , fig.height=4, dev=&#39;svg&#39;} par(mar = c(4, 4, 1, .1)) plot(cars, pch = 19, col = &#39;darkgray&#39;, las = 1) abline(fit, lwd = 2) "],["tables-3.html", "Chapter 472 Tables", " Chapter 472 Tables If you want to generate a table, make sure it is in the HTML format (instead of Markdown or other formats), e.g., {r eval=FALSE, include=FALSE, echo=TRUE} knitr::kable(head(iris), format = &#39;html&#39;) "],["html-widgets-1.html", "Chapter 473 HTML Widgets", " Chapter 473 HTML Widgets I have not thoroughly tested HTML widgets against xaringan. Some may work well, and some may not. It is a little tricky. Similarly, the Shiny mode ({r # untime: shiny) does not work. I might get these issues fixed in the future, but these are not of high priority to me. I never turn my presentation into a Shiny app. When I need to demonstrate more complicated examples, I just launch them separately. It is convenient to share slides with other people when they are plain HTML/JS applications. See the next page for two HTML widgets. {r out.width=&#39;100%&#39;, fig.height=6, eval=require(&#39;leaflet&#39;)} library(leaflet) leaflet() %&gt;% addTiles() %&gt;% setView(-93.65, 42.0285, zoom = 17) {r eval=require(&#39;DT&#39;), tidy=FALSE} DT::datatable( head(iris, 10), fillContainer = FALSE, options = list(pageLength = 8) ) "],["some-tips-6.html", "Chapter 474 Some Tips", " Chapter 474 Some Tips When you use the Infinite Moon Reader addin in RStudio, your R session will be blocked by default. You can click the red button on the right of the console to stop serving the slides, or use the daemonized mode so that it does not block your R session. To do the latter, you can set the option {r # options(servr.daemon = TRUE) in your current R session, or in ~/.Rprofile so that it is applied to all future R sessions. I do the latter by myself. To know more about the web server, see the servr package. ‚Äì Do not forget to try the yolo option of xaringan::moon_reader. output: xaringan::moon_reader: yolo: true "],["some-tips-7.html", "Chapter 475 Some Tips", " Chapter 475 Some Tips Slides can be automatically played if you set the autoplay option under nature, e.g. go to the next slide every 30 seconds in a lightning talk: output: xaringan::moon_reader: nature: autoplay: 30000 ‚Äì A countdown timer can be added to every page of the slides using the countdown option under nature, e.g. if you want to spend one minute on every page when you give the talk, you can set: output: xaringan::moon_reader: nature: countdown: 60000 Then you will see a timer counting down from 01:00, to 00:59, 00:58, ‚Ä¶ When the time is out, the timer will continue but the time turns red. "],["some-tips-8.html", "Chapter 476 Some Tips", " Chapter 476 Some Tips The title slide is created automatically by xaringan, but it is just another remark.js slide added before your other slides. The title slide is set to class: center, middle, inverse, title-slide by default. You can change the classes applied to the title slide with the titleSlideClass option of nature (title-slide is always applied). output: xaringan::moon_reader: nature: titleSlideClass: [top, left, inverse] ‚Äì If you‚Äôd like to create your own title slide, disable xaringan‚Äôs title slide with the seal = FALSE option of moon_reader. output: xaringan::moon_reader: seal: false "],["some-tips-9.html", "Chapter 477 Some Tips", " Chapter 477 Some Tips There are several ways to build incremental slides. See this presentation for examples. The option highlightLines: true of nature will highlight code lines that start with *, or are wrapped in {{ }}, or have trailing comments #&lt;&lt;; output: xaringan::moon_reader: nature: highlightLines: true See examples on the next page. "],["some-tips-10.html", "Chapter 478 Some Tips", " Chapter 478 Some Tips .pull-left[ An example using a leading *: ``` {r # if (TRUE) { ** message( Very important! ) } Output: {r # if (TRUE) { * message( Very important! ) } This is invalid R code, so it is a plain fenced code block that is not executed. ] .pull-right[ An example using `{{}}`: `{r # ‚Äô‚Äô{r tidy=FALSE} if (TRUE) { *{{ message( Very important! ) }} } ``` Output: {r tidy=FALSE} if (TRUE) { {{ message( Very important! ) }} } It is valid R code so you can run it. Note that {{}} can wrap an R expression of multiple lines. ] "],["some-tips-11.html", "Chapter 479 Some Tips", " Chapter 479 Some Tips An example of using the trailing comment #&lt;&lt; to highlight lines: `{r # &#39;&#39;```` {r tidy=FALSE} library(ggplot2) ggplot(mtcars) + aes(mpg, disp) + geom_point() + #&lt;&lt; geom_smooth() #&lt;&lt; ``` Output: {r tidy=FALSE, eval=FALSE} library(ggplot2) ggplot(mtcars) + aes(mpg, disp) + geom_point() + #&lt;&lt; geom_smooth() #&lt;&lt; "],["some-tips-12.html", "Chapter 480 Some Tips", " Chapter 480 Some Tips When you enable line-highlighting, you can also use the chunk option highlight.output to highlight specific lines of the text output from a code chunk. For example, highlight.output = TRUE means highlighting all lines, and highlight.output = c(1, 3) means highlighting the first and third line. `{r # &#39;&#39;```` {r, highlight.output=c(1, 3)} head(iris) ``` {r, highlight.output=c(1, 3), echo=TRUE} head(iris) Question: what does highlight.output = c(TRUE, FALSE) mean? (Hint: think about R‚Äôs recycling of vectors) "],["some-tips-13.html", "Chapter 481 Some Tips", " Chapter 481 Some Tips To make slides work offline, you need to download a copy of remark.js in advance, because xaringan uses the online version by default (see the help page ?xaringan::moon_reader). You can use xaringan::summon_remark() to download the latest or a specified version of remark.js. By default, it is downloaded to libs/remark-latest.min.js. Then change the chakra option in YAML to point to this file, e.g. output: xaringan::moon_reader: chakra: libs/remark-latest.min.js If you used Google fonts in slides (the default theme uses Yanone Kaffeesatz, Droid Serif, and Source Code Pro), they won‚Äôt work offline unless you download or install them locally. The Heroku app google-webfonts-helper can help you download fonts and generate the necessary CSS. "],["macros-1.html", "Chapter 482 Macros", " Chapter 482 Macros remark.js allows users to define custom macros (JS functions) that can be applied to Markdown text using the syntax ![:macroName arg1, arg2, ...] or ![:macroName arg1, arg2, ...](this). For example, before remark.js initializes the slides, you can define a macro named scale: remark.macros.scale = function (percentage) { var url = this; return &#39;&lt;img src= &#39; + url + &#39; style= width: &#39; + percentage + &#39; /&gt;&#39;; }; Then the Markdown text ![:scale 50%](image.jpg) will be translated to &lt;img src= image.jpg style= width: 50% /&gt; "],["macros-continued-1.html", "Chapter 483 Macros (continued)", " Chapter 483 Macros (continued) To insert macros in xaringan slides, you can use the option beforeInit under the option nature, e.g., output: xaringan::moon_reader: nature: beforeInit: macros.js You save your remark.js macros in the file macros.js. The beforeInit option can be used to insert arbitrary JS code before {r # emark.create(). Inserting macros is just one of its possible applications. "],["css-2.html", "Chapter 484 CSS", " Chapter 484 CSS Among all options in xaringan::moon_reader, the most challenging but perhaps also the most rewarding one is css, because it allows you to customize the appearance of your slides using any CSS rules or hacks you know. You can see the default CSS file here. You can completely replace it with your own CSS files, or define new rules to override the default. See the help page ?xaringan::moon_reader for more information. "],["css-3.html", "Chapter 485 CSS", " Chapter 485 CSS For example, suppose you want to change the font for code from the default Source Code Pro to Ubuntu Mono . You can create a CSS file named, say, ubuntu-mono.css: @import url(https://fonts.googleapis.com/css?family=Ubuntu+Mono:400,700,400italic); .remark-code, .remark-inline-code { font-family: &#39;Ubuntu Mono&#39;; } Then set the css option in the YAML metadata: output: xaringan::moon_reader: css: [ default , ubuntu-mono.css ] Here I assume ubuntu-mono.css is under the same directory as your Rmd. See yihui/xaringan#83 for an example of using the Fira Code font, which supports ligatures in program code. "],["themes-2.html", "Chapter 486 Themes", " Chapter 486 Themes Don‚Äôt want to learn CSS? Okay, you can use some user-contributed themes. A theme typically consists of two CSS files foo.css and foo-fonts.css, where foo is the theme name. Below are some existing themes: {r eval=FALSE, include=FALSE, echo=TRUE} names(xaringan:::list_css()) "],["themes-3.html", "Chapter 487 Themes", " Chapter 487 Themes To use a theme, you can specify the css option as an array of CSS filenames (without the .css extensions), e.g., output: xaringan::moon_reader: css: [default, metropolis, metropolis-fonts] If you want to contribute a theme to xaringan, please read this blog post. class: inverse, middle, center background-image: url(https://upload.wikimedia.org/wikipedia/commons/3/39/Naruto_Shiki_Fujin.svg) background-size: contain "],["naruto-1.html", "Chapter 488 Naruto", " Chapter 488 Naruto background-image: url(https://upload.wikimedia.org/wikipedia/commons/b/be/Sharingan_triple.svg) background-size: 100px background-position: 90% 8% "],["sharingan-1.html", "Chapter 489 Sharingan", " Chapter 489 Sharingan The R package name xaringan was derived1 from Sharingan, a d≈çjutsu in the Japanese anime Naruto with two abilities: the Eye of Insight the Eye of Hypnotism I think a presentation is basically a way to communicate insights to the audience, and a great presentation may even hypnotize the audience.2,3 .footnote[ [1] In Chinese, the pronounciation of X is Sh / É/ (as in shrimp). Now you should have a better idea of how to pronounce my last name Xie. [2] By comparison, bad presentations only put the audience to sleep. [3] Personally I find that setting background images for slides is a killer feature of remark.js. It is an effective way to bring visual impact into your presentations. ] "],["naruto-terminology-1.html", "Chapter 490 Naruto terminology", " Chapter 490 Naruto terminology The xaringan package borrowed a few terms from Naruto, such as Sharingan (ÂÜôËº™Áúº; the package name) The moon reader (ÊúàË™≠; an attractive R Markdown output format) Chakra (Êü•ÂÖãÊãâ; the path to the remark.js library, which is the power to drive the presentation) Nature transformation (ÊÄßË≥™Â§âÂåñ; transform the chakra by setting different options) The infinite moon reader (ÁÑ°ÈôêÊúàË™≠; start a local web server to continuously serve your slides) The summoning technique (download remark.js from the web) You can click the links to know more about them if you want. The jutsu Moon Reader may seem a little evil, but that does not mean your slides are evil. class: center "],["hand-seals-1.html", "Chapter 491 Hand seals (Âç∞)", " Chapter 491 Hand seals (Âç∞) Press h or ? to see the possible ninjutsu you can use in remark.js. class: center, middle "],["thanks-1.html", "Chapter 492 Thanks!", " Chapter 492 Thanks! Slides created via the R package xaringan. The chakra comes from remark.js, knitr, and R Markdown. "],["survival-analysis-in-r-1.html", "Chapter 493 Survival Analysis in R", " Chapter 493 Survival Analysis in R output: html_document: toc: TRUE toc_float: TRUE http://www.emilyzabor.com/tutorials/survival_analysis_in_r_tutorial.html This tutorial provides an introduction to survival analysis, and to conducting a survival analysis in R. This tutorial was originally presented at the Memorial Sloan Kettering Cancer Center R-Presenters series on August 30, 2018. {r eval=FALSE, include=FALSE, echo=TRUE} # load packages library(knitr) library(tidyverse) library(broom) # load my package to access msk_palette with MSK brand colors devtools::install_github( zabore/ezfun ) {r loaddata, echo = TRUE} # load(here::here( data , survival_data_example.RData )) "],["introduction-3.html", "Chapter 494 Introduction 494.1 What is survival data? 494.2 Examples from other fields 494.3 Aliases for survival analysis 494.4 Questions of interest", " Chapter 494 Introduction 494.1 What is survival data? Time-to-event data that consists of a distinct start time and end time. Examples from cancer: Time from surgery to death Time from start of treatment to progression Time from response to recurrence 494.2 Examples from other fields Time-to-event data is common in many fields including, but not limited to: Time from HIV infection to development of AIDS Time to heart attack Time to onset of substance abuse Time to initiation of sexual activity Time to machine malfunction 494.3 Aliases for survival analysis Because survival analysis is common in many other fields, it also goes by other names: Reliability analysis Duration analysis Event history analysis Time-to-event analysis 494.4 Questions of interest The two most common questions I encounter related to survival analysis are: What is the probability of survival to a certain point in time? What is the average survival time? "],["censoring.html", "Chapter 495 Censoring 495.1 What is censoring? 495.2 Censored survival data 495.3 We can incorporate censored data using survival analysis techniques 495.4 Danger of ignoring censoring 495.5 Components of survival data", " Chapter 495 Censoring 495.1 What is censoring? In survival analysis it is common for the exact event time to be unknown, or unobserved, which is called censoring. A subject may be censored due to: Loss to follow-up Withdrawal from study No event by end of fixed study period Specifically these are examples of right censoring. Other common types of censoring include: Left Interval 495.2 Censored survival data When the exact event time is unknown then some patients are censored, and survival analysis methods are required. {r swimmer, eval=FALSE, include=FALSE} # make fake data set.seed(20180809) fkdt &lt;- data_frame(Subject = as.factor(1:10), Years = sample(4:20, 10, replace = T), censor = sample(c( Censor , rep( Event , 2)), 10, replace = T)) # %&gt;% mutate(Subject = fct_reorder(Subject, Years, desc = TRUE)) # plot with shapes to indicate censoring or event ggplot(fkdt, aes(Subject, Years)) + geom_bar(stat = identity , width = 0.5, fill = ezfun::msk_palette( main )[3]) + geom_point(data = fkdt, aes(Subject, Years, color = censor, shape = censor), size = 6) + scale_color_manual(values = ezfun::msk_palette( contrast )[2:3]) + coord_flip() + theme_minimal() + theme(legend.title = element_blank(), legend.position = bottom ) In this example, how would we compute the proportion who are event-free at 10 years? Subjects 2, 3, 5, 6, 8, 9, and 10 were all event-free at 10 years. Subjects 4 and 7 had the event before 10 years. Subject 1 was censored before 10 years, so we don‚Äôt know whether they had the event or not by 10 years. How do we incorporate this subject into our estimate? 495.3 We can incorporate censored data using survival analysis techniques Toy example of a Kaplan-Meier curve for this simple data (details to follow): {r eval=FALSE, include=FALSE} library(survival) plot(survfit(Surv(Years, ifelse(censor == Event , 1, 0)) ~ 1, data = fkdt), xlab = Years , ylab = Survival probability , mark.time = T, conf.int = FALSE) Horizontal lines represent survival duration for the interval An interval is terminated by an event The height of vertical lines show the change in cumulative probability Censored observations, indicated by tick marks, reduces the cumulative survival between intervals 495.4 Danger of ignoring censoring Case study: musicians and mortality Conclusion: Musical genre is associated with early death among musicians. Problem: this graph does not account for the right-censored nature of the data. {r include=FALSE, eval=FALSE, echo = TRUE} # include_graphics( ./img/musician_death_graph.jpg ) 495.5 Components of survival data For subject \\(i\\): Event time \\(T_i\\) Censoring time \\(C_i\\) Event indicator \\(\\delta_i\\): 1 if event observed (i.e. \\(T_i \\leq C_i\\)) 0 if censored (i.e. \\(T_i &gt; C_i\\)) Observed time \\(Y_i = \\min(T_i, C_i)\\) "],["data-example.html", "Chapter 496 Data example 496.1 Research question of interest 496.2 Data structure", " Chapter 496 Data example 496.1 Research question of interest Investigating the obesity paradox in kidney cancer patients. Increased BMI associated with risk of kidney cancer Is increased BMI also associated with worse prognosis among kidney cancer patients? 496.2 Data structure {r # nrow(df) kidney cancer patients Outcome: overall survival Predictor: BMI {r peekdata, eval=FALSE, include=FALSE} df[, c( os_yrs , os , bmi_cat )] %&gt;% head Variables: os_yrs : the observed time \\(Y_i = min(T_i, C_i)\\) os : the event indicator \\(\\delta_i\\) bmi_cat : 1 = Normal BMI &lt; 25, 2 = Overweight BMI 25-30, 3 = Obese BMI &gt; 30 "],["preparing-data-for-analysis.html", "Chapter 497 Preparing data for analysis 497.1 Dates 497.2 Formatting dates - base R 497.3 Formatting dates - lubridate 497.4 Event indicator 497.5 Calculating survival times - base R 497.6 Calculating survival times - lubridate", " Chapter 497 Preparing data for analysis 497.1 Dates Data will often come with start and end dates rather than pre-calculated survival times. Our data example includes the following variables: proc_date : Date of surgery last_status_date : Date of death or last follow-up last_status : Character variable denoting whether the patient is alive, and the cause of death if dead The first step is to make sure these are formatted as dates in R. 497.2 Formatting dates - base R First let‚Äôs look at the current format of our surgery date: {r eval=FALSE, include=FALSE, echo=TRUE} str(df$proc_date) We see this is a character variable in a certain format, but we need it to be formatted as a Date. {r format_date1, eval=FALSE, include=FALSE} df &lt;- df %&gt;% mutate(proc_date_format1 = as.Date(proc_date, format = %d%b%Y )) And after formatting we see that surgery date has Date format now: {r eval=FALSE, include=FALSE, echo=TRUE} str(df$proc_date_format1) Note that in base R the format must include the separator as well as the symbol. e.g. if your date is in format m/d/Y then you would need format = %m/%d/%Y See a full list of date format symbols at https://www.statmethods.net/input/dates.html 497.3 Formatting dates - lubridate We can also use the lubridate package to format dates. Again we look at our original surgery date variable: {r eval=FALSE, include=FALSE, echo=TRUE} str(df$proc_date) And now use the lubridate::dmy function to format this into a Date: {r format_date2, eval=FALSE, include=FALSE} df &lt;- df %&gt;% mutate(proc_date_format2 = lubridate::dmy(proc_date)) And again we see that R now recognizes surgery date as a Date format: {r eval=FALSE, include=FALSE, echo=TRUE} str(df$proc_date_format2) The help page for ?ymd will show all format options. Note that unlike the base R option, the separators do not need to be specified 497.4 Event indicator Most functions used in survival analysis will also require a binary indicator of event that is: 0 for no event 1 for event Currently our data example contains a character variable indicating whehter the patient is alive, and if not indicating the cause of death, so we must create a binary indicator. {r eval=FALSE, include=FALSE, echo=TRUE} table(df$last_status, useNA = &#39;ifany&#39;) df &lt;- df %&gt;% mutate(os_event = ifelse(last_status == Alive , 0, 1)) table(df$os_event) 497.5 Calculating survival times - base R Now that we have our dates formatted, we need to calculate the difference between start and end time in some units, usually months or years. A base R solution to calculate the number of years from surgery to death: {r include=FALSE, eval=FALSE, echo = TRUE} # need to actually properly format the real dates for use df &lt;- df %&gt;% mutate(proc_date = as.Date(proc_date, format = %d%b%Y ), last_status_date = as.Date(last_status_date, format = %d%b%Y )) {r difftime_ex1, eval=FALSE, include=FALSE} df &lt;- df %&gt;% mutate(os_yrs_opt1 = as.numeric(difftime(last_status_date, proc_date, units = days )) / 365.25) {r eval=FALSE, include=FALSE, echo=TRUE} summary(df$os_yrs_opt1) Here we use difftime to calculate the number of days between our two dates and convert it to a numeric value using as.numeric. We then convert to years by dividing by 365.25, the average number of days in a year. Sidenote: the &gt;0 nature of survival times is another reason why standard regression techniques such as linear regression would not be an appropriate way to analyze time-to-event data 497.6 Calculating survival times - lubridate We can also use the lubridate package to calculate the number of years from surgery to death: {r difftime_ex2, message = FALSE, warning = FALSE} library(lubridate) df &lt;- df %&gt;% mutate(os_yrs_opt2 = as.duration(proc_date %--% last_status_date) / dyears(1)) {r eval=FALSE, include=FALSE, echo=TRUE} summary(df$os_yrs_opt2) Here the operator %--% is used to designate a time interval, which is then converted to the number of elapsed seconds using as.duration and finally converted to years by dividing by dyears(1), which gives the number of seconds in a year. If you look closely you will see the result differs slightly from the previous result due to rounding differences, but nothing that would impact our results Note: we need to load the lubridate package using a call to library in order to be able to access the special operators (similar to situation with pipes) "],["analyzing-survival-data.html", "Chapter 498 Analyzing survival data 498.1 Questions of interest 498.2 Creating survival objects 498.3 Estimating survival curves with the Kaplan-Meier method 498.4 Kaplan-Meier plot - base R 498.5 Kaplan-Meier plot - ggsurvplot 498.6 Estimating \\(x\\)-year survival 498.7 \\(x\\)-year survival and the survival curve 498.8 \\(x\\)-year survival is often estimated incorrectly 498.9 Estimating median survival time 498.10 Median survival time and the survival curve 498.11 Median survival is often estimated incorrectly", " Chapter 498 Analyzing survival data 498.1 Questions of interest Recall the questions of interest: What is the probability of surviving to a certain point in time? What is the average survival time? 498.2 Creating survival objects The Kaplan-Meier method is the most common way to estimate survival times and probabilities. It is a non-parametric approach that results in a step function, where there is a step down each time an event occurs. The Surv function from the survival package creates a survival object for use as the response in a model formula. There will be one entry for each subject that is the survival time, which is followed by a + if the subject was censored. Let‚Äôs look at the first 10 observations: {r eval=FALSE, include=FALSE, echo=TRUE} survival::Surv(df$os_yrs, df$os)[1:10] 498.3 Estimating survival curves with the Kaplan-Meier method The survival::survfit function creates survival curves based on a formula. Let‚Äôs generate the overall survival curve for the entire cohort, assign it to object f1, and look at the names of that object: {r eval=FALSE, include=FALSE, echo=TRUE} f1 &lt;- survival::survfit(survival::Surv(os_yrs, os) ~ 1, data = df) names(f1) Some key components of this survfit object that will be used to create survival curves include: time, which contains the start and endpoints of each time interval surv, which contains the survival probability corresponding to each time 498.4 Kaplan-Meier plot - base R Now we plot the survfit object in base R to get the Kaplan-Meier plot: {r eval=FALSE, include=FALSE, echo=TRUE} plot(survival::survfit(survival::Surv(os_yrs, os) ~ 1, data = df), xlab = Years , ylab = Overall survival probability ) The default plot in base R shows the step function (solid line) with associated confidence intervals (dotted lines). Note that the tick marks for censored patients are not shown by default, but could be added using mark.time = TRUE 498.5 Kaplan-Meier plot - ggsurvplot Alternatively, the ggsurvplot function from the survminer package is built on ggplot2, and can be used to create Kaplan-Meier plots: {r eval=FALSE, message=FALSE, warning=FALSE, include=FALSE} survminer::ggsurvplot( fit = survival::survfit(survival::Surv(os_yrs, os) ~ 1, data = df), xlab = Years , ylab = Overall survival probability ) The default plot using survminer::ggsurvplot shows the step function (solid line) with associated confidence bands (shaded area). The tick marks for censored patients are shown by default, somewhat obscuring the line itself in this example, and could be supressed using censor = FALSE 498.6 Estimating \\(x\\)-year survival One quantity often of interest in a survival analysis is the probability of surviving a certain number (\\(x\\)) of years. For example, to estimate the probability of survivng to 5 years, use summary with the times argument: {r eval=FALSE, include=FALSE, echo=TRUE} summary(survival::survfit(survival::Surv(os_yrs, os) ~ 1, data = df), times = 5) We find that the 5-year probability of survival in this study is {r # round(summary(f1, times = 5)$surv * 100)%. The associated lower and upper bounds of the 95% confidence interval are also displayed. 498.7 \\(x\\)-year survival and the survival curve The 5-year survival probability is the point on the y-axis that corresponds to 5 years on the x-axis for the survival curve. {r eval=FALSE, message=FALSE, include=FALSE} survplot1 &lt;- survminer::ggsurvplot(survival::survfit( survival::Surv(os_yrs, os) ~ 1, data = df), xlab = Years , ylab = Overall survival probability , break.x.by = 5, palette = ezfun::msk_palette( main )) + geom_segment(x = 5, xend = 5, y = -1, yend = 0.808, col = 2, lwd = 2) + geom_segment(x = 5, xend = -0.7, y = 0.808, yend = 0.808, arrow = arrow(length = unit(0.3, inches )), col = 2, lwd = 2) survplot1$plot &lt;- survplot1$plot + annotate( text , x = 0.2, y = 0.7, label = 81% , size = 6, col = 2) survplot1 498.8 \\(x\\)-year survival is often estimated incorrectly What happens if you use a naive estimate? {r # table(df$os[df$os_yrs &lt;= 5])[2] of the {r # nrow(df) patients died by 5 years so: \\[\\Big(1 - \\frac{297}{2119}\\Big) \\times 100 = 86\\%\\] You get an incorrect estimate of the 5-year probability of survival when you ignore the fact that {r # table(df$os[df$os_yrs &lt;= 5])[1] patients were censored before 5 years. Recall the correct estimate of the 5-year probability of survival was {r # round(summary(f1, times = 5)$surv * 100)%. 498.9 Estimating median survival time Another quantity often of interest in a survival analysis is the average survival time, which we quantify using the median (survival times are not expected to be normally distributed so the mean is not an appropriate summary). We can obtain this directly from our survfit object: {r eval=FALSE, include=FALSE, echo=TRUE} survival::survfit(survival::Surv(os_yrs, os) ~ 1, data = df) We see the median survival time is {r # round(summary(f1)$table[ median ], 1) years. The lower and upper bounds of the 95% confidence interval are also displayed. 498.10 Median survival time and the survival curve Median survival is the time corresponding to a survival probability of 0.5: {r eval=FALSE, message=FALSE, include=FALSE} survplot2 &lt;- survminer::ggsurvplot(survival::survfit( survival::Surv(os_yrs, os) ~ 1, data = df), xlab = Years , ylab = Overall survival probability , break.x.by = 5, palette = ezfun::msk_palette( main )) + geom_segment(x = -1, xend = 12.6794, y = 0.5, yend = 0.5, col = 2, lwd = 2) + geom_segment(x = 12.6794, xend = 12.6794, y = 0.5, yend = -0.02, arrow = arrow(length = unit(0.3, inches )), col = 2, lwd = 2) survplot2$plot &lt;- survplot2$plot + annotate( text , x = 9.5, y = 0.02, label = 12.7 years , size = 6, col = 2) survplot2 498.11 Median survival is often estimated incorrectly What happens if you use a naive estimate? Summarize the median survival time among the {r # table(df$os)[2] patients who died: {r eval=FALSE, include=FALSE, echo=TRUE} df$os_yrs[df$os == 1] %&gt;% median You get an incorrect estimate of median survival time of {r # round(median(df$os_yrs[df$os == 1]), 1) years when you ignore the fact that censored patients also contribute follow-up time. Recall the correct estimate of median survival time is {r # round(summary(f1)$table[ median ], 1) years. "],["comparing-survival-times-between-groups.html", "Chapter 499 Comparing survival times between groups 499.1 Questions of interest with respect to between-group differences 499.2 Kaplan-Meier plot by group 499.3 \\(x\\)-year survival probability by group 499.4 Log-rank test for between-group significance test", " Chapter 499 Comparing survival times between groups 499.1 Questions of interest with respect to between-group differences Is there a difference in survival probability between groups? From our example: does the probability of survival differ according to BMI among kidney cancer patients? 499.2 Kaplan-Meier plot by group We can add a covariate to the right-hand side of the survival::survfit object to obtain a stratified Kaplan-Meier plot. Let‚Äôs also look at some other customization we can do with survminer::ggsurvplot. {r fig.height = 6} survminer::ggsurvplot( fit = survival::survfit(survival::Surv(os_yrs, os) ~ bmi_cat, data = df), xlab = Years , ylab = Overall survival probability , legend.title = BMI , legend.labs = c( Normal , Overweight , Obese ), break.x.by = 5, palette = ezfun::msk_palette( contrast ), censor = FALSE, risk.table = TRUE, risk.table.y.text = FALSE) By looking at the curves, we can see that normal BMI patients have the lowest overall survival probability, followed by overweight BMI patients and obese BMI patients. The risk table below the plot shows us the number of patients at risk at certain time points, which can give an idea of how much information is being used to calculate the estimates at each time 499.3 \\(x\\)-year survival probability by group As before, we can get an estimate of, for example, 5-year survival by using summary with the times argument in our survival::survfit object: {r eval=FALSE, include=FALSE, echo=TRUE} summary(survival::survfit(survival::Surv(os_yrs, os) ~ bmi_cat, data = df), times = 5) To summarize: {r eval=FALSE, include=FALSE} library(survival) tab &lt;- ezfun::uvsurv(NULL, bmi_cat , os , os_yrs , 5, df)[-1, c(1, 3)] colnames(tab) &lt;- c( BMI , 5-year estimate (95% CI) ) tab[, 1] &lt;- c( Normal , Overweight , Obese ) kable(tab, row.names = FALSE) 499.4 Log-rank test for between-group significance test We can conduct between-group significance tests using a log-rank test. The log-rank test equally weights observations over the entire follow-up time and is the most common way to compare survival times between groups. There are versions that more heavily weight the early or late follow-up that could be more appropriate depending on the research question. We get the log-rank p-value using the survival::survdiff function: {r eval=FALSE, include=FALSE, echo=TRUE} survival::survdiff(survival::Surv(df$os_yrs, df$os) ~ df$bmi_cat) And we see that the p-value is &lt;.001, indicating a significant difference in overall survival according to BMI. "],["regression-2.html", "Chapter 500 Regression 500.1 The Cox regression model 500.2 Cox regression example using a single covariate 500.3 Hazard ratios 500.4 Summary", " Chapter 500 Regression 500.1 The Cox regression model We may want to quantify an effect size for a single variable, or include more than one variable into a regression model to account for the effects of multiple variables. The Cox regression model is a semi-parametric model that can be used to fit univariable and multivariable regression models that have survival outcomes. Some key assumptions of the model: non-informative censoring proportional hazards Note: parametric regression models for survival outcomes are also available, but I won‚Äôt cover them in detail here. 500.2 Cox regression example using a single covariate We can fit regression models for survival data using the survival::coxph function, which takes a survival::Surv object on the left hand side and has standard syntax for regression formulas in R on the right hand side. {r eval = FALSE} survival::coxph(survival::Surv(os_yrs, os) ~ factor(bmi_cat), data = df) We can see a tidy version of the output using the tidy function from the broom package: {r eval=FALSE, include=FALSE, echo=TRUE} broom::tidy(survival::coxph(survival::Surv(os_yrs, os) ~ factor(bmi_cat), data = df)) 500.3 Hazard ratios The quantity of interest from a Cox regression model is a hazard ratio (HR). If you have a regression parameter \\(\\beta\\) (from column estimate in our survival::coxph) then HR = \\(\\exp(\\beta)\\). For example, from our example we obtain the regression parameter \\(\\beta_1=-0.4441733\\) for obese vs normal BMI, so we have HR = \\(\\exp(\\beta_1)=0.64\\). A HR &lt; 1 indicates reduced hazard of death whereas a HR &gt; 1 indicates an increased hazard of death. So we would say that obese BMI kidney cancer patients have 0.64 times reduced hazard of death as compared to normal BMI kidney cancer patients. 500.4 Summary Time-to-event data is common Survival analysis techniques are required to account for censored data The survival package provides tools for survival analysis, including the Surv and survfit functions The survminer package allows for customization of Kaplan-Meier plots based on ggplot2 Between-group comparisons can be made with the log-rank test using survival::survdiff Multiavariable Cox regression analysis can be accomplished using survival::coxph "],["survival-analysis-in-r-2.html", "Chapter 501 Survival Analysis in R", " Chapter 501 Survival Analysis in R output: html_document: toc: TRUE toc_float: TRUE This tutorial provides an introduction to survival analysis, and to conducting a survival analysis in R. This tutorial was originally presented at the Memorial Sloan Kettering Cancer Center R-Presenters series on August 30, 2018. {r eval=FALSE, include=FALSE, echo=TRUE} # load packages library(knitr) library(tidyverse) library(broom) # load my package to access msk_palette with MSK brand colors devtools::install_github( zabore/ezfun ) {r loaddata 2, echo = TRUE} # load(here::here( data , survival_data_example.RData )) "],["introduction-4.html", "Chapter 502 Introduction 502.1 What is survival data? 502.2 Examples from other fields 502.3 Aliases for survival analysis 502.4 Questions of interest", " Chapter 502 Introduction 502.1 What is survival data? Time-to-event data that consists of a distinct start time and end time. Examples from cancer: Time from surgery to death Time from start of treatment to progression Time from response to recurrence 502.2 Examples from other fields Time-to-event data is common in many fields including, but not limited to: Time from HIV infection to development of AIDS Time to heart attack Time to onset of substance abuse Time to initiation of sexual activity Time to machine malfunction 502.3 Aliases for survival analysis Because survival analysis is common in many other fields, it also goes by other names: Reliability analysis Duration analysis Event history analysis Time-to-event analysis 502.4 Questions of interest The two most common questions I encounter related to survival analysis are: What is the probability of survival to a certain point in time? What is the average survival time? "],["censoring-1.html", "Chapter 503 Censoring 503.1 What is censoring? 503.2 Censored survival data 503.3 We can incorporate censored data using survival analysis techniques 503.4 Danger of ignoring censoring 503.5 Components of survival data", " Chapter 503 Censoring 503.1 What is censoring? In survival analysis it is common for the exact event time to be unknown, or unobserved, which is called censoring. A subject may be censored due to: Loss to follow-up Withdrawal from study No event by end of fixed study period Specifically these are examples of right censoring. Other common types of censoring include: Left Interval 503.2 Censored survival data When the exact event time is unknown then some patients are censored, and survival analysis methods are required. {r swimmer 2, eval=FALSE, include=FALSE} # make fake data set.seed(20180809) fkdt &lt;- data_frame(Subject = as.factor(1:10), Years = sample(4:20, 10, replace = T), censor = sample(c( Censor , rep( Event , 2)), 10, replace = T)) # %&gt;% mutate(Subject = fct_reorder(Subject, Years, desc = TRUE)) # plot with shapes to indicate censoring or event ggplot(fkdt, aes(Subject, Years)) + geom_bar(stat = identity , width = 0.5, fill = ezfun::msk_palette( main )[3]) + geom_point(data = fkdt, aes(Subject, Years, color = censor, shape = censor), size = 6) + scale_color_manual(values = ezfun::msk_palette( contrast )[2:3]) + coord_flip() + theme_minimal() + theme(legend.title = element_blank(), legend.position = bottom ) In this example, how would we compute the proportion who are event-free at 10 years? Subjects 2, 3, 5, 6, 8, 9, and 10 were all event-free at 10 years. Subjects 4 and 7 had the event before 10 years. Subject 1 was censored before 10 years, so we don‚Äôt know whether they had the event or not by 10 years. How do we incorporate this subject into our estimate? 503.3 We can incorporate censored data using survival analysis techniques Toy example of a Kaplan-Meier curve for this simple data (details to follow): {r eval=FALSE, include=FALSE} library(survival) plot(survfit(Surv(Years, ifelse(censor == Event , 1, 0)) ~ 1, data = fkdt), xlab = Years , ylab = Survival probability , mark.time = T, conf.int = FALSE) Horizontal lines represent survival duration for the interval An interval is terminated by an event The height of vertical lines show the change in cumulative probability Censored observations, indicated by tick marks, reduces the cumulative survival between intervals 503.4 Danger of ignoring censoring Case study: musicians and mortality Conclusion: Musical genre is associated with early death among musicians. Problem: this graph does not account for the right-censored nature of the data. {r include=FALSE, eval=FALSE, echo = TRUE} # include_graphics( ./img/musician_death_graph.jpg ) 503.5 Components of survival data For subject \\(i\\): Event time \\(T_i\\) Censoring time \\(C_i\\) Event indicator \\(\\delta_i\\): 1 if event observed (i.e. \\(T_i \\leq C_i\\)) 0 if censored (i.e. \\(T_i &gt; C_i\\)) Observed time \\(Y_i = \\min(T_i, C_i)\\) "],["data-example-1.html", "Chapter 504 Data example 504.1 Research question of interest 504.2 Data structure", " Chapter 504 Data example 504.1 Research question of interest Investigating the obesity paradox in kidney cancer patients. Increased BMI associated with risk of kidney cancer Is increased BMI also associated with worse prognosis among kidney cancer patients? 504.2 Data structure {r # nrow(df) kidney cancer patients Outcome: overall survival Predictor: BMI {r peekdata 2, eval=FALSE, include=FALSE} df[, c( os_yrs , os , bmi_cat )] %&gt;% head Variables: os_yrs : the observed time \\(Y_i = min(T_i, C_i)\\) os : the event indicator \\(\\delta_i\\) bmi_cat : 1 = Normal BMI &lt; 25, 2 = Overweight BMI 25-30, 3 = Obese BMI &gt; 30 "],["preparing-data-for-analysis-1.html", "Chapter 505 Preparing data for analysis 505.1 Dates 505.2 Formatting dates - base R 505.3 Formatting dates - lubridate 505.4 Event indicator 505.5 Calculating survival times - base R 505.6 Calculating survival times - lubridate", " Chapter 505 Preparing data for analysis 505.1 Dates Data will often come with start and end dates rather than pre-calculated survival times. Our data example includes the following variables: proc_date : Date of surgery last_status_date : Date of death or last follow-up last_status : Character variable denoting whether the patient is alive, and the cause of death if dead The first step is to make sure these are formatted as dates in R. 505.2 Formatting dates - base R First let‚Äôs look at the current format of our surgery date: {r eval=FALSE, include=FALSE, echo=TRUE} str(df$proc_date) We see this is a character variable in a certain format, but we need it to be formatted as a Date. {r format_date1 2, eval=FALSE, include=FALSE} df &lt;- df %&gt;% mutate(proc_date_format1 = as.Date(proc_date, format = %d%b%Y )) And after formatting we see that surgery date has Date format now: {r eval=FALSE, include=FALSE, echo=TRUE} str(df$proc_date_format1) Note that in base R the format must include the separator as well as the symbol. e.g. if your date is in format m/d/Y then you would need format = %m/%d/%Y See a full list of date format symbols at https://www.statmethods.net/input/dates.html 505.3 Formatting dates - lubridate We can also use the lubridate package to format dates. Again we look at our original surgery date variable: {r eval=FALSE, include=FALSE, echo=TRUE} str(df$proc_date) And now use the lubridate::dmy function to format this into a Date: {r format_date2 2} df &lt;- df %&gt;% mutate(proc_date_format2 = lubridate::dmy(proc_date)) And again we see that R now recognizes surgery date as a Date format: {r eval=FALSE, include=FALSE, echo=TRUE} str(df$proc_date_format2) The help page for ?ymd will show all format options. Note that unlike the base R option, the separators do not need to be specified 505.4 Event indicator Most functions used in survival analysis will also require a binary indicator of event that is: 0 for no event 1 for event Currently our data example contains a character variable indicating whehter the patient is alive, and if not indicating the cause of death, so we must create a binary indicator. {r eval=FALSE, include=FALSE, echo=TRUE} table(df$last_status, useNA = &#39;ifany&#39;) df &lt;- df %&gt;% mutate(os_event = ifelse(last_status == Alive , 0, 1)) table(df$os_event) 505.5 Calculating survival times - base R Now that we have our dates formatted, we need to calculate the difference between start and end time in some units, usually months or years. A base R solution to calculate the number of years from surgery to death: {r include=FALSE, eval=FALSE, echo = TRUE} # need to actually properly format the real dates for use df &lt;- df %&gt;% mutate(proc_date = as.Date(proc_date, format = %d%b%Y ), last_status_date = as.Date(last_status_date, format = %d%b%Y )) {r difftime_ex1 2} df &lt;- df %&gt;% mutate(os_yrs_opt1 = as.numeric(difftime(last_status_date, proc_date, units = days )) / 365.25) {r eval=FALSE, include=FALSE, echo=TRUE} summary(df$os_yrs_opt1) Here we use difftime to calculate the number of days between our two dates and convert it to a numeric value using as.numeric. We then convert to years by dividing by 365.25, the average number of days in a year. Sidenote: the &gt;0 nature of survival times is another reason why standard regression techniques such as linear regression would not be an appropriate way to analyze time-to-event data 505.6 Calculating survival times - lubridate We can also use the lubridate package to calculate the number of years from surgery to death: {r difftime_ex2 2, message = FALSE, warning = FALSE} library(lubridate) df &lt;- df %&gt;% mutate(os_yrs_opt2 = as.duration(proc_date %--% last_status_date) / dyears(1)) {r eval=FALSE, include=FALSE, echo=TRUE} summary(df$os_yrs_opt2) Here the operator %--% is used to designate a time interval, which is then converted to the number of elapsed seconds using as.duration and finally converted to years by dividing by dyears(1), which gives the number of seconds in a year. If you look closely you will see the result differs slightly from the previous result due to rounding differences, but nothing that would impact our results Note: we need to load the lubridate package using a call to library in order to be able to access the special operators (similar to situation with pipes) "],["analyzing-survival-data-1.html", "Chapter 506 Analyzing survival data 506.1 Questions of interest 506.2 Creating survival objects 506.3 Estimating survival curves with the Kaplan-Meier method 506.4 Kaplan-Meier plot - base R 506.5 Kaplan-Meier plot - ggsurvplot 506.6 Estimating \\(x\\)-year survival 506.7 \\(x\\)-year survival and the survival curve 506.8 \\(x\\)-year survival is often estimated incorrectly 506.9 Estimating median survival time 506.10 Median survival time and the survival curve 506.11 Median survival is often estimated incorrectly", " Chapter 506 Analyzing survival data 506.1 Questions of interest Recall the questions of interest: What is the probability of surviving to a certain point in time? What is the average survival time? 506.2 Creating survival objects The Kaplan-Meier method is the most common way to estimate survival times and probabilities. It is a non-parametric approach that results in a step function, where there is a step down each time an event occurs. The Surv function from the survival package creates a survival object for use as the response in a model formula. There will be one entry for each subject that is the survival time, which is followed by a + if the subject was censored. Let‚Äôs look at the first 10 observations: {r eval=FALSE, include=FALSE, echo=TRUE} survival::Surv(df$os_yrs, df$os)[1:10] 506.3 Estimating survival curves with the Kaplan-Meier method The survival::survfit function creates survival curves based on a formula. Let‚Äôs generate the overall survival curve for the entire cohort, assign it to object f1, and look at the names of that object: {r eval=FALSE, include=FALSE, echo=TRUE} f1 &lt;- survival::survfit(survival::Surv(os_yrs, os) ~ 1, data = df) names(f1) Some key components of this survfit object that will be used to create survival curves include: time, which contains the start and endpoints of each time interval surv, which contains the survival probability corresponding to each time 506.4 Kaplan-Meier plot - base R Now we plot the survfit object in base R to get the Kaplan-Meier plot: {r eval=FALSE, include=FALSE, echo=TRUE} plot(survival::survfit(survival::Surv(os_yrs, os) ~ 1, data = df), xlab = Years , ylab = Overall survival probability ) The default plot in base R shows the step function (solid line) with associated confidence intervals (dotted lines). Note that the tick marks for censored patients are not shown by default, but could be added using mark.time = TRUE 506.5 Kaplan-Meier plot - ggsurvplot Alternatively, the ggsurvplot function from the survminer package is built on ggplot2, and can be used to create Kaplan-Meier plots: {r, message = FALSE, warning = FALSE} survminer::ggsurvplot( fit = survival::survfit(survival::Surv(os_yrs, os) ~ 1, data = df), xlab = Years , ylab = Overall survival probability ) The default plot using survminer::ggsurvplot shows the step function (solid line) with associated confidence bands (shaded area). The tick marks for censored patients are shown by default, somewhat obscuring the line itself in this example, and could be supressed using censor = FALSE 506.6 Estimating \\(x\\)-year survival One quantity often of interest in a survival analysis is the probability of surviving a certain number (\\(x\\)) of years. For example, to estimate the probability of survivng to 5 years, use summary with the times argument: {r eval=FALSE, include=FALSE, echo=TRUE} summary(survival::survfit(survival::Surv(os_yrs, os) ~ 1, data = df), times = 5) We find that the 5-year probability of survival in this study is {r # round(summary(f1, times = 5)$surv * 100)%. The associated lower and upper bounds of the 95% confidence interval are also displayed. 506.7 \\(x\\)-year survival and the survival curve The 5-year survival probability is the point on the y-axis that corresponds to 5 years on the x-axis for the survival curve. {r, message = FALSE, echo = TRUE} survplot1 &lt;- survminer::ggsurvplot(survival::survfit( survival::Surv(os_yrs, os) ~ 1, data = df), xlab = Years , ylab = Overall survival probability , break.x.by = 5, palette = ezfun::msk_palette( main )) + geom_segment(x = 5, xend = 5, y = -1, yend = 0.808, col = 2, lwd = 2) + geom_segment(x = 5, xend = -0.7, y = 0.808, yend = 0.808, arrow = arrow(length = unit(0.3, inches )), col = 2, lwd = 2) survplot1$plot &lt;- survplot1$plot + annotate( text , x = 0.2, y = 0.7, label = 81% , size = 6, col = 2) survplot1 506.8 \\(x\\)-year survival is often estimated incorrectly What happens if you use a naive estimate? {r # table(df$os[df$os_yrs &lt;= 5])[2] of the {r # nrow(df) patients died by 5 years so: \\[\\Big(1 - \\frac{297}{2119}\\Big) \\times 100 = 86\\%\\] You get an incorrect estimate of the 5-year probability of survival when you ignore the fact that {r # table(df$os[df$os_yrs &lt;= 5])[1] patients were censored before 5 years. Recall the correct estimate of the 5-year probability of survival was {r # round(summary(f1, times = 5)$surv * 100)%. 506.9 Estimating median survival time Another quantity often of interest in a survival analysis is the average survival time, which we quantify using the median (survival times are not expected to be normally distributed so the mean is not an appropriate summary). We can obtain this directly from our survfit object: {r eval=FALSE, include=FALSE, echo=TRUE} survival::survfit(survival::Surv(os_yrs, os) ~ 1, data = df) We see the median survival time is {r # round(summary(f1)$table[ median ], 1) years. The lower and upper bounds of the 95% confidence interval are also displayed. 506.10 Median survival time and the survival curve Median survival is the time corresponding to a survival probability of 0.5: {r, message = FALSE, echo = TRUE} survplot2 &lt;- survminer::ggsurvplot(survival::survfit( survival::Surv(os_yrs, os) ~ 1, data = df), xlab = Years , ylab = Overall survival probability , break.x.by = 5, palette = ezfun::msk_palette( main )) + geom_segment(x = -1, xend = 12.6794, y = 0.5, yend = 0.5, col = 2, lwd = 2) + geom_segment(x = 12.6794, xend = 12.6794, y = 0.5, yend = -0.02, arrow = arrow(length = unit(0.3, inches )), col = 2, lwd = 2) survplot2$plot &lt;- survplot2$plot + annotate( text , x = 9.5, y = 0.02, label = 12.7 years , size = 6, col = 2) survplot2 506.11 Median survival is often estimated incorrectly What happens if you use a naive estimate? Summarize the median survival time among the {r # table(df$os)[2] patients who died: {r eval=FALSE, include=FALSE, echo=TRUE} df$os_yrs[df$os == 1] %&gt;% median You get an incorrect estimate of median survival time of {r # round(median(df$os_yrs[df$os == 1]), 1) years when you ignore the fact that censored patients also contribute follow-up time. Recall the correct estimate of median survival time is {r # round(summary(f1)$table[ median ], 1) years. "],["comparing-survival-times-between-groups-1.html", "Chapter 507 Comparing survival times between groups 507.1 Questions of interest with respect to between-group differences 507.2 Kaplan-Meier plot by group 507.3 \\(x\\)-year survival probability by group 507.4 Log-rank test for between-group significance test", " Chapter 507 Comparing survival times between groups 507.1 Questions of interest with respect to between-group differences Is there a difference in survival probability between groups? From our example: does the probability of survival differ according to BMI among kidney cancer patients? 507.2 Kaplan-Meier plot by group We can add a covariate to the right-hand side of the survival::survfit object to obtain a stratified Kaplan-Meier plot. Let‚Äôs also look at some other customization we can do with survminer::ggsurvplot. {r fig.height = 6} survminer::ggsurvplot( fit = survival::survfit(survival::Surv(os_yrs, os) ~ bmi_cat, data = df), xlab = Years , ylab = Overall survival probability , legend.title = BMI , legend.labs = c( Normal , Overweight , Obese ), break.x.by = 5, palette = ezfun::msk_palette( contrast ), censor = FALSE, risk.table = TRUE, risk.table.y.text = FALSE) By looking at the curves, we can see that normal BMI patients have the lowest overall survival probability, followed by overweight BMI patients and obese BMI patients. The risk table below the plot shows us the number of patients at risk at certain time points, which can give an idea of how much information is being used to calculate the estimates at each time 507.3 \\(x\\)-year survival probability by group As before, we can get an estimate of, for example, 5-year survival by using summary with the times argument in our survival::survfit object: {r eval=FALSE, include=FALSE, echo=TRUE} summary(survival::survfit(survival::Surv(os_yrs, os) ~ bmi_cat, data = df), times = 5) To summarize: {r eval=FALSE, include=FALSE} library(survival) tab &lt;- ezfun::uvsurv(NULL, bmi_cat , os , os_yrs , 5, df)[-1, c(1, 3)] colnames(tab) &lt;- c( BMI , 5-year estimate (95% CI) ) tab[, 1] &lt;- c( Normal , Overweight , Obese ) kable(tab, row.names = FALSE) 507.4 Log-rank test for between-group significance test We can conduct between-group significance tests using a log-rank test. The log-rank test equally weights observations over the entire follow-up time and is the most common way to compare survival times between groups. There are versions that more heavily weight the early or late follow-up that could be more appropriate depending on the research question. We get the log-rank p-value using the survival::survdiff function: {r eval=FALSE, include=FALSE, echo=TRUE} survival::survdiff(survival::Surv(df$os_yrs, df$os) ~ df$bmi_cat) And we see that the p-value is &lt;.001, indicating a significant difference in overall survival according to BMI. "],["regression-3.html", "Chapter 508 Regression 508.1 The Cox regression model 508.2 Cox regression example using a single covariate 508.3 Hazard ratios 508.4 Summary 508.5 Survival Analysis in R", " Chapter 508 Regression 508.1 The Cox regression model We may want to quantify an effect size for a single variable, or include more than one variable into a regression model to account for the effects of multiple variables. The Cox regression model is a semi-parametric model that can be used to fit univariable and multivariable regression models that have survival outcomes. Some key assumptions of the model: non-informative censoring proportional hazards Note: parametric regression models for survival outcomes are also available, but I won‚Äôt cover them in detail here. 508.2 Cox regression example using a single covariate We can fit regression models for survival data using the survival::coxph function, which takes a survival::Surv object on the left hand side and has standard syntax for regression formulas in R on the right hand side. {r eval = FALSE} survival::coxph(survival::Surv(os_yrs, os) ~ factor(bmi_cat), data = df) We can see a tidy version of the output using the tidy function from the broom package: {r eval=FALSE, include=FALSE, echo=TRUE} broom::tidy(survival::coxph(survival::Surv(os_yrs, os) ~ factor(bmi_cat), data = df)) 508.3 Hazard ratios The quantity of interest from a Cox regression model is a hazard ratio (HR). If you have a regression parameter \\(\\beta\\) (from column estimate in our survival::coxph) then HR = \\(\\exp(\\beta)\\). For example, from our example we obtain the regression parameter \\(\\beta_1=-0.4441733\\) for obese vs normal BMI, so we have HR = \\(\\exp(\\beta_1)=0.64\\). A HR &lt; 1 indicates reduced hazard of death whereas a HR &gt; 1 indicates an increased hazard of death. So we would say that obese BMI kidney cancer patients have 0.64 times reduced hazard of death as compared to normal BMI kidney cancer patients. 508.4 Summary Time-to-event data is common Survival analysis techniques are required to account for censored data The survival package provides tools for survival analysis, including the Surv and survfit functions The survminer package allows for customization of Kaplan-Meier plots based on ggplot2 Between-group comparisons can be made with the log-rank test using survival::survdiff Multiavariable Cox regression analysis can be accomplished using survival::coxph 508.5 Survival Analysis in R https://rviews.rstudio.com/2017/09/25/survival-analysis-with-r/ {r eval=FALSE, include=FALSE, echo=TRUE} library(survival) library(ranger) library(ggplot2) library(dplyr) library(ggfortify) {r eval=FALSE, include=FALSE, echo=TRUE} data(veteran) head(veteran) 508.5.1 Censored Data {r eval=FALSE, include=FALSE, echo=TRUE} km &lt;- with(veteran, Surv(time, status)) head(km,80) 508.5.2 Kaplan Meier {r eval=FALSE, include=FALSE, echo=TRUE} km_fit &lt;- survfit(Surv(time, status) ~ 1, data = veteran) km_fit 508.5.3 Life Table {r eval=FALSE, include=FALSE, echo=TRUE} summary(km_fit, times = c(1,30,60,90*(1:10))) 508.5.4 KM Graph Overall {r eval=FALSE, include=FALSE, echo=TRUE} plot(km_fit, xlab= Days , main = &#39;Kaplan Meyer Plot&#39;) #base graphics is always ready {r eval=FALSE, include=FALSE, echo=TRUE} autoplot(km_fit) 508.5.5 KM per treatment group {r eval=FALSE, include=FALSE, echo=TRUE} km_trt_fit &lt;- survfit(Surv(time, status) ~ trt, data = veteran) km_trt_fit 508.5.6 KM Graph per treatment group {r eval=FALSE, include=FALSE, echo=TRUE} autoplot(km_trt_fit) "],["survminer-2.html", "Chapter 509 survminer", " Chapter 509 survminer http://www.sthda.com/english/rpkgs/survminer/#uber-platinum-customized-survival-curves "],["openintro-statistics.html", "Chapter 510 OpenIntro Statistics", " Chapter 510 OpenIntro Statistics https://www.openintro.org/stat/surv.php https://www.openintro.org/download.php?file=survival_analysis_in_R&amp;referrer=/stat/surv.php https://www.openintro.org/download.php?file=survival_analysis_in_R_code&amp;referrer=/stat/surv.php https://www.openintro.org/download.php?file=survival_analysis_in_R_code_df-cp&amp;referrer=/stat/surv.php {r eval=FALSE, include=FALSE, echo=TRUE} # install.packages( OIsurv ) {r eval=FALSE, include=FALSE, echo=TRUE} # library(OIsurv) library(survival) library(splines) library(KMsurv) {r eval=FALSE, include=FALSE, echo=TRUE} library(help=KMsurv) {r eval=FALSE, include=FALSE, echo=TRUE} data(aids) aids {r eval=FALSE, include=FALSE, echo=TRUE} data(tongue) tongue {r eval=FALSE, include=FALSE, echo=TRUE} my.surv.object &lt;- Surv(tongue$time[tongue$type==1], tongue$delta[tongue$type==1]) my.surv.object {r eval=FALSE, include=FALSE, echo=TRUE} data(psych) psych my.surv.object &lt;- Surv(psych$age, psych$age + psych$time, psych$death) my.surv.object {r eval=FALSE, include=FALSE, echo=TRUE} rm(list=ls()) #===&gt; loading packages and such &lt;===# #install.packages( OIsurv ) library(OIsurv) data(aids) aids attach(aids) infect detach(aids) aids$infect #===&gt; survival object &lt;===# data(tongue); attach(tongue) # the following will not affect computations # create a subset for just the first group by using [type==1] my.surv.object &lt;- Surv(time[type==1], delta[type==1]) my.surv.object detach(tongue) data(psych); attach(psych) my.surv.object &lt;- Surv(age, age+time, death) my.surv.object detach(psych) #===&gt; K-M Estimate &lt;===# data(tongue); attach(tongue) my.surv &lt;- Surv(time[type==1], delta[type==1]) survfit(my.surv ~ 1) my.fit &lt;- survfit(my.surv ~ 1) summary(my.fit)$surv # returns the Kaplan-Meier estimate at each t_i summary(my.fit)$time # {t_i} summary(my.fit)$n.risk # {Y_i} summary(my.fit)$n.event # {d_i} summary(my.fit)$std.err # standard error of the K-M estimate at {t_i} summary(my.fit)$lower # lower pointwise estimates (alternatively, $upper) str(my.fit) # full summary of the my.fit object str(summary(my.fit)) # full summary of the my.fit object pdf( ../figures/kmPlot.pdf , 7, 4.5) plot(my.fit, main= Kaplan-Meier estimate with 95% confidence bounds , xlab= time , ylab= survival function ) dev.off() my.fit1 &lt;- survfit( Surv(time, delta) ~ type ) # here the key is type detach(tongue) #===&gt; confidence bands &lt;===# data(bmt); attach(bmt) my.surv &lt;- Surv(t2[group==1], d3[group==1]) my.cb &lt;- confBands(my.surv, confLevel=0.95, type= hall ) pdf( ../figures/confBand.pdf , 8, 5) plot(survfit(my.surv ~ 1), xlim=c(100, 600), xlab= time , ylab= Estimated Survival Function , main= Reproducing Confidence Bands for Example 4.2 in Klein/Moeschberger ) lines(my.cb$time, my.cb$lower, lty=3, type= s ) lines(my.cb$time, my.cb$upper, lty=3, type= s ) legend(100, 0.3, legend=c( K-M survival estimate , pointwise intervals , confidence bands ), lty=1:3) dev.off() detach(bmt) #===&gt; cumulative hazard &lt;===# data(tongue); attach(tongue) my.surv &lt;- Surv(time[type==1], delta[type==1]) my.fit &lt;- summary(survfit(my.surv ~ 1)) H.hat &lt;- -log(my.fit$surv) H.hat &lt;- c(H.hat, tail(H.hat, 1)) h.sort.of &lt;- my.fit$n.event / my.fit$n.risk H.tilde &lt;- cumsum(h.sort.of) H.tilde &lt;- c(H.tilde, tail(H.tilde, 1)) pdf( ../figures/cumHazard.pdf , 6, 4) plot(c(my.fit$time, 250), H.hat, xlab= time , ylab= cumulative hazard , main= comparing cumulative hazards , ylim=range(c(H.hat, H.tilde)), type= s ) points(c(my.fit$time, 250), H.tilde, lty=2, type= s ) legend( topleft , legend=c( H.hat , H.tilde ), lty=1:2) dev.off() detach(tongue) #===&gt; mean/median &lt;===# data(drug6mp); attach(drug6mp) my.surv &lt;- Surv(t1, rep(1, 21)) # all placebo patients observed survfit(my.surv ~ 1) print(survfit(my.surv ~ 1), print.rmean=TRUE) detach(drug6mp) #===&gt; test for 2+ samples &lt;===# data(btrial); attach(btrial) survdiff(Surv(time, death) ~ im) # output omitted survdiff(Surv(time, death) ~ im, rho=1) # output omitted detach(btrial) #===&gt; coxph, time-independent &lt;===# data(burn); attach(burn) my.surv &lt;- Surv(T1, D1) coxph.fit &lt;- coxph(my.surv ~ Z1 + as.factor(Z11), method= breslow ) coxph.fit co &lt;- coxph.fit$coefficients # may use coxph.fit$coeff instead va &lt;- coxph.fit$var # I^(-1), estimated cov matrix of the estimates ll &lt;- coxph.fit$loglik # log-likelihood for alt and null MLEs, resp. my.survfit.object &lt;- survfit(coxph.fit) hold &lt;- survfit(my.surv ~ 1) #source( http://www.stat.ucla.edu/~david/teac/surv/local-coxph-test.R ) coxph.fit C &lt;- matrix(c(0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1), nrow=3, byrow=TRUE) d &lt;- rep(0, 3) t1 &lt;- C %*% co - d t2 &lt;- C %*% va %*% t(C) XW2 &lt;- c(t(t1) %*% solve(t2) %*% t1) pchisq(XW2, 3, lower.tail=FALSE) #local.coxph.test(coxph.fit, 2:4) my.survfit.object &lt;- survfit(coxph.fit) detach(burn) #===&gt; coxph, time-dependent &lt;===# data(relapse) relapse N &lt;- dim(relapse)[1] t1 &lt;- rep(0, N+sum(!is.na(relapse$int))) # initialize start time at 0 t2 &lt;- rep(-1, length(t1)) # build vector for end times d &lt;- rep(-1, length(t1)) # whether event was censored g &lt;- rep(-1, length(t1)) # gender covariate i &lt;- rep(FALSE, length(t1)) # initialize intervention at FALSE j &lt;- 1 for(ii in 1:dim(relapse)[1]){ if(is.na(relapse$int[ii])){ # no intervention, copy survival record t2[j] &lt;- relapse$event[ii] d[j] &lt;- relapse$delta[ii] g[j] &lt;- relapse$gender[ii] j &lt;- j+1 } else { # intervention, split records g[j+0:1] &lt;- relapse$gender[ii] # gender is same for each time d[j] &lt;- 0 # no relapse observed pre-intervention d[j+1] &lt;- relapse$delta[ii] # relapse occur post-intervention? i[j+1] &lt;- TRUE # intervention covariate, post-intervention t2[j] &lt;- relapse$int[ii]-1 # end of pre-intervention t1[j+1] &lt;- relapse$int[ii]-1 # start of post-intervention t2[j+1] &lt;- relapse$event[ii] # end of post-intervention j &lt;- j+2 # two records added } } mySurv &lt;- Surv(t1, t2, d) # pg 3 discusses left-trunc. right-cens. data myCPH &lt;- coxph(mySurv ~ g + i) #data(burn); attach(burn) ##source( http://www.stat.ucla.edu/~david/teac/surv/time-dep-coxph.R ) #td.coxph &lt;- timeDepCoxph(burn, T1 , D1 , 2:4, Z1 , verbose=FALSE) #td.coxph # some model output is omitted for brevity #detach(burn) #===&gt; AFT models &lt;===# data(larynx) attach(larynx) srFit &lt;- survreg(Surv(time, delta) ~ as.factor(stage) + age, dist= weibull ) summary(srFit) srFitExp &lt;- survreg(Surv(time, delta) ~ as.factor(stage) + age, dist= exponential ) summary(srFitExp) srFitExp$coeff # covariate coefficients srFitExp$icoef # intercept and scale coefficients srFitExp$var # variance-covariance matrix srFitExp$loglik # log-likelihood srFit$scale # not using srFitExp (defaulted to 1) detach(larynx) {r eval=FALSE, include=FALSE, echo=TRUE} # Posted with permission of the code author: # Beau Benjamin Bruce # Author email: bbbruce@emory.edu # # This code is available under GPL-2 # For license information, see # http://cran.r-project.org/web/licenses/GPL-2 library(KMsurv) data(burn) #=====&gt; Convert a data frame to a counting process version &lt;=====# #=====&gt; Allows for time dependent variables to be introduced &lt;=====# df.cp = function(data,t.var,status.var,covars=setdiff(names(data),c(t.var,status.var))) { # data: data frame that represents the dataset # t.var: the column name of data that represents the survival time variable # status.var: the column name of data that represents the status variable # covars: list other covariates to retain # Returned object: a data frame that breaks each event down to a counting process # sorted times, append to 0 t.sort &lt;- c(0,sort(unlist(unique(data[t.var])))) # for each data point find times less than or equal to the obs&#39; time t.list &lt;- lapply(unlist(data[t.var]),function(x) t.sort[t.sort&lt;=x]) # create a list of datasets with covariates and all relevant start/stop times # use with to set the environment to the correct list item df.list &lt;- lapply(seq_along(t.list),function(i) cbind(with(list(x=t.list[[i]]), # start by removing one from end of x, stop by removing first of x # include the status variable in the dataframe - helpful later data.frame(start=head(x,-1),stop=tail(x,-1),data[i,c(status.var,covars)])))) # do.call uses df.list as the argument for rbind df &lt;- do.call(rbind,df.list) # create the correct status need last time for each # subject with status=1 to to be status=1 but all others status=0 # # the lapply creates vectors 0,0,0,...,1 based on length of t.list # need to substract 2 because the lag takes one away, then need one for the 1 at end # do.call with c binds them together into a single vector # this is then multiplied by status to correct it keep.status &lt;- do.call(c,lapply(t.list,function(x) c(rep(0,length(x)-2),1))) df[status.var] &lt;- df[status.var] * keep.status df } #=====&gt; Create the counting process data frame &lt;=====# burn.cp &lt;- df.cp(burn,&#39;T1&#39;,&#39;D1&#39;) burn.cp &lt;- within(burn.cp,{ T1Z1 &lt;- log(stop)*Z1; }) #=====&gt; Apply the Cox Proportional Hazards model &lt;=====# coxph(Surv(start,stop,D1) ~ Z1+Z2+Z3+T1Z1,data=burn.cp) "],["survsup.html", "Chapter 511 survsup", " Chapter 511 survsup Plotting survival curves with the survsup package https://cran.r-project.org/web/packages/survsup/vignettes/survsup_intro.html http://github.com/dlindholm/survsup/ {r eval=FALSE, include=FALSE, echo=TRUE} # install.packages( survsup ) {r eval=FALSE, include=FALSE, echo=TRUE} library(survsup) library(ggplot2) library(survival) library(dplyr) {r eval=FALSE, include=FALSE, echo=TRUE} lung {r eval=FALSE, include=FALSE, echo=TRUE} fit &lt;- survfit(Surv(time, status) ~ 1, data = lung) plot_survfit(fit) {r eval=FALSE, include=FALSE, echo=TRUE} lung %&gt;% survfit(Surv(time, status) ~ 1, data = .) %&gt;% plot_survfit() {r eval=FALSE, include=FALSE, echo=TRUE} lung %&gt;% survfit(Surv(time, status) ~ 1, data = .) %&gt;% plot_survfit(cuminc = FALSE) {r eval=FALSE, include=FALSE, echo=TRUE} lung %&gt;% survfit(Surv(time, status) ~ sex, data = .) %&gt;% plot_survfit(cuminc = FALSE) {r eval=FALSE, include=FALSE, echo=TRUE} lung %&gt;% survfit(Surv(time, status) ~ sex, data = .) %&gt;% plot_survfit(cuminc = FALSE, ci = TRUE) {r eval=FALSE, include=FALSE, echo=TRUE} lung %&gt;% survfit(Surv(time, status) ~ sex, data = .) %&gt;% plot_survfit(cuminc = FALSE, ci = TRUE) + # &lt; NOTE! labs(x = Time (days) , y = Survival (%) ) {r eval=FALSE, include=FALSE, echo=TRUE} lung %&gt;% survfit(Surv(time, status) ~ sex, data = .) %&gt;% plot_survfit(cuminc = FALSE) %&gt;% nar() {r eval=FALSE, include=FALSE, echo=TRUE} lung %&gt;% survfit(Surv(time, status) ~ sex, data = .) %&gt;% plot_survfit(cuminc = FALSE) %&gt;% nar(size = 3) {r eval=FALSE, include=FALSE, echo=TRUE} colon {r eval=FALSE, include=FALSE, echo=TRUE} colon %&gt;% survfit(Surv(time, status) ~ rx, data = .) %&gt;% plot_survfit() %&gt;% nar() + scale_color_manual(values = c( darkorange , steelblue , darkred )) {r eval=FALSE, include=FALSE, echo=TRUE} colon %&gt;% survfit(Surv(time, status) ~ extent, data = .) %&gt;% plot_survfit() %&gt;% nar() %&gt;% skislopes() {r eval=FALSE, include=FALSE, echo=TRUE} colon %&gt;% survfit(Surv(time, status) ~ extent, data = .) %&gt;% plot_survfit() %&gt;% nar() %&gt;% skislopes(reverse = TRUE) {r eval=FALSE, include=FALSE, echo=TRUE} colon %&gt;% survfit(Surv(time, status) ~ extent, data = .) %&gt;% plot_survfit() %&gt;% nar() %&gt;% cat4() {r eval=FALSE, include=FALSE, echo=TRUE} colon %&gt;% survfit(Surv(time, status) ~ extent, data = .) %&gt;% plot_survfit() %&gt;% nar() %&gt;% hcl_rainbow() {r eval=FALSE, include=FALSE, echo=TRUE} colon %&gt;% survfit(Surv(time, status) ~ extent, data = .) %&gt;% plot_survfit() %&gt;% nar() %&gt;% hcl_rainbow(reverse = TRUE) {r eval=FALSE, include=FALSE, echo=TRUE} colon %&gt;% survfit(Surv(time, status) ~ extent, data = .) %&gt;% plot_survfit(lwd = 2) {r eval=FALSE, include=FALSE, echo=TRUE} colon %&gt;% survfit(Surv(time, status) ~ extent, data = .) %&gt;% plot_survfit(lwd = 0.5) {r eval=FALSE, include=FALSE, echo=TRUE} colon %&gt;% survfit(Surv(time, status) ~ extent, data = .) %&gt;% plot_survfit(legend.title = Extent of disease ) {r eval=FALSE, include=FALSE, echo=TRUE} colon %&gt;% survfit(Surv(time, status) ~ extent, data = .) %&gt;% plot_survfit(ylim = c(0, 100)) %&gt;% nar() {r eval=FALSE, include=FALSE, echo=TRUE} colon %&gt;% survfit(Surv(time, status) ~ extent, data = .) %&gt;% plot_survfit(xmax = 2000) %&gt;% nar() {r eval=FALSE, include=FALSE, echo=TRUE} colon %&gt;% survfit(Surv(time, status) ~ extent, data = .) %&gt;% plot_survfit(xmax = 2000, xbreaks = c(0, 1000, 2000)) %&gt;% nar() {r eval=FALSE, include=FALSE, echo=TRUE} colon %&gt;% survfit(Surv(time, status) ~ sex, data = .) %&gt;% plot_survfit() %&gt;% nar() {r eval=FALSE, include=FALSE, echo=TRUE} colon %&gt;% survfit(Surv(time, status) ~ sex, data = .) %&gt;% plot_survfit() %&gt;% nar(flip = TRUE) {r eval=FALSE, include=FALSE, echo=TRUE} colon %&gt;% survfit(Surv(time, status) ~ sex, data = .) %&gt;% plot_survfit() %&gt;% nar(size = 5, flip = TRUE) {r eval=FALSE, include=FALSE, echo=TRUE} colon %&gt;% survfit(Surv(time, status) ~ sex, data = .) %&gt;% plot_survfit() %&gt;% nar(size = 2, flip = TRUE) {r eval=FALSE, include=FALSE, echo=TRUE} colon %&gt;% survfit(Surv(time, status) ~ sex, data = .) %&gt;% plot_survfit() %&gt;% nar(y_offset = 0.1, flip = TRUE) {r eval=FALSE, include=FALSE, echo=TRUE} colon %&gt;% survfit(Surv(time, status) ~ sex, data = .) %&gt;% plot_survfit() %&gt;% nar(y_offset = 0.03, flip = TRUE) {r eval=FALSE, include=FALSE, echo=TRUE} colon %&gt;% survfit(Surv(time, status) ~ sex, data = .) %&gt;% plot_survfit() %&gt;% nar(separator = FALSE) {r eval=FALSE, include=FALSE, echo=TRUE} colon %&gt;% survfit(Surv(time, status) ~ sex, data = .) %&gt;% plot_survfit() %&gt;% nar(sep_color = grey90 , sep_lwd = 1.5) https://cran.r-project.org/package=gridExtra {r eval=FALSE, include=FALSE, echo=TRUE} library(gridExtra) {r eval=FALSE, include=FALSE, echo=TRUE} p &lt;- list( p1 = colon %&gt;% survfit(Surv(time, status) ~ sex, data = .) %&gt;% plot_survfit(ylim = c(0, 100)) %&gt;% nar() + labs(tag = A ), p2 = colon %&gt;% survfit(Surv(time, status) ~ node4, data = .) %&gt;% plot_survfit(ylim = c(0, 100)) %&gt;% nar() + labs(tag = B ) ) grid.arrange(grobs = p, ncol = 2) {r eval=FALSE, include=FALSE, echo=TRUE} # Store plots in a list p &lt;- list( p1 = colon %&gt;% survfit(Surv(time, status) ~ 1, data = .) %&gt;% plot_survfit(ylim = c(0, 100)) + labs(tag = A ), p2 = colon %&gt;% survfit(Surv(time, status) ~ rx, data = .) %&gt;% plot_survfit(ylim = c(0, 100)) %&gt;% nar(2, separator = FALSE) + labs(tag = B ), p3 = colon %&gt;% survfit(Surv(time, status) ~ extent, data = .) %&gt;% plot_survfit(ylim = c(0, 100)) %&gt;% nar(2, separator = FALSE) + labs(tag = C ), p4 = colon %&gt;% survfit(Surv(time, status) ~ sex, data = .) %&gt;% plot_survfit(ylim = c(0, 100)) %&gt;% nar(2, separator = FALSE) + labs(tag = D ), p5 = colon %&gt;% survfit(Surv(time, status) ~ node4, data = .) %&gt;% plot_survfit(ylim = c(0, 100)) %&gt;% nar(2, separator = FALSE) + labs(tag = E ), p6 = colon %&gt;% survfit(Surv(time, status) ~ surg, data = .) %&gt;% plot_survfit(ylim = c(0, 100)) %&gt;% nar(2, separator = FALSE) + labs(tag = F ) ) #Define layout matrix lay &lt;- rbind(c(1,1,2), c(1,1,3), c(4,5,6)) #Plot it all! grid.arrange(grobs = p, layout_matrix = lay) {r eval=FALSE, include=FALSE, echo=TRUE} p[[ p3 ]] "],["survivalanalysis.html", "Chapter 512 survivalAnalysis 512.1 Univariate Survival Analysis", " Chapter 512 survivalAnalysis 512.1 Univariate Survival Analysis https://cran.r-project.org/web/packages/survivalAnalysis/vignettes/univariate.html "],["breast-cancer-wisconsin.html", "Chapter 513 breast cancer wisconsin", " Chapter 513 breast cancer wisconsin {r eval=FALSE, include=FALSE, echo=TRUE} library(tidyverse) {r eval=FALSE, include=FALSE, echo=TRUE} UCI_data_URL &lt;- RCurl::getURL(&#39;https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data&#39;) "],["lung-cancer.html", "Chapter 514 Lung Cancer", " Chapter 514 Lung Cancer {r eval=FALSE, include=FALSE, echo=TRUE} library(survival) km_fit &lt;- survfit(Surv(time, status) ~ sex, data = lung) km_fit {r eval=FALSE, include=FALSE, echo=TRUE} summary(km_fit, times = c(12,36,60)) {r eval=FALSE, include=FALSE, echo=TRUE} # View fit View(km_fit) # Get formula km_fit[[ call ]][[ formula ]] # Get formula 2 km_fit[[ call ]] {r eval=FALSE, include=FALSE, echo=TRUE} library(tidyverse) # make a dataframe from fit list fitDF &lt;- bind_cols( time = km_fit[[ time ]], n.risk = km_fit[[ n.risk ]], n.event = km_fit[[ n.event ]], n.censor = km_fit[[ n.censor ]], surv = km_fit[[ surv ]], lower = km_fit[[ lower ]], upper = km_fit[[ upper ]], ) %&gt;% mutate( timeyear = time / 365.25 ) {r eval=FALSE, include=FALSE, echo=TRUE} # this gives the result for the time = 0.5 fitDF[which.min(abs(fitDF$timeyear - 0.5)),] # this gives the result for the time = 1 fitDF[which.min(abs(fitDF$timeyear - 1)),] # this gives the result for the time = 2 fitDF[which.min(abs(fitDF$timeyear - 2)),] {r eval=FALSE, include=FALSE, echo=TRUE} fitDF[which.min(abs(fitDF$timeyear - 0.5)),] fitDF[which.min(abs(fitDF$timeyear - 1)),] fitDF[which.min(abs(fitDF$timeyear - 2)),] {r eval=FALSE, include=FALSE, echo=TRUE} # https://stackoverflow.com/questions/43419385/how-to-export-survfit-output-as-a-csv-table res &lt;- summary(km_fit, times = c(12,36,60)) save.df &lt;- as.data.frame(res[c( strata , time , n.risk , n.event , surv , std.err , lower , upper )]) # write.csv(save.df, file = ./file.csv ) "],["syncing-a-github-fork.html", "Chapter 515 Syncing a GitHub Fork", " Chapter 515 Syncing a GitHub Fork "],["configuring-a-remote-for-a-fork.html", "Chapter 516 Configuring a remote for a fork 516.1 Open Terminal. 516.2 List the current configured remote repository for your fork. 516.3 Specify a new remote upstream repository that will be synced with the fork. 516.4 Verify the new upstream repository you‚Äôve specified for your fork.", " Chapter 516 Configuring a remote for a fork https://help.github.com/articles/configuring-a-remote-for-a-fork/ 516.1 Open Terminal. 516.2 List the current configured remote repository for your fork. {bash} git remote -v origin https://github.com/YOUR_USERNAME/YOUR_FORK.git (fetch) origin https://github.com/YOUR_USERNAME/YOUR_FORK.git (push) 516.3 Specify a new remote upstream repository that will be synced with the fork. {bash} git remote add upstream https://github.com/BIOP/IPA4LSx.git git remote add upstream https://github.com/ORIGINAL_OWNER/ORIGINAL_REPOSITORY.git git remote add upstream https://github.com/BIOP/IPA4LSx.git 516.4 Verify the new upstream repository you‚Äôve specified for your fork. {bash} git remote -v origin https://github.com/YOUR_USERNAME/YOUR_FORK.git (fetch) origin https://github.com/YOUR_USERNAME/YOUR_FORK.git (push) upstream https://github.com/ORIGINAL_OWNER/ORIGINAL_REPOSITORY.git (fetch) upstream https://github.com/ORIGINAL_OWNER/ORIGINAL_REPOSITORY.git (push) "],["syncing-a-fork.html", "Chapter 517 Syncing a fork 517.1 Open Terminal. 517.2 Change the current working directory to your local project. 517.3 Fetch the branches and their respective commits from the upstream repository. Commits to master will be stored in a local branch, upstream/master. 517.4 Check out your fork‚Äôs local master branch. 517.5 Merge the changes from upstream/master into your local master branch. This brings your fork‚Äôs master branch into sync with the upstream repository, without losing your local changes. 517.6 If your local branch didn‚Äôt have any unique commits, Git will instead perform a fast-forward :", " Chapter 517 Syncing a fork https://help.github.com/articles/syncing-a-fork/ 517.1 Open Terminal. 517.2 Change the current working directory to your local project. 517.3 Fetch the branches and their respective commits from the upstream repository. Commits to master will be stored in a local branch, upstream/master. {bash} git fetch upstream git fetch upstream remote: Counting objects: 75, done. remote: Compressing objects: 100% (53/53), done. remote: Total 62 (delta 27), reused 44 (delta 9) Unpacking objects: 100% (62/62), done. From https://github.com/ORIGINAL_OWNER/ORIGINAL_REPOSITORY * [new branch] master -&gt; upstream/master 517.4 Check out your fork‚Äôs local master branch. {bash} git checkout master git checkout master Switched to branch &#39;master&#39; 517.5 Merge the changes from upstream/master into your local master branch. This brings your fork‚Äôs master branch into sync with the upstream repository, without losing your local changes. {bash} git merge upstream/master git merge upstream/master Updating a422352..5fdff0f Fast-forward README | 9 - README.md | 7 ++++++ 2 files changed, 7 insertions(+), 9 deletions(-) delete mode 100644 README create mode 100644 README.md 517.6 If your local branch didn‚Äôt have any unique commits, Git will instead perform a fast-forward : {bash} git merge upstream/master git merge upstream/master Updating 34e91da..16c56ad Fast-forward README.md | 5 +++-- 1 file changed, 3 insertions(+), 2 deletions(-) Tip: Syncing your fork only updates your local copy of the repository. To update your fork on GitHub, you must push your changes. "],["my-r-codes-for-data-analysis-8.html", "Chapter 518 My R Codes For Data Analysis 518.1 Tables 518.2 tttable", " Chapter 518 My R Codes For Data Analysis finalfit http://www.datasurg.net/2018/07/12/finalfit-now-includes-bootstrap-simulation-for-model-prediction/ https://kbroman.org/knitr_knutshell/pages/figs_tables.html 518.1 Tables {r eval=FALSE, include=FALSE, echo=TRUE} library(xtable) xtable(BMT) {r eval=FALSE, include=FALSE, echo=TRUE} library(kableExtra) kable(BMT) {r display results as table, eval=FALSE, include=FALSE, echo=TRUE, warning=FALSE, message=FALSE} require(rhandsontable) rhandsontable(BMT) 518.2 tttable https://github.com/leeper/tttable 518.2.1 - renderers 518.2.1.1 - xtable https://rdrr.io/cran/xtable/man/xtable.html 518.2.1.2 - flextable https://davidgohel.github.io/flextable/index.html {r eval=FALSE, include=FALSE, echo=TRUE} https://cran.r-project.org/web/packages/flextable/vignettes/overview.html {r eval=FALSE, include=FALSE, echo=TRUE} library(flextable) library(officer) {r eval=FALSE, include=FALSE, echo=TRUE} data &lt;- iris[c(1:3, 51:53, 101:104),] data {r eval=FALSE, include=FALSE, echo=TRUE} flextable::regulartable(data) {r eval=FALSE, include=FALSE, echo=TRUE} myft &lt;- regulartable( head(mtcars), col_keys = c( am , carb , gear , mpg , drat )) myft {r eval=FALSE, include=FALSE, echo=TRUE} myft &lt;- theme_vanilla(myft) myft {r eval=FALSE, include=FALSE, echo=TRUE} myft &lt;- merge_v(myft, j = c( am , carb ) ) myft {r eval=FALSE, include=FALSE, echo=TRUE} myft &lt;- set_header_labels( myft, carb = # carb. ) myft &lt;- width(myft, width = .75) # set width of all columns to .75 in myft {r eval=FALSE, include=FALSE, echo=TRUE} myft &lt;- autofit(myft) myft {r eval=FALSE, include=FALSE, echo=TRUE} myft &lt;- italic(myft, j = 1) myft &lt;- bg(myft, bg = #C90000 , part = header ) myft &lt;- color(myft, color = white , part = header ) myft {r eval=FALSE, include=FALSE, echo=TRUE} myft &lt;- color(myft, ~ drat &gt; 3.5, ~ drat, color = red ) myft &lt;- bold(myft, ~ drat &gt; 3.5, ~ drat, bold = TRUE) myft &lt;- autofit(myft) myft {r eval=FALSE, include=FALSE, echo=TRUE} library(officer) ft &lt;- regulartable(head(mtcars)) ft &lt;- theme_booktabs(ft) ft &lt;- autofit(ft) ppt &lt;- read_pptx() ppt &lt;- add_slide(ppt, layout = Title and Content , master = Office Theme ) ppt &lt;- ph_with_flextable(ppt, value = ft, type = body ) print(ppt, target = output/example.pptx ) # https://view.officeapps.live.com/op/view.aspx?src=https://davidgohel.github.io/flextable/articles/assets/pptx/example.pptx {r eval=FALSE, include=FALSE, echo=TRUE} doc &lt;- read_docx() doc &lt;- body_add_flextable(doc, value = ft) print(doc, target = output/example.docx ) Make Beautiful Tables with the Formattable Package https://www.displayr.com/formattable/?utm_medium=Feed&amp;utm_source=Syndication 518.2.1.3 - knitr::kable() 518.2.1.4 - formatttable {r eval=FALSE, include=FALSE, echo=TRUE} library(formattable) p &lt;- percent(c(0.1, 0.02, 0.03, 0.12)) p p + 0.05 max(p) {r eval=FALSE, include=FALSE, echo=TRUE} balance &lt;- accounting(c(1000, 500, 200, -150, 0, 1200)) balance balance + 1000 {r eval=FALSE, include=FALSE, echo=TRUE} p &lt;- data.frame( id = c(1, 2, 3, 4, 5), name = c( A1 , A2 , B1 , B2 , C1 ), balance = accounting(c(52500, 36150, 25000, 18300, 7600), format = d ), growth = percent(c(0.3, 0.3, 0.1, 0.15, 0.15), format = d ), ready = formattable(c(TRUE, TRUE, FALSE, FALSE, TRUE), yes , no )) p print.AsIs(p) {r eval=FALSE, include=FALSE, echo=TRUE} df &lt;- data.frame( id = 1:10, name = c( Bob , Ashley , James , David , Jenny , Hans , Leo , John , Emily , Lee ), age = c(28, 27, 30, 28, 29, 29, 27, 27, 31, 30), grade = c( C , A , A , C , B , B , B , A , C , C ), test1_score = c(8.9, 9.5, 9.6, 8.9, 9.1, 9.3, 9.3, 9.9, 8.5, 8.6), test2_score = c(9.1, 9.1, 9.2, 9.1, 8.9, 8.5, 9.2, 9.3, 9.1, 8.8), final_score = c(9, 9.3, 9.4, 9, 9, 8.9, 9.25, 9.6, 8.8, 8.7), registered = c(TRUE, FALSE, TRUE, FALSE, TRUE, TRUE, TRUE, FALSE, FALSE, FALSE), stringsAsFactors = FALSE) {r eval=FALSE, include=FALSE, echo=TRUE} library(formattable) formattable(df, list( age = color_tile( white , orange ), grade = formatter( span , style = x ~ ifelse(x == A , style(color = green , font.weight = bold ), NA)), area(col = c(test1_score, test2_score)) ~ normalize_bar( pink , 0.2), final_score = formatter( span , style = x ~ style(color = ifelse(rank(-x) &lt;= 3, green , gray )), x ~ sprintf( %.2f (rank: %02d) , x, rank(-x))), registered = formatter( span , style = x ~ style(color = ifelse(x, green , red )), x ~ icontext(ifelse(x, ok , remove ), ifelse(x, Yes , No ))) )) 518.2.1.5 - knitLatex 518.2.1.6 - htmlTable 518.2.1.7 - psytabs 518.2.1.8 - SortableHTMLTables 518.2.1.9 - tablaxlsx 518.2.1.10 - table1xls 518.2.1.11 - tableHTML 518.2.1.12 - TableMonster 518.2.1.13 - texreg 518.2.1.14 - ztable 518.2.1.15 - apaStyle 518.2.1.16 - apaTables 518.2.1.17 - apsrtable 518.2.1.18 - DT https://www.infoworld.com/video/91607/r-tip-quick-interactive-tables https://rstudio.github.io/DT/ https://datatables.net/ {r eval=FALSE, include=FALSE, echo=TRUE} # install.packages(&#39;DT&#39;) 518.2.1.19 - gtsummary https://github.com/vincentarelbundock/gtsummary 518.2.2 - higher-level functionality 518.2.2.1 - finalfit https://github.com/ewenharrison/finalfit http://www.datasurg.net/2018/05/16/elegant-regression-results-tables-and-plots-the-finalfit-package/ {r eval=FALSE, include=FALSE, echo=TRUE} # devtools::install_github( ewenharrison/finalfit ) # install.packages( rstan ) # install.packages( boot ) library(tidyverse) library(finalfit) library(rstan) library(boot) {r eval=FALSE, include=FALSE, echo=TRUE} # Load example dataset, modified version of survival::colon data(colon_s) colon_s2$age &lt;- as.numeric(colon_s$age) colon_s2$age.factor &lt;- as_factor(colon_s$age.factor) colon_s2$sex.factor &lt;- as_factor(colon_s$sex.factor) colon_s2$obstruct.factor &lt;- as_factor(colon_s$obstruct.factor) colon_s2$perfor.factor &lt;- as_factor(colon_s$perfor.factor) colon_s2$mort_5yr &lt;- as_factor(colon_s$mort_5yr) colon_s2$hospital &lt;- as_factor(colon_s$hospital) colon_s2$status &lt;- as.numeric(colon_s$status) colon_s2$time &lt;- as.numeric(colon_s$time) colon_s2 {r eval=FALSE, include=FALSE, echo=TRUE} # Table 1 - Patient demographics by variable of interest - explanatory = c( age , age.factor , sex.factor , obstruct.factor ) dependent = perfor.factor # Bowel perforation table &lt;- colon_s2 %&gt;% summary_factorlist(dependent, explanatory, p=TRUE, add_dependent_label=TRUE, total_col = TRUE) print.AsIs(table) {r eval=FALSE, include=FALSE, echo=TRUE} # Table 2 - 5 yr mortality - explanatory = c( age.factor , sex.factor , obstruct.factor ) dependent = &#39;mort_5yr&#39; table &lt;- colon_s %&gt;% summary_factorlist(dependent, explanatory, p=TRUE, add_dependent_label=TRUE) print.AsIs(table) {r results=&#39;asis&#39;, eval=FALSE, include=FALSE, echo=TRUE} knitr::kable(table, row.names=FALSE, align=c( l , l , r , r , r , r )) {r eval=FALSE, include=FALSE, echo=TRUE} explanatory = c( age.factor , sex.factor , obstruct.factor , perfor.factor ) dependent = &#39;mort_5yr&#39; table_surv &lt;- colon_s2 %&gt;% finalfit(dependent, explanatory) {r results=&#39;asis&#39;, eval=FALSE, include=FALSE, echo=TRUE} knitr::kable(table_surv, row.names = FALSE, align = c( l , l , r , r , r , r )) {r eval=FALSE, include=FALSE, echo=TRUE} explanatory = c( age.factor , sex.factor , obstruct.factor , perfor.factor ) explanatory_multi = c( age.factor , obstruct.factor ) dependent = &#39;mort_5yr&#39; colon_s2 %&gt;% finalfit(dependent, explanatory, explanatory_multi) {r eval=FALSE, include=FALSE, echo=TRUE} explanatory = c( age.factor , sex.factor , obstruct.factor , perfor.factor ) explanatory_multi = c( age.factor , obstruct.factor ) random_effect = hospital dependent = &#39;mort_5yr&#39; colon_s2 %&gt;% finalfit(dependent, explanatory, explanatory_multi, random_effect) {r eval=FALSE, include=FALSE, echo=TRUE} explanatory = c( age.factor , sex.factor , obstruct.factor , perfor.factor ) dependent = Surv(time, status) colon_s2 %&gt;% finalfit(dependent, explanatory) {r eval=FALSE, include=FALSE, echo=TRUE} explanatory = c( age.factor , sex.factor , obstruct.factor , perfor.factor ) dependent = &#39;mort_5yr&#39; table7 &lt;- colon_s2 %&gt;% finalfit(dependent, explanatory, metrics=TRUE) {r, eval=FALSE, include=FALSE, echo=TRUE, results= asis } knitr::kable(table7[[1]], row.names=FALSE, align=c( l , l , r , r , r )) knitr::kable(table7[[2]], row.names=FALSE) {r eval=FALSE, include=FALSE, echo=TRUE} explanatory = c( age.factor , sex.factor , obstruct.factor , perfor.factor ) explanatory_multi = c( age.factor , obstruct.factor ) random_effect = hospital dependent = &#39;mort_5yr&#39; # Separate tables colon_s2 %&gt;% summary_factorlist(dependent, explanatory, fit_id=TRUE) -&gt; example.summary colon_s2 %&gt;% glmuni(dependent, explanatory) %&gt;% fit2df(estimate_suffix= (univariable) ) -&gt; example.univariable colon_s2 %&gt;% glmmulti(dependent, explanatory) %&gt;% fit2df(estimate_suffix= (multivariable) ) -&gt; example.multivariable colon_s2 %&gt;% glmmixed(dependent, explanatory, random_effect) %&gt;% fit2df(estimate_suffix= (multilevel) ) -&gt; example.multilevel # Pipe together example.summary %&gt;% finalfit_merge(example.univariable) %&gt;% finalfit_merge(example.multivariable) %&gt;% finalfit_merge(example.multilevel) %&gt;% select(-c(fit_id, index)) %&gt;% # remove unnecessary columns dependent_label(colon_s2, dependent, prefix= ) # place dependent variable label {r fig.height=5, fig.width=6, eval=FALSE, include=FALSE, echo=TRUE, warning=FALSE, message=FALSE} # OR plot explanatory = c( age.factor , sex.factor , obstruct.factor , perfor.factor ) dependent = &#39;mort_5yr&#39; colon_s2 %&gt;% or_plot(dependent, explanatory) # Previously fitted models (`glmmulti()` or # `glmmixed()`) can be provided directly to `glmfit` {r fig.height=5, fig.width=6, eval=FALSE, include=FALSE, echo=TRUE} # HR plot explanatory = c( age.factor , sex.factor , obstruct.factor , perfor.factor ) dependent = Surv(time, status) colon_s2 %&gt;% hr_plot(dependent, explanatory, dependent_label = Survival ) # Previously fitted models (`coxphmulti`) can be provided directly using `coxfit` {r eval=FALSE, include=FALSE, echo=TRUE} # KM plot explanatory = c( perfor.factor ) dependent = Surv(time, status) plotKM &lt;- colon_s2 %&gt;% surv_plot(dependent, explanatory, xlab = Time (days) , pval = TRUE, legend = none ) plotKM 518.2.2.2 - janitor 518.2.2.3 - huxtable https://hughjonesd.github.io/huxtable/design-principles.html 518.2.2.4 - tables 518.2.2.5 - stargazer 518.2.2.6 - pixiedust pixiedust: Tables so Beautifully Fine-Tuned You Will Believe It‚Äôs Magic https://cran.r-project.org/web/packages/pixiedust/vignettes/pixiedust.html https://cran.r-project.org/web/packages/pixiedust/vignettes/advancedMagic.html 518.2.2.7 - reporttools 518.2.2.8 - rtable 518.2.2.9 - summarytools 518.2.2.10 - tab 518.2.2.11 - tableone 518.2.2.12 - carpenter https://cran.r-project.org/web/packages/carpenter/vignettes/carpenter.html 518.2.2.13 - dtables 518.2.2.14 - etable 518.2.2.15 - tangram tangram (grammar of tables) https://github.com/spgarbet/tangram http://htmlpreview.github.io/?https://github.com/spgarbet/tg/blob/master/vignettes/example.html https://github.com/spgarbet/tangram/issues/36 518.2.3 - pivots 518.2.3.1 - rpivotttable 518.2.4 - other 518.2.4.1 - gtable 518.2.4.2 - sjtlm https://strengejacke.github.io/sjPlot/articles/sjtlm.html 518.2.4.3 - arsenal to compare data frames: https://cran.r-project.org/web/packages/arsenal/vignettes/compare.html 518.2.4.3.1 - arsenal::paired summary statistics for a set of variables paired across two time points: https://cran.r-project.org/web/packages/arsenal/vignettes/paired.html {r eval=FALSE, include=FALSE, echo=TRUE} library(arsenal) dat &lt;- data.frame( tp = paste0( Time Point , c(1, 2, 1, 2, 1, 2, 1, 2, 1, 2)), id = c(1, 1, 2, 2, 3, 3, 4, 4, 5, 6), Cat = c( A , A , A , B , B , B , B , A , NA, B ), Fac = factor(c( A , B , C , A , B , C , A , B , C , A )), Num = c(1, 2, 3, 4, 4, 3, 3, 4, 0, NA), Ord = ordered(c( I , II , II , III , III , III , I , III , II , I )), Lgl = c(TRUE, TRUE, FALSE, TRUE, FALSE, TRUE, TRUE, FALSE, FALSE, FALSE), Dat = as.Date( 2018-05-01 ) + c(1, 1, 2, 2, 3, 4, 5, 6, 3, 4), stringsAsFactors = FALSE ) {r eval=FALSE, include=FALSE, echo=TRUE} dat {r include=FALSE, eval=FALSE, comment=NA, prompt=FALSE, cache=FALSE, echo=TRUE, results=&#39;asis&#39;} p &lt;- paired(tp ~ Cat + Fac + Num + Ord + Lgl + Dat, data = dat, id = id, signed.rank.exact = FALSE) summary(p) 518.2.4.3.2 - arsenal::freqlist https://cran.r-project.org/web/packages/arsenal/vignettes/freqlist.html {r eval=FALSE, include=FALSE, echo=TRUE} require(arsenal) {r eval=FALSE, include=FALSE, echo=TRUE} # load the data data(mockstudy) # retain NAs when creating the table using the useNA argument tab.ex &lt;- table(mockstudy[, c( arm , sex , mdquality.s )], useNA = ifany ) {r eval=FALSE, include=FALSE, echo=TRUE} tab.ex {r eval=FALSE, include=FALSE, echo=TRUE} noby &lt;- freqlist(tab.ex) str(noby) {r eval=FALSE, include=FALSE, echo=TRUE} # view the data frame portion of freqlist output head(noby[[ freqlist ]]) ## or use as.data.frame(noby) {r eval=FALSE, include=FALSE, echo=TRUE} summary(noby) summary(noby, title = Basic freqlist output ) summary(freqlist(~ arm + sex + mdquality.s, data = mockstudy, addNA = TRUE)) summary(freqlist(~arm + sex + addNA(mdquality.s), data = mockstudy)) summary(freqlist(~arm + sex + includeNA(mdquality.s, Missing ), data = mockstudy)) withnames &lt;- freqlist(tab.ex, labelTranslations = c( Treatment Arm , Gender , LASA QOL ), digits = 0) summary(withnames) 518.2.4.3.3 - arsenal::tableby https://cran.r-project.org/web/packages/arsenal/vignettes/tableby.html 518.2.4.3.4 - arsenal::modelsum https://cran.r-project.org/web/packages/arsenal/vignettes/modelsum.html 518.2.4.4 - desctable https://cran.r-project.org/web/packages/desctable/vignettes/desctable.html "],["gt.html", "Chapter 519 GT", " Chapter 519 GT https://github.com/rstudio/gt {r eval=FALSE, include=FALSE, echo=TRUE} install.packages( devtools ) remotes::install_github( rstudio/gt ) {r eval=FALSE, include=FALSE, echo=TRUE} library(gt) library(tidyverse) library(glue) # Define the start and end dates for the data range start_date &lt;- 2010-06-07 end_date &lt;- 2010-06-14 # Create a gt table based on preprocessed # `sp500` table data sp500 %&gt;% dplyr::filter(date &gt;= start_date &amp; date &lt;= end_date) %&gt;% dplyr::select(-adj_close) %&gt;% dplyr::mutate(date = as.character(date)) %&gt;% gt() %&gt;% tab_header( title = S&amp;P 500 , subtitle = glue::glue( {start_date} to {end_date} ) ) %&gt;% fmt_date( columns = vars(date), date_style = 3 ) %&gt;% fmt_currency( columns = vars(open, high, low, close), currency = USD ) %&gt;% fmt_number( columns = vars(volume), scale_by = 1 / 1E9, pattern = {x}B ) "],["sparklines.html", "Chapter 520 sparklines", " Chapter 520 sparklines R tip: Sparklines in HTML tables https://www.infoworld.com/video/91867/r-tip-sparklines-in-html-tables "],["tensorflow.html", "Chapter 521 Tensorflow", " Chapter 521 Tensorflow https://ai.google/education/ https://github.com/tensorflow/workshops https://experiments.withgoogle.com/collection/ai https://developers.google.com/machine-learning/crash-course/ https://ai.google/education/responsible-ai-practices https://research.google.com/seedbank/ https://codelabs.developers.google.com/codelabs/end-to-end-ml/index.html#0 "],["text-mining.html", "Chapter 522 Text Mining", " Chapter 522 Text Mining https://regexr.com/ https://regex101.com/ "],["read-text-files-with-readtext.html", "Chapter 523 Read text files with readtext()", " Chapter 523 Read text files with readtext() https://cran.r-project.org/web/packages/readtext/vignettes/readtext_vignette.html "],["qdapregex.html", "Chapter 524 qdapRegex", " Chapter 524 qdapRegex {r eval=FALSE, include=FALSE, echo=TRUE} library(qdapRegex) https://github.com/trinker/qdapRegex http://trinker.github.io/qdapRegex_dev/index.html "],["stringr-2.html", "Chapter 525 stringr", " Chapter 525 stringr https://cran.r-project.org/web/packages/stringr/vignettes/stringr.html stringr::str_view() "],["regexplain.html", "Chapter 526 RegExplain", " Chapter 526 RegExplain https://www.garrickadenbuie.com/project/regexplain/ {r eval=FALSE, include=FALSE, echo=TRUE} # devtools::install_github( gadenbuie/regexplain ) # source( https://install-github.me/gadenbuie/regexplain ) "],["a-tidy-text-analysis-of-my-google-search-history.html", "Chapter 527 A Tidy Text Analysis of My Google Search History", " Chapter 527 A Tidy Text Analysis of My Google Search History https://www.r-bloggers.com/a-tidy-text-analysis-of-my-google-search-history/ "],["text-mining-with-r-a-tidy-approach.html", "Chapter 528 Text Mining with R A Tidy Approach", " Chapter 528 Text Mining with R A Tidy Approach https://www.tidytextmining.com/ "],["pride-and-prejudice.html", "Chapter 529 Pride and Prejudice", " Chapter 529 Pride and Prejudice https://juliasilge.com/blog/tidy-text-classification/ https://juliasilge.com/blog/if-i-loved-nlp-less/ "],["cleannlp.html", "Chapter 530 cleanNLP", " Chapter 530 cleanNLP https://statsmaths.github.io/cleanNLP/ "],["text-analysis-of-holy-books.html", "Chapter 531 Text analysis of holy books 531.1 quRan 531.2 sacred 531.3 scriptuRs", " Chapter 531 Text analysis of holy books 531.1 quRan https://github.com/andrewheiss/quRan https://github.com/andrewheiss/quRan/blob/master/data-raw/clean_data.R 531.1.1 Tidy text, parts of speech, and unique words in the Qur‚Äôan https://www.andrewheiss.com/blog/2018/12/28/tidytext-pos-arabic/ 531.2 sacred http://sacred.john-coene.com/ https://github.com/JohnCoene/sacred 531.3 scriptuRs https://github.com/andrewheiss/scriptuRs 531.3.1 Tidy text, parts of speech, and unique words in the Bible https://www.andrewheiss.com/blog/2018/12/26/tidytext-pos-john/ {r eval=FALSE, include=FALSE, echo=TRUE} # devtools::install_github( andrewheiss/scriptuRs ) {r eval=FALSE, include=FALSE, echo=TRUE} library(tidyverse) # For dplyr, ggplot2, and friends library(scriptuRs) # For full text of bible library(tidytext) # For analyzing text library(cleanNLP) # For fancier natural language processing # Load data gospels &lt;- kjv_bible() %&gt;% filter(book_title %in% c( Matthew , Mark , Luke , John )) {r eval=FALSE, include=FALSE, echo=TRUE} # Set up NLP backend # reticulate::use_python( /Users/serdarbalciold/.conda ) # I use homebrew python3 # reticulate::use_condaenv(condaenv = /anaconda3/envs , conda = auto , required = FALSE) # cnlp_init_spacy() # Use spaCy cnlp_init_udpipe() # Or use this R-only one without external dependencies {r eval=FALSE, include=FALSE, echo=TRUE} # Determine the parts of speech of the text column and use verse_title as the id gospels_annotated &lt;- cnlp_annotate(gospels, as_strings = TRUE, text_var = text , doc_var = verse_title ) {r eval=FALSE, include=FALSE, echo=TRUE} gospels_annotated {r eval=FALSE, include=FALSE, echo=TRUE} glimpse(gospels_annotated) {r eval=FALSE, include=FALSE, echo=TRUE} gospel_terms &lt;- gospels_annotated %&gt;% cnlp_get_token() head(gospel_terms) {r eval=FALSE, include=FALSE, echo=TRUE} gospels_lookup &lt;- gospels %&gt;% select(verse_title, book_title, chapter_number, verse_number) gospel_terms &lt;- gospel_terms %&gt;% left_join(gospels_lookup, by = c( id = verse_title )) glimpse(gospel_terms) "],["word-associations-from-the-small-world-of-words.html", "Chapter 532 WORD ASSOCIATIONS FROM THE SMALL WORLD OF WORDS", " Chapter 532 WORD ASSOCIATIONS FROM THE SMALL WORLD OF WORDS https://juliasilge.com/blog/word-associations/ "],["section-1.html", "Chapter 533 ", " Chapter 533 "],["the-lesser-known-stars-of-the-tidyverse.html", "Chapter 534 The Lesser Known Stars of the Tidyverse", " Chapter 534 The Lesser Known Stars of the Tidyverse slug: the-lesser-known-stars-of-the-tidyverse tags: - tidyverse - R - Code categories: [] I copied this code just to learn myself. See original links below: "],["webinar-tidyverse-exploratory-analysis-emily-robinson-1.html", "Chapter 535 Webinar: Tidyverse Exploratory Analysis (Emily Robinson) 535.1 Reading in the data 535.2 Initial examination", " Chapter 535 Webinar: Tidyverse Exploratory Analysis (Emily Robinson) &lt;iframe width= 560 height= 315 src= https://www.youtube.com/embed/uG3igAGX7UE frameborder= 0 allow= accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture allowfullscreen&gt; &lt;iframe width= 560 height= 315 src= https://www.youtube.com/embed/ax4LXQ5t38k frameborder= 0 allow= accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture allowfullscreen&gt; https://hookedondata.org/the-lesser-known-stars-of-the-tidyverse/ https://www.rstudio.com/resources/videos/the-lesser-known-stars-of-the-tidyverse/ https://github.com/robinsones/robinsones_blog/blob/master/content/post/multipleChoiceResponses.csv https://github.com/robinsones/robinsones_blog/blob/master/content/post/2018-11-16-the-lesser-known-stars-of-the-tidyverse.Rmd 535.1 Reading in the data {r eval=FALSE, include=FALSE, echo=TRUE} knitr::opts_chunk$set(echo = TRUE, warning = TRUE, message = FALSE, error = TRUE) library(tidyverse) library(magrittr) theme_set(theme_bw()) {r eval=FALSE, include=FALSE, echo=TRUE} multiple_choice_responses_base &lt;- read.csv( data/multipleChoiceResponses.csv ) {r eval=FALSE, include=FALSE, echo=TRUE} # for one column sum(is.na(multiple_choice_responses_base$Country)) {r eval=FALSE, include=FALSE, echo=TRUE} # for five columns multiple_choice_responses_base %&gt;% summarise_at(1:5, ~sum(is.na(.))) multiple_choice_responses_base %&gt;% summarise_at(vars(GenderSelect:StudentStatus), ~sum(is.na(.))) multiple_choice_responses_base %&gt;% summarise_at(vars(GenderSelect, Age), ~sum(is.na(.))) {r eval=FALSE, include=FALSE, echo=TRUE} multiple_choice_responses_base %&gt;% count(StudentStatus) Yep. We see here we have a lot of entries instead of NAs. We can correct this with na_if from dplyr, which takes as an argument what we want to turn into NAs. We can also use %&lt;&gt;%, which is a reassignment pipe. While this is nice to save some typing, it can make it confusing when reading a script, so use with caution. {r eval=FALSE, include=FALSE, echo=TRUE} multiple_choice_responses_base %&lt;&gt;% na_if( ) ## is the same as: multiple_choice_responses_base &lt;- multiple_choice_responses_base %&gt;% na_if( ) Now let‚Äôs count the NAs again. {r eval=FALSE, include=FALSE, echo=TRUE} multiple_choice_responses_base %&gt;% summarise_at(1:5, ~sum(is.na(.))) And it‚Äôs fixed! How could we have avoided this all in the first place? By using {r # eadr::read_csv instead of {r # ead.csv. If you‚Äôre not familiar with ::, it‚Äôs for explicitly setting what package you‚Äôre getting the function on the right from. This is helpful in three ways: There can be name conflicts, where two packages have functions with the same name. Using :: ensures you‚Äôre getting the function you want. if you only want to use one function from a package, you can use :: to skip the library call. As long as you‚Äôve installed the package, you don‚Äôt need to have loaded it to get the function. For teaching purposes, it‚Äôs nice to remind people where the function is coming from. {r eval=FALSE, include=FALSE, echo=TRUE} multiple_choice_responses &lt;- readr::read_csv( multipleChoiceResponses.csv ) It‚Äôs definitely faster, but it seems we have some errors. Let‚Äôs inspect them. {r eval=FALSE, include=FALSE, echo=TRUE} problems(multiple_choice_responses) We see each row and column where a problem occurs. What‚Äôs happening is that {r # ead_csv uses the first 1000 rows of a column to guess its type. But in some cases, it‚Äôs guessing the column is an integer, because the first 1000 rows are whole numbers, when actually it should be double, as some entries have decimal points. We can fix this by changing the number of rows {r # ead_csv uses to guess the column type (with the guess_max argument) to the number of rows in the data set. {r eval=FALSE, include=FALSE, echo=TRUE} multiple_choice_responses &lt;- readr::read_csv( multipleChoiceResponses.csv , guess_max = nrow(multiple_choice_responses)) Error-free! 535.2 Initial examination Let‚Äôs see what we can glean from the column names themselves. I‚Äôll only look at the first 20 since there are so many. {r eval=FALSE, include=FALSE, echo=TRUE} colnames(multiple_choice_responses) %&gt;% head(20) We can see that there were categories of questions, like LearningPlatform, with each platform having its own column. Now let‚Äôs take a look at our numeric columns with skimr. Skimr is a package from rOpenSci that allows you to quickly view summaries of your data. We can use select_if to select only columns where a certain condition, in this case whether it‚Äôs a numeric column, is true. {r eval=FALSE, include=FALSE, echo=TRUE} multiple_choice_responses %&gt;% select_if(is.numeric) %&gt;% skimr::skim() I love the histograms. We can quickly see from them that people self teach a lot and spend a good amount of time building models and gathering data, compared to visualizing data or working in production. Let‚Äôs see how many distinct answers we have for each question. We can use n_distinct(), a shorter and faster version of length(unique()). We‚Äôll use summarise_all, which is the same as summarise_at except that you don‚Äôt select a group of columns and so it applies to every one in the dataset. {r eval=FALSE, include=FALSE, echo=TRUE} multiple_choice_responses %&gt;% summarise_all(n_distinct) %&gt;% select(1:10) This data would be more helpful if it was tidy and had two columns, question and num_distinct_answers. We can use tidyr::gather to change our data from wide to long format and then arrange it so we can see the columns with the most distinct answers first. If you‚Äôve used (or are still using!) reshape2, check out tidyr; reshape2 is retired and is only updated with changes necessary for it to remain on CRAN. While not exactly equivalent, tidyr::spread replaces {r # eshape2::dcast, tidyr::separate {r # eshape2::colsplit, and tidyr::gather {r # eshape2::melt. {r eval=FALSE, include=FALSE, echo=TRUE} multiple_choice_responses %&gt;% summarise_all(n_distinct) %&gt;% tidyr::gather(question, num_distinct_answers) %&gt;% arrange(desc(num_distinct_answers)) Let‚Äôs take a look at the question with the most distinct answers, WorkMethodsSelect. {r eval=FALSE, include=FALSE, echo=TRUE} multiple_choice_responses %&gt;% count(WorkMethodsSelect, sort = TRUE) We can see this is a multiple select question, where if a person selected multiple answers they‚Äôre listed as one entry, separated by commas. Let‚Äôs tidy it up. First, let‚Äôs get rid of the NAs. We can use !is.na(WorkMethodsSelect), short for is.na(WorkMethodsSelect) == FALSE, to filter out NAs. We then use str_split, from stringr, to divide the entries up. str_split(WorkMethodsSelect, , ) says Take this string and split it into a list by dividing it where there are ,s. {r eval=FALSE, include=FALSE, echo=TRUE} nested_workmethods &lt;- multiple_choice_responses %&gt;% select(WorkMethodsSelect) %&gt;% filter(!is.na(WorkMethodsSelect)) %&gt;% mutate(work_method = str_split(WorkMethodsSelect, , )) nested_workmethods %&gt;% select(work_method) Now we have a list column, with each entry in the list being one work method. We can unnest this so we can get back a tidy dataframe. {r eval=FALSE, include=FALSE, echo=TRUE} unnested_workmethods &lt;- nested_workmethods %&gt;% tidyr::unnest(work_method) %&gt;% select(work_method) unnested_workmethods Great! As a last step, let‚Äôs count this data so we can find which are the most common work methods people use. {r eval=FALSE, include=FALSE, echo=TRUE} unnested_workmethods %&gt;% count(work_method, sort = TRUE) We see the classic methods of data visualization, logistic regression, and cross-validation lead the pack. 535.2.1 Graphing Frequency of Different Work Challenges Now let‚Äôs move on to understanding what challenges people face at work. This was one of those categories where there were multiple questions asked, all having names starting with WorkChallengeFrequency and ending with the challenge (e.g DirtyData ). We can find the relevant columns by using the dplyr select helper contains. We then use gather to tidy the data for analysis, filter for only the non-NAs, and remove the WorkChallengeFrequency from each question using stringr::str_remove. {r eval=FALSE, include=FALSE, echo=TRUE} WorkChallenges &lt;- multiple_choice_responses %&gt;% select(contains( WorkChallengeFrequency )) %&gt;% gather(question, response) %&gt;% filter(!is.na(response)) %&gt;% mutate(question = stringr::str_remove(question, WorkChallengeFrequency )) WorkChallenges Let‚Äôs make a facet bar plot, one for each question with the frequency of responses.To make the x-axis tick labels readable, we‚Äôll change them to be vertical instead of horizontal. {r WorkChallenges_graph1, fig.width = 9, fig.height = 6} ggplot(WorkChallenges, aes(x = response)) + geom_bar() + facet_wrap(~question) + theme(axis.text.x = element_text(angle = 90, hjust = 1)) This graph has two main problems. First, there are too many histograms for it to be really useful. But second, the order of the x-axis is wrong. We want it to go from least often to most, but instead {r # arely is in the middle. We can manually reorder the level of this variable using forcats::fct_relevel. {r WorkChallenges_graph2, fig.width = 9, fig.height = 6} WorkChallenges %&gt;% mutate(response = fct_relevel(response, Rarely , Sometimes , Often , Most of the time )) %&gt;% ggplot(aes(x = response)) + geom_bar() + facet_wrap(~question) + theme(axis.text.x = element_text(angle = 90, hjust = 1)) Now we‚Äôve got the x-axis in the order we want it. Let‚Äôs try dichotomizing the variable by grouping most of the time and often together as the person considering something a challenge. We can use if_else and %in%. %in% is equivalent to {r # esponse == Most of the time | response == Often and can save you a lot of typing if you have a bunch of variables to match. Grouping by the question, we can use summarise to reduce the dataset to one row per question, adding the variable perc_problem for the percentage of responses that thought something was a challenge often or most of the time. This way, we can make one graph with data for all the questions and easily compare them. {r eval=FALSE, include=FALSE, echo=TRUE} perc_problem_work_challenge &lt;- WorkChallenges %&gt;% mutate(response = if_else(response %in% c( Most of the time , Often ), 1, 0)) %&gt;% group_by(question) %&gt;% summarise(perc_problem = mean(response)) {r perc_problem_work_challenge_graph, fig.width = 8, fig.height = 5} ggplot(perc_problem_work_challenge, aes(x = question, y = perc_problem)) + geom_point() + coord_flip() This is better, but it‚Äôs hard to read because the points are scattered all over the place. Although you can spot the highest one, you then have to track it back to the correct variable. And it‚Äôs also hard to tell the order of the ones in the middle. We can use forcats:fct_reorder to change the x-axis to be ordered by another variable, in this case the y-axis. While we‚Äôre at it, we can use scale_y_continuous andscales::percent to update our axis to display in percent and labs to change our axis labels. {r perc_problem_work_challenge_graph2, fig.width = 8, fig.height = 5} ggplot(perc_problem_work_challenge, aes(x = perc_problem, y = fct_reorder(question, perc_problem))) + geom_point() + scale_x_continuous(labels = scales::percent) + labs(y = Work Challenge , x = Percentage of people encountering challenge frequently ) Much better! You can now easily tell which work challenges are encountered most frequently. 535.2.2 Conclusion I‚Äôm a big advocate of using and teaching the tidyverse for data analysis and visualization in R (it runs in the family). In addition to doing these talks, I‚Äôve released a course on Categorical Data in the Tidyverse. I walk through some of the functions in this course and more from forcats. It‚Äôs part of the new Tidyverse Fundamentals skill track, which is suitable for people are new to R or those looking to switch to the tidyverse. Check it out and let us know what you think. Some other good resources for learning the tidyverse are Hadley Wickham and Garrett Grolemund‚Äôs free R for Data Science book and RStudio‚Äôs cheat sheets. If you have questions, I recommend using the tidyverse section of RStudio community and/or the #rstats hashtag on Twitter. If you do, make sure you include a reproducible example (see best practices here) with the reprex package! "],["tidycells.html", "Chapter 536 tidycells", " Chapter 536 tidycells "],["tidyxl.html", "Chapter 537 tidyxl", " Chapter 537 tidyxl https://github.com/nacnudus/tidyxl "],["tidycells-1.html", "Chapter 538 tidycells", " Chapter 538 tidycells {r eval=FALSE, include=FALSE, echo=TRUE} # remotes::install_github( nacnudus/tidyxl ) # remotes::install_github( r-rudra/tidycells ) # install.packages( tidycells ) # install.packages( tidyxl ) library(tidycells) {r eval=FALSE, include=FALSE, echo=TRUE} system.file( extdata , marks.xlsx , package = tidycells , mustWork = TRUE) {r eval=FALSE, include=FALSE, echo=TRUE} # you should have tidyxl installed system.file( extdata , marks.xlsx , package = tidycells , mustWork = TRUE) %&gt;% read_cells() "],["tufte-handout.html", "Chapter 539 Tufte Handout", " Chapter 539 Tufte Handout {r eval=FALSE, include=FALSE, echo=TRUE} library(tufte) # invalidate cache when the tufte version changes knitr::opts_chunk$set(tidy = FALSE, cache.extra = packageVersion(&#39;tufte&#39;)) options(htmltools.dir.version = FALSE) "],["introduction-5.html", "Chapter 540 Introduction", " Chapter 540 Introduction The Tufte handout style is a style that Edward Tufte uses in his books and handouts. Tufte‚Äôs style is known for its extensive use of sidenotes, tight integration of graphics with text, and well-set typography. This style has been implemented in LaTeX and HTML/CSS8, respectively. We have ported both implementations into the tufte package. If you want LaTeX/PDF output, you may use the tufte_handout format for handouts, and tufte_book for books. For HTML output, use tufte_html. These formats can be either specified in the YAML metadata at the beginning of an R Markdown document (see an example below), or passed to the {r # markdown::render() function. See Allaire et al. (2020) for more information about rmarkdown. # An Example Using the Tufte Style author: John Smith output: tufte::tufte_handout: default tufte::tufte_html: default There are two goals of this package: To produce both PDF and HTML output with similar styles from the same R Markdown document; To provide simple syntax to write elements of the Tufte style such as side notes and margin figures, e.g. when you want a margin figure, all you need to do is the chunk option fig.margin = TRUE, and we will take care of the details for you, so you never need to think about \\begin{marginfigure} \\end{marginfigure} or &lt;span class= marginfigure &gt; &lt;/span&gt;; the LaTeX and HTML code under the hood may be complicated, but you never need to learn or write such code. If you have any feature requests or find bugs in tufte, please do not hesitate to file them to https://github.com/rstudio/tufte/issues. For general questions, you may ask them on StackOverflow: http://stackoverflow.com/tags/rmarkdown. References "],["headings.html", "Chapter 541 Headings", " Chapter 541 Headings This style provides first and second-level headings (that is, # and ##), demonstrated in the next section. You may get unexpected output if you try to use ### and smaller headings. {r # newthought('In his later books')9, Tufte starts each section with a bit of vertical space, a non-indented paragraph, and sets the first few words of the sentence in small caps. To accomplish this using this style, call the newthought() function in tufte in an inline R expression `{r # ` as demonstrated at the beginning of this paragraph.10 Beautiful Evidence‚Ü©Ô∏é Note you should not assume tufte has been attached to your R session. You should either library(tufte) in your R Markdown document before you call newthought(), or use tufte::newthought().‚Ü©Ô∏é "],["figures.html", "Chapter 542 Figures 542.1 Margin Figures 542.2 Arbitrary Margin Content 542.3 Full Width Figures 542.4 Main Column Figures", " Chapter 542 Figures 542.1 Margin Figures Images and graphics play an integral role in Tufte‚Äôs work. To place figures in the margin you can use the knitr chunk option fig.margin = TRUE. For example: {r fig-margin, fig.margin = TRUE, fig.cap = MPG vs horsepower, colored by transmission. , fig.width=3.5, fig.height=3.5, cache=TRUE, message=FALSE} library(ggplot2) mtcars2 &lt;- mtcars mtcars2$am &lt;- factor( mtcars$am, labels = c(&#39;automatic&#39;, &#39;manual&#39;) ) ggplot(mtcars2, aes(hp, mpg, color = am)) + geom_point() + geom_smooth() + theme(legend.position = &#39;bottom&#39;) Note the use of the fig.cap chunk option to provide a figure caption. You can adjust the proportions of figures using the fig.width and fig.height chunk options. These are specified in inches, and will be automatically scaled down to fit within the handout margin. 542.2 Arbitrary Margin Content In fact, you can include anything in the margin using the knitr engine named marginfigure. Unlike R code chunks ``` {r eval=FALSE, include=FALSE, echo=TRUE}, you write a chunk starting with ``` {marginfigure} instead, then put the content in the chunk. See an example on the right about the first fundamental theorem of calculus. {marginfigure} We know from _the first fundamental theorem of calculus_ that for $x$ in $[a, b]$: $$\\frac{d}{dx}\\left( \\int_{a}^{x} f(u)\\,du\\right)=f(x).$$ For the sake of portability between LaTeX and HTML, you should keep the margin content as simple as possible (syntax-wise) in the marginefigure blocks. You may use simple Markdown syntax like **bold** and _italic_ text, but please refrain from using footnotes, citations, or block-level elements (e.g. blockquotes and lists) there. Note: if you set echo = TRUE in your global chunk options, you will have to add echo = TRUE to the chunk to display a margin figure, for example ``` {marginfigure, echo = TRUE}. 542.3 Full Width Figures You can arrange for figures to span across the entire page by using the chunk option fig.fullwidth = TRUE. {r fig-fullwidth, fig.width = 10, fig.height = 2, fig.fullwidth = TRUE, fig.cap = A full width figure. , warning=FALSE, message=FALSE, cache=TRUE} ggplot(diamonds, aes(carat, price)) + geom_smooth() + facet_grid(~ cut) Other chunk options related to figures can still be used, such as fig.width, fig.cap, out.width, and so on. For full width figures, usually fig.width is large and fig.height is small. In the above example, the plot size is \\(10 \\times 2\\). 542.4 Main Column Figures Besides margin and full width figures, you can of course also include figures constrained to the main column. This is the default type of figures in the LaTeX/HTML output. {r fig-main, fig.cap = A figure in the main column. , cache=TRUE} ggplot(diamonds, aes(cut, price)) + geom_boxplot() "],["sidenotes.html", "Chapter 543 Sidenotes", " Chapter 543 Sidenotes One of the most prominent and distinctive features of this style is the extensive use of sidenotes. There is a wide margin to provide ample room for sidenotes and small figures. Any use of a footnote will automatically be converted to a sidenote.11 If you‚Äôd like to place ancillary information in the margin without the sidenote mark (the superscript number), you can use the margin_note() function from tufte in an inline R expression. {r # margin_note( This is a margin note. Notice that there is no number preceding the note. ) This function does not process the text with Pandoc, so Markdown syntax will not work here. If you need to write anything in Markdown syntax, please use the marginfigure block described previously. This is a sidenote that was entered using a footnote.‚Ü©Ô∏é "],["references-1.html", "Chapter 544 References", " Chapter 544 References References can be displayed as margin notes for HTML output. For example, we can cite R here (R Core Team 2020). To enable this feature, you must set link-citations: yes in the YAML metadata, and the version of pandoc-citeproc should be at least 0.7.2. You can always install your own version of Pandoc from http://pandoc.org/installing.html if the version is not sufficient. To check the version of pandoc-citeproc in your system, you may run this in R: {r eval=FALSE, include=FALSE, echo=TRUE} system2(&#39;pandoc-citeproc&#39;, &#39;--version&#39;) If your version of pandoc-citeproc is too low, or you did not set link-citations: yes in YAML, references in the HTML output will be placed at the end of the output document. References "],["tables-6.html", "Chapter 545 Tables", " Chapter 545 Tables You can use the kable() function from the knitr package to format tables that integrate well with the rest of the Tufte handout style. The table captions are placed in the margin like figures in the HTML output. {r eval=FALSE, include=FALSE, echo=TRUE} knitr::kable( mtcars[1:6, 1:6], caption = &#39;A subset of mtcars.&#39; ) "],["block-quotes.html", "Chapter 546 Block Quotes", " Chapter 546 Block Quotes We know from the Markdown syntax that paragraphs that start with &gt; are converted to block quotes. If you want to add a right-aligned footer for the quote, you may use the function quote_footer() from tufte in an inline R expression. Here is an example: If it weren‚Äôt for my lawyer, I‚Äôd still be in prison. It went a lot faster with two people digging. {r # tufte::quote_footer(' Joe Martin') Without using quote_footer(), it looks like this (the second line is just a normal paragraph): Great people talk about ideas, average people talk about things, and small people talk about wine. Fran Lebowitz "],["responsiveness.html", "Chapter 547 Responsiveness", " Chapter 547 Responsiveness The HTML page is responsive in the sense that when the page width is smaller than 760px, sidenotes and margin notes will be hidden by default. For sidenotes, you can click their numbers (the superscripts) to toggle their visibility. For margin notes, you may click the circled plus signs to toggle visibility. "],["more-examples.html", "Chapter 548 More Examples", " Chapter 548 More Examples The rest of this document consists of a few test cases to make sure everything still works well in slightly more complicated scenarios. First we generate two plots in one figure environment with the chunk option fig.show = 'hold': {r fig-two-together, fig.cap= Two plots in one figure environment. , fig.show=&#39;hold&#39;, cache=TRUE, message=FALSE} p &lt;- ggplot(mtcars2, aes(hp, mpg, color = am)) + geom_point() p p + geom_smooth() Then two plots in separate figure environments (the code is identical to the previous code chunk, but the chunk option is the default fig.show = 'asis' now): {r fig-two-separate, ref.label=&#39;fig-two-together&#39;, fig.cap=sprintf( Two plots in separate figure environments (the %s plot). , c( first , second )), cache=TRUE, message=FALSE} You may have noticed that the two figures have different captions, and that is because we used a character vector of length 2 for the chunk option fig.cap (something like fig.cap = c('first plot', 'second plot')). Next we show multiple plots in margin figures. Similarly, two plots in the same figure environment in the margin: {r fig-margin-together, fig.margin=TRUE, fig.show=&#39;hold&#39;, fig.cap= Two plots in one figure environment in the margin. , fig.width=3.5, fig.height=2.5, cache=TRUE} p p + geom_smooth(method = &#39;lm&#39;) Then two plots from the same code chunk placed in different figure environments: {r fig-margin-separate, fig.margin=TRUE, fig.cap=sprintf( Two plots in separate figure environments in the margin (the %s plot). , c( first , second )), fig.width=3.5, fig.height=2.5, cache=TRUE} knitr::kable(head(iris, 15)) p knitr::kable(head(iris, 12)) p + geom_smooth(method = &#39;lm&#39;) knitr::kable(head(iris, 5)) We blended some tables in the above code chunk only as placeholders to make sure there is enough vertical space among the margin figures, otherwise they will be stacked tightly together. For a practical document, you should not insert too many margin figures consecutively and make the margin crowded. You do not have to assign captions to figures. We show three figures with no captions below in the margin, in the main column, and in full width, respectively. {r fig-nocap-margin, fig.margin=TRUE, fig.width=3.5, fig.height=2, cache=TRUE} # a boxplot of weight vs transmission; this figure # will be placed in the margin ggplot(mtcars2, aes(am, wt)) + geom_boxplot() + coord_flip() {r fig-nocap-main, cache=TRUE} # a figure in the main column p &lt;- ggplot(mtcars, aes(wt, hp)) + geom_point() p {r fig-nocap-fullwidth, fig.fullwidth=TRUE, fig.width=10, fig.height=3, cache=TRUE} # a fullwidth figure p + geom_smooth(method = &#39;lm&#39;) + facet_grid(~ gear) "],["some-notes-on-tufte-css.html", "Chapter 549 Some Notes on Tufte CSS", " Chapter 549 Some Notes on Tufte CSS There are a few other things in Tufte CSS that we have not mentioned so far. If you prefer {r # sans_serif('sans-serif fonts'), use the function sans_serif() in tufte. For epigraphs, you may use a pair of underscores to make the paragraph italic in a block quote, e.g. I can win an argument on any topic, against any opponent. People know this, and steer clear of me at parties. Often, as a sign of their great respect, they don‚Äôt even invite me. {r # quote_footer(' Dave Barry') We hope you will enjoy the simplicity of R Markdown and this R package, and we sincerely thank the authors of the Tufte-CSS and Tufte-LaTeX projects for developing the beautiful CSS and LaTeX classes. Our tufte package would not have been possible without their heavy lifting. You can turn on/off some features of the Tufte style in HTML output. The default features enabled are: output: tufte::tufte_html: tufte_features: [ fonts , background , italics ] If you do not want the page background to be lightyellow, you can remove background from tufte_features. You can also customize the style of the HTML page via a CSS file. For example, if you do not want the subtitle to be italic, you can define h3.subtitle em { font-style: normal; } in, say, a CSS file my_style.css (under the same directory of your Rmd document), and apply it to your HTML output via the css option, e.g., output: tufte::tufte_html: tufte_features: [ fonts , background ] css: my_style.css There is also a variant of the Tufte style in HTML/CSS named Envisoned CSS . This style can be used by specifying the argument tufte_variant = 'envisioned' in tufte_html()12, e.g. output: tufte::tufte_html: tufte_variant: envisioned To see the R Markdown source of this example document, you may follow this link to Github, use the wizard in RStudio IDE (File -&gt; New File -&gt; R Markdown -&gt; From Template), or open the Rmd file in the package: {r eval=FALSE, include=FALSE, echo=TRUE} file.edit( tufte:::template_resources( &#39;tufte_html&#39;, &#39;..&#39;, &#39;skeleton&#39;, &#39;skeleton.Rmd&#39; ) ) This document is also available in Chinese, and its envisioned style can be found here. {r bib, include=FALSE} # create a bib file for the R packages used in this document knitr::write_bib(c(&#39;base&#39;, &#39;rmarkdown&#39;), file = &#39;skeleton.bib&#39;) The actual Envisioned CSS was not used in the tufte package. We only changed the fonts, background color, and text color based on the default Tufte style.‚Ü©Ô∏é "],["tutorials.html", "Chapter 550 Tutorials", " Chapter 550 Tutorials Exploratory Data Analysis &amp; Data Preparation with ‚ÄòfunModeling‚Äô https://blog.datascienceheroes.com/exploratory-data-analysis-data-preparation-with-funmodeling/ Content for Wrangling data in the Tidyverse - a tutorial given at useR! 2018 https://github.com/drsimonj/tidyverse_tutorial-useR2018 Teaching R to New Users - From tapply to the Tidyverse https://simplystatistics.org/2018/07/12/use-r-keynote-2018/ jstor An R Package for Analysing Scientific Articles https://speakerdeck.com/tklebel/jstor-an-r-package-for-analysing-scientific-articles?slide=25 workshop in survival analysis using R https://github.com/dave-harrington/survival_workshop https://github.com/dave-harrington/eventtimedata Teaching R to New Users - From tapply to the Tidyverse https://simplystatistics.org/2018/07/12/use-r-keynote-2018/ Data Driven Decision Making (D3M) http://www.vishalsingh.org/teaching/#content Tidymodeling Titanic Tragedy https://cdn.rawgit.com/ClaytonJY/tidymodels-talk/145e6574/slides.html#1 Disease risk modelling and visualization using R https://paula-moraga.github.io/teaching/ Creating a Geodemographic Classification Using K-means Clustering in R https://data.cdrc.ac.uk/tutorial/creating-a-geodemographic-classification-using-k-means-clustering-in-r Advanced R http://adv-r.had.co.nz/ How to use R for matching samples (propensity score) https://datascienceplus.com/how-to-use-r-for-matching-samples-propensity-score/ "],["tweetbook1.html", "Chapter 551 tweetbook1", " Chapter 551 tweetbook1 https://rtweet.info/ {r include=FALSE, eval=FALSE, echo = TRUE} library(tidyverse) library(rtweet) "],["get-data.html", "Chapter 552 get data", " Chapter 552 get data {r include=FALSE, eval=FALSE, echo = TRUE} gipath &lt;- rtweet::search_tweets(q = #gipath , n = 18000, include_rts = FALSE, # retryonratelimit = TRUE ) # gipath %&gt;% # select(user_id,status_id, contains( url )) %&gt;% # filter(!is.na(ext_media_url)) %&gt;% # View() "],["filter-tweet.html", "Chapter 553 filter tweet", " Chapter 553 filter tweet {r filter tweet} for (i in 1:dim(gipath)[1]) { nam &lt;- paste0( gitweetid , i) assign(nam, gipath$status_id[i]) nam2 &lt;- paste0( gitweet , i) assign(nam2, gipath[gipath$status_id==gitweetid1, ]) } "],["tweet-owner.html", "Chapter 554 tweet owner", " Chapter 554 tweet owner {r eval=FALSE, include=FALSE, echo=TRUE} gitweet1$screen_name "],["tweet-time.html", "Chapter 555 tweet time", " Chapter 555 tweet time {r eval=FALSE, include=FALSE, echo=TRUE} gitweet1$created_at "],["tweet-text.html", "Chapter 556 tweet text", " Chapter 556 tweet text {r eval=FALSE, include=FALSE, echo=TRUE} gitweet1$text "],["tweet-media.html", "Chapter 557 tweet media", " Chapter 557 tweet media {r tweet media} unlist(gitweet2$ext_media_url) {r tweet media length} length(unlist(gitweet2$ext_media_url)) {r eval=FALSE, include=FALSE, echo=TRUE} "],["download-image-read-image.html", "Chapter 558 download image, read image", " Chapter 558 download image, read image {r download image read image, error=FALSE, comment=FALSE} for (i in 1:length(unlist(gitweet2$ext_media_url))) { urls &lt;- unlist(gitweet2$ext_media_url)[i] dest &lt;- paste0( twfigure/jpeg ,i, .jpg , collapse = ) download.file(url = urls, destfile = dest, mode = &#39;wb&#39;) } images &lt;- capture.output( cat( for (i in 1:length(unlist(gitweet2$ext_media_url))) { cat( paste0( ![](twfigure/jpeg , i, .jpg){width=30%} , collapse = ), \\n ) } , sep = \\n ) ) "],["auto-output.html", "Chapter 559 Auto Output", " Chapter 559 Auto Output Tweet by: {r # gitweet2$screen_name Tweet Time: {r # gitweet2$created_at {r # gitweet2$text {r # unlist(gitweet2$hashtags) {r # unlist(gitweet2$symbols) {r # unlist(gitweet2$urls_expanded_url) {r # images {r eval=FALSE, include=FALSE, echo=TRUE} tweetbody1 &lt;- gitweet1$screen_name; gitweet1$created_at; gitweet1$text; unlist(gitweet1$hashtags); unlist(gitweet1$symbols); unlist(gitweet1$urls_expanded_url) evaluate::replay(evaluate::evaluate(tweetbody1)) {r eval=FALSE, include=FALSE, echo=TRUE} library(evaluate) s &lt;- paste(capture.output(replay(evaluate::evaluate(tweetbody1))), collapse= \\n ) cat(s) {r eval=FALSE, include=FALSE, echo=TRUE} dont_print_source = function(x){ if (class(x)!= source ){ cat(x) } } L &lt;- evaluate::evaluate(tweetbody1) # library(R.utils) # s3 &lt;- paste(captureOutput( # for(i in 1:length(L)) dont_print_source(L[[i]]) # ), collapse= \\n ) s3 &lt;- gsub( \\\\[1]|\\ , , paste(capture.output( for(i in 1:length(L)) dont_print_source(L[[i]]) ), collapse= \\n )) {r # s3 https://community.rstudio.com/t/how-to-use-an-object-rather-than-a-file-as-source-for-knitting-resolved/3057/6 {r eval=FALSE, include=FALSE, echo=TRUE} tweetAutoOutput &lt;- function(x) { } Tweet by: {r # gitweet2$screen_name Tweet Time: {r # gitweet2$created_at {r # gitweet2$text {r # unlist(gitweet2$hashtags) {r # unlist(gitweet2$symbols) {r # unlist(gitweet2$urls_expanded_url) {r # images "],["olders.html", "Chapter 560 olders", " Chapter 560 olders rtweet::get_collections( serdarbalci ) {r eval=FALSE, include=FALSE, echo=TRUE} gitweet1 &lt;- gipath %&gt;% filter(user_id == 3011337389) {r eval=FALSE, include=FALSE, echo=TRUE} gitweet1$user_id {r eval=FALSE, include=FALSE, echo=TRUE} gitweet1$status_id {r eval=FALSE, include=FALSE, echo=TRUE} gitweet1$created_at {r eval=FALSE, include=FALSE, echo=TRUE} gitweet1$screen_name {r eval=FALSE, include=FALSE, echo=TRUE} gitweet1$ext_media_url {r eval=FALSE, include=FALSE, echo=TRUE} length(gitweet1$ext_media_url) {r eval=FALSE, include=FALSE, echo=TRUE} imageurl1 &lt;- gitweet1$ext_media_url[[1]][1] {r eval=FALSE, include=FALSE, echo=TRUE} imageurl1 {r eval=FALSE, include=FALSE, echo=TRUE} imageurl2 &lt;- gitweet1$ext_media_url[[1]][2] {r eval=FALSE, include=FALSE, echo=TRUE} imageurl2 {r eval=FALSE, include=FALSE, echo=TRUE} gitweet1$text {r eval=FALSE, include=FALSE, echo=TRUE} gitweet1$source {r eval=FALSE, include=FALSE, echo=TRUE} gitweet1$display_text_width {r eval=FALSE, include=FALSE, echo=TRUE} gitweet1$reply_to_status_id {r eval=FALSE, include=FALSE, echo=TRUE} gitweet1$reply_to_status_id {r eval=FALSE, include=FALSE, echo=TRUE} gitweet1$reply_to_user_id {r eval=FALSE, include=FALSE, echo=TRUE} gitweet1$reply_to_screen_name {r eval=FALSE, include=FALSE, echo=TRUE} gitweet1$is_quote {r eval=FALSE, include=FALSE, echo=TRUE} gitweet1$is_retweet {r eval=FALSE, include=FALSE, echo=TRUE} gitweet1$favorite_count {r eval=FALSE, include=FALSE, echo=TRUE} gitweet1$retweet_count {r eval=FALSE, include=FALSE, echo=TRUE} gitweet1$hashtags {r eval=FALSE, include=FALSE, echo=TRUE} gitweet1$symbols {r eval=FALSE, include=FALSE, echo=TRUE} gitweet1$urls_url {r eval=FALSE, include=FALSE, echo=TRUE} gitweet1$urls_t.co {r eval=FALSE, include=FALSE, echo=TRUE} gitweet1$urls_expanded_url {r eval=FALSE, include=FALSE, echo=TRUE} gitweet1$media_url {r eval=FALSE, include=FALSE, echo=TRUE} gitweet1$media_t.co {r eval=FALSE, include=FALSE, echo=TRUE} gitweet1$media_expanded_url {r eval=FALSE, include=FALSE, echo=TRUE} gitweet1$media_type {r eval=FALSE, include=FALSE, echo=TRUE} gitweet1$ext_media_url {r eval=FALSE, include=FALSE, echo=TRUE} gitweet1$ext_media_t.co {r eval=FALSE, include=FALSE, echo=TRUE} gitweet1$ext_media_expanded_url {r eval=FALSE, include=FALSE, echo=TRUE} gitweet1$ext_media_type {r eval=FALSE, include=FALSE, echo=TRUE} gitweet1$mentions_user_id {r eval=FALSE, include=FALSE, echo=TRUE} gitweet1$mentions_screen_name {r eval=FALSE, include=FALSE, echo=TRUE} gitweet1$lang {r eval=FALSE, include=FALSE, echo=TRUE} gitweet1$quoted_status_id {r eval=FALSE, include=FALSE, echo=TRUE} gitweet1$quoted_text {r eval=FALSE, include=FALSE, echo=TRUE} gitweet1$quoted_created_at {r eval=FALSE, include=FALSE, echo=TRUE} gitweet1$quoted_source {r eval=FALSE, include=FALSE, echo=TRUE} gitweet1$quoted_favorite_count {r eval=FALSE, include=FALSE, echo=TRUE} gitweet1$quoted_retweet_count {r eval=FALSE, include=FALSE, echo=TRUE} gitweet1$quoted_user_id {r eval=FALSE, include=FALSE, echo=TRUE} gitweet1$quoted_screen_name {r eval=FALSE, include=FALSE, echo=TRUE} gitweet1$quoted_name {r eval=FALSE, include=FALSE, echo=TRUE} gitweet1$quoted_followers_count {r eval=FALSE, include=FALSE, echo=TRUE} gitweet1$quoted_friends_count {r eval=FALSE, include=FALSE, echo=TRUE} gitweet1$quoted_statuses_count {r eval=FALSE, include=FALSE, echo=TRUE} gitweet1$quoted_location {r eval=FALSE, include=FALSE, echo=TRUE} gitweet1$quoted_description {r eval=FALSE, include=FALSE, echo=TRUE} gitweet1$quoted_verified {r eval=FALSE, include=FALSE, echo=TRUE} gitweet1$retweet_status_id {r eval=FALSE, include=FALSE, echo=TRUE} gitweet1$retweet_text {r eval=FALSE, include=FALSE, echo=TRUE} gitweet1$retweet_created_at {r eval=FALSE, include=FALSE, echo=TRUE} gitweet1$retweet_source {r eval=FALSE, include=FALSE, echo=TRUE} gitweet1$retweet_favorite_count {r eval=FALSE, include=FALSE, echo=TRUE} gitweet1$retweet_retweet_count {r eval=FALSE, include=FALSE, echo=TRUE} gitweet1$retweet_user_id {r eval=FALSE, include=FALSE, echo=TRUE} gitweet1$retweet_screen_name {r eval=FALSE, include=FALSE, echo=TRUE} gitweet1$retweet_name {r eval=FALSE, include=FALSE, echo=TRUE} gitweet1$retweet_followers_count {r eval=FALSE, include=FALSE, echo=TRUE} gitweet1$retweet_friends_count {r eval=FALSE, include=FALSE, echo=TRUE} gitweet1$retweet_statuses_count {r eval=FALSE, include=FALSE, echo=TRUE} gitweet1$retweet_location {r eval=FALSE, include=FALSE, echo=TRUE} gitweet1$retweet_description {r eval=FALSE, include=FALSE, echo=TRUE} gitweet1$retweet_verified {r eval=FALSE, include=FALSE, echo=TRUE} gitweet1$place_url {r eval=FALSE, include=FALSE, echo=TRUE} gitweet1$place_name {r eval=FALSE, include=FALSE, echo=TRUE} gitweet1$place_full_name {r eval=FALSE, include=FALSE, echo=TRUE} gitweet1$place_type {r eval=FALSE, include=FALSE, echo=TRUE} gitweet1$country {r eval=FALSE, include=FALSE, echo=TRUE} gitweet1$country_code {r eval=FALSE, include=FALSE, echo=TRUE} gitweet1$geo_coords {r eval=FALSE, include=FALSE, echo=TRUE} gitweet1$coords_coords {r eval=FALSE, include=FALSE, echo=TRUE} gitweet1$bbox_coords {r eval=FALSE, include=FALSE, echo=TRUE} gitweet1$status_url {r eval=FALSE, include=FALSE, echo=TRUE} gitweet1$name {r eval=FALSE, include=FALSE, echo=TRUE} gitweet1$location {r eval=FALSE, include=FALSE, echo=TRUE} gitweet1$description {r eval=FALSE, include=FALSE, echo=TRUE} gitweet1$url {r eval=FALSE, include=FALSE, echo=TRUE} gitweet1$protected {r eval=FALSE, include=FALSE, echo=TRUE} gitweet1$followers_count {r eval=FALSE, include=FALSE, echo=TRUE} gitweet1$friends_count {r eval=FALSE, include=FALSE, echo=TRUE} gitweet1$listed_count {r eval=FALSE, include=FALSE, echo=TRUE} gitweet1$statuses_count {r eval=FALSE, include=FALSE, echo=TRUE} gitweet1$favourites_count {r eval=FALSE, include=FALSE, echo=TRUE} gitweet1$account_created_at {r eval=FALSE, include=FALSE, echo=TRUE} gitweet1$verified {r eval=FALSE, include=FALSE, echo=TRUE} gitweet1$profile_url {r eval=FALSE, include=FALSE, echo=TRUE} gitweet1$profile_expanded_url {r eval=FALSE, include=FALSE, echo=TRUE} gitweet1$account_lang {r eval=FALSE, include=FALSE, echo=TRUE} gitweet1$profile_banner_url {r eval=FALSE, include=FALSE, echo=TRUE} gitweet1$profile_background_url {r eval=FALSE, include=FALSE, echo=TRUE} gitweet1$profile_image_url {r eval=FALSE, include=FALSE, echo=TRUE} # gipath %&gt;% # select(user_id,status_id, contains( url )) %&gt;% # filter(!is.na(ext_media_url)) %&gt;% # select(status_id, contains( media_url )) %&gt;% # View() {r eval=FALSE, include=FALSE, echo=TRUE} gitweet2 &lt;- gipath %&gt;% filter(status_id ==1096465400848699392) {r eval=FALSE, include=FALSE, echo=TRUE} gitweet2$ext_media_url {r eval=FALSE, include=FALSE, echo=TRUE} unlist(gitweet2$ext_media_url) {r eval=FALSE, include=FALSE, echo=TRUE} length(unlist(gitweet2$ext_media_url)) {r eval=FALSE, include=FALSE, echo=TRUE} # jpeg1 &lt;- paste0( \\ , unlist(gitweet2$ext_media_url)[1], \\ ) # jpeg1 &lt;- unlist(gitweet2$ext_media_url)[1] # jpeg2 &lt;- unlist(gitweet2$ext_media_url)[2] # jpeg3 &lt;- unlist(gitweet2$ext_media_url)[3] # jpeg4 &lt;- unlist(gitweet2$ext_media_url)[4] # &lt;center&gt;&lt;img src= `{r # jpeg1` &gt;&lt;img src= `{r # jpeg2` &gt;&lt;/center&gt; {r eval=FALSE, include=FALSE, echo=TRUE} for (i in 1:length(unlist(gitweet2$ext_media_url))) { nam &lt;- paste0( jpeg , i) assign(nam, unlist(gitweet2$ext_media_url)[i]) } {r eval=FALSE, include=FALSE, echo=TRUE} download.file(url = jpeg1, destfile = &#39;twfigure/jpeg1.jpg&#39;, mode = &#39;wb&#39;) dene1 dene2 image 0 image 1 &lt;img src= {r # jpeg1 &gt; image 2 &lt;img src= {r # jpeg1 &gt;&lt;img src= {r # jpeg2 &gt;&lt;img src= {r # jpeg3 &gt; image 3 &lt;img src= {r # jpeg1 &gt;&lt;img src= {r # jpeg2 &gt;&lt;img src= {r # jpeg3 &gt; "],["tweetrmd.html", "Chapter 561 tweetrmd", " Chapter 561 tweetrmd https://github.com/gadenbuie/tweetrmd "],["tweetrmd-1.html", "Chapter 562 tweetrmd 562.1 Installation 562.2 Embed a Tweet 562.3 Take a screenshot of a tweet 562.4 Customize tweet appearance", " Chapter 562 tweetrmd Easily embed Tweets anywhere R Markdown turns plain text into HTML. 562.1 Installation You can install the released version of tweetrmd from GitHub: # install.packages( devtools ) devtools::install_github( gadenbuie/tweetrmd ) 562.2 Embed a Tweet {r eval=FALSE, include=FALSE, echo=TRUE} library(tweetrmd) tweet_embed( https://twitter.com/alexpghayes/status/1211748406730706944 ) Or if you would rather use the screen name and status id. {r eval=FALSE, include=FALSE, echo=TRUE} tweet_embed(tweet_url( alexpghayes , 1211748406730706944 )) 562.3 Take a screenshot of a tweet Screenshots are automatically embedded in R Markdown documents, or you can save the screenshot as a .png or .pdf file. Uses the rstudio/webshot2 package. {r screenshot, out.width= 400px } tweet_screenshot(tweet_url( alexpghayes , 1211748406730706944 )) 562.4 Customize tweet appearance Twitter‚Äôs oembed API provides a number of options, all of which are made available for customization in tweet_embed() and tweet_screenshot(). {r screenshot-customized, out.width= 300px } tweet_screenshot( tweet_url( alexpghayes , 1211748406730706944 ), maxwidth = 300, hide_media = TRUE, theme = dark ) Note: When using tweet_embed(), you may need to add the following line to your YAML header for strict markdown output formats. "],["r-notebook-5.html", "Chapter 563 R Notebook", " Chapter 563 R Notebook 563.0.1 API connection http://rtweet.info/ https://apps.twitter.com/ {r eval=FALSE, include=FALSE, echo=TRUE} # install.packages( rtweet ) library(rtweet) {r eval=FALSE, include=FALSE, echo=TRUE} # web browser method: create token and save it as an environment variable create_token( app = , consumer_key = , consumer_secret = ) # ## authenticate via access token # token &lt;- create_token( # app = , # consumer_key = , # acess_token = , # access_secret = ) # # # ## create token and save it as an environment variable # twitter_token &lt;- create_token( # app = appname, # consumer_key = key, # consumer_secret = secret, # access_token = access_token, # access_secret = access_secret # ) # # # ## check to see if the token is loaded # identical(twitter_token, get_token()) 563.0.2 Search Tweets {r eval=FALSE, include=FALSE, echo=TRUE} ## search for 18000 tweets using the hashtag rt &lt;- search_tweets( #GIpath , n = 18000, include_rts = FALSE ) {r eval=FALSE, include=FALSE, echo=TRUE} ## preview tweets data head(rt) dplyr::glimpse(rt) {r eval=FALSE, include=FALSE, echo=TRUE} ## preview users data users_data(rt) {r eval=FALSE, include=FALSE, echo=TRUE} ## plot time series (if ggplot2 is installed) ts_plot(rt) {r eval=FALSE, include=FALSE, echo=TRUE} ## plot time series of tweets ts_plot(rt, 3 hours ) + ggplot2::theme_minimal() + ggplot2::theme(plot.title = ggplot2::element_text(face = bold )) + ggplot2::labs( x = NULL, y = NULL, title = Frequency of #pathologists Twitter statuses from past 9 days , subtitle = Twitter status (tweet) counts aggregated using three-hour intervals , caption = \\nSource: Data collected from Twitter&#39;s REST API via rtweet ) {r eval=FALSE, include=FALSE, echo=TRUE} ## search for 250,000 tweets containing the word data rt1 &lt;- search_tweets( USCAP , n = 250000, retryonratelimit = TRUE ) {r eval=FALSE, include=FALSE, echo=TRUE} ## search for 10,000 tweets sent from the US rt1 &lt;- search_tweets( lang:en , geocode = lookup_coords( usa ), n = 10000 ) {r eval=FALSE, include=FALSE, echo=TRUE} ## create lat/lng variables using all available tweet and profile geo-location data rt1 &lt;- lat_lng(rt1) ## plot state boundaries par(mar = c(0, 0, 0, 0)) maps::map( state , lwd = .25) ## plot lat and lng points onto state map with(rt1, points(lng, lat, pch = 20, cex = .75, col = rgb(0, .3, .7, .75))) 563.0.3 Stream Tweets {r eval=FALSE, include=FALSE, echo=TRUE} ## random sample for 30 seconds (default) rt3 &lt;- stream_tweets( ) {r eval=FALSE, include=FALSE, echo=TRUE} rt3 {r eval=FALSE, include=FALSE, echo=TRUE} ## stream tweets from london for 60 seconds rt &lt;- stream_tweets(lookup_coords( london, uk ), timeout = 60) {r eval=FALSE, include=FALSE, echo=TRUE} ## stream london tweets for a week (60 secs x 60 mins * 24 hours * 7 days) # stream_tweets( # realdonaldtrump,trump , # timeout = 60 * 60 * 24 * 7, # file_name = tweetsabouttrump.json , # parse = FALSE # ) stream_tweets( realdonaldtrump,trump , timeout = 60 * 60 * 1 * 1, file_name = tweetsabouttrump.json , parse = FALSE ) {r eval=FALSE, include=FALSE, echo=TRUE} ## read in the data as a tidy tbl data frame djt &lt;- parse_stream( tweetsabouttrump.json ) {r eval=FALSE, include=FALSE, echo=TRUE} djt 563.0.4 Twitter connections and user information {r eval=FALSE, include=FALSE, echo=TRUE} ## get user IDs of accounts followed by CNN # cnn_fds &lt;- get_friends( cnn ) sb_fds &lt;- get_friends( serdarbalci ) sb_fds {r eval=FALSE, include=FALSE, echo=TRUE} ## lookup data on those accounts # cnn_fds_data &lt;- lookup_users(cnn_fds$user_id) sb_fds_data &lt;- lookup_users(sb_fds$user_id) sb_fds_data {r eval=FALSE, include=FALSE, echo=TRUE} sb_follows &lt;- sb_fds_data$screen_name sb_follows[1:50] {r eval=FALSE, include=FALSE, echo=TRUE} ## get user IDs of accounts following CNN # cnn_flw &lt;- get_followers( cnn , n = 75000) sb_flw &lt;- get_followers( serdarbalci ) sb_flw {r eval=FALSE, include=FALSE, echo=TRUE} ## lookup data on those accounts # cnn_flw_data &lt;- lookup_users(cnn_flw$user_id) sb_flw_data &lt;- lookup_users(sb_flw$user_id[1:10]) sb_flw_data {r eval=FALSE, include=FALSE, echo=TRUE} # ## how many total follows does cnn have? # cnn &lt;- lookup_users( cnn ) # # ## get them all (this would take a little over 5 days) # cnn_flw &lt;- get_followers( # cnn , n = cnn$followers_count, retryonratelimit = TRUE # ) {r eval=FALSE, include=FALSE, echo=TRUE} ## get user IDs of accounts followed by CNN tmls &lt;- get_timelines(c( cnn , BBCWorld , foxnews ), n = 3200) {r eval=FALSE, include=FALSE, echo=TRUE} ## plot the frequency of tweets for each user over time tmls %&gt;% dplyr::filter(created_at &gt; 2017-10-29 ) %&gt;% dplyr::group_by(screen_name) %&gt;% ts_plot( days , trim = 1L) + ggplot2::geom_point() + ggplot2::theme_minimal() + ggplot2::theme( legend.title = ggplot2::element_blank(), legend.position = bottom , plot.title = ggplot2::element_text(face = bold )) + ggplot2::labs( x = NULL, y = NULL, title = Frequency of Twitter statuses posted by news organization , subtitle = Twitter status (tweet) counts aggregated by day from October/November 2017 , caption = \\nSource: Data collected from Twitter&#39;s REST API via rtweet ) {r eval=FALSE, include=FALSE, echo=TRUE} # jkr &lt;- get_favorites( jk_rowling , n = 3000) sbf &lt;- get_favorites( serdarbalci , n = 3000) sbf {r eval=FALSE, include=FALSE, echo=TRUE} ## search for users with #rstats in their profiles # usrs &lt;- search_users( #rstats , n = 1000) path_usrs &lt;- search_users( pathology , n = 1000) {r eval=FALSE, include=FALSE, echo=TRUE} head(path_usrs) 563.0.5 Twitter Trends {r eval=FALSE, include=FALSE, echo=TRUE} sf &lt;- get_trends( san francisco ) {r eval=FALSE, include=FALSE, echo=TRUE} sf {r eval=FALSE, include=FALSE, echo=TRUE} ## lookup users by screen_name or user_id users &lt;- c( KimKardashian , justinbieber , taylorswift13 , espn , JoelEmbiid , cstonehoops , KUHoops , upshotnyt , fivethirtyeight , hadleywickham , cnn , foxnews , msnbc , maddow , seanhannity , potus , epa , hillaryclinton , realdonaldtrump , natesilver538 , ezraklein , annecoulter ) famous_tweeters &lt;- lookup_users(users) {r eval=FALSE, include=FALSE, echo=TRUE} ## preview users data famous_tweeters {r eval=FALSE, include=FALSE, echo=TRUE} # extract most recent tweets data from the famous tweeters tweets_data(famous_tweeters) 563.0.6 Post tweet via R {r eval=FALSE, include=FALSE, echo=TRUE} # post_tweet( my first rtweet #rstats, let us see if it works :) http://rtweet.info/ ) 563.0.7 Follow via R {r eval=FALSE, include=FALSE, echo=TRUE} ## ty for the follow ;) post_follow( kearneymw ) {r eval=FALSE, include=FALSE, echo=TRUE} ## quick overview of rtweet functions vignette( auth , package = rtweet ) ## quick overview of rtweet functions vignette( intro , package = rtweet ) ## working with the stream vignette( stream , package = rtweet ) ## working with the stream vignette( FAQ , package = rtweet ) 563.0.8 Election Analysis on Twitter {r eval=FALSE, include=FALSE, echo=TRUE} ## Stream keywords used to filter tweets q &lt;- hillaryclinton,imwithher,realdonaldtrump,maga,electionday ## Stream time in seconds so for one minute set timeout = 60 ## For larger chunks of time, I recommend multiplying 60 by the number ## of desired minutes. This method scales up to hours as well ## (x * 60 = x mins, x * 60 * 60 = x hours) ## Stream for 30 minutes # streamtime &lt;- 30 * 60 streamtime &lt;- 3 * 60 ## Filename to save json data (backup) filename &lt;- rtelect.json {r eval=FALSE, include=FALSE, echo=TRUE} ## Stream election tweets rt_election &lt;- stream_tweets(q = q, timeout = streamtime, file_name = filename) {r eval=FALSE, include=FALSE, echo=TRUE} ## No upfront-parse save as json file instead method stream_tweets( q = q, parse = FALSE, timeout = streamtime, file_name = filename ) {r eval=FALSE, include=FALSE, echo=TRUE} ## Parse from json file rt_election &lt;- parse_stream(filename) {r eval=FALSE, include=FALSE, echo=TRUE} ## stream_tweets2 method # twoweeks &lt;- 60L * 60L * 24L * 7L * 2L # congress &lt;- congress,senate,house of representatives,representatives,senators,legislative # stream_tweets2( # q = congress, # parse = FALSE, # timeout = twoweeks, # dir = congress-stream # ) {r eval=FALSE, include=FALSE, echo=TRUE} ## Parse from json file rt &lt;- parse_stream( congress-stream.json ) {r eval=FALSE, include=FALSE, echo=TRUE} ## Preview tweets data rt {r eval=FALSE, include=FALSE, echo=TRUE} ## Preview users data users_data(rt) {r eval=FALSE, include=FALSE, echo=TRUE} ## Plot time series of all tweets aggregated by second ts_plot(rt, by = secs ) {r eval=FALSE, include=FALSE, echo=TRUE} ## plot multiple time series by first grouping the data by screen name rt %&gt;% dplyr::group_by(screen_name) %&gt;% ts_plot() + ggplot2::labs( title = Tweets during election day for the 2016 U.S. election , subtitle = Tweets collected, parsed, and plotted using `{r # tweet` ) 563.0.9 Graphing Tweets http://graphtweets.john-coene.com/index.html {r eval=FALSE, include=FALSE, echo=TRUE} install.packages( graphTweets ) # CRAN release v0.4 library(graphTweets) library(igraph) # for plot {r eval=FALSE, include=FALSE, echo=TRUE} # tweets &lt;- rtweet::search_tweets( rstats ) tweets &lt;- rtweet::search_tweets( #pathologists ) {r eval=FALSE, include=FALSE, echo=TRUE} tweets %&gt;% gt_edges(text, screen_name, status_id) %&gt;% gt_graph() %&gt;% plot() {r eval=FALSE, include=FALSE, echo=TRUE} tweets %&gt;% gt_edges(text, screen_name, status_id) %&gt;% gt_graph() -&gt; graph class(graph) {r eval=FALSE, include=FALSE, echo=TRUE} graph {r eval=FALSE, include=FALSE, echo=TRUE} tweets %&gt;% gt_edges(text, screen_name, status_id) %&gt;% gt_collect() -&gt; edges names(edges) {r eval=FALSE, include=FALSE, echo=TRUE} edges {r eval=FALSE, include=FALSE, echo=TRUE} tweets %&gt;% gt_edges(text, screen_name, status_id) %&gt;% gt_nodes() %&gt;% gt_collect() -&gt; graph graph {r eval=FALSE, include=FALSE, echo=TRUE} lapply(graph, nrow) # number of edges and nodes {r eval=FALSE, include=FALSE, echo=TRUE} lapply(graph, names) # names of data.frames returned {r eval=FALSE, include=FALSE, echo=TRUE} tweets %&gt;% gt_edges(text, screen_name, status_id) %&gt;% gt_nodes(meta = TRUE) %&gt;% gt_collect() -&gt; graph graph # lapply(graph, names) # names of data.frames returned {r eval=FALSE, include=FALSE, echo=TRUE} tweets %&gt;% gt_edges(text, screen_name, status_id, datetime = created_at ) %&gt;% gt_nodes(meta = TRUE) %&gt;% gt_collect() -&gt; graph graph {r eval=FALSE, include=FALSE, echo=TRUE} # install.packages( sigmajs ) library(dplyr) library(sigmajs) # for plots tweets %&gt;% gt_edges(text, screen_name, status_id, datetime = created_at ) %&gt;% gt_nodes(meta = TRUE) %&gt;% gt_collect() -&gt; gt nodes &lt;- gt$nodes %&gt;% mutate( id = nodes, label = ifelse(is.na(name), nodes, name), size = n_edges, color = #1967be ) edges &lt;- gt$edges %&gt;% mutate( id = 1:n() ) sigmajs() %&gt;% sg_force_start() %&gt;% sg_nodes(nodes, id, label, size, color) %&gt;% sg_edges(edges, id, source, target) %&gt;% sg_force_stop(10000) {r eval=FALSE, include=FALSE, echo=TRUE} library(igraph) tweets %&gt;% gt_edges(text, screen_name, status_id) %&gt;% gt_graph() -&gt; g # communities wc &lt;- walktrap.community(g) V(g)$color &lt;- membership(wc) # plot # tons of arrguments because defaults are awful plot(g, layout = igraph::layout.fruchterman.reingold(g), vertex.color = V(g)$color, vertex.label.family = sans , vertex.label.color = hsv(h = 0, s = 0, v = 0, alpha = 0.0), vertex.size = igraph::degree(g), edge.arrow.size = 0.2, edge.arrow.width = 0.3, edge.width = 1, edge.color = hsv(h = 1, s = .59, v = .91, alpha = 0.7), vertex.frame.color= #fcfcfc ) {r eval=FALSE, include=FALSE, echo=TRUE} tweets %&gt;% gt_edges(text, screen_name, status_id, created_at ) %&gt;% gt_nodes() %&gt;% gt_dyn() %&gt;% gt_collect() -&gt; net {r eval=FALSE, include=FALSE, echo=TRUE} knitr::kable(head(net$edges)) {r eval=FALSE, include=FALSE, echo=TRUE} knitr::kable(head(net$nodes)) {r eval=FALSE, include=FALSE, echo=TRUE} library(sigmajs) # convert to numeric &amp; rescale edges &lt;- net$edges %&gt;% dplyr::mutate( id = 1:n(), created_at = as.numeric(created_at), created_at = (created_at - min(created_at)) / (max(created_at) - min(created_at)), created_at = created_at * 5000 ) nodes &lt;- net$nodes %&gt;% dplyr::mutate( id = source, size = n_edges ) mx &lt;- max(edges$created_at) + 500 sigmajs() %&gt;% sg_force_start() %&gt;% sg_nodes(nodes, id, size) %&gt;% sg_add_edges(edges, created_at, id, source, target, cumsum = FALSE, refresh = FALSE) %&gt;% sg_force_stop(delay = mx) %&gt;% sg_settings(defaultNodeColor = #1967be ) "],["a-shiny-app-with-rtweet.html", "Chapter 564 A Shiny App with rtweet", " Chapter 564 A Shiny App with rtweet https://aj17.shinyapps.io/twitteranalytics/ {r eval=FALSE, include=FALSE, echo=TRUE} library(blogdown) blogdown::shortcode(&#39;tweet&#39;, &#39;1014492923981803521&#39;) "],["conference-tweets.html", "Chapter 565 Conference tweets", " Chapter 565 Conference tweets https://twitter.com/APo_ORV/status/1016412207867973632 https://twitter.com/grrrck/status/959137137118646272 https://gadenbuie.shinyapps.io/rsconf_tweets/ https://github.com/gadenbuie/rsconf_tweets https://behindbars.shinyapps.io/user2018/ https://github.com/oliviergimenez/isec2018_tweet_analysis "],["twitter-article-mentions-and-citations.html", "Chapter 566 Twitter Article Mentions and Citations", " Chapter 566 Twitter Article Mentions and Citations https://github.com/dsquintana/ajp "],["twitterreport.html", "Chapter 567 twitterreport", " Chapter 567 twitterreport https://github.com/gvegayon/twitterreport "],["streamr.html", "Chapter 568 streamR", " Chapter 568 streamR https://cran.r-project.org/web/packages/streamR/ Retweet count for specific tweet https://stackoverflow.com/questions/10427147/retweet-count-for-specific-tweet "],["statquotes.html", "Chapter 569 statquotes", " Chapter 569 statquotes {r eval=FALSE, include=FALSE, echo=TRUE} install.packages( statquotes ) {r eval=FALSE, include=FALSE, echo=TRUE} statquotes::statquote() "],["rtweet-workshop.html", "Chapter 570 rtweet-workshop", " Chapter 570 rtweet-workshop https://github.com/mkearney/rtweet-workshop https://rtweet-workshop.mikewk.com/ "],["twitter-dashboard.html", "Chapter 571 twitter dashboard", " Chapter 571 twitter dashboard "],["making-a-twitter-dashboard-with-r.html", "Chapter 572 Making a twitter dashboard with R", " Chapter 572 Making a twitter dashboard with R https://jsta.rbind.io/blog/making-a-twitter-dashboard-with-r/ https://jsta.rbind.io/tweets http://rtweet.info/articles/auth.html {r eval=FALSE, include=FALSE, echo=TRUE} library(rtweet) library(magrittr) library(dplyr) library(DT) {r eval=FALSE, include=FALSE, echo=TRUE} user_name &lt;- serdarbalci my_likes &lt;- get_favorites(user_name, n = 100) %&gt;% select( created_at , screen_name , text , urls_expanded_url ) %&gt;% arrange(desc(created_at)) {r eval=FALSE, include=FALSE, echo=TRUE} my_likes$created_at &lt;- strptime(as.POSIXct(my_likes$created_at), format = %Y-%m-%d ) my_likes$created_at &lt;- format(my_likes$created_at, %Y-%m-%d ) {r eval=FALSE, include=FALSE, echo=TRUE} createLink &lt;- function(x) { if(is.na(x)){ return( ) }else{ sprintf(paste0(&#39;&lt;a href= &#39;, URLdecode(x),&#39; target= _blank &gt;&#39;, substr(x, 1, 25) ,&#39;&lt;/a&gt;&#39;)) } } my_likes$urls_expanded_url &lt;- lapply(my_likes$urls_expanded_url, function(x) sapply(x, createLink)) {r eval=FALSE, include=FALSE, echo=TRUE} my_table &lt;- datatable(my_likes, options = list(scrollX = TRUE, autoWidth = TRUE, columnDefs = list(list( width = &#39;70%&#39;, targets = c(2)))), rownames = FALSE, fillContainer = TRUE, width = 100% , height = 100% , colnames = c( Date , Handle , Text , URL )) my_table &lt;- formatStyle(my_table, columns = 1:4, fontSize = &#39;70%&#39;) my_table &lt;- formatStyle(my_table, columns = 3, width = &#39;500px&#39;) {r eval=FALSE, include=FALSE, echo=TRUE} my_table {r eval=FALSE, include=FALSE, echo=TRUE} library(widgetframe) frameWidget(my_table, width = 100% , height = 800, options = frameOptions(allowfullscreen = TRUE)) {r eval=FALSE, include=FALSE, echo=TRUE} frameableWidget(my_table) "],["visualisation-graphs-plots.html", "Chapter 573 Visualisation: Graphs &amp; Plots", " Chapter 573 Visualisation: Graphs &amp; Plots Visualization of Biomedical Data https://www.annualreviews.org/doi/10.1146/annurev-biodatasci-080917-013424 Regional population structures at a glance https://ikashnitsky.github.io/2018/the-lancet-paper/ Global Migration, animated with R http://blog.revolutionanalytics.com/2018/06/global-migration-animated-with-r.html Creating a Geodemographic Classification Using K-means Clustering in R https://data.cdrc.ac.uk/tutorial/creating-a-geodemographic-classification-using-k-means-clustering-in-r Tidy Eval Meets ggplot2 http://www.onceupondata.com/2018/07/06/ggplot-tidyeval/ http://serialmentor.com/dataviz/ You Can Design a Good Chart with R https://towardsdatascience.com/you-can-design-a-good-chart-with-r-5d00ed7dd18e Data to Viz https://www.data-to-viz.com/ ggpubr ggpubr: ‚Äòggplot2‚Äô Based Publication Ready Plots http://www.sthda.com/english/rpkgs/ggpubr/ R Graph Gallery https://www.r-graph-gallery.com/ The Data Visualisation Catalogue https://datavizcatalogue.com/ Dataviz Project http://datavizproject.com/ Python Graph Gallery https://python-graph-gallery.com/ Financial Times Visual Vocabulary https://github.com/ft-interactive/chart-doctor/tree/master/visual-vocabulary Xenographics Weird but (sometimes) useful charts https://xeno.graphics/ https://vis.design/ Show Me Shiny Gallery of R Web Apps https://www.showmeshiny.com/ Where Work Pays: Occupations &amp; Earnings across the United States http://www.hamiltonproject.org/charts/where_work_pays_interactive You Can Design a Good Chart with R But do R users invest in design? https://towardsdatascience.com/you-can-design-a-good-chart-with-r-5d00ed7dd18e Machine Learning Results in R: one plot to rule them all! https://datascienceplus.com/machine-learning-results-one-plot-to-rule-them-all/ data visualisation applications, tools and libraries http://www.visualisingdata.com/resources/ "],["r-notebook-6.html", "Chapter 574 R Notebook", " Chapter 574 R Notebook The ultimate online collection toolbox: Combining RSelenium and Rvest - Part 1 https://www.youtube.com/watch?v=OxbvFiYxEzI https://gist.github.com/HanjoStudy/aeb331b7a277be9639f3cfb3bf875ba2 https://hanjostudy.github.io/Presentations/UseR2018/RSelenium/rselenium.html#1 https://hanjostudy.github.io/Presentations/UseR2018/Rvest/rvest.html#1 Automated Text Feature Engineering using textfeatures in R https://www.r-bloggers.com/automated-text-feature-engineering-using-textfeatures-in-r/ "],["selectorgadget.html", "Chapter 575 Selectorgadget", " Chapter 575 Selectorgadget https://cran.r-project.org/web/packages/rvest/vignettes/selectorgadget.html "],["polite.html", "Chapter 576 polite", " Chapter 576 polite https://github.com/dmi3kno/polite "],["where-to-learn-r.html", "Chapter 577 Where to learn R", " Chapter 577 Where to learn R "],["introduction-to-data-science.html", "Chapter 578 Introduction to Data Science", " Chapter 578 Introduction to Data Science https://rafalab.github.io/dsbook/ "],["rstudio-education.html", "Chapter 579 RStudio Education", " Chapter 579 RStudio Education https://education.rstudio.com/ "],["statistics-in-action-with-r.html", "Chapter 580 Statistics in Action with R", " Chapter 580 Statistics in Action with R http://sia.webpopix.org/index.html "],["swirlstats.html", "Chapter 581 swirlstats", " Chapter 581 swirlstats https://swirlstats.com/ "],["data-skills-for-reproducible-science.html", "Chapter 582 Data Skills for Reproducible Science", " Chapter 582 Data Skills for Reproducible Science https://gupsych.github.io/data_skills/index.html "],["rstats-ed.html", "Chapter 583 rstats-ed", " Chapter 583 rstats-ed https://github.com/rstudio-education/rstats-ed "],["new-online-courses-psu.html", "Chapter 584 new online courses psu", " Chapter 584 new online courses psu https://newonlinecourses.science.psu.edu/statprogram/ "],["think-stats.html", "Chapter 585 Think Stats", " Chapter 585 Think Stats https://greenteapress.com/wp/think-stats-2e/ "],["statsthinking21.html", "Chapter 586 statsthinking21", " Chapter 586 statsthinking21 http://statsthinking21.org/index.html "],["rstudio-8.html", "Chapter 587 RStudio", " Chapter 587 RStudio https://www.rstudio.com/online-learning/ "],["r-statistics.html", "Chapter 588 r-statistics", " Chapter 588 r-statistics http://r-statistics.co/R-Tutorial.html "],["advanced-data-science.html", "Chapter 589 Advanced Data Science", " Chapter 589 Advanced Data Science https://jhu-advdatasci.github.io/2018/ "],["how-do-i.html", "Chapter 590 How Do I? ‚Ä¶", " Chapter 590 How Do I? ‚Ä¶ https://smach.github.io/R4JournalismBook/HowDoI.html "],["data-management-and-manipulation-using-r.html", "Chapter 591 Data management and manipulation using R", " Chapter 591 Data management and manipulation using R https://ozanj.github.io/rclass/resources/ "],["teaching-reproducible-data-analysis-in-r.html", "Chapter 592 Teaching Reproducible Data Analysis in R", " Chapter 592 Teaching Reproducible Data Analysis in R https://gupsych.github.io/trdair_workshop/ "],["academic-websites.html", "Chapter 593 Academic Websites", " Chapter 593 Academic Websites https://gupsych.github.io/acadweb/ "],["data-skills-for-reproducible-science-1.html", "Chapter 594 Data Skills for Reproducible Science", " Chapter 594 Data Skills for Reproducible Science https://gupsych.github.io/data_skills/ "],["methodology-metascience.html", "Chapter 595 Methodology &amp; Metascience", " Chapter 595 Methodology &amp; Metascience https://gupsych.github.io/mms/ "],["causal-inference-book.html", "Chapter 596 Causal Inference Book", " Chapter 596 Causal Inference Book https://www.hsph.harvard.edu/miguel-hernan/causal-inference-book/ "],["introtor.html", "Chapter 597 IntroToR", " Chapter 597 IntroToR https://github.com/sbalci/IntroToR.git "],["youtube-collections.html", "Chapter 598 YouTube Collections", " Chapter 598 YouTube Collections &lt;iframe width= 560 height= 315 src= https://www.youtube.com/embed/videoseries?list=PLxRBOaoEoP4KUSgmcNjSGBFu7PYipGVrX frameborder= 0 allow= accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture allowfullscreen&gt; Jamovi https://www.youtube.com/playlist?list=PLkk92zzyru5OAtc_ItUubaSSq6S_TGfRn Do More with R https://www.youtube.com/playlist?list=PL7D2RMSmRO9JOvPC1gbA8Mc3azvSfm8Vv https://www.youtube.com/playlist?list=PLYaGSokOr0MPz1tgwTW4JKcelhdJyUIrb Neat proofs/perspectives https://www.youtube.com/playlist?list=PLZHQObOWTQDPSKntUcMArGheySM4gL7wS Learning medical statistics with python and Jupyter notebooks https://www.youtube.com/playlist?list=PLsu0TcgLDUiIueDMfTX3322AZhdGb0_zm Data Visualization and R https://www.youtube.com/playlist?list=PLCj1LhGni3hPGy6Kj1AFxHYkKklxenO9D "],["what-they-forgot-to-teach-you-about-r.html", "Chapter 599 What They Forgot to Teach You About R", " Chapter 599 What They Forgot to Teach You About R https://whattheyforgot.org/ "],["keeping-up-to-date-with-r-news.html", "Chapter 600 Keeping up to date with R news", " Chapter 600 Keeping up to date with R news https://masalmon.eu/2019/01/25/uptodate/ "],["advanced-r-markdown-workshop.html", "Chapter 601 Advanced R Markdown Workshop", " Chapter 601 Advanced R Markdown Workshop https://arm.rbind.io/days/day1/learnr/ "],["cran-task-views.html", "Chapter 602 CRAN Task Views", " Chapter 602 CRAN Task Views https://cran.r-project.org/web/views/ "],["wikibook-r-programming.html", "Chapter 603 WikiBook R Programming", " Chapter 603 WikiBook R Programming https://en.wikibooks.org/wiki/R_Programming "],["happy-git-and-github-for-the-user-1.html", "Chapter 604 Happy Git and GitHub for the useR", " Chapter 604 Happy Git and GitHub for the useR https://happygitwithr.com https://r-bootcamp.netlify.com/ "],["how-i-use-r.html", "Chapter 605 How I Use R", " Chapter 605 How I Use R https://howiuser.com "],["r-pirate-yarrr.html", "Chapter 606 R Pirate YaRrr", " Chapter 606 R Pirate YaRrr {r eval=FALSE, include=FALSE, echo=TRUE} install.packages( yarrr ) library( yarrr ) {r eval=FALSE, include=FALSE, echo=TRUE} pirateplot(formula = weight ~ Time, data = ChickWeight, pal = xmen ) {r eval=FALSE, include=FALSE, echo=TRUE} yarrr::pirateplot(formula = weight ~ Diet, data = ChickWeight) "]]
